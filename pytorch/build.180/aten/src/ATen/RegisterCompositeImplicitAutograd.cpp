// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// @generated by tools/codegen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/Functions.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>


#include <ATen/CompositeImplicitAutogradFunctions.h>

namespace at {

// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {


void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      at::native::as_strided_(out, sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

namespace {

at::Tensor wrapper___cast_Byte(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Byte(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Char(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Char(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Double(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Double(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Float(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Float(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Int(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Int(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Long(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Long(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Short(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Short(self, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___cast_Half(const at::Tensor & self, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cast_Half(self, non_blocking);
}

} // anonymous namespace
namespace {

void wrapper___backward(const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
    // No device check


  // DeviceGuard omitted
  return at::native::_backward(self, inputs, gradient, retain_graph, create_graph);
}

} // anonymous namespace
namespace {

void wrapper__set_data(at::Tensor & self, const at::Tensor & new_data) {
    // No device check


  // DeviceGuard omitted
  return at::native::set_data(self, new_data);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__data(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::data(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_leaf(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_leaf(self);
}

} // anonymous namespace
namespace {

int64_t wrapper__output_nr(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::output_nr(self);
}

} // anonymous namespace
namespace {

int64_t wrapper___version(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_version(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__requires_grad_(at::Tensor & self, bool requires_grad) {
    // No device check


  // DeviceGuard omitted
  return at::native::requires_grad_(self, requires_grad);
}

} // anonymous namespace
namespace {

void wrapper__retain_grad(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::retain_grad(self);
}

} // anonymous namespace
namespace {

bool wrapper__retains_grad(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::retains_grad(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___make_dual(const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
    // No device check


  // DeviceGuard omitted
  return at::native::_make_dual(primal, tangent, level);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___unpack_dual(const at::Tensor & dual, int64_t level) {
    // No device check


  // DeviceGuard omitted
  return at::native::_unpack_dual(dual, level);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__rename_(at::Tensor & self, c10::optional<at::DimnameList> names) {
    // No device check


  // DeviceGuard omitted
  return at::native::rename_(self, names);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rename(const at::Tensor & self, c10::optional<at::DimnameList> names) {
    // No device check


  // DeviceGuard omitted
  return at::native::rename(self, names);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__align_to(const at::Tensor & self, at::DimnameList names) {
    // No device check


  // DeviceGuard omitted
  return at::native::align_to(self, names);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ellipsis_idx_align_to_ellipsis_idx(const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
    // No device check


  // DeviceGuard omitted
  return at::native::align_to(self, order, ellipsis_idx);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__align_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::align_as(self, other);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__align_tensors(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::align_tensors(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__refine_names(const at::Tensor & self, at::DimnameList names) {
    // No device check


  // DeviceGuard omitted
  return at::native::refine_names(self, names);
}

} // anonymous namespace
namespace {

bool wrapper___use_cudnn_rnn_flatten_weight() {
    // No device check


  // DeviceGuard omitted
  return at::native::_use_cudnn_rnn_flatten_weight();
}

} // anonymous namespace
namespace {

int64_t wrapper___debug_has_internal_overlap(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_debug_has_internal_overlap(self);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___sobol_engine_draw(const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sobol_engine_draw(quasi, n, sobolstate, dimension, num_generated, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper___sobol_engine_ff_(at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sobol_engine_ff_(self, n, sobolstate, dimension, num_generated);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper___sobol_engine_scramble_(at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sobol_engine_scramble_(self, ltm, dimension);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper___sobol_engine_initialize_state_(at::Tensor & self, int64_t dimension) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sobol_engine_initialize_state_(self, dimension);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___reshape_from_tensor(const at::Tensor & self, const at::Tensor & shape) {
    // No device check


  // DeviceGuard omitted
  return at::native::_reshape_from_tensor(self, shape);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___shape_as_tensor(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_shape_as_tensor(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__dropout(const at::Tensor & input, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::dropout(input, p, train);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__dropout_(at::Tensor & self, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::dropout_(self, p, train);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__feature_dropout(const at::Tensor & input, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::feature_dropout(input, p, train);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__feature_dropout_(at::Tensor & self, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::feature_dropout_(self, p, train);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__alpha_dropout(const at::Tensor & input, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::alpha_dropout(input, p, train);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__alpha_dropout_(at::Tensor & self, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::alpha_dropout_(self, p, train);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__feature_alpha_dropout(const at::Tensor & input, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::feature_alpha_dropout(input, p, train);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__feature_alpha_dropout_(at::Tensor & self, double p, bool train) {
    // No device check


  // DeviceGuard omitted
  return at::native::feature_alpha_dropout_(self, p, train);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__absolute(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::absolute(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_absolute_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::absolute_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__absolute_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::absolute_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__real(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::real(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__imag(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::imag(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conj(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::conj(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conj_physical(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::conj_physical(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__resolve_conj(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::resolve_conj(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__resolve_neg(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::resolve_neg(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arccos(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arccos(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arccos_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arccos_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__arccos_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arccos_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__avg_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
    // No device check


  // DeviceGuard omitted
  return at::native::avg_pool1d(self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__adaptive_avg_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::adaptive_avg_pool1d(self, output_size);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__adaptive_max_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::adaptive_max_pool1d(self, output_size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__addr(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::math_addr(self, vec1, vec2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_addr_out_out(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::math_addr_out(self, vec1, vec2, beta, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__affine_grid_generator_backward(const at::Tensor & grad, at::IntArrayRef size, bool align_corners) {
    // No device check


  // DeviceGuard omitted
  return at::native::affine_grid_generator_backward(grad, size, align_corners);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_all_dimname(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::all(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_all_out_dimname_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::all_out(self, dim, keepdim, out);
}

} // anonymous namespace
namespace {

bool wrapper__allclose(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    // No device check


  // DeviceGuard omitted
  return at::native::allclose(self, other, rtol, atol, equal_nan);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_any_dimname(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::any(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_any_out_dimname_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::any_out(self, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arange(const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::arange(end, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_start_arange_start(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::arange(start, end, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_start_step_arange_start_step(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::arange(start, end, step, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arange_out_out(const at::Scalar & end, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arange_out(end, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___dim_arange(const at::Tensor & like, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_dim_arange(like, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arccosh(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arccosh(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arccosh_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arccosh_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__arccosh_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arccosh_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arcsinh(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arcsinh(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arcsinh_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arcsinh_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__arcsinh_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arcsinh_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arctanh(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arctanh(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arctanh_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arctanh_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__arctanh_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arctanh_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arcsin(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arcsin(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arcsin_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arcsin_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__arcsin_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arcsin_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__arctan(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arctan(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_arctan_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::arctan_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__arctan_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::arctan_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__atleast_1d(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::atleast_1d(self);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_Sequence_atleast_1d_Sequence(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::atleast_1d(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__atleast_2d(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::atleast_2d(self);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_Sequence_atleast_2d_Sequence(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::atleast_2d(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__atleast_3d(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::atleast_3d(self);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_Sequence_atleast_3d_Sequence(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::atleast_3d(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper___baddbmm_mkl_(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::_baddbmm_mkl_(self, batch1, batch2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__bartlett_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::bartlett_window(window_length, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_bartlett_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::bartlett_window(window_length, periodic, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    // No device check


  // DeviceGuard omitted
  return at::native::batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> wrapper___batch_norm_impl_index(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    // No device check


  // DeviceGuard omitted
  return at::native::_batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper___batch_norm_impl_index_backward(int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, ::std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
    // No device check


  // DeviceGuard omitted
  return at::native::_batch_norm_impl_index_backward(impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask, reservedSpace);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_p_bernoulli_p(const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return at::native::bernoulli(self, p, generator);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__bilinear(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::bilinear(input1, input2, weight, bias);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__binary_cross_entropy_with_logits_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::binary_cross_entropy_with_logits_backward(grad_output, self, target, weight, pos_weight, reduction);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logical_not(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_not(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__logical_not_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_not_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logical_xor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_xor(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__logical_xor_(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_xor_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logical_and(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_and(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__logical_and_(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_and_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logical_or(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_or(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__logical_or_(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::logical_or_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__blackman_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::blackman_window(window_length, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_blackman_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::blackman_window(window_length, periodic, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__broadcast_tensors(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::broadcast_tensors(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__broadcast_to(const at::Tensor & self, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return at::native::broadcast_to(self, size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_cat_names(at::TensorList tensors, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cat(tensors, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_cat_out_names_out(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::cat_out(tensors, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__concat(at::TensorList tensors, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::concat(tensors, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_concat_out_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::concat_out(tensors, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_concat_names(at::TensorList tensors, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::concat(tensors, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_concat_out_names_out(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::concat_out(tensors, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__block_diag(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::block_diag(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__chain_matmul(at::TensorList matrices) {
    // No device check


  // DeviceGuard omitted
  return at::native::chain_matmul(matrices);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_chain_matmul_out_out(at::TensorList matrices, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::chain_matmul_out(matrices, out);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__unsafe_chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unsafe_chunk(self, chunks, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::chunk(self, chunks, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_sections_tensor_split_sections(const at::Tensor & self, int64_t sections, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::tensor_split(self, sections, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_indices_tensor_split_indices(const at::Tensor & self, at::IntArrayRef indices, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::tensor_split(self, indices, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_tensor_indices_or_sections_tensor_split_tensor_indices_or_sections(const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::tensor_split(self, tensor_indices_or_sections, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__clip(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clip(self, min, max);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_clip_out_out(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::clip_out(self, min, max, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__clip_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clip_(self, min, max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_clip_Tensor(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clip(self, min, max);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_clip_out_Tensor_out(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::clip_out(self, min, max, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_clip__Tensor(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clip_(self, min, max);
}

} // anonymous namespace
namespace {

bool wrapper__cudnn_is_acceptable(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::cudnn_is_acceptable(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__contiguous(const at::Tensor & self, at::MemoryFormat memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::contiguous(self, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
    // No device check


  // DeviceGuard omitted
  return at::native::_convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_deprecated__convolution_deprecated(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
    // No device check


  // DeviceGuard omitted
  return at::native::_convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___convolution_mode(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::_convolution_mode(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___convolution_nogroup(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding) {
    // No device check


  // DeviceGuard omitted
  return at::native::_convolution_nogroup(input, weight, bias, stride, padding, dilation, transposed, output_padding);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper___convolution_double_backward(const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::_convolution_double_backward(ggI, ggW, ggb, gO, weight, self, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, output_mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv1d(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv2d(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv3d(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_padding_conv1d_padding(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv1d(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_padding_conv2d_padding(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv2d(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_padding_conv3d_padding(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv3d(input, weight, bias, stride, padding, dilation, groups);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__conv_tbc_backward(const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv_tbc_backward(self, input, weight, bias, pad);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conv_transpose1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv_transpose1d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_input_conv_transpose2d_input(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv_transpose2d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_input_conv_transpose3d_input(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv_transpose3d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cosine_embedding_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::cosine_embedding_loss(input1, input2, target, margin, reduction);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cov(const at::Tensor & self, int64_t correction, const c10::optional<at::Tensor> & fweights, const c10::optional<at::Tensor> & aweights) {
    // No device check


  // DeviceGuard omitted
  return at::native::cov(self, correction, fweights, aweights);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__corrcoef(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::corrcoef(self);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dimname_cummax_dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummax(self, dim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_dimname_out_cummax_out_dimname_out(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummax_out(self, dim, values, indices);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dimname_cummin_dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummin(self, dim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_dimname_out_cummin_out_dimname_out(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummin_out(self, dim, values, indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cummaxmin_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummaxmin_backward(grad, input, indices, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_cumprod_dimname(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumprod(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_cumprod_out_dimname_out(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumprod_out(self, dim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_cumprod__dimname(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumprod_(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cumprod_backward(const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumprod_backward(grad, input, dim, output);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_cumsum_dimname(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumsum(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_cumsum_out_dimname_out(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumsum_out(self, dim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_cumsum__dimname(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumsum_(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_x_cumulative_trapezoid_x(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumulative_trapezoid(y, x, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dx_cumulative_trapezoid_dx(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cumulative_trapezoid(y, dx, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_IntList_ctc_loss_IntList(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    // No device check


  // DeviceGuard omitted
  return at::native::ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_ctc_loss_Tensor(const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    // No device check


  // DeviceGuard omitted
  return at::native::ctc_loss(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__diag_embed(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check


  // DeviceGuard omitted
  return at::native::diag_embed(self, offset, dim1, dim2);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__diagflat(const at::Tensor & self, int64_t offset) {
    // No device check


  // DeviceGuard omitted
  return at::native::diagflat(self, offset);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_diagonal_Dimname(const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
    // No device check


  // DeviceGuard omitted
  return at::native::diagonal(self, outdim, dim1, dim2, offset);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__fill_diagonal_(at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
    // No device check


  // DeviceGuard omitted
  return at::native::fill_diagonal_(self, fill_value, wrap);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__diff(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
    // No device check


  // DeviceGuard omitted
  return at::native::diff(self, n, dim, prepend, append);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_diff_out_out(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::diff_out(self, n, dim, prepend, append, out);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_scalarint_gradient_scalarint(const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_scalararray_gradient_scalararray(const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_array_gradient_array(const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, dim, edge_order);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_scalarrayint_gradient_scalarrayint(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_scalarrayarray_gradient_scalarrayarray(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_tensorarrayint_gradient_tensorarrayint(const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_tensorarray_gradient_tensorarray(const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
    // No device check


  // DeviceGuard omitted
  return at::native::gradient(self, spacing, dim, edge_order);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_divide_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_divide_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_divide__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_divide_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_divide__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_mode_divide_Tensor_mode(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_mode_divide_out_out_mode(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide_out(self, other, rounding_mode, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_mode_divide__Tensor_mode(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide_(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_mode_divide_Scalar_mode(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_mode_divide__Scalar_mode(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::divide_(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_true_divide_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::true_divide(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_true_divide_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::true_divide_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_true_divide__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::true_divide_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_true_divide_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::true_divide(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_true_divide__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::true_divide_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__einsum(c10::string_view equation, at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::einsum(equation, tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__embedding_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    // No device check


  // DeviceGuard omitted
  return at::native::embedding_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__embedding_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    // No device check


  // DeviceGuard omitted
  return at::native::embedding_sparse_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___rowwise_prune(const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_rowwise_prune(weight, mask, compressed_indices_dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__row_stack(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::row_stack(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_row_stack_out_out(at::TensorList tensors, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::row_stack_out(tensors, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper__embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
    // No device check


  // DeviceGuard omitted
  return at::native::embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper_padding_idx_embedding_bag_padding_idx(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
    // No device check


  // DeviceGuard omitted
  return at::native::embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___embedding_bag_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    // No device check


  // DeviceGuard omitted
  return at::native::_embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___embedding_bag_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    // No device check


  // DeviceGuard omitted
  return at::native::_embedding_bag_sparse_backward(grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_empty_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::empty(size, names, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__new_empty(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::new_empty(self, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__new_empty_strided(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::new_empty_strided(self, size, stride, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__new_full(const at::Tensor & self, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::new_full(self, size, fill_value, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__new_zeros(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::new_zeros(self, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__new_ones(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::new_ones(self, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_empty_out_out(at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::empty_out(size, memory_format, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::empty_like(self, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__expand_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::expand_as(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__eye(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::eye(n, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_m_eye_m(int64_t n, int64_t m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::eye(n, m, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_using_ints_flatten_using_ints(const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::flatten(self, start_dim, end_dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_named_out_dim_flatten_named_out_dim(const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::flatten(self, start_dim, end_dim, out_dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_using_names_flatten_using_names(const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::flatten(self, start_dim, end_dim, out_dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_DimnameList_flatten_DimnameList(const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::flatten(self, dims, out_dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_unflatten_int(const at::Tensor & self, int64_t dim, at::IntArrayRef sizes, c10::optional<at::DimnameList> names) {
    // No device check


  // DeviceGuard omitted
  return at::native::unflatten(self, dim, sizes, names);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_unflatten_Dimname(const at::Tensor & self, at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) {
    // No device check


  // DeviceGuard omitted
  return at::native::unflatten(self, dim, sizes, names);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_floor_divide_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::floor_divide(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_floor_divide__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::floor_divide_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_full_names(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::full(size, fill_value, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__full(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::full(size, fill_value, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_full_out_out(at::IntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::full_out(size, fill_value, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__full_like(const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::full_like(self, fill_value, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__grid_sampler(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check


  // DeviceGuard omitted
  return at::native::grid_sampler(input, grid, interpolation_mode, padding_mode, align_corners);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___grid_sampler_2d_cpu_fallback_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check


  // DeviceGuard omitted
  return at::native::_grid_sampler_2d_cpu_fallback_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__hann_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::hann_window(window_length, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_hann_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::hann_window(window_length, periodic, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__hamming_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::hamming_window(window_length, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_hamming_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::hamming_window(window_length, periodic, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_alpha_hamming_window_periodic_alpha(int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::hamming_window(window_length, periodic, alpha, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_alpha_beta_hamming_window_periodic_alpha_beta(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::hamming_window(window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__kaiser_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::kaiser_window(window_length, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_periodic_kaiser_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::kaiser_window(window_length, periodic, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_beta_kaiser_window_beta(int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::kaiser_window(window_length, periodic, beta, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__hinge_embedding_loss(const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::hinge_embedding_loss(self, target, margin, reduction);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__group_norm(const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
    // No device check


  // DeviceGuard omitted
  return at::native::group_norm(input, num_groups, weight, bias, eps, cudnn_enabled);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__native_group_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps) {
    // No device check


  // DeviceGuard omitted
  return at::native::math_group_norm(input, weight, bias, N, C, HxW, group, eps);
}

} // anonymous namespace
namespace {

int64_t wrapper___cufft_get_plan_cache_size(int64_t device_index) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cufft_get_plan_cache_size(device_index);
}

} // anonymous namespace
namespace {

int64_t wrapper___cufft_get_plan_cache_max_size(int64_t device_index) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cufft_get_plan_cache_max_size(device_index);
}

} // anonymous namespace
namespace {

void wrapper___cufft_set_plan_cache_max_size(int64_t device_index, int64_t max_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cufft_set_plan_cache_max_size(device_index, max_size);
}

} // anonymous namespace
namespace {

void wrapper___cufft_clear_plan_cache(int64_t device_index) {
    // No device check


  // DeviceGuard omitted
  return at::native::_cufft_clear_plan_cache(device_index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__index_copy(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_copy(self, dim, index, source);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_index_copy__dimname(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_copy_(self, dim, index, source);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_index_copy_dimname(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_copy(self, dim, index, source);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__index_put(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_put(self, indices, values, accumulate);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__instance_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
    // No device check


  // DeviceGuard omitted
  return at::native::instance_norm(input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__isclose(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    // No device check


  // DeviceGuard omitted
  return at::native::isclose(self, other, rtol, atol, equal_nan);
}

} // anonymous namespace
namespace {

bool wrapper__is_distributed(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_distributed(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_floating_point(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_floating_point(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_complex(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_complex(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_conj(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_conj(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_neg(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_neg(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__isreal(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::isreal(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_nonzero(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_nonzero(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_same_size(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_same_size(self, other);
}

} // anonymous namespace
namespace {

bool wrapper__is_signed(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_signed(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_inference(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_inference(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__kron(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::kron(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_kron_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::kron_out(self, other, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dimname_kthvalue_dimname(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::kthvalue(self, k, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_dimname_out_kthvalue_out_dimname_out(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::kthvalue_out(self, k, dim, keepdim, values, indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
    // No device check


  // DeviceGuard omitted
  return at::native::layer_norm(input, normalized_shape, weight, bias, eps, cudnn_enable);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__native_layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    // No device check


  // DeviceGuard omitted
  return at::native::math_native_layer_norm(input, normalized_shape, weight, bias, eps);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linear(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::linear(input, weight, bias);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linear_out_out(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linear_out(input, weight, bias, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fbgemm_linear_int8_weight_fp32_activation(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_linear_int8_weight_fp32_activation(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fbgemm_linear_int8_weight(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_linear_int8_weight(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,double,int64_t> wrapper__fbgemm_linear_quantize_weight(const at::Tensor & input) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_linear_quantize_weight(input);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fbgemm_pack_gemm_matrix_fp16(const at::Tensor & input) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_pack_gemm_matrix_fp16(input);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fbgemm_linear_fp16_weight_fp32_activation(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_linear_fp16_weight_fp32_activation(input, packed_weight, bias);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fbgemm_linear_fp16_weight(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_linear_fp16_weight(input, packed_weight, bias);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fbgemm_pack_quantized_matrix(const at::Tensor & input) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_pack_quantized_matrix(input);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_KN_fbgemm_pack_quantized_matrix_KN(const at::Tensor & input, int64_t K, int64_t N) {
    // No device check


  // DeviceGuard omitted
  return at::native::fbgemm_pack_quantized_matrix(input, K, N);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_ldexp_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::ldexp(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_ldexp_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::ldexp_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__ldexp_(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::ldexp_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::linspace(start, end, steps, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::logspace(start, end, steps, base, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_log_softmax_int(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::log_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_log_softmax_Dimname(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::log_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_logcumsumexp_dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::logcumsumexp(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_logcumsumexp_out_dimname_out(const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::logcumsumexp_out(self, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_logsumexp_names(const at::Tensor & self, at::DimnameList dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::logsumexp(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_logsumexp_out_names_out(const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::logsumexp_out(self, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__margin_ranking_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::margin_ranking_loss(input1, input2, target, margin, reduction);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__matmul(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::matmul(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_matmul_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::matmul_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_tol_matrix_rank_tol(const at::Tensor & self, double tol, bool symmetric) {
    // No device check


  // DeviceGuard omitted
  return at::native::matrix_rank(self, tol, symmetric);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__matrix_rank(const at::Tensor & self, bool symmetric) {
    // No device check


  // DeviceGuard omitted
  return at::native::matrix_rank(self, symmetric);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__matrix_power(const at::Tensor & self, int64_t n) {
    // No device check


  // DeviceGuard omitted
  return at::native::matrix_power(self, n);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_matrix_power_out_out(const at::Tensor & self, int64_t n, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::matrix_power_out(self, n, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__matrix_exp_backward(const at::Tensor & self, const at::Tensor & grad) {
    // No device check


  // DeviceGuard omitted
  return at::native::matrix_exp_backward(self, grad);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_names_dim_max_names_dim(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::max(self, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_names_dim_max_max_out_names_dim_max(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    // No device check


  // DeviceGuard omitted
  return at::native::max_out(self, dim, keepdim, max, max_values);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__value_selecting_reduction_backward(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, at::IntArrayRef sizes, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::value_selecting_reduction_backward(grad, dim, indices, sizes, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__max_pool1d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::max_pool1d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::max_pool1d(self, kernel_size, stride, padding, dilation, ceil_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_dim_mean_names_dim(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::mean(self, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_mean_out_names_out(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::mean_out(self, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nanmean(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanmean(self, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_nanmean_out_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanmean_out(self, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_names_dim_median_names_dim(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::median(self, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_names_dim_values_median_out_names_dim_values(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::median_out(self, dim, keepdim, values, indices);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_names_dim_nanmedian_names_dim(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanmedian(self, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_names_dim_values_nanmedian_out_names_dim_values(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanmedian_out(self, dim, keepdim, values, indices);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_names_dim_min_names_dim(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::min(self, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_names_dim_min_min_out_names_dim_min(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::min_out(self, dim, keepdim, min, min_indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mkldnn_convolution_backward_input(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
    // No device check


  // DeviceGuard omitted
  return at::native::mkldnn_convolution_backward_input(self_size, grad_output, weight, padding, stride, dilation, groups, bias_defined);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__mkldnn_convolution_backward_weights(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
    // No device check


  // DeviceGuard omitted
  return at::native::mkldnn_convolution_backward_weights(weight_size, grad_output, self, padding, stride, dilation, groups, bias_defined);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_mm(const at::Tensor & sparse, const at::Tensor & dense) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_mm(sparse, dense);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dimname_mode_dimname(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::mode(self, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_dimname_out_mode_out_dimname_out(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::mode_out(self, dim, keepdim, values, indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_multiply_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::multiply(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_multiply_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::multiply_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_multiply__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::multiply_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_multiply_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::multiply(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_multiply__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::multiply_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__narrow(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    // No device check


  // DeviceGuard omitted
  return at::native::narrow(self, dim, start, length);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_narrow_Tensor(const at::Tensor & self, int64_t dim, const at::Tensor & start, int64_t length) {
    // No device check


  // DeviceGuard omitted
  return at::native::narrow(self, dim, start, length);
}

} // anonymous namespace
namespace {

bool wrapper__is_vulkan_available() {
    // No device check


  // DeviceGuard omitted
  return at::native::is_vulkan_available();
}

} // anonymous namespace
namespace {

bool wrapper___nnpack_available() {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnpack_available();
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper___nnpack_spatial_convolution_backward(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnpack_spatial_convolution_backward(input, grad_output, weight, padding, output_mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___nnpack_spatial_convolution_backward_input(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding) {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnpack_spatial_convolution_backward_input(input, grad_output, weight, padding);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___nnpack_spatial_convolution_backward_weight(const at::Tensor & input, at::IntArrayRef weightsize, const at::Tensor & grad_output, at::IntArrayRef padding) {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnpack_spatial_convolution_backward_weight(input, weightsize, grad_output, padding);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_ones_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::ones(size, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__ones(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::ones(size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_ones_out_out(at::IntArrayRef size, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::ones_out(size, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__ones_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::ones_like(self, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pairwise_distance(const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::pairwise_distance(x1, x2, p, eps, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cdist(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::cdist(x1, x2, p, compute_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pdist(const at::Tensor & self, double p) {
    // No device check


  // DeviceGuard omitted
  return at::native::pdist(self, p);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cosine_similarity(const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
    // No device check


  // DeviceGuard omitted
  return at::native::cosine_similarity(x1, x2, dim, eps);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_intlist_movedim_intlist(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    // No device check


  // DeviceGuard omitted
  return at::native::movedim(self, source, destination);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_movedim_int(const at::Tensor & self, int64_t source, int64_t destination) {
    // No device check


  // DeviceGuard omitted
  return at::native::movedim(self, source, destination);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_intlist_moveaxis_intlist(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    // No device check


  // DeviceGuard omitted
  return at::native::moveaxis(self, source, destination);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_moveaxis_int(const at::Tensor & self, int64_t source, int64_t destination) {
    // No device check


  // DeviceGuard omitted
  return at::native::moveaxis(self, source, destination);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__numpy_T(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::numpy_T(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pixel_shuffle(const at::Tensor & self, int64_t upscale_factor) {
    // No device check


  // DeviceGuard omitted
  return at::native::pixel_shuffle(self, upscale_factor);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pixel_unshuffle(const at::Tensor & self, int64_t downscale_factor) {
    // No device check


  // DeviceGuard omitted
  return at::native::pixel_unshuffle(self, downscale_factor);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pin_memory(const at::Tensor & self, c10::optional<at::Device> device) {
    // No device check


  // DeviceGuard omitted
  return at::native::pin_memory(self, device);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pinverse(const at::Tensor & self, double rcond) {
    // No device check


  // DeviceGuard omitted
  return at::native::pinverse(self, rcond);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__poisson_nll_loss(const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::poisson_nll_loss(input, target, log_input, full, eps, reduction);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__scalar_tensor(const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::scalar_tensor(s, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_rand_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand(size, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_generator_with_names_rand_generator_with_names(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand(size, generator, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rand(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand(size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_generator_rand_generator(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand(size, generator, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_rand_out_out(at::IntArrayRef size, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand_out(size, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_generator_out_rand_out_generator_out(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand_out(size, generator, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rand_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::rand_like(self, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__randint(int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint(high, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_generator_randint_generator(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint(high, size, generator, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_low_randint_low(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint(low, high, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_low_generator_randint_low_generator(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint(low, high, size, generator, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_randint_out_out(int64_t high, at::IntArrayRef size, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint_out(high, size, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_generator_out_randint_out_generator_out(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint_out(high, size, generator, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_low_out_randint_out_low_out(int64_t low, int64_t high, at::IntArrayRef size, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint_out(low, high, size, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_low_generator_out_randint_out_low_generator_out(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint_out(low, high, size, generator, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__randint_like(const at::Tensor & self, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint_like(self, high, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_low_dtype_randint_like_low_dtype(const at::Tensor & self, int64_t low, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::randint_like(self, low, high, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__randn(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn(size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_generator_randn_generator(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn(size, generator, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_randn_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn(size, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_generator_with_names_randn_generator_with_names(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn(size, generator, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_randn_out_out(at::IntArrayRef size, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn_out(size, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_generator_out_randn_out_generator_out(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn_out(size, generator, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__randn_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::randn_like(self, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__randperm(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randperm(n, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_generator_randperm_generator(int64_t n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::randperm(n, generator, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_randperm_out_out(int64_t n, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::randperm_out(n, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_step_range_step(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::range(start, end, step, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__range(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::range(start, end, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__ravel(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::ravel(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__negative(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::negative(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_negative_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::negative_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__negative_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::negative_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_self_Tensor_repeat_interleave_self_Tensor(const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::repeat_interleave(self, repeats, dim, output_size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_self_int_repeat_interleave_self_int(const at::Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::repeat_interleave(self, repeats, dim, output_size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__reshape(const at::Tensor & self, at::IntArrayRef shape) {
    // No device check


  // DeviceGuard omitted
  return at::native::reshape(self, shape);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__reshape_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::reshape_as(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rrelu(const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return at::native::rrelu(self, lower, upper, training, generator);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__rrelu_(at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return at::native::rrelu_(self, lower, upper, training, generator);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__relu6(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::relu6(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__relu6_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::relu6_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__infinitely_differentiable_gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::infinitely_differentiable_gelu_backward(grad, self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_select_Dimname(const at::Tensor & self, at::Dimname dim, int64_t index) {
    // No device check


  // DeviceGuard omitted
  return at::native::select(self, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__selu(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::selu(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__selu_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::selu_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::math_silu_backward(grad_output, self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mish_backward(const at::Tensor & grad_output, const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::math_mish_backward(grad_output, self);
}

} // anonymous namespace
namespace {

int64_t wrapper_int_size_int(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::size(self, dim);
}

} // anonymous namespace
namespace {

int64_t wrapper_Dimname_size_Dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::size(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__smm(const at::Tensor & self, const at::Tensor & mat2) {
    // No device check


  // DeviceGuard omitted
  return at::native::smm(self, mat2);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_softmax_int(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_softmax_Dimname(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_int_hsplit_int(const at::Tensor & self, int64_t sections) {
    // No device check


  // DeviceGuard omitted
  return at::native::hsplit(self, sections);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_array_hsplit_array(const at::Tensor & self, at::IntArrayRef indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::hsplit(self, indices);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_int_vsplit_int(const at::Tensor & self, int64_t sections) {
    // No device check


  // DeviceGuard omitted
  return at::native::vsplit(self, sections);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_array_vsplit_array(const at::Tensor & self, at::IntArrayRef indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::vsplit(self, indices);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_int_dsplit_int(const at::Tensor & self, int64_t sections) {
    // No device check


  // DeviceGuard omitted
  return at::native::dsplit(self, sections);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_array_dsplit_array(const at::Tensor & self, at::IntArrayRef indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::dsplit(self, indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_squeeze_dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::squeeze(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_squeeze__dimname(at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::squeeze_(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__sspaddmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::sspaddmm(self, mat1, mat2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__hstack(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::hstack(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_hstack_out_out(at::TensorList tensors, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::hstack_out(tensors, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__vstack(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::vstack(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_vstack_out_out(at::TensorList tensors, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::vstack_out(tensors, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__dstack(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::dstack(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_dstack_out_out(at::TensorList tensors, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::dstack_out(tensors, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__stft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    // No device check


  // DeviceGuard omitted
  return at::native::stft(self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__istft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
    // No device check


  // DeviceGuard omitted
  return at::native::istft(self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
}

} // anonymous namespace
namespace {

int64_t wrapper_int_stride_int(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::stride(self, dim);
}

} // anonymous namespace
namespace {

int64_t wrapper_Dimname_stride_Dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::stride(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_DimnameList_sum_dim_DimnameList(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::sum(self, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_DimnameList_out_sum_out_DimnameList_out(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::sum_out(self, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__sum_to_size(const at::Tensor & self, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return at::native::sum_to_size(self, size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__square(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::square(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__square_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::square_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__std(const at::Tensor & self, bool unbiased) {
    // No device check


  // DeviceGuard omitted
  return at::native::std(self, unbiased);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_std_dim(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::std(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_std_out_out(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_out(self, dim, unbiased, keepdim, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__std_mean(const at::Tensor & self, bool unbiased) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_mean(self, unbiased);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dim_std_mean_dim(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_mean(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_names_dim_std_mean_names_dim(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_mean(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_correction_names_std_mean_correction_names(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_mean(self, dim, correction, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_dim_std_names_dim(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::std(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_std_out_names_out(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_out(self, dim, unbiased, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_correction_names_std_correction_names(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::std(self, dim, correction, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_correction_names_out_std_out_correction_names_out(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::std_out(self, dim, correction, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_Dimname_prod_dim_Dimname(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::prod(self, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Dimname_out_prod_out_Dimname_out(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::prod_out(self, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__tensordot(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
    // No device check


  // DeviceGuard omitted
  return at::native::tensordot(self, other, dims_self, dims_other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__tile(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check


  // DeviceGuard omitted
  return at::native::tile(self, dims);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_transpose_Dimname(const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
    // No device check


  // DeviceGuard omitted
  return at::native::transpose(self, dim0, dim1);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__one_hot(const at::Tensor & self, int64_t num_classes) {
    // No device check


  // DeviceGuard omitted
  return at::native::one_hot(self, num_classes);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fliplr(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::fliplr(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__flipud(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::flipud(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_x_trapezoid_x(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::trapezoid(y, x, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dx_trapezoid_dx(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::trapezoid(y, dx, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_x_trapz_x(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::trapz(y, x, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dx_trapz_dx(const at::Tensor & y, double dx, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::trapz(y, dx, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__triplet_margin_loss(const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fix(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::fix(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fix_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fix_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__fix_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::fix_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__type_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::type_as(self, other);
}

} // anonymous namespace
namespace {

bool wrapper___has_compatible_shallow_copy_type(const at::Tensor & self, const at::Tensor & from) {
    // No device check


  // DeviceGuard omitted
  return at::native::_has_compatible_shallow_copy_type(self, from);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__vander(const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
    // No device check


  // DeviceGuard omitted
  return at::native::vander(x, N, increasing);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__var(const at::Tensor & self, bool unbiased) {
    // No device check


  // DeviceGuard omitted
  return at::native::var(self, unbiased);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_var_dim(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::var(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_var_out_out(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_out(self, dim, unbiased, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_dim_var_names_dim(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::var(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_var_out_names_out(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_out(self, dim, unbiased, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_correction_names_var_correction_names(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::var(self, dim, correction, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_correction_names_out_var_out_correction_names_out(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_out(self, dim, correction, keepdim, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__var_mean(const at::Tensor & self, bool unbiased) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_mean(self, unbiased);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dim_var_mean_dim(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_mean(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_names_dim_var_mean_names_dim(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_mean(self, dim, unbiased, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_correction_names_var_mean_correction_names(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::var_mean(self, dim, correction, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__view_as(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::view_as(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_self_where_self(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ScalarSelf_where_ScalarSelf(const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ScalarOther_where_ScalarOther(const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_where_Scalar(const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::where(condition, self, other);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__where(const at::Tensor & condition) {
    // No device check


  // DeviceGuard omitted
  return at::native::where(condition);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__norm_except_dim(const at::Tensor & v, int64_t pow, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm_except_dim(v, pow, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___weight_norm(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_weight_norm(v, g, dim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___weight_norm_differentiable_backward(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_weight_norm_differentiable_backward(grad_w, saved_v, saved_g, saved_norms, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_zeros_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::zeros(size, names, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__zeros(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::zeros(size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_zeros_out_out(at::IntArrayRef size, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::zeros_out(size, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__zeros_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::zeros_like(self, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_sum(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_sum(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dtype__sparse_sum_dtype(const at::Tensor & self, at::ScalarType dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_sum(self, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_dtype__sparse_sum_dim_dtype(const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_sum(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int__sparse_softmax_int(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname__sparse_softmax_Dimname(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int__sparse_log_softmax_int(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_log_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname__sparse_log_softmax_Dimname(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_log_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_ScalarOpt_dim_dtype_norm_names_ScalarOpt_dim_dtype(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm(self, p, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_dtype_out_norm_out_names_dtype_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm_out(self, p, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_names_ScalarOpt_dim_norm_names_ScalarOpt_dim(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm(self, p, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_names_out_norm_out_names_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm_out(self, p, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__frobenius_norm(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::frobenius_norm(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_frobenius_norm_dim(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::frobenius_norm(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_frobenius_norm_out_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::frobenius_norm_out(self, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nuclear_norm(const at::Tensor & self, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::nuclear_norm(self, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_nuclear_norm_out_out(const at::Tensor & self, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nuclear_norm_out(self, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_nuclear_norm_dim(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::nuclear_norm(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dim_out_nuclear_norm_out_dim_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nuclear_norm_out(self, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__positive(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::positive(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_subtract_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::subtract(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_subtract_out_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::subtract_out(self, other, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_subtract__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::subtract_(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_subtract_Scalar(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::subtract(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_subtract__Scalar(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::subtract_(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_crow_col_value_size_sparse_csr_tensor_crow_col_value_size(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_csr_tensor(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_crow_col_value_sparse_csr_tensor_crow_col_value(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_csr_tensor(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_size_sparse_coo_tensor_size(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_coo_tensor(size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_indices_sparse_coo_tensor_indices(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_coo_tensor(indices, values, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_indices_size_sparse_coo_tensor_indices_size(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_coo_tensor(indices, values, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_coo_tensor_unsafe(indices, values, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

void wrapper___validate_sparse_coo_tensor_args(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return at::native::_validate_sparse_coo_tensor_args(indices, values, size);
}

} // anonymous namespace
namespace {

void wrapper___validate_sparse_csr_tensor_args(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return at::native::_validate_sparse_csr_tensor_args(crow_indices, col_indices, values, size);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper___to_cpu(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::_to_cpu(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__to_dense_backward(const at::Tensor & grad, const at::Tensor & input) {
    // No device check


  // DeviceGuard omitted
  return at::native::to_dense_backward(grad, input);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__coalesce(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::coalesce(self);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_Dimname_unbind_Dimname(const at::Tensor & self, at::Dimname dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unbind(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__to_mkldnn_backward(const at::Tensor & grad, const at::Tensor & input) {
    // No device check


  // DeviceGuard omitted
  return at::native::to_mkldnn_backward(grad, input);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fake_quantize_per_tensor_affine(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    // No device check


  // DeviceGuard omitted
  return at::native::fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_tensor_qparams_fake_quantize_per_tensor_affine_tensor_qparams(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
    // No device check


  // DeviceGuard omitted
  return at::native::fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fake_quantize_per_tensor_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::fake_quantize_per_tensor_affine_cachemask_backward(grad, mask);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper___fake_quantize_learnable_per_tensor_affine_backward(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    // No device check


  // DeviceGuard omitted
  return at::native::_fake_quantize_learnable_per_tensor_affine_backward(grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fake_quantize_per_channel_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
    // No device check


  // DeviceGuard omitted
  return at::native::fake_quantize_per_channel_affine(self, scale, zero_point, axis, quant_min, quant_max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fake_quantize_per_channel_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::fake_quantize_per_channel_affine_cachemask_backward(grad, mask);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper___fake_quantize_learnable_per_channel_affine_backward(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    // No device check


  // DeviceGuard omitted
  return at::native::_fake_quantize_learnable_per_channel_affine_backward(grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fused_moving_avg_obs_fake_quant(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    // No device check


  // DeviceGuard omitted
  return at::native::fused_moving_avg_obs_fake_quant(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

} // anonymous namespace
namespace {

::std::tuple<double,int64_t> wrapper___choose_qparams_per_tensor(const at::Tensor & self, bool reduce_range) {
    // No device check


  // DeviceGuard omitted
  return at::native::_choose_qparams_per_tensor(self, reduce_range);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___saturate_weight_to_fp16(const at::Tensor & weight) {
    // No device check


  // DeviceGuard omitted
  return at::native::_saturate_weight_to_fp16(weight);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__choose_qparams_optimized(const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
    // No device check


  // DeviceGuard omitted
  return at::native::choose_qparams_optimized(input, numel, n_bins, ratio, bit_width);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dtype_layout_to_dtype_layout(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::to(self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_device_to_device(const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::to(self, device, dtype, non_blocking, copy, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dtype_to_dtype(const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::to(self, dtype, non_blocking, copy, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_to_other(const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::to(self, other, non_blocking, copy, memory_format);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__meshgrid(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::meshgrid(tensors);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_indexing_meshgrid_indexing(at::TensorList tensors, c10::string_view indexing) {
    // No device check


  // DeviceGuard omitted
  return at::native::meshgrid(tensors, indexing);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cartesian_prod(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::cartesian_prod(tensors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__combinations(const at::Tensor & self, int64_t r, bool with_replacement) {
    // No device check


  // DeviceGuard omitted
  return at::native::combinations(self, r, with_replacement);
}

} // anonymous namespace
namespace {

at::Scalar wrapper__item(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::item(self);
}

} // anonymous namespace
namespace {

at::ScalarType wrapper_Tensor_result_type_Tensor(const at::Tensor & tensor, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::result_type(tensor, other);
}

} // anonymous namespace
namespace {

at::ScalarType wrapper_Scalar_result_type_Scalar(const at::Tensor & tensor, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::result_type(tensor, other);
}

} // anonymous namespace
namespace {

at::ScalarType wrapper_Scalar_Tensor_result_type_Scalar_Tensor(const at::Scalar & scalar, const at::Tensor & tensor) {
    // No device check


  // DeviceGuard omitted
  return at::native::result_type(scalar, tensor);
}

} // anonymous namespace
namespace {

at::ScalarType wrapper_Scalar_Scalar_result_type_Scalar_Scalar(const at::Scalar & scalar1, const at::Scalar & scalar2) {
    // No device check


  // DeviceGuard omitted
  return at::native::result_type(scalar1, scalar2);
}

} // anonymous namespace
namespace {

bool wrapper__can_cast(at::ScalarType from, at::ScalarType to) {
    // No device check


  // DeviceGuard omitted
  return at::native::can_cast(from, to);
}

} // anonymous namespace
namespace {

at::ScalarType wrapper__promote_types(at::ScalarType type1, at::ScalarType type2) {
    // No device check


  // DeviceGuard omitted
  return at::native::promote_types(type1, type2);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper___thnn_differentiable_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
    // No device check


  // DeviceGuard omitted
  return at::native::_thnn_differentiable_lstm_cell_backward(grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper___thnn_differentiable_gru_cell_backward(const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    // No device check


  // DeviceGuard omitted
  return at::native::_thnn_differentiable_gru_cell_backward(grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_input_lstm_input(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check


  // DeviceGuard omitted
  return at::native::lstm(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper_data_lstm_data(const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check


  // DeviceGuard omitted
  return at::native::lstm(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_input_gru_input(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check


  // DeviceGuard omitted
  return at::native::gru(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_data_gru_data(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check


  // DeviceGuard omitted
  return at::native::gru(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_input_rnn_tanh_input(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check


  // DeviceGuard omitted
  return at::native::rnn_tanh(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_data_rnn_tanh_data(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check


  // DeviceGuard omitted
  return at::native::rnn_tanh(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_input_rnn_relu_input(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    // No device check


  // DeviceGuard omitted
  return at::native::rnn_relu(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_data_rnn_relu_data(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    // No device check


  // DeviceGuard omitted
  return at::native::rnn_relu(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__quantized_lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantized_lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__quantized_gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantized_gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__quantized_rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantized_rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__quantized_rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantized_rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___pack_padded_sequence_backward(const at::Tensor & grad, at::IntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
    // No device check


  // DeviceGuard omitted
  return at::native::_pack_padded_sequence_backward(grad, input_size, batch_sizes, batch_first);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___pad_packed_sequence(const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
    // No device check


  // DeviceGuard omitted
  return at::native::_pad_packed_sequence(data, batch_sizes, batch_first, padding_value, total_length);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_masked_fill_Scalar(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::masked_fill(self, mask, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_masked_fill_Tensor(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::masked_fill(self, mask, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__masked_scatter(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::masked_scatter(self, mask, source);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__put(const at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
    // No device check


  // DeviceGuard omitted
  return at::native::put(self, index, source, accumulate);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__index_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_add_(self, dim, index, source);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__index_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_add(self, dim, index, source);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_alpha_index_add_alpha(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_add(self, dim, index, source, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_index_add_dimname(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_add(self, dim, index, source, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_Scalar_index_fill_int_Scalar(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_fill(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_int_Tensor_index_fill_int_Tensor(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_fill(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Dimname_Scalar_index_fill__Dimname_Scalar(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_fill_(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_Scalar_index_fill_Dimname_Scalar(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_fill(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Dimname_Tensor_index_fill__Dimname_Tensor(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_fill_(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Dimname_Tensor_index_fill_Dimname_Tensor(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_fill(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_src_scatter_dimname_src(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check


  // DeviceGuard omitted
  return at::native::scatter(self, dim, index, src);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_value_scatter_dimname_value(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::scatter(self, dim, index, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_scatter_add_dimname(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    // No device check


  // DeviceGuard omitted
  return at::native::scatter_add(self, dim, index, src);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_bitwise_and__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_and_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar___and___Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__and__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar___iand___Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__iand__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor___and___Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__and__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor___iand___Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__iand__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_bitwise_or_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_or(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_bitwise_or__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_or_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar___or___Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__or__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar___ior___Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__ior__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor___or___Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__or__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor___ior___Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__ior__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_bitwise_xor_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_xor(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_bitwise_xor__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_xor_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar___xor___Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__xor__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar___ixor___Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__ixor__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor___xor___Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__xor__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor___ixor___Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::__ixor__(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__diag_backward(const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t diagonal) {
    // No device check


  // DeviceGuard omitted
  return at::native::diag_backward(grad, input_sizes, diagonal);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__trace_backward(const at::Tensor & grad, at::IntArrayRef sizes) {
    // No device check


  // DeviceGuard omitted
  return at::native::trace_backward(grad, sizes);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_not_equal_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::not_equal(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_not_equal_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::not_equal_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_not_equal__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::not_equal_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_not_equal_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::not_equal(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_not_equal_out_Tensor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::not_equal_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_not_equal__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::not_equal_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_greater_equal_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_equal(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_greater_equal_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_equal_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_greater_equal__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_equal_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_greater_equal_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_equal(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_greater_equal_out_Tensor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_equal_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_greater_equal__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_equal_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_less_equal_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_equal(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_less_equal_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_equal_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_less_equal__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_equal_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_less_equal_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_equal(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_less_equal_out_Tensor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_equal_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_less_equal__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_equal_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_greater_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_greater_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_greater__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_greater_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_greater_out_Tensor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_greater__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::greater_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_less_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_less_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_less__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_less_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_out_less_out_Tensor_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_less__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::less_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__take_along_dim(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::take_along_dim(self, indices, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_take_along_dim_out_out(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::take_along_dim_out(self, indices, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_index_select_dimname(const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_select(self, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_index_select_out_dimname_out(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_select_out(self, dim, index, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__index_select_backward(const at::Tensor & grad, at::IntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_select_backward(grad, self_sizes, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__masked_select_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::masked_select_backward(grad, input, mask);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__nonzero_numpy(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::nonzero_numpy(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__gather_backward(const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    // No device check


  // DeviceGuard omitted
  return at::native::gather_backward(grad, self, dim, index, sparse_grad);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_gather_dimname(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
    // No device check


  // DeviceGuard omitted
  return at::native::gather(self, dim, index, sparse_grad);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dimname_out_gather_out_dimname_out(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::gather_out(self, dim, index, sparse_grad, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___gather_sparse_backward(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
    // No device check


  // DeviceGuard omitted
  return at::native::_gather_sparse_backward(self, dim, index, grad);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cross_entropy_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, double label_smoothing) {
    // No device check


  // DeviceGuard omitted
  return at::native::cross_entropy_loss(self, target, weight, reduction, ignore_index, label_smoothing);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__svd(const at::Tensor & self, bool some, bool compute_uv) {
    // No device check


  // DeviceGuard omitted
  return at::native::svd(self, some, compute_uv);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_U_svd_out_U(const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
    // No device check


  // DeviceGuard omitted
  return at::native::svd_out(self, some, compute_uv, U, S, V);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__swapaxes(const at::Tensor & self, int64_t axis0, int64_t axis1) {
    // No device check


  // DeviceGuard omitted
  return at::native::swapaxes(self, axis0, axis1);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__swapaxes_(at::Tensor & self, int64_t axis0, int64_t axis1) {
    // No device check


  // DeviceGuard omitted
  return at::native::swapaxes_(self, axis0, axis1);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__swapdims(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check


  // DeviceGuard omitted
  return at::native::swapdims(self, dim0, dim1);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__swapdims_(at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check


  // DeviceGuard omitted
  return at::native::swapdims_(self, dim0, dim1);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__qr(const at::Tensor & self, bool some) {
    // No device check


  // DeviceGuard omitted
  return at::native::qr(self, some);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_Q_qr_out_Q(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
    // No device check


  // DeviceGuard omitted
  return at::native::qr_out(self, some, Q, R);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__orgqr(const at::Tensor & self, const at::Tensor & input2) {
    // No device check


  // DeviceGuard omitted
  return at::native::orgqr(self, input2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_orgqr_out_out(const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::orgqr_out(self, input2, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_max_other(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::max(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_max_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::max_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_min_other(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::min(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_min_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::min_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_scalar_quantile_scalar(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile(self, q, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_scalar_out_quantile_out_scalar_out(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile_out(self, q, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__quantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile(self, q, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_quantile_out_out(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile_out(self, q, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_scalar_nanquantile_scalar(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile(self, q, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_scalar_out_nanquantile_out_scalar_out(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile_out(self, q, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nanquantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile(self, q, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_nanquantile_out_out(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile_out(self, q, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_new_scalar_quantile_new_scalar(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile(self, q, dim, keepdim, interpolation);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_new_scalar_out_quantile_out_new_scalar_out(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile_out(self, q, dim, keepdim, interpolation, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_new_quantile_new(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile(self, q, dim, keepdim, interpolation);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_new_out_quantile_out_new_out(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::quantile_out(self, q, dim, keepdim, interpolation, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_new_scalar_nanquantile_new_scalar(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile(self, q, dim, keepdim, interpolation);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_new_scalar_out_nanquantile_out_new_scalar_out(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile_out(self, q, dim, keepdim, interpolation, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_new_nanquantile_new(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile(self, q, dim, keepdim, interpolation);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_new_out_nanquantile_out_new_out(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanquantile_out(self, q, dim, keepdim, interpolation, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dimname_sort_dimname(const at::Tensor & self, at::Dimname dim, bool descending) {
    // No device check


  // DeviceGuard omitted
  return at::native::sort(self, dim, descending);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_dimname_values_sort_out_dimname_values(const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::sort_out(self, dim, descending, values, indices);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dimname_stable_sort_dimname_stable(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
    // No device check


  // DeviceGuard omitted
  return at::native::sort(self, stable, dim, descending);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_dimname_values_stable_sort_out_dimname_values_stable(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::sort_out(self, stable, dim, descending, values, indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__msort(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::msort(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_msort_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::msort_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__argsort(const at::Tensor & self, int64_t dim, bool descending) {
    // No device check


  // DeviceGuard omitted
  return at::native::argsort(self, dim, descending);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dimname_argsort_dimname(const at::Tensor & self, at::Dimname dim, bool descending) {
    // No device check


  // DeviceGuard omitted
  return at::native::argsort(self, dim, descending);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Tensor_float_power_Tensor_Tensor(const at::Tensor & self, const at::Tensor & exponent) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power(self, exponent);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Tensor_out_float_power_out_Tensor_Tensor_out(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power_out(self, exponent, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_float_power__Tensor(at::Tensor & self, const at::Tensor & exponent) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power_(self, exponent);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_float_power_Scalar(const at::Scalar & self, const at::Tensor & exponent) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power(self, exponent);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_float_power_out_Scalar_out(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power_out(self, exponent, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Scalar_float_power_Tensor_Scalar(const at::Tensor & self, const at::Scalar & exponent) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power(self, exponent);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Scalar_out_float_power_out_Tensor_Scalar_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power_out(self, exponent, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_float_power__Scalar(at::Tensor & self, const at::Scalar & exponent) {
    // No device check


  // DeviceGuard omitted
  return at::native::float_power_(self, exponent);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_float_float_normal_float_float(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::normal(mean, std, size, generator, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_float_float_out_normal_out_float_float_out(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::normal_out(mean, std, size, generator, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__multilabel_margin_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::multilabel_margin_loss(self, target, reduction);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_multilabel_margin_loss_out_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::multilabel_margin_loss_out(self, target, reduction, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nll_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    // No device check


  // DeviceGuard omitted
  return at::native::nll_loss(self, target, weight, reduction, ignore_index);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_nll_loss_out_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nll_loss_out(self, target, weight, reduction, ignore_index, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nll_loss_nd(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    // No device check


  // DeviceGuard omitted
  return at::native::nll_loss_nd(self, target, weight, reduction, ignore_index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nll_loss2d(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    // No device check


  // DeviceGuard omitted
  return at::native::nll_loss2d(self, target, weight, reduction, ignore_index);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_nll_loss2d_out_out(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::nll_loss2d_out(self, target, weight, reduction, ignore_index, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__log_sigmoid(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::log_sigmoid(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_log_sigmoid_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::log_sigmoid_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::adaptive_avg_pool2d(self, output_size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__adaptive_avg_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
    // No device check


  // DeviceGuard omitted
  return at::native::adaptive_avg_pool3d(self, output_size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__thnn_conv2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    // No device check


  // DeviceGuard omitted
  return at::native::thnn_conv2d(self, weight, kernel_size, bias, stride, padding);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_thnn_conv2d_out_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::thnn_conv2d_out(self, weight, kernel_size, bias, stride, padding, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__slow_conv3d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    // No device check


  // DeviceGuard omitted
  return at::native::slow_conv3d(self, weight, kernel_size, bias, stride, padding);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_slow_conv3d_out_out(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::slow_conv3d_out(self, weight, kernel_size, bias, stride, padding, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__column_stack(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::column_stack(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_column_stack_out_out(at::TensorList tensors, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::column_stack_out(tensors, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__isfinite(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::isfinite(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__isinf(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::isinf(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___add_batch_dim(const at::Tensor & self, int64_t batch_dim, int64_t level) {
    // No device check


  // DeviceGuard omitted
  return at::native::_add_batch_dim(self, batch_dim, level);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___remove_batch_dim(const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_remove_batch_dim(self, level, batch_size, out_dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_expm1(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_expm1(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_expm1_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_expm1_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_exp2(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_exp2(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_exp2_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_exp2_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_psi(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_psi(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_psi_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_psi_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_digamma(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_digamma(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_digamma_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_digamma_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_gammaln(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_gammaln(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_gammaln_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_gammaln_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_erf(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_erf(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_erf_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_erf_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_erfc(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_erfc(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_erfc_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_erfc_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_erfinv(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_erfinv(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_erfinv_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_erfinv_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_ndtr(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_ndtr(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_ndtr_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_ndtr_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_xlogy(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlogy(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_xlogy_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlogy_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_self_scalar_special_xlogy_self_scalar(const at::Scalar & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlogy(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_self_scalar_out_special_xlogy_out_self_scalar_out(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlogy_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_scalar_special_xlogy_other_scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlogy(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_other_scalar_out_special_xlogy_out_other_scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlogy_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_i0(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_i0(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_i0_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_i0_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_logit(const at::Tensor & self, c10::optional<double> eps) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_logit(self, eps);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_logit_out_out(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_logit_out(self, eps, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_polygamma(int64_t n, const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_polygamma(n, self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_polygamma_out_out(int64_t n, const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_polygamma_out(n, self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_logsumexp(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_logsumexp_out_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_logsumexp_out(self, dim, keepdim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_expit(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_expit(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_expit_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_expit_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_sinc(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_sinc(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_sinc_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_sinc_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_round(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_round(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_round_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_round_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_log1p(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_log1p(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_log1p_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_log1p_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_log_softmax(self, dim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_gammainc(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_gammainc(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_gammainc_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_gammainc_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_gammaincc(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_gammaincc(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_gammaincc_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_gammaincc_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__special_multigammaln(const at::Tensor & self, int64_t p) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_multigammaln(self, p);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_special_multigammaln_out_out(const at::Tensor & self, int64_t p, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_multigammaln_out(self, p, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_fft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fft(self, n, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_fft_out_out(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fft_out(self, n, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_ifft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifft(self, n, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_ifft_out_out(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifft_out(self, n, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_rfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfft(self, n, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_rfft_out_out(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfft_out(self, n, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_irfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_irfft(self, n, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_irfft_out_out(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_irfft_out(self, n, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_hfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_hfft(self, n, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_hfft_out_out(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_hfft_out(self, n, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_ihfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ihfft(self, n, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_ihfft_out_out(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ihfft_out(self, n, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_fft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fft2(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_fft2_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fft2_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_ifft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifft2(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_ifft2_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifft2_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_rfft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfft2(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_rfft2_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfft2_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_irfft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_irfft2(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_irfft2_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_irfft2_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_fftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fftn(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_fftn_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fftn_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_ifftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifftn(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_ifftn_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifftn_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_rfftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfftn(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_rfftn_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfftn_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_irfftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_irfftn(self, s, dim, norm);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_irfftn_out_out(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_irfftn_out(self, s, dim, norm, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_fftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fftfreq(n, d, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_fftfreq_out_out(int64_t n, double d, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fftfreq_out(n, d, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_rfftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfftfreq(n, d, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_fft_rfftfreq_out_out(int64_t n, double d, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_rfftfreq_out(n, d, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_fftshift(const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_fftshift(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__fft_ifftshift(const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::fft_ifftshift(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_cholesky(const at::Tensor & self, bool upper) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_cholesky(self, upper);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_cholesky_out_out(const at::Tensor & self, bool upper, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_cholesky_out(self, upper, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_det(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_det(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_det_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_det_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__det(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::det(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_matmul(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matmul(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_matmul_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matmul_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_eigvals(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_eigvals(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_eigvals_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_eigvals_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_eigvalsh(const at::Tensor & self, c10::string_view UPLO) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_eigvalsh(self, UPLO);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_inv(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_inv(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_inv_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_inv_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__inner(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::inner(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_inner_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::inner_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__outer(const at::Tensor & self, const at::Tensor & vec2) {
    // No device check


  // DeviceGuard omitted
  return at::native::outer(self, vec2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_outer_out_out(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::outer_out(self, vec2, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__ger(const at::Tensor & self, const at::Tensor & vec2) {
    // No device check


  // DeviceGuard omitted
  return at::native::ger(self, vec2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_ger_out_out(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::ger_out(self, vec2, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_norm(const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_norm(self, ord, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_norm_out_out(const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ord_str_linalg_norm_ord_str(const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_norm(self, ord, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_ord_str_out_linalg_norm_out_ord_str_out(const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_norm_out(self, ord, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_matrix_norm(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_matrix_norm_out_out(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_str_ord_linalg_matrix_norm_str_ord(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_str_ord_out_linalg_matrix_norm_out_str_ord_out(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_norm_out(self, ord, dim, keepdim, dtype, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__linalg_svd(const at::Tensor & self, bool full_matrices) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_svd(self, full_matrices);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> wrapper_U_linalg_svd_out_U(const at::Tensor & self, bool full_matrices, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_svd_out(self, full_matrices, U, S, Vh);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_svdvals(const at::Tensor & input) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_svdvals(input);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_svdvals_out_out(const at::Tensor & input, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_svdvals_out(input, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_cond(const at::Tensor & self, const c10::optional<at::Scalar> & p) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_cond(self, p);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_cond_out_out(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_cond_out(self, p, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_p_str_linalg_cond_p_str(const at::Tensor & self, c10::string_view p) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_cond(self, p);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_p_str_out_linalg_cond_out_p_str_out(const at::Tensor & self, c10::string_view p, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_cond_out(self, p, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_pinv(const at::Tensor & self, double rcond, bool hermitian) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_pinv(self, rcond, hermitian);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_pinv_out_out(const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_pinv_out(self, rcond, hermitian, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_rcond_tensor_linalg_pinv_rcond_tensor(const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_pinv(self, rcond, hermitian);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_rcond_tensor_linalg_pinv_out_out_rcond_tensor(const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_pinv_out(self, rcond, hermitian, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_tensorinv(const at::Tensor & self, int64_t ind) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_tensorinv(self, ind);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_tensorinv_out_out(const at::Tensor & self, int64_t ind, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_tensorinv_out(self, ind, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_tensorsolve(const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_tensorsolve(self, other, dims);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_tensorsolve_out_out(const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_tensorsolve_out(self, other, dims, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_matrix_power(const at::Tensor & self, int64_t n) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_power(self, n);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_matrix_power_out_out(const at::Tensor & self, int64_t n, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_power_out(self, n, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_matrix_rank(const at::Tensor & self, c10::optional<double> tol, bool hermitian) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_rank(self, tol, hermitian);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_matrix_rank_out_out(const at::Tensor & self, c10::optional<double> tol, bool hermitian, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_rank_out(self, tol, hermitian, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_tol_tensor_linalg_matrix_rank_tol_tensor(const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_rank(input, tol, hermitian);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_tol_tensor_linalg_matrix_rank_out_out_tol_tensor(const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_matrix_rank_out(input, tol, hermitian, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__linalg_multi_dot(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_multi_dot(tensors);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_linalg_multi_dot_out_out(at::TensorList tensors, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_multi_dot_out(tensors, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___test_serialization_subcmul(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::_test_serialization_subcmul(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___test_string_default(const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
    // No device check


  // DeviceGuard omitted
  return at::native::_test_string_default(dummy, a, b);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_a__test_ambiguous_defaults_a(const at::Tensor & dummy, int64_t a, int64_t b) {
    // No device check


  // DeviceGuard omitted
  return at::native::_test_ambiguous_defaults(dummy, a, b);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_b__test_ambiguous_defaults_b(const at::Tensor & dummy, int64_t a, c10::string_view b) {
    // No device check


  // DeviceGuard omitted
  return at::native::_test_ambiguous_defaults(dummy, a, b);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__pad_sequence(at::TensorList sequences, bool batch_first, double padding_value) {
    // No device check


  // DeviceGuard omitted
  return at::native::pad_sequence(sequences, batch_first, padding_value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__flatten_dense_tensors(at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::flatten_dense_tensors(tensors);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__unflatten_dense_tensors(const at::Tensor & flat, at::TensorList tensors) {
    // No device check


  // DeviceGuard omitted
  return at::native::unflatten_dense_tensors(flat, tensors);
}

} // anonymous namespace

TORCH_LIBRARY_IMPL(aten, CompositeImplicitAutograd, m) {
  m.impl("_cast_Byte",
  TORCH_FN(wrapper___cast_Byte));
  m.impl("_cast_Char",
  TORCH_FN(wrapper___cast_Char));
  m.impl("_cast_Double",
  TORCH_FN(wrapper___cast_Double));
  m.impl("_cast_Float",
  TORCH_FN(wrapper___cast_Float));
  m.impl("_cast_Int",
  TORCH_FN(wrapper___cast_Int));
  m.impl("_cast_Long",
  TORCH_FN(wrapper___cast_Long));
  m.impl("_cast_Short",
  TORCH_FN(wrapper___cast_Short));
  m.impl("_cast_Half",
  TORCH_FN(wrapper___cast_Half));
  m.impl("_backward",
  TORCH_FN(wrapper___backward));
  m.impl("set_data",
  TORCH_FN(wrapper__set_data));
  m.impl("data",
  TORCH_FN(wrapper__data));
  m.impl("is_leaf",
  TORCH_FN(wrapper__is_leaf));
  m.impl("output_nr",
  TORCH_FN(wrapper__output_nr));
  m.impl("_version",
  TORCH_FN(wrapper___version));
  m.impl("requires_grad_",
  TORCH_FN(wrapper__requires_grad_));
  m.impl("retain_grad",
  TORCH_FN(wrapper__retain_grad));
  m.impl("retains_grad",
  TORCH_FN(wrapper__retains_grad));
  m.impl("_make_dual",
  TORCH_FN(wrapper___make_dual));
  m.impl("_unpack_dual",
  TORCH_FN(wrapper___unpack_dual));
  m.impl("rename_",
  TORCH_FN(wrapper__rename_));
  m.impl("rename",
  TORCH_FN(wrapper__rename));
  m.impl("align_to",
  TORCH_FN(wrapper__align_to));
  m.impl("align_to.ellipsis_idx",
  TORCH_FN(wrapper_ellipsis_idx_align_to_ellipsis_idx));
  m.impl("align_as",
  TORCH_FN(wrapper__align_as));
  m.impl("align_tensors",
  TORCH_FN(wrapper__align_tensors));
  m.impl("refine_names",
  TORCH_FN(wrapper__refine_names));
  m.impl("_use_cudnn_rnn_flatten_weight",
  TORCH_FN(wrapper___use_cudnn_rnn_flatten_weight));
  m.impl("_debug_has_internal_overlap",
  TORCH_FN(wrapper___debug_has_internal_overlap));
  m.impl("_sobol_engine_draw",
  TORCH_FN(wrapper___sobol_engine_draw));
  m.impl("_sobol_engine_ff_",
  TORCH_FN(wrapper___sobol_engine_ff_));
  m.impl("_sobol_engine_scramble_",
  TORCH_FN(wrapper___sobol_engine_scramble_));
  m.impl("_sobol_engine_initialize_state_",
  TORCH_FN(wrapper___sobol_engine_initialize_state_));
  m.impl("_reshape_from_tensor",
  TORCH_FN(wrapper___reshape_from_tensor));
  m.impl("_shape_as_tensor",
  TORCH_FN(wrapper___shape_as_tensor));
  m.impl("dropout",
  TORCH_FN(wrapper__dropout));
  m.impl("dropout_",
  TORCH_FN(wrapper__dropout_));
  m.impl("feature_dropout",
  TORCH_FN(wrapper__feature_dropout));
  m.impl("feature_dropout_",
  TORCH_FN(wrapper__feature_dropout_));
  m.impl("alpha_dropout",
  TORCH_FN(wrapper__alpha_dropout));
  m.impl("alpha_dropout_",
  TORCH_FN(wrapper__alpha_dropout_));
  m.impl("feature_alpha_dropout",
  TORCH_FN(wrapper__feature_alpha_dropout));
  m.impl("feature_alpha_dropout_",
  TORCH_FN(wrapper__feature_alpha_dropout_));
  m.impl("absolute",
  TORCH_FN(wrapper__absolute));
  m.impl("absolute.out",
  TORCH_FN(wrapper_out_absolute_out_out));
  m.impl("absolute_",
  TORCH_FN(wrapper__absolute_));
  m.impl("real",
  TORCH_FN(wrapper__real));
  m.impl("imag",
  TORCH_FN(wrapper__imag));
  m.impl("conj",
  TORCH_FN(wrapper__conj));
  m.impl("conj_physical",
  TORCH_FN(wrapper__conj_physical));
  m.impl("resolve_conj",
  TORCH_FN(wrapper__resolve_conj));
  m.impl("resolve_neg",
  TORCH_FN(wrapper__resolve_neg));
  m.impl("arccos",
  TORCH_FN(wrapper__arccos));
  m.impl("arccos.out",
  TORCH_FN(wrapper_out_arccos_out_out));
  m.impl("arccos_",
  TORCH_FN(wrapper__arccos_));
  m.impl("avg_pool1d",
  TORCH_FN(wrapper__avg_pool1d));
  m.impl("adaptive_avg_pool1d",
  TORCH_FN(wrapper__adaptive_avg_pool1d));
  m.impl("adaptive_max_pool1d",
  TORCH_FN(wrapper__adaptive_max_pool1d));
  m.impl("addr",
  TORCH_FN(wrapper__addr));
  m.impl("addr.out",
  TORCH_FN(wrapper_out_addr_out_out));
  m.impl("affine_grid_generator_backward",
  TORCH_FN(wrapper__affine_grid_generator_backward));
  m.impl("all.dimname",
  TORCH_FN(wrapper_dimname_all_dimname));
  m.impl("all.dimname_out",
  TORCH_FN(wrapper_dimname_out_all_out_dimname_out));
  m.impl("allclose",
  TORCH_FN(wrapper__allclose));
  m.impl("any.dimname",
  TORCH_FN(wrapper_dimname_any_dimname));
  m.impl("any.dimname_out",
  TORCH_FN(wrapper_dimname_out_any_out_dimname_out));
  m.impl("arange",
  TORCH_FN(wrapper__arange));
  m.impl("arange.start",
  TORCH_FN(wrapper_start_arange_start));
  m.impl("arange.start_step",
  TORCH_FN(wrapper_start_step_arange_start_step));
  m.impl("arange.out",
  TORCH_FN(wrapper_out_arange_out_out));
  m.impl("_dim_arange",
  TORCH_FN(wrapper___dim_arange));
  m.impl("arccosh",
  TORCH_FN(wrapper__arccosh));
  m.impl("arccosh.out",
  TORCH_FN(wrapper_out_arccosh_out_out));
  m.impl("arccosh_",
  TORCH_FN(wrapper__arccosh_));
  m.impl("arcsinh",
  TORCH_FN(wrapper__arcsinh));
  m.impl("arcsinh.out",
  TORCH_FN(wrapper_out_arcsinh_out_out));
  m.impl("arcsinh_",
  TORCH_FN(wrapper__arcsinh_));
  m.impl("arctanh",
  TORCH_FN(wrapper__arctanh));
  m.impl("arctanh.out",
  TORCH_FN(wrapper_out_arctanh_out_out));
  m.impl("arctanh_",
  TORCH_FN(wrapper__arctanh_));
  m.impl("arcsin",
  TORCH_FN(wrapper__arcsin));
  m.impl("arcsin.out",
  TORCH_FN(wrapper_out_arcsin_out_out));
  m.impl("arcsin_",
  TORCH_FN(wrapper__arcsin_));
  m.impl("arctan",
  TORCH_FN(wrapper__arctan));
  m.impl("arctan.out",
  TORCH_FN(wrapper_out_arctan_out_out));
  m.impl("arctan_",
  TORCH_FN(wrapper__arctan_));
  m.impl("atleast_1d",
  TORCH_FN(wrapper__atleast_1d));
  m.impl("atleast_1d.Sequence",
  TORCH_FN(wrapper_Sequence_atleast_1d_Sequence));
  m.impl("atleast_2d",
  TORCH_FN(wrapper__atleast_2d));
  m.impl("atleast_2d.Sequence",
  TORCH_FN(wrapper_Sequence_atleast_2d_Sequence));
  m.impl("atleast_3d",
  TORCH_FN(wrapper__atleast_3d));
  m.impl("atleast_3d.Sequence",
  TORCH_FN(wrapper_Sequence_atleast_3d_Sequence));
  m.impl("_baddbmm_mkl_",
  TORCH_FN(wrapper___baddbmm_mkl_));
  m.impl("bartlett_window",
  TORCH_FN(wrapper__bartlett_window));
  m.impl("bartlett_window.periodic",
  TORCH_FN(wrapper_periodic_bartlett_window_periodic));
  m.impl("batch_norm",
  TORCH_FN(wrapper__batch_norm));
  m.impl("_batch_norm_impl_index",
  TORCH_FN(wrapper___batch_norm_impl_index));
  m.impl("_batch_norm_impl_index_backward",
  TORCH_FN(wrapper___batch_norm_impl_index_backward));
  m.impl("bernoulli.p",
  TORCH_FN(wrapper_p_bernoulli_p));
  m.impl("bilinear",
  TORCH_FN(wrapper__bilinear));
  m.impl("binary_cross_entropy_with_logits_backward",
  TORCH_FN(wrapper__binary_cross_entropy_with_logits_backward));
  m.impl("logical_not",
  TORCH_FN(wrapper__logical_not));
  m.impl("logical_not_",
  TORCH_FN(wrapper__logical_not_));
  m.impl("logical_xor",
  TORCH_FN(wrapper__logical_xor));
  m.impl("logical_xor_",
  TORCH_FN(wrapper__logical_xor_));
  m.impl("logical_and",
  TORCH_FN(wrapper__logical_and));
  m.impl("logical_and_",
  TORCH_FN(wrapper__logical_and_));
  m.impl("logical_or",
  TORCH_FN(wrapper__logical_or));
  m.impl("logical_or_",
  TORCH_FN(wrapper__logical_or_));
  m.impl("blackman_window",
  TORCH_FN(wrapper__blackman_window));
  m.impl("blackman_window.periodic",
  TORCH_FN(wrapper_periodic_blackman_window_periodic));
  m.impl("broadcast_tensors",
  TORCH_FN(wrapper__broadcast_tensors));
  m.impl("broadcast_to",
  TORCH_FN(wrapper__broadcast_to));
  m.impl("cat.names",
  TORCH_FN(wrapper_names_cat_names));
  m.impl("cat.names_out",
  TORCH_FN(wrapper_names_out_cat_out_names_out));
  m.impl("concat",
  TORCH_FN(wrapper__concat));
  m.impl("concat.out",
  TORCH_FN(wrapper_out_concat_out_out));
  m.impl("concat.names",
  TORCH_FN(wrapper_names_concat_names));
  m.impl("concat.names_out",
  TORCH_FN(wrapper_names_out_concat_out_names_out));
  m.impl("block_diag",
  TORCH_FN(wrapper__block_diag));
  m.impl("chain_matmul",
  TORCH_FN(wrapper__chain_matmul));
  m.impl("chain_matmul.out",
  TORCH_FN(wrapper_out_chain_matmul_out_out));
  m.impl("unsafe_chunk",
  TORCH_FN(wrapper__unsafe_chunk));
  m.impl("chunk",
  TORCH_FN(wrapper__chunk));
  m.impl("tensor_split.sections",
  TORCH_FN(wrapper_sections_tensor_split_sections));
  m.impl("tensor_split.indices",
  TORCH_FN(wrapper_indices_tensor_split_indices));
  m.impl("tensor_split.tensor_indices_or_sections",
  TORCH_FN(wrapper_tensor_indices_or_sections_tensor_split_tensor_indices_or_sections));
  m.impl("clip",
  TORCH_FN(wrapper__clip));
  m.impl("clip.out",
  TORCH_FN(wrapper_out_clip_out_out));
  m.impl("clip_",
  TORCH_FN(wrapper__clip_));
  m.impl("clip.Tensor",
  TORCH_FN(wrapper_Tensor_clip_Tensor));
  m.impl("clip.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_clip_out_Tensor_out));
  m.impl("clip_.Tensor",
  TORCH_FN(wrapper_Tensor_clip__Tensor));
  m.impl("cudnn_is_acceptable",
  TORCH_FN(wrapper__cudnn_is_acceptable));
  m.impl("contiguous",
  TORCH_FN(wrapper__contiguous));
  m.impl("convolution",
  TORCH_FN(wrapper__convolution));
  m.impl("_convolution",
  TORCH_FN(wrapper___convolution));
  m.impl("_convolution.deprecated",
  TORCH_FN(wrapper_deprecated__convolution_deprecated));
  m.impl("_convolution_mode",
  TORCH_FN(wrapper___convolution_mode));
  m.impl("_convolution_nogroup",
  TORCH_FN(wrapper___convolution_nogroup));
  m.impl("_convolution_double_backward",
  TORCH_FN(wrapper___convolution_double_backward));
  m.impl("conv1d",
  TORCH_FN(wrapper__conv1d));
  m.impl("conv2d",
  TORCH_FN(wrapper__conv2d));
  m.impl("conv3d",
  TORCH_FN(wrapper__conv3d));
  m.impl("conv1d.padding",
  TORCH_FN(wrapper_padding_conv1d_padding));
  m.impl("conv2d.padding",
  TORCH_FN(wrapper_padding_conv2d_padding));
  m.impl("conv3d.padding",
  TORCH_FN(wrapper_padding_conv3d_padding));
  m.impl("conv_tbc_backward",
  TORCH_FN(wrapper__conv_tbc_backward));
  m.impl("conv_transpose1d",
  TORCH_FN(wrapper__conv_transpose1d));
  m.impl("conv_transpose2d.input",
  TORCH_FN(wrapper_input_conv_transpose2d_input));
  m.impl("conv_transpose3d.input",
  TORCH_FN(wrapper_input_conv_transpose3d_input));
  m.impl("cosine_embedding_loss",
  TORCH_FN(wrapper__cosine_embedding_loss));
  m.impl("cov",
  TORCH_FN(wrapper__cov));
  m.impl("corrcoef",
  TORCH_FN(wrapper__corrcoef));
  m.impl("cummax.dimname",
  TORCH_FN(wrapper_dimname_cummax_dimname));
  m.impl("cummax.dimname_out",
  TORCH_FN(wrapper_dimname_out_cummax_out_dimname_out));
  m.impl("cummin.dimname",
  TORCH_FN(wrapper_dimname_cummin_dimname));
  m.impl("cummin.dimname_out",
  TORCH_FN(wrapper_dimname_out_cummin_out_dimname_out));
  m.impl("cummaxmin_backward",
  TORCH_FN(wrapper__cummaxmin_backward));
  m.impl("cumprod.dimname",
  TORCH_FN(wrapper_dimname_cumprod_dimname));
  m.impl("cumprod.dimname_out",
  TORCH_FN(wrapper_dimname_out_cumprod_out_dimname_out));
  m.impl("cumprod_.dimname",
  TORCH_FN(wrapper_dimname_cumprod__dimname));
  m.impl("cumprod_backward",
  TORCH_FN(wrapper__cumprod_backward));
  m.impl("cumsum.dimname",
  TORCH_FN(wrapper_dimname_cumsum_dimname));
  m.impl("cumsum.dimname_out",
  TORCH_FN(wrapper_dimname_out_cumsum_out_dimname_out));
  m.impl("cumsum_.dimname",
  TORCH_FN(wrapper_dimname_cumsum__dimname));
  m.impl("cumulative_trapezoid.x",
  TORCH_FN(wrapper_x_cumulative_trapezoid_x));
  m.impl("cumulative_trapezoid.dx",
  TORCH_FN(wrapper_dx_cumulative_trapezoid_dx));
  m.impl("ctc_loss.IntList",
  TORCH_FN(wrapper_IntList_ctc_loss_IntList));
  m.impl("ctc_loss.Tensor",
  TORCH_FN(wrapper_Tensor_ctc_loss_Tensor));
  m.impl("diag_embed",
  TORCH_FN(wrapper__diag_embed));
  m.impl("diagflat",
  TORCH_FN(wrapper__diagflat));
  m.impl("diagonal.Dimname",
  TORCH_FN(wrapper_Dimname_diagonal_Dimname));
  m.impl("fill_diagonal_",
  TORCH_FN(wrapper__fill_diagonal_));
  m.impl("diff",
  TORCH_FN(wrapper__diff));
  m.impl("diff.out",
  TORCH_FN(wrapper_out_diff_out_out));
  m.impl("gradient.scalarint",
  TORCH_FN(wrapper_scalarint_gradient_scalarint));
  m.impl("gradient.scalararray",
  TORCH_FN(wrapper_scalararray_gradient_scalararray));
  m.impl("gradient.array",
  TORCH_FN(wrapper_array_gradient_array));
  m.impl("gradient.scalarrayint",
  TORCH_FN(wrapper_scalarrayint_gradient_scalarrayint));
  m.impl("gradient.scalarrayarray",
  TORCH_FN(wrapper_scalarrayarray_gradient_scalarrayarray));
  m.impl("gradient.tensorarrayint",
  TORCH_FN(wrapper_tensorarrayint_gradient_tensorarrayint));
  m.impl("gradient.tensorarray",
  TORCH_FN(wrapper_tensorarray_gradient_tensorarray));
  m.impl("divide.Tensor",
  TORCH_FN(wrapper_Tensor_divide_Tensor));
  m.impl("divide.out",
  TORCH_FN(wrapper_out_divide_out_out));
  m.impl("divide_.Tensor",
  TORCH_FN(wrapper_Tensor_divide__Tensor));
  m.impl("divide.Scalar",
  TORCH_FN(wrapper_Scalar_divide_Scalar));
  m.impl("divide_.Scalar",
  TORCH_FN(wrapper_Scalar_divide__Scalar));
  m.impl("divide.Tensor_mode",
  TORCH_FN(wrapper_Tensor_mode_divide_Tensor_mode));
  m.impl("divide.out_mode",
  TORCH_FN(wrapper_out_mode_divide_out_out_mode));
  m.impl("divide_.Tensor_mode",
  TORCH_FN(wrapper_Tensor_mode_divide__Tensor_mode));
  m.impl("divide.Scalar_mode",
  TORCH_FN(wrapper_Scalar_mode_divide_Scalar_mode));
  m.impl("divide_.Scalar_mode",
  TORCH_FN(wrapper_Scalar_mode_divide__Scalar_mode));
  m.impl("true_divide.Tensor",
  TORCH_FN(wrapper_Tensor_true_divide_Tensor));
  m.impl("true_divide.out",
  TORCH_FN(wrapper_out_true_divide_out_out));
  m.impl("true_divide_.Tensor",
  TORCH_FN(wrapper_Tensor_true_divide__Tensor));
  m.impl("true_divide.Scalar",
  TORCH_FN(wrapper_Scalar_true_divide_Scalar));
  m.impl("true_divide_.Scalar",
  TORCH_FN(wrapper_Scalar_true_divide__Scalar));
  m.impl("einsum",
  TORCH_FN(wrapper__einsum));
  m.impl("embedding_backward",
  TORCH_FN(wrapper__embedding_backward));
  m.impl("embedding_sparse_backward",
  TORCH_FN(wrapper__embedding_sparse_backward));
  m.impl("_rowwise_prune",
  TORCH_FN(wrapper___rowwise_prune));
  m.impl("row_stack",
  TORCH_FN(wrapper__row_stack));
  m.impl("row_stack.out",
  TORCH_FN(wrapper_out_row_stack_out_out));
  m.impl("embedding_bag",
  TORCH_FN(wrapper__embedding_bag));
  m.impl("embedding_bag.padding_idx",
  TORCH_FN(wrapper_padding_idx_embedding_bag_padding_idx));
  m.impl("_embedding_bag_backward",
  TORCH_FN(wrapper___embedding_bag_backward));
  m.impl("_embedding_bag_sparse_backward",
  TORCH_FN(wrapper___embedding_bag_sparse_backward));
  m.impl("empty.names",
  TORCH_FN(wrapper_names_empty_names));
  m.impl("new_empty",
  TORCH_FN(wrapper__new_empty));
  m.impl("new_empty_strided",
  TORCH_FN(wrapper__new_empty_strided));
  m.impl("new_full",
  TORCH_FN(wrapper__new_full));
  m.impl("new_zeros",
  TORCH_FN(wrapper__new_zeros));
  m.impl("new_ones",
  TORCH_FN(wrapper__new_ones));
  m.impl("empty.out",
  TORCH_FN(wrapper_out_empty_out_out));
  m.impl("empty_like",
  TORCH_FN(wrapper__empty_like));
  m.impl("expand_as",
  TORCH_FN(wrapper__expand_as));
  m.impl("eye",
  TORCH_FN(wrapper__eye));
  m.impl("eye.m",
  TORCH_FN(wrapper_m_eye_m));
  m.impl("flatten.using_ints",
  TORCH_FN(wrapper_using_ints_flatten_using_ints));
  m.impl("flatten.named_out_dim",
  TORCH_FN(wrapper_named_out_dim_flatten_named_out_dim));
  m.impl("flatten.using_names",
  TORCH_FN(wrapper_using_names_flatten_using_names));
  m.impl("flatten.DimnameList",
  TORCH_FN(wrapper_DimnameList_flatten_DimnameList));
  m.impl("unflatten.int",
  TORCH_FN(wrapper_int_unflatten_int));
  m.impl("unflatten.Dimname",
  TORCH_FN(wrapper_Dimname_unflatten_Dimname));
  m.impl("floor_divide.Scalar",
  TORCH_FN(wrapper_Scalar_floor_divide_Scalar));
  m.impl("floor_divide_.Scalar",
  TORCH_FN(wrapper_Scalar_floor_divide__Scalar));
  m.impl("full.names",
  TORCH_FN(wrapper_names_full_names));
  m.impl("full",
  TORCH_FN(wrapper__full));
  m.impl("full.out",
  TORCH_FN(wrapper_out_full_out_out));
  m.impl("full_like",
  TORCH_FN(wrapper__full_like));
  m.impl("grid_sampler",
  TORCH_FN(wrapper__grid_sampler));
  m.impl("_grid_sampler_2d_cpu_fallback_backward",
  TORCH_FN(wrapper___grid_sampler_2d_cpu_fallback_backward));
  m.impl("hann_window",
  TORCH_FN(wrapper__hann_window));
  m.impl("hann_window.periodic",
  TORCH_FN(wrapper_periodic_hann_window_periodic));
  m.impl("hamming_window",
  TORCH_FN(wrapper__hamming_window));
  m.impl("hamming_window.periodic",
  TORCH_FN(wrapper_periodic_hamming_window_periodic));
  m.impl("hamming_window.periodic_alpha",
  TORCH_FN(wrapper_periodic_alpha_hamming_window_periodic_alpha));
  m.impl("hamming_window.periodic_alpha_beta",
  TORCH_FN(wrapper_periodic_alpha_beta_hamming_window_periodic_alpha_beta));
  m.impl("kaiser_window",
  TORCH_FN(wrapper__kaiser_window));
  m.impl("kaiser_window.periodic",
  TORCH_FN(wrapper_periodic_kaiser_window_periodic));
  m.impl("kaiser_window.beta",
  TORCH_FN(wrapper_beta_kaiser_window_beta));
  m.impl("hinge_embedding_loss",
  TORCH_FN(wrapper__hinge_embedding_loss));
  m.impl("group_norm",
  TORCH_FN(wrapper__group_norm));
  m.impl("native_group_norm",
  TORCH_FN(wrapper__native_group_norm));
  m.impl("_cufft_get_plan_cache_size",
  TORCH_FN(wrapper___cufft_get_plan_cache_size));
  m.impl("_cufft_get_plan_cache_max_size",
  TORCH_FN(wrapper___cufft_get_plan_cache_max_size));
  m.impl("_cufft_set_plan_cache_max_size",
  TORCH_FN(wrapper___cufft_set_plan_cache_max_size));
  m.impl("_cufft_clear_plan_cache",
  TORCH_FN(wrapper___cufft_clear_plan_cache));
  m.impl("index_copy",
  TORCH_FN(wrapper__index_copy));
  m.impl("index_copy_.dimname",
  TORCH_FN(wrapper_dimname_index_copy__dimname));
  m.impl("index_copy.dimname",
  TORCH_FN(wrapper_dimname_index_copy_dimname));
  m.impl("index_put",
  TORCH_FN(wrapper__index_put));
  m.impl("instance_norm",
  TORCH_FN(wrapper__instance_norm));
  m.impl("isclose",
  TORCH_FN(wrapper__isclose));
  m.impl("is_distributed",
  TORCH_FN(wrapper__is_distributed));
  m.impl("is_floating_point",
  TORCH_FN(wrapper__is_floating_point));
  m.impl("is_complex",
  TORCH_FN(wrapper__is_complex));
  m.impl("is_conj",
  TORCH_FN(wrapper__is_conj));
  m.impl("is_neg",
  TORCH_FN(wrapper__is_neg));
  m.impl("isreal",
  TORCH_FN(wrapper__isreal));
  m.impl("is_nonzero",
  TORCH_FN(wrapper__is_nonzero));
  m.impl("is_same_size",
  TORCH_FN(wrapper__is_same_size));
  m.impl("is_signed",
  TORCH_FN(wrapper__is_signed));
  m.impl("is_inference",
  TORCH_FN(wrapper__is_inference));
  m.impl("kron",
  TORCH_FN(wrapper__kron));
  m.impl("kron.out",
  TORCH_FN(wrapper_out_kron_out_out));
  m.impl("kthvalue.dimname",
  TORCH_FN(wrapper_dimname_kthvalue_dimname));
  m.impl("kthvalue.dimname_out",
  TORCH_FN(wrapper_dimname_out_kthvalue_out_dimname_out));
  m.impl("layer_norm",
  TORCH_FN(wrapper__layer_norm));
  m.impl("native_layer_norm",
  TORCH_FN(wrapper__native_layer_norm));
  m.impl("linear",
  TORCH_FN(wrapper__linear));
  m.impl("linear.out",
  TORCH_FN(wrapper_out_linear_out_out));
  m.impl("fbgemm_linear_int8_weight_fp32_activation",
  TORCH_FN(wrapper__fbgemm_linear_int8_weight_fp32_activation));
  m.impl("fbgemm_linear_int8_weight",
  TORCH_FN(wrapper__fbgemm_linear_int8_weight));
  m.impl("fbgemm_linear_quantize_weight",
  TORCH_FN(wrapper__fbgemm_linear_quantize_weight));
  m.impl("fbgemm_pack_gemm_matrix_fp16",
  TORCH_FN(wrapper__fbgemm_pack_gemm_matrix_fp16));
  m.impl("fbgemm_linear_fp16_weight_fp32_activation",
  TORCH_FN(wrapper__fbgemm_linear_fp16_weight_fp32_activation));
  m.impl("fbgemm_linear_fp16_weight",
  TORCH_FN(wrapper__fbgemm_linear_fp16_weight));
  m.impl("fbgemm_pack_quantized_matrix",
  TORCH_FN(wrapper__fbgemm_pack_quantized_matrix));
  m.impl("fbgemm_pack_quantized_matrix.KN",
  TORCH_FN(wrapper_KN_fbgemm_pack_quantized_matrix_KN));
  m.impl("ldexp.Tensor",
  TORCH_FN(wrapper_Tensor_ldexp_Tensor));
  m.impl("ldexp.out",
  TORCH_FN(wrapper_out_ldexp_out_out));
  m.impl("ldexp_",
  TORCH_FN(wrapper__ldexp_));
  m.impl("linspace",
  TORCH_FN(wrapper__linspace));
  m.impl("logspace",
  TORCH_FN(wrapper__logspace));
  m.impl("log_softmax.int",
  TORCH_FN(wrapper_int_log_softmax_int));
  m.impl("log_softmax.Dimname",
  TORCH_FN(wrapper_Dimname_log_softmax_Dimname));
  m.impl("logcumsumexp.dimname",
  TORCH_FN(wrapper_dimname_logcumsumexp_dimname));
  m.impl("logcumsumexp.dimname_out",
  TORCH_FN(wrapper_dimname_out_logcumsumexp_out_dimname_out));
  m.impl("logsumexp.names",
  TORCH_FN(wrapper_names_logsumexp_names));
  m.impl("logsumexp.names_out",
  TORCH_FN(wrapper_names_out_logsumexp_out_names_out));
  m.impl("margin_ranking_loss",
  TORCH_FN(wrapper__margin_ranking_loss));
  m.impl("matmul",
  TORCH_FN(wrapper__matmul));
  m.impl("matmul.out",
  TORCH_FN(wrapper_out_matmul_out_out));
  m.impl("matrix_rank.tol",
  TORCH_FN(wrapper_tol_matrix_rank_tol));
  m.impl("matrix_rank",
  TORCH_FN(wrapper__matrix_rank));
  m.impl("matrix_power",
  TORCH_FN(wrapper__matrix_power));
  m.impl("matrix_power.out",
  TORCH_FN(wrapper_out_matrix_power_out_out));
  m.impl("matrix_exp_backward",
  TORCH_FN(wrapper__matrix_exp_backward));
  m.impl("max.names_dim",
  TORCH_FN(wrapper_names_dim_max_names_dim));
  m.impl("max.names_dim_max",
  TORCH_FN(wrapper_names_dim_max_max_out_names_dim_max));
  m.impl("value_selecting_reduction_backward",
  TORCH_FN(wrapper__value_selecting_reduction_backward));
  m.impl("max_pool1d_with_indices",
  TORCH_FN(wrapper__max_pool1d_with_indices));
  m.impl("max_pool1d",
  TORCH_FN(wrapper__max_pool1d));
  m.impl("max_pool2d",
  TORCH_FN(wrapper__max_pool2d));
  m.impl("max_pool3d",
  TORCH_FN(wrapper__max_pool3d));
  m.impl("mean.names_dim",
  TORCH_FN(wrapper_names_dim_mean_names_dim));
  m.impl("mean.names_out",
  TORCH_FN(wrapper_names_out_mean_out_names_out));
  m.impl("nanmean",
  TORCH_FN(wrapper__nanmean));
  m.impl("nanmean.out",
  TORCH_FN(wrapper_out_nanmean_out_out));
  m.impl("median.names_dim",
  TORCH_FN(wrapper_names_dim_median_names_dim));
  m.impl("median.names_dim_values",
  TORCH_FN(wrapper_names_dim_values_median_out_names_dim_values));
  m.impl("nanmedian.names_dim",
  TORCH_FN(wrapper_names_dim_nanmedian_names_dim));
  m.impl("nanmedian.names_dim_values",
  TORCH_FN(wrapper_names_dim_values_nanmedian_out_names_dim_values));
  m.impl("min.names_dim",
  TORCH_FN(wrapper_names_dim_min_names_dim));
  m.impl("min.names_dim_min",
  TORCH_FN(wrapper_names_dim_min_min_out_names_dim_min));
  m.impl("mkldnn_convolution_backward_input",
  TORCH_FN(wrapper__mkldnn_convolution_backward_input));
  m.impl("mkldnn_convolution_backward_weights",
  TORCH_FN(wrapper__mkldnn_convolution_backward_weights));
  m.impl("_sparse_mm",
  TORCH_FN(wrapper___sparse_mm));
  m.impl("mode.dimname",
  TORCH_FN(wrapper_dimname_mode_dimname));
  m.impl("mode.dimname_out",
  TORCH_FN(wrapper_dimname_out_mode_out_dimname_out));
  m.impl("multiply.Tensor",
  TORCH_FN(wrapper_Tensor_multiply_Tensor));
  m.impl("multiply.out",
  TORCH_FN(wrapper_out_multiply_out_out));
  m.impl("multiply_.Tensor",
  TORCH_FN(wrapper_Tensor_multiply__Tensor));
  m.impl("multiply.Scalar",
  TORCH_FN(wrapper_Scalar_multiply_Scalar));
  m.impl("multiply_.Scalar",
  TORCH_FN(wrapper_Scalar_multiply__Scalar));
  m.impl("narrow",
  TORCH_FN(wrapper__narrow));
  m.impl("narrow.Tensor",
  TORCH_FN(wrapper_Tensor_narrow_Tensor));
  m.impl("is_vulkan_available",
  TORCH_FN(wrapper__is_vulkan_available));
  m.impl("_nnpack_available",
  TORCH_FN(wrapper___nnpack_available));
  m.impl("_nnpack_spatial_convolution_backward",
  TORCH_FN(wrapper___nnpack_spatial_convolution_backward));
  m.impl("_nnpack_spatial_convolution_backward_input",
  TORCH_FN(wrapper___nnpack_spatial_convolution_backward_input));
  m.impl("_nnpack_spatial_convolution_backward_weight",
  TORCH_FN(wrapper___nnpack_spatial_convolution_backward_weight));
  m.impl("ones.names",
  TORCH_FN(wrapper_names_ones_names));
  m.impl("ones",
  TORCH_FN(wrapper__ones));
  m.impl("ones.out",
  TORCH_FN(wrapper_out_ones_out_out));
  m.impl("ones_like",
  TORCH_FN(wrapper__ones_like));
  m.impl("pairwise_distance",
  TORCH_FN(wrapper__pairwise_distance));
  m.impl("cdist",
  TORCH_FN(wrapper__cdist));
  m.impl("pdist",
  TORCH_FN(wrapper__pdist));
  m.impl("cosine_similarity",
  TORCH_FN(wrapper__cosine_similarity));
  m.impl("movedim.intlist",
  TORCH_FN(wrapper_intlist_movedim_intlist));
  m.impl("movedim.int",
  TORCH_FN(wrapper_int_movedim_int));
  m.impl("moveaxis.intlist",
  TORCH_FN(wrapper_intlist_moveaxis_intlist));
  m.impl("moveaxis.int",
  TORCH_FN(wrapper_int_moveaxis_int));
  m.impl("numpy_T",
  TORCH_FN(wrapper__numpy_T));
  m.impl("pixel_shuffle",
  TORCH_FN(wrapper__pixel_shuffle));
  m.impl("pixel_unshuffle",
  TORCH_FN(wrapper__pixel_unshuffle));
  m.impl("pin_memory",
  TORCH_FN(wrapper__pin_memory));
  m.impl("pinverse",
  TORCH_FN(wrapper__pinverse));
  m.impl("poisson_nll_loss",
  TORCH_FN(wrapper__poisson_nll_loss));
  m.impl("scalar_tensor",
  TORCH_FN(wrapper__scalar_tensor));
  m.impl("rand.names",
  TORCH_FN(wrapper_names_rand_names));
  m.impl("rand.generator_with_names",
  TORCH_FN(wrapper_generator_with_names_rand_generator_with_names));
  m.impl("rand",
  TORCH_FN(wrapper__rand));
  m.impl("rand.generator",
  TORCH_FN(wrapper_generator_rand_generator));
  m.impl("rand.out",
  TORCH_FN(wrapper_out_rand_out_out));
  m.impl("rand.generator_out",
  TORCH_FN(wrapper_generator_out_rand_out_generator_out));
  m.impl("rand_like",
  TORCH_FN(wrapper__rand_like));
  m.impl("randint",
  TORCH_FN(wrapper__randint));
  m.impl("randint.generator",
  TORCH_FN(wrapper_generator_randint_generator));
  m.impl("randint.low",
  TORCH_FN(wrapper_low_randint_low));
  m.impl("randint.low_generator",
  TORCH_FN(wrapper_low_generator_randint_low_generator));
  m.impl("randint.out",
  TORCH_FN(wrapper_out_randint_out_out));
  m.impl("randint.generator_out",
  TORCH_FN(wrapper_generator_out_randint_out_generator_out));
  m.impl("randint.low_out",
  TORCH_FN(wrapper_low_out_randint_out_low_out));
  m.impl("randint.low_generator_out",
  TORCH_FN(wrapper_low_generator_out_randint_out_low_generator_out));
  m.impl("randint_like",
  TORCH_FN(wrapper__randint_like));
  m.impl("randint_like.low_dtype",
  TORCH_FN(wrapper_low_dtype_randint_like_low_dtype));
  m.impl("randn",
  TORCH_FN(wrapper__randn));
  m.impl("randn.generator",
  TORCH_FN(wrapper_generator_randn_generator));
  m.impl("randn.names",
  TORCH_FN(wrapper_names_randn_names));
  m.impl("randn.generator_with_names",
  TORCH_FN(wrapper_generator_with_names_randn_generator_with_names));
  m.impl("randn.out",
  TORCH_FN(wrapper_out_randn_out_out));
  m.impl("randn.generator_out",
  TORCH_FN(wrapper_generator_out_randn_out_generator_out));
  m.impl("randn_like",
  TORCH_FN(wrapper__randn_like));
  m.impl("randperm",
  TORCH_FN(wrapper__randperm));
  m.impl("randperm.generator",
  TORCH_FN(wrapper_generator_randperm_generator));
  m.impl("randperm.out",
  TORCH_FN(wrapper_out_randperm_out_out));
  m.impl("range.step",
  TORCH_FN(wrapper_step_range_step));
  m.impl("range",
  TORCH_FN(wrapper__range));
  m.impl("ravel",
  TORCH_FN(wrapper__ravel));
  m.impl("negative",
  TORCH_FN(wrapper__negative));
  m.impl("negative.out",
  TORCH_FN(wrapper_out_negative_out_out));
  m.impl("negative_",
  TORCH_FN(wrapper__negative_));
  m.impl("repeat_interleave.self_Tensor",
  TORCH_FN(wrapper_self_Tensor_repeat_interleave_self_Tensor));
  m.impl("repeat_interleave.self_int",
  TORCH_FN(wrapper_self_int_repeat_interleave_self_int));
  m.impl("reshape",
  TORCH_FN(wrapper__reshape));
  m.impl("reshape_as",
  TORCH_FN(wrapper__reshape_as));
  m.impl("rrelu",
  TORCH_FN(wrapper__rrelu));
  m.impl("rrelu_",
  TORCH_FN(wrapper__rrelu_));
  m.impl("relu6",
  TORCH_FN(wrapper__relu6));
  m.impl("relu6_",
  TORCH_FN(wrapper__relu6_));
  m.impl("infinitely_differentiable_gelu_backward",
  TORCH_FN(wrapper__infinitely_differentiable_gelu_backward));
  m.impl("select.Dimname",
  TORCH_FN(wrapper_Dimname_select_Dimname));
  m.impl("selu",
  TORCH_FN(wrapper__selu));
  m.impl("selu_",
  TORCH_FN(wrapper__selu_));
  m.impl("silu_backward",
  TORCH_FN(wrapper__silu_backward));
  m.impl("mish_backward",
  TORCH_FN(wrapper__mish_backward));
  m.impl("size.int",
  TORCH_FN(wrapper_int_size_int));
  m.impl("size.Dimname",
  TORCH_FN(wrapper_Dimname_size_Dimname));
  m.impl("smm",
  TORCH_FN(wrapper__smm));
  m.impl("softmax.int",
  TORCH_FN(wrapper_int_softmax_int));
  m.impl("softmax.Dimname",
  TORCH_FN(wrapper_Dimname_softmax_Dimname));
  m.impl("hsplit.int",
  TORCH_FN(wrapper_int_hsplit_int));
  m.impl("hsplit.array",
  TORCH_FN(wrapper_array_hsplit_array));
  m.impl("vsplit.int",
  TORCH_FN(wrapper_int_vsplit_int));
  m.impl("vsplit.array",
  TORCH_FN(wrapper_array_vsplit_array));
  m.impl("dsplit.int",
  TORCH_FN(wrapper_int_dsplit_int));
  m.impl("dsplit.array",
  TORCH_FN(wrapper_array_dsplit_array));
  m.impl("squeeze.dimname",
  TORCH_FN(wrapper_dimname_squeeze_dimname));
  m.impl("squeeze_.dimname",
  TORCH_FN(wrapper_dimname_squeeze__dimname));
  m.impl("sspaddmm",
  TORCH_FN(wrapper__sspaddmm));
  m.impl("hstack",
  TORCH_FN(wrapper__hstack));
  m.impl("hstack.out",
  TORCH_FN(wrapper_out_hstack_out_out));
  m.impl("vstack",
  TORCH_FN(wrapper__vstack));
  m.impl("vstack.out",
  TORCH_FN(wrapper_out_vstack_out_out));
  m.impl("dstack",
  TORCH_FN(wrapper__dstack));
  m.impl("dstack.out",
  TORCH_FN(wrapper_out_dstack_out_out));
  m.impl("stft",
  TORCH_FN(wrapper__stft));
  m.impl("istft",
  TORCH_FN(wrapper__istft));
  m.impl("stride.int",
  TORCH_FN(wrapper_int_stride_int));
  m.impl("stride.Dimname",
  TORCH_FN(wrapper_Dimname_stride_Dimname));
  m.impl("sum.dim_DimnameList",
  TORCH_FN(wrapper_dim_DimnameList_sum_dim_DimnameList));
  m.impl("sum.DimnameList_out",
  TORCH_FN(wrapper_DimnameList_out_sum_out_DimnameList_out));
  m.impl("sum_to_size",
  TORCH_FN(wrapper__sum_to_size));
  m.impl("square",
  TORCH_FN(wrapper__square));
  m.impl("square_",
  TORCH_FN(wrapper__square_));
  m.impl("std",
  TORCH_FN(wrapper__std));
  m.impl("std.dim",
  TORCH_FN(wrapper_dim_std_dim));
  m.impl("std.out",
  TORCH_FN(wrapper_out_std_out_out));
  m.impl("std_mean",
  TORCH_FN(wrapper__std_mean));
  m.impl("std_mean.dim",
  TORCH_FN(wrapper_dim_std_mean_dim));
  m.impl("std_mean.names_dim",
  TORCH_FN(wrapper_names_dim_std_mean_names_dim));
  m.impl("std_mean.correction_names",
  TORCH_FN(wrapper_correction_names_std_mean_correction_names));
  m.impl("std.names_dim",
  TORCH_FN(wrapper_names_dim_std_names_dim));
  m.impl("std.names_out",
  TORCH_FN(wrapper_names_out_std_out_names_out));
  m.impl("std.correction_names",
  TORCH_FN(wrapper_correction_names_std_correction_names));
  m.impl("std.correction_names_out",
  TORCH_FN(wrapper_correction_names_out_std_out_correction_names_out));
  m.impl("prod.dim_Dimname",
  TORCH_FN(wrapper_dim_Dimname_prod_dim_Dimname));
  m.impl("prod.Dimname_out",
  TORCH_FN(wrapper_Dimname_out_prod_out_Dimname_out));
  m.impl("tensordot",
  TORCH_FN(wrapper__tensordot));
  m.impl("tile",
  TORCH_FN(wrapper__tile));
  m.impl("transpose.Dimname",
  TORCH_FN(wrapper_Dimname_transpose_Dimname));
  m.impl("one_hot",
  TORCH_FN(wrapper__one_hot));
  m.impl("fliplr",
  TORCH_FN(wrapper__fliplr));
  m.impl("flipud",
  TORCH_FN(wrapper__flipud));
  m.impl("trapezoid.x",
  TORCH_FN(wrapper_x_trapezoid_x));
  m.impl("trapezoid.dx",
  TORCH_FN(wrapper_dx_trapezoid_dx));
  m.impl("trapz.x",
  TORCH_FN(wrapper_x_trapz_x));
  m.impl("trapz.dx",
  TORCH_FN(wrapper_dx_trapz_dx));
  m.impl("triplet_margin_loss",
  TORCH_FN(wrapper__triplet_margin_loss));
  m.impl("fix",
  TORCH_FN(wrapper__fix));
  m.impl("fix.out",
  TORCH_FN(wrapper_out_fix_out_out));
  m.impl("fix_",
  TORCH_FN(wrapper__fix_));
  m.impl("type_as",
  TORCH_FN(wrapper__type_as));
  m.impl("_has_compatible_shallow_copy_type",
  TORCH_FN(wrapper___has_compatible_shallow_copy_type));
  m.impl("vander",
  TORCH_FN(wrapper__vander));
  m.impl("var",
  TORCH_FN(wrapper__var));
  m.impl("var.dim",
  TORCH_FN(wrapper_dim_var_dim));
  m.impl("var.out",
  TORCH_FN(wrapper_out_var_out_out));
  m.impl("var.names_dim",
  TORCH_FN(wrapper_names_dim_var_names_dim));
  m.impl("var.names_out",
  TORCH_FN(wrapper_names_out_var_out_names_out));
  m.impl("var.correction_names",
  TORCH_FN(wrapper_correction_names_var_correction_names));
  m.impl("var.correction_names_out",
  TORCH_FN(wrapper_correction_names_out_var_out_correction_names_out));
  m.impl("var_mean",
  TORCH_FN(wrapper__var_mean));
  m.impl("var_mean.dim",
  TORCH_FN(wrapper_dim_var_mean_dim));
  m.impl("var_mean.names_dim",
  TORCH_FN(wrapper_names_dim_var_mean_names_dim));
  m.impl("var_mean.correction_names",
  TORCH_FN(wrapper_correction_names_var_mean_correction_names));
  m.impl("view_as",
  TORCH_FN(wrapper__view_as));
  m.impl("where.self",
  TORCH_FN(wrapper_self_where_self));
  m.impl("where.ScalarSelf",
  TORCH_FN(wrapper_ScalarSelf_where_ScalarSelf));
  m.impl("where.ScalarOther",
  TORCH_FN(wrapper_ScalarOther_where_ScalarOther));
  m.impl("where.Scalar",
  TORCH_FN(wrapper_Scalar_where_Scalar));
  m.impl("where",
  TORCH_FN(wrapper__where));
  m.impl("norm_except_dim",
  TORCH_FN(wrapper__norm_except_dim));
  m.impl("_weight_norm",
  TORCH_FN(wrapper___weight_norm));
  m.impl("_weight_norm_differentiable_backward",
  TORCH_FN(wrapper___weight_norm_differentiable_backward));
  m.impl("zeros.names",
  TORCH_FN(wrapper_names_zeros_names));
  m.impl("zeros",
  TORCH_FN(wrapper__zeros));
  m.impl("zeros.out",
  TORCH_FN(wrapper_out_zeros_out_out));
  m.impl("zeros_like",
  TORCH_FN(wrapper__zeros_like));
  m.impl("_sparse_sum",
  TORCH_FN(wrapper___sparse_sum));
  m.impl("_sparse_sum.dtype",
  TORCH_FN(wrapper_dtype__sparse_sum_dtype));
  m.impl("_sparse_sum.dim_dtype",
  TORCH_FN(wrapper_dim_dtype__sparse_sum_dim_dtype));
  m.impl("_sparse_softmax.int",
  TORCH_FN(wrapper_int__sparse_softmax_int));
  m.impl("_sparse_softmax.Dimname",
  TORCH_FN(wrapper_Dimname__sparse_softmax_Dimname));
  m.impl("_sparse_log_softmax.int",
  TORCH_FN(wrapper_int__sparse_log_softmax_int));
  m.impl("_sparse_log_softmax.Dimname",
  TORCH_FN(wrapper_Dimname__sparse_log_softmax_Dimname));
  m.impl("norm.names_ScalarOpt_dim_dtype",
  TORCH_FN(wrapper_names_ScalarOpt_dim_dtype_norm_names_ScalarOpt_dim_dtype));
  m.impl("norm.names_dtype_out",
  TORCH_FN(wrapper_names_dtype_out_norm_out_names_dtype_out));
  m.impl("norm.names_ScalarOpt_dim",
  TORCH_FN(wrapper_names_ScalarOpt_dim_norm_names_ScalarOpt_dim));
  m.impl("norm.names_out",
  TORCH_FN(wrapper_names_out_norm_out_names_out));
  m.impl("frobenius_norm",
  TORCH_FN(wrapper__frobenius_norm));
  m.impl("frobenius_norm.dim",
  TORCH_FN(wrapper_dim_frobenius_norm_dim));
  m.impl("frobenius_norm.out",
  TORCH_FN(wrapper_out_frobenius_norm_out_out));
  m.impl("nuclear_norm",
  TORCH_FN(wrapper__nuclear_norm));
  m.impl("nuclear_norm.out",
  TORCH_FN(wrapper_out_nuclear_norm_out_out));
  m.impl("nuclear_norm.dim",
  TORCH_FN(wrapper_dim_nuclear_norm_dim));
  m.impl("nuclear_norm.dim_out",
  TORCH_FN(wrapper_dim_out_nuclear_norm_out_dim_out));
  m.impl("positive",
  TORCH_FN(wrapper__positive));
  m.impl("subtract.Tensor",
  TORCH_FN(wrapper_Tensor_subtract_Tensor));
  m.impl("subtract.out",
  TORCH_FN(wrapper_out_subtract_out_out));
  m.impl("subtract_.Tensor",
  TORCH_FN(wrapper_Tensor_subtract__Tensor));
  m.impl("subtract.Scalar",
  TORCH_FN(wrapper_Scalar_subtract_Scalar));
  m.impl("subtract_.Scalar",
  TORCH_FN(wrapper_Scalar_subtract__Scalar));
  m.impl("sparse_csr_tensor.crow_col_value_size",
  TORCH_FN(wrapper_crow_col_value_size_sparse_csr_tensor_crow_col_value_size));
  m.impl("sparse_csr_tensor.crow_col_value",
  TORCH_FN(wrapper_crow_col_value_sparse_csr_tensor_crow_col_value));
  m.impl("_sparse_csr_tensor_unsafe",
  TORCH_FN(wrapper___sparse_csr_tensor_unsafe));
  m.impl("sparse_coo_tensor.size",
  TORCH_FN(wrapper_size_sparse_coo_tensor_size));
  m.impl("sparse_coo_tensor.indices",
  TORCH_FN(wrapper_indices_sparse_coo_tensor_indices));
  m.impl("sparse_coo_tensor.indices_size",
  TORCH_FN(wrapper_indices_size_sparse_coo_tensor_indices_size));
  m.impl("_sparse_coo_tensor_unsafe",
  TORCH_FN(wrapper___sparse_coo_tensor_unsafe));
  m.impl("_validate_sparse_coo_tensor_args",
  TORCH_FN(wrapper___validate_sparse_coo_tensor_args));
  m.impl("_validate_sparse_csr_tensor_args",
  TORCH_FN(wrapper___validate_sparse_csr_tensor_args));
  m.impl("_to_cpu",
  TORCH_FN(wrapper___to_cpu));
  m.impl("to_dense_backward",
  TORCH_FN(wrapper__to_dense_backward));
  m.impl("coalesce",
  TORCH_FN(wrapper__coalesce));
  m.impl("unbind.Dimname",
  TORCH_FN(wrapper_Dimname_unbind_Dimname));
  m.impl("to_mkldnn_backward",
  TORCH_FN(wrapper__to_mkldnn_backward));
  m.impl("fake_quantize_per_tensor_affine",
  TORCH_FN(wrapper__fake_quantize_per_tensor_affine));
  m.impl("fake_quantize_per_tensor_affine.tensor_qparams",
  TORCH_FN(wrapper_tensor_qparams_fake_quantize_per_tensor_affine_tensor_qparams));
  m.impl("fake_quantize_per_tensor_affine_cachemask_backward",
  TORCH_FN(wrapper__fake_quantize_per_tensor_affine_cachemask_backward));
  m.impl("_fake_quantize_learnable_per_tensor_affine_backward",
  TORCH_FN(wrapper___fake_quantize_learnable_per_tensor_affine_backward));
  m.impl("fake_quantize_per_channel_affine",
  TORCH_FN(wrapper__fake_quantize_per_channel_affine));
  m.impl("fake_quantize_per_channel_affine_cachemask_backward",
  TORCH_FN(wrapper__fake_quantize_per_channel_affine_cachemask_backward));
  m.impl("_fake_quantize_learnable_per_channel_affine_backward",
  TORCH_FN(wrapper___fake_quantize_learnable_per_channel_affine_backward));
  m.impl("fused_moving_avg_obs_fake_quant",
  TORCH_FN(wrapper__fused_moving_avg_obs_fake_quant));
  m.impl("_choose_qparams_per_tensor",
  TORCH_FN(wrapper___choose_qparams_per_tensor));
  m.impl("_saturate_weight_to_fp16",
  TORCH_FN(wrapper___saturate_weight_to_fp16));
  m.impl("choose_qparams_optimized",
  TORCH_FN(wrapper__choose_qparams_optimized));
  m.impl("to.dtype_layout",
  TORCH_FN(wrapper_dtype_layout_to_dtype_layout));
  m.impl("to.device",
  TORCH_FN(wrapper_device_to_device));
  m.impl("to.dtype",
  TORCH_FN(wrapper_dtype_to_dtype));
  m.impl("to.other",
  TORCH_FN(wrapper_other_to_other));
  m.impl("meshgrid",
  TORCH_FN(wrapper__meshgrid));
  m.impl("meshgrid.indexing",
  TORCH_FN(wrapper_indexing_meshgrid_indexing));
  m.impl("cartesian_prod",
  TORCH_FN(wrapper__cartesian_prod));
  m.impl("combinations",
  TORCH_FN(wrapper__combinations));
  m.impl("item",
  TORCH_FN(wrapper__item));
  m.impl("result_type.Tensor",
  TORCH_FN(wrapper_Tensor_result_type_Tensor));
  m.impl("result_type.Scalar",
  TORCH_FN(wrapper_Scalar_result_type_Scalar));
  m.impl("result_type.Scalar_Tensor",
  TORCH_FN(wrapper_Scalar_Tensor_result_type_Scalar_Tensor));
  m.impl("result_type.Scalar_Scalar",
  TORCH_FN(wrapper_Scalar_Scalar_result_type_Scalar_Scalar));
  m.impl("can_cast",
  TORCH_FN(wrapper__can_cast));
  m.impl("promote_types",
  TORCH_FN(wrapper__promote_types));
  m.impl("_thnn_differentiable_lstm_cell_backward",
  TORCH_FN(wrapper___thnn_differentiable_lstm_cell_backward));
  m.impl("_thnn_differentiable_gru_cell_backward",
  TORCH_FN(wrapper___thnn_differentiable_gru_cell_backward));
  m.impl("lstm.input",
  TORCH_FN(wrapper_input_lstm_input));
  m.impl("lstm.data",
  TORCH_FN(wrapper_data_lstm_data));
  m.impl("gru.input",
  TORCH_FN(wrapper_input_gru_input));
  m.impl("gru.data",
  TORCH_FN(wrapper_data_gru_data));
  m.impl("rnn_tanh.input",
  TORCH_FN(wrapper_input_rnn_tanh_input));
  m.impl("rnn_tanh.data",
  TORCH_FN(wrapper_data_rnn_tanh_data));
  m.impl("rnn_relu.input",
  TORCH_FN(wrapper_input_rnn_relu_input));
  m.impl("rnn_relu.data",
  TORCH_FN(wrapper_data_rnn_relu_data));
  m.impl("lstm_cell",
  TORCH_FN(wrapper__lstm_cell));
  m.impl("gru_cell",
  TORCH_FN(wrapper__gru_cell));
  m.impl("rnn_tanh_cell",
  TORCH_FN(wrapper__rnn_tanh_cell));
  m.impl("rnn_relu_cell",
  TORCH_FN(wrapper__rnn_relu_cell));
  m.impl("quantized_lstm_cell",
  TORCH_FN(wrapper__quantized_lstm_cell));
  m.impl("quantized_gru_cell",
  TORCH_FN(wrapper__quantized_gru_cell));
  m.impl("quantized_rnn_relu_cell",
  TORCH_FN(wrapper__quantized_rnn_relu_cell));
  m.impl("quantized_rnn_tanh_cell",
  TORCH_FN(wrapper__quantized_rnn_tanh_cell));
  m.impl("_pack_padded_sequence_backward",
  TORCH_FN(wrapper___pack_padded_sequence_backward));
  m.impl("_pad_packed_sequence",
  TORCH_FN(wrapper___pad_packed_sequence));
  m.impl("masked_fill.Scalar",
  TORCH_FN(wrapper_Scalar_masked_fill_Scalar));
  m.impl("masked_fill.Tensor",
  TORCH_FN(wrapper_Tensor_masked_fill_Tensor));
  m.impl("masked_scatter",
  TORCH_FN(wrapper__masked_scatter));
  m.impl("put",
  TORCH_FN(wrapper__put));
  m.impl("index_add_",
  TORCH_FN(wrapper__index_add_));
  m.impl("index_add",
  TORCH_FN(wrapper__index_add));
  m.impl("index_add.alpha",
  TORCH_FN(wrapper_alpha_index_add_alpha));
  m.impl("index_add.dimname",
  TORCH_FN(wrapper_dimname_index_add_dimname));
  m.impl("index_fill.int_Scalar",
  TORCH_FN(wrapper_int_Scalar_index_fill_int_Scalar));
  m.impl("index_fill.int_Tensor",
  TORCH_FN(wrapper_int_Tensor_index_fill_int_Tensor));
  m.impl("index_fill_.Dimname_Scalar",
  TORCH_FN(wrapper_Dimname_Scalar_index_fill__Dimname_Scalar));
  m.impl("index_fill.Dimname_Scalar",
  TORCH_FN(wrapper_Dimname_Scalar_index_fill_Dimname_Scalar));
  m.impl("index_fill_.Dimname_Tensor",
  TORCH_FN(wrapper_Dimname_Tensor_index_fill__Dimname_Tensor));
  m.impl("index_fill.Dimname_Tensor",
  TORCH_FN(wrapper_Dimname_Tensor_index_fill_Dimname_Tensor));
  m.impl("scatter.dimname_src",
  TORCH_FN(wrapper_dimname_src_scatter_dimname_src));
  m.impl("scatter.dimname_value",
  TORCH_FN(wrapper_dimname_value_scatter_dimname_value));
  m.impl("scatter_add.dimname",
  TORCH_FN(wrapper_dimname_scatter_add_dimname));
  m.impl("bitwise_and_.Scalar",
  TORCH_FN(wrapper_Scalar_bitwise_and__Scalar));
  m.impl("__and__.Scalar",
  TORCH_FN(wrapper_Scalar___and___Scalar));
  m.impl("__iand__.Scalar",
  TORCH_FN(wrapper_Scalar___iand___Scalar));
  m.impl("__and__.Tensor",
  TORCH_FN(wrapper_Tensor___and___Tensor));
  m.impl("__iand__.Tensor",
  TORCH_FN(wrapper_Tensor___iand___Tensor));
  m.impl("bitwise_or.Scalar",
  TORCH_FN(wrapper_Scalar_bitwise_or_Scalar));
  m.impl("bitwise_or_.Scalar",
  TORCH_FN(wrapper_Scalar_bitwise_or__Scalar));
  m.impl("__or__.Scalar",
  TORCH_FN(wrapper_Scalar___or___Scalar));
  m.impl("__ior__.Scalar",
  TORCH_FN(wrapper_Scalar___ior___Scalar));
  m.impl("__or__.Tensor",
  TORCH_FN(wrapper_Tensor___or___Tensor));
  m.impl("__ior__.Tensor",
  TORCH_FN(wrapper_Tensor___ior___Tensor));
  m.impl("bitwise_xor.Scalar",
  TORCH_FN(wrapper_Scalar_bitwise_xor_Scalar));
  m.impl("bitwise_xor_.Scalar",
  TORCH_FN(wrapper_Scalar_bitwise_xor__Scalar));
  m.impl("__xor__.Scalar",
  TORCH_FN(wrapper_Scalar___xor___Scalar));
  m.impl("__ixor__.Scalar",
  TORCH_FN(wrapper_Scalar___ixor___Scalar));
  m.impl("__xor__.Tensor",
  TORCH_FN(wrapper_Tensor___xor___Tensor));
  m.impl("__ixor__.Tensor",
  TORCH_FN(wrapper_Tensor___ixor___Tensor));
  m.impl("diag_backward",
  TORCH_FN(wrapper__diag_backward));
  m.impl("trace_backward",
  TORCH_FN(wrapper__trace_backward));
  m.impl("not_equal.Scalar",
  TORCH_FN(wrapper_Scalar_not_equal_Scalar));
  m.impl("not_equal.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_not_equal_out_Scalar_out));
  m.impl("not_equal_.Scalar",
  TORCH_FN(wrapper_Scalar_not_equal__Scalar));
  m.impl("not_equal.Tensor",
  TORCH_FN(wrapper_Tensor_not_equal_Tensor));
  m.impl("not_equal.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_not_equal_out_Tensor_out));
  m.impl("not_equal_.Tensor",
  TORCH_FN(wrapper_Tensor_not_equal__Tensor));
  m.impl("greater_equal.Scalar",
  TORCH_FN(wrapper_Scalar_greater_equal_Scalar));
  m.impl("greater_equal.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_greater_equal_out_Scalar_out));
  m.impl("greater_equal_.Scalar",
  TORCH_FN(wrapper_Scalar_greater_equal__Scalar));
  m.impl("greater_equal.Tensor",
  TORCH_FN(wrapper_Tensor_greater_equal_Tensor));
  m.impl("greater_equal.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_greater_equal_out_Tensor_out));
  m.impl("greater_equal_.Tensor",
  TORCH_FN(wrapper_Tensor_greater_equal__Tensor));
  m.impl("less_equal.Scalar",
  TORCH_FN(wrapper_Scalar_less_equal_Scalar));
  m.impl("less_equal.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_less_equal_out_Scalar_out));
  m.impl("less_equal_.Scalar",
  TORCH_FN(wrapper_Scalar_less_equal__Scalar));
  m.impl("less_equal.Tensor",
  TORCH_FN(wrapper_Tensor_less_equal_Tensor));
  m.impl("less_equal.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_less_equal_out_Tensor_out));
  m.impl("less_equal_.Tensor",
  TORCH_FN(wrapper_Tensor_less_equal__Tensor));
  m.impl("greater.Scalar",
  TORCH_FN(wrapper_Scalar_greater_Scalar));
  m.impl("greater.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_greater_out_Scalar_out));
  m.impl("greater_.Scalar",
  TORCH_FN(wrapper_Scalar_greater__Scalar));
  m.impl("greater.Tensor",
  TORCH_FN(wrapper_Tensor_greater_Tensor));
  m.impl("greater.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_greater_out_Tensor_out));
  m.impl("greater_.Tensor",
  TORCH_FN(wrapper_Tensor_greater__Tensor));
  m.impl("less.Scalar",
  TORCH_FN(wrapper_Scalar_less_Scalar));
  m.impl("less.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_less_out_Scalar_out));
  m.impl("less_.Scalar",
  TORCH_FN(wrapper_Scalar_less__Scalar));
  m.impl("less.Tensor",
  TORCH_FN(wrapper_Tensor_less_Tensor));
  m.impl("less.Tensor_out",
  TORCH_FN(wrapper_Tensor_out_less_out_Tensor_out));
  m.impl("less_.Tensor",
  TORCH_FN(wrapper_Tensor_less__Tensor));
  m.impl("take_along_dim",
  TORCH_FN(wrapper__take_along_dim));
  m.impl("take_along_dim.out",
  TORCH_FN(wrapper_out_take_along_dim_out_out));
  m.impl("index_select.dimname",
  TORCH_FN(wrapper_dimname_index_select_dimname));
  m.impl("index_select.dimname_out",
  TORCH_FN(wrapper_dimname_out_index_select_out_dimname_out));
  m.impl("index_select_backward",
  TORCH_FN(wrapper__index_select_backward));
  m.impl("masked_select_backward",
  TORCH_FN(wrapper__masked_select_backward));
  m.impl("nonzero_numpy",
  TORCH_FN(wrapper__nonzero_numpy));
  m.impl("gather_backward",
  TORCH_FN(wrapper__gather_backward));
  m.impl("gather.dimname",
  TORCH_FN(wrapper_dimname_gather_dimname));
  m.impl("gather.dimname_out",
  TORCH_FN(wrapper_dimname_out_gather_out_dimname_out));
  m.impl("_gather_sparse_backward",
  TORCH_FN(wrapper___gather_sparse_backward));
  m.impl("cross_entropy_loss",
  TORCH_FN(wrapper__cross_entropy_loss));
  m.impl("svd",
  TORCH_FN(wrapper__svd));
  m.impl("svd.U",
  TORCH_FN(wrapper_U_svd_out_U));
  m.impl("swapaxes",
  TORCH_FN(wrapper__swapaxes));
  m.impl("swapaxes_",
  TORCH_FN(wrapper__swapaxes_));
  m.impl("swapdims",
  TORCH_FN(wrapper__swapdims));
  m.impl("swapdims_",
  TORCH_FN(wrapper__swapdims_));
  m.impl("qr",
  TORCH_FN(wrapper__qr));
  m.impl("qr.Q",
  TORCH_FN(wrapper_Q_qr_out_Q));
  m.impl("orgqr",
  TORCH_FN(wrapper__orgqr));
  m.impl("orgqr.out",
  TORCH_FN(wrapper_out_orgqr_out_out));
  m.impl("max.other",
  TORCH_FN(wrapper_other_max_other));
  m.impl("max.out",
  TORCH_FN(wrapper_out_max_out_out));
  m.impl("min.other",
  TORCH_FN(wrapper_other_min_other));
  m.impl("min.out",
  TORCH_FN(wrapper_out_min_out_out));
  m.impl("quantile.scalar",
  TORCH_FN(wrapper_scalar_quantile_scalar));
  m.impl("quantile.scalar_out",
  TORCH_FN(wrapper_scalar_out_quantile_out_scalar_out));
  m.impl("quantile",
  TORCH_FN(wrapper__quantile));
  m.impl("quantile.out",
  TORCH_FN(wrapper_out_quantile_out_out));
  m.impl("nanquantile.scalar",
  TORCH_FN(wrapper_scalar_nanquantile_scalar));
  m.impl("nanquantile.scalar_out",
  TORCH_FN(wrapper_scalar_out_nanquantile_out_scalar_out));
  m.impl("nanquantile",
  TORCH_FN(wrapper__nanquantile));
  m.impl("nanquantile.out",
  TORCH_FN(wrapper_out_nanquantile_out_out));
  m.impl("quantile.new_scalar",
  TORCH_FN(wrapper_new_scalar_quantile_new_scalar));
  m.impl("quantile.new_scalar_out",
  TORCH_FN(wrapper_new_scalar_out_quantile_out_new_scalar_out));
  m.impl("quantile.new",
  TORCH_FN(wrapper_new_quantile_new));
  m.impl("quantile.new_out",
  TORCH_FN(wrapper_new_out_quantile_out_new_out));
  m.impl("nanquantile.new_scalar",
  TORCH_FN(wrapper_new_scalar_nanquantile_new_scalar));
  m.impl("nanquantile.new_scalar_out",
  TORCH_FN(wrapper_new_scalar_out_nanquantile_out_new_scalar_out));
  m.impl("nanquantile.new",
  TORCH_FN(wrapper_new_nanquantile_new));
  m.impl("nanquantile.new_out",
  TORCH_FN(wrapper_new_out_nanquantile_out_new_out));
  m.impl("sort.dimname",
  TORCH_FN(wrapper_dimname_sort_dimname));
  m.impl("sort.dimname_values",
  TORCH_FN(wrapper_dimname_values_sort_out_dimname_values));
  m.impl("sort.dimname_stable",
  TORCH_FN(wrapper_dimname_stable_sort_dimname_stable));
  m.impl("sort.dimname_values_stable",
  TORCH_FN(wrapper_dimname_values_stable_sort_out_dimname_values_stable));
  m.impl("msort",
  TORCH_FN(wrapper__msort));
  m.impl("msort.out",
  TORCH_FN(wrapper_out_msort_out_out));
  m.impl("argsort",
  TORCH_FN(wrapper__argsort));
  m.impl("argsort.dimname",
  TORCH_FN(wrapper_dimname_argsort_dimname));
  m.impl("float_power.Tensor_Tensor",
  TORCH_FN(wrapper_Tensor_Tensor_float_power_Tensor_Tensor));
  m.impl("float_power.Tensor_Tensor_out",
  TORCH_FN(wrapper_Tensor_Tensor_out_float_power_out_Tensor_Tensor_out));
  m.impl("float_power_.Tensor",
  TORCH_FN(wrapper_Tensor_float_power__Tensor));
  m.impl("float_power.Scalar",
  TORCH_FN(wrapper_Scalar_float_power_Scalar));
  m.impl("float_power.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_float_power_out_Scalar_out));
  m.impl("float_power.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_float_power_Tensor_Scalar));
  m.impl("float_power.Tensor_Scalar_out",
  TORCH_FN(wrapper_Tensor_Scalar_out_float_power_out_Tensor_Scalar_out));
  m.impl("float_power_.Scalar",
  TORCH_FN(wrapper_Scalar_float_power__Scalar));
  m.impl("normal.float_float",
  TORCH_FN(wrapper_float_float_normal_float_float));
  m.impl("normal.float_float_out",
  TORCH_FN(wrapper_float_float_out_normal_out_float_float_out));
  m.impl("multilabel_margin_loss",
  TORCH_FN(wrapper__multilabel_margin_loss));
  m.impl("multilabel_margin_loss.out",
  TORCH_FN(wrapper_out_multilabel_margin_loss_out_out));
  m.impl("nll_loss",
  TORCH_FN(wrapper__nll_loss));
  m.impl("nll_loss.out",
  TORCH_FN(wrapper_out_nll_loss_out_out));
  m.impl("nll_loss_nd",
  TORCH_FN(wrapper__nll_loss_nd));
  m.impl("nll_loss2d",
  TORCH_FN(wrapper__nll_loss2d));
  m.impl("nll_loss2d.out",
  TORCH_FN(wrapper_out_nll_loss2d_out_out));
  m.impl("log_sigmoid",
  TORCH_FN(wrapper__log_sigmoid));
  m.impl("log_sigmoid.out",
  TORCH_FN(wrapper_out_log_sigmoid_out_out));
  m.impl("adaptive_avg_pool2d",
  TORCH_FN(wrapper__adaptive_avg_pool2d));
  m.impl("adaptive_avg_pool3d",
  TORCH_FN(wrapper__adaptive_avg_pool3d));
  m.impl("thnn_conv2d",
  TORCH_FN(wrapper__thnn_conv2d));
  m.impl("thnn_conv2d.out",
  TORCH_FN(wrapper_out_thnn_conv2d_out_out));
  m.impl("slow_conv3d",
  TORCH_FN(wrapper__slow_conv3d));
  m.impl("slow_conv3d.out",
  TORCH_FN(wrapper_out_slow_conv3d_out_out));
  m.impl("column_stack",
  TORCH_FN(wrapper__column_stack));
  m.impl("column_stack.out",
  TORCH_FN(wrapper_out_column_stack_out_out));
  m.impl("isfinite",
  TORCH_FN(wrapper__isfinite));
  m.impl("isinf",
  TORCH_FN(wrapper__isinf));
  m.impl("_add_batch_dim",
  TORCH_FN(wrapper___add_batch_dim));
  m.impl("_remove_batch_dim",
  TORCH_FN(wrapper___remove_batch_dim));
  m.impl("special_expm1",
  TORCH_FN(wrapper__special_expm1));
  m.impl("special_expm1.out",
  TORCH_FN(wrapper_out_special_expm1_out_out));
  m.impl("special_exp2",
  TORCH_FN(wrapper__special_exp2));
  m.impl("special_exp2.out",
  TORCH_FN(wrapper_out_special_exp2_out_out));
  m.impl("special_psi",
  TORCH_FN(wrapper__special_psi));
  m.impl("special_psi.out",
  TORCH_FN(wrapper_out_special_psi_out_out));
  m.impl("special_digamma",
  TORCH_FN(wrapper__special_digamma));
  m.impl("special_digamma.out",
  TORCH_FN(wrapper_out_special_digamma_out_out));
  m.impl("special_gammaln",
  TORCH_FN(wrapper__special_gammaln));
  m.impl("special_gammaln.out",
  TORCH_FN(wrapper_out_special_gammaln_out_out));
  m.impl("special_erf",
  TORCH_FN(wrapper__special_erf));
  m.impl("special_erf.out",
  TORCH_FN(wrapper_out_special_erf_out_out));
  m.impl("special_erfc",
  TORCH_FN(wrapper__special_erfc));
  m.impl("special_erfc.out",
  TORCH_FN(wrapper_out_special_erfc_out_out));
  m.impl("special_erfinv",
  TORCH_FN(wrapper__special_erfinv));
  m.impl("special_erfinv.out",
  TORCH_FN(wrapper_out_special_erfinv_out_out));
  m.impl("special_ndtr",
  TORCH_FN(wrapper__special_ndtr));
  m.impl("special_ndtr.out",
  TORCH_FN(wrapper_out_special_ndtr_out_out));
  m.impl("special_xlogy",
  TORCH_FN(wrapper__special_xlogy));
  m.impl("special_xlogy.out",
  TORCH_FN(wrapper_out_special_xlogy_out_out));
  m.impl("special_xlogy.self_scalar",
  TORCH_FN(wrapper_self_scalar_special_xlogy_self_scalar));
  m.impl("special_xlogy.self_scalar_out",
  TORCH_FN(wrapper_self_scalar_out_special_xlogy_out_self_scalar_out));
  m.impl("special_xlogy.other_scalar",
  TORCH_FN(wrapper_other_scalar_special_xlogy_other_scalar));
  m.impl("special_xlogy.other_scalar_out",
  TORCH_FN(wrapper_other_scalar_out_special_xlogy_out_other_scalar_out));
  m.impl("special_i0",
  TORCH_FN(wrapper__special_i0));
  m.impl("special_i0.out",
  TORCH_FN(wrapper_out_special_i0_out_out));
  m.impl("special_logit",
  TORCH_FN(wrapper__special_logit));
  m.impl("special_logit.out",
  TORCH_FN(wrapper_out_special_logit_out_out));
  m.impl("special_polygamma",
  TORCH_FN(wrapper__special_polygamma));
  m.impl("special_polygamma.out",
  TORCH_FN(wrapper_out_special_polygamma_out_out));
  m.impl("special_logsumexp",
  TORCH_FN(wrapper__special_logsumexp));
  m.impl("special_logsumexp.out",
  TORCH_FN(wrapper_out_special_logsumexp_out_out));
  m.impl("special_expit",
  TORCH_FN(wrapper__special_expit));
  m.impl("special_expit.out",
  TORCH_FN(wrapper_out_special_expit_out_out));
  m.impl("special_sinc",
  TORCH_FN(wrapper__special_sinc));
  m.impl("special_sinc.out",
  TORCH_FN(wrapper_out_special_sinc_out_out));
  m.impl("special_round",
  TORCH_FN(wrapper__special_round));
  m.impl("special_round.out",
  TORCH_FN(wrapper_out_special_round_out_out));
  m.impl("special_log1p",
  TORCH_FN(wrapper__special_log1p));
  m.impl("special_log1p.out",
  TORCH_FN(wrapper_out_special_log1p_out_out));
  m.impl("special_log_softmax",
  TORCH_FN(wrapper__special_log_softmax));
  m.impl("special_gammainc",
  TORCH_FN(wrapper__special_gammainc));
  m.impl("special_gammainc.out",
  TORCH_FN(wrapper_out_special_gammainc_out_out));
  m.impl("special_gammaincc",
  TORCH_FN(wrapper__special_gammaincc));
  m.impl("special_gammaincc.out",
  TORCH_FN(wrapper_out_special_gammaincc_out_out));
  m.impl("special_multigammaln",
  TORCH_FN(wrapper__special_multigammaln));
  m.impl("special_multigammaln.out",
  TORCH_FN(wrapper_out_special_multigammaln_out_out));
  m.impl("fft_fft",
  TORCH_FN(wrapper__fft_fft));
  m.impl("fft_fft.out",
  TORCH_FN(wrapper_out_fft_fft_out_out));
  m.impl("fft_ifft",
  TORCH_FN(wrapper__fft_ifft));
  m.impl("fft_ifft.out",
  TORCH_FN(wrapper_out_fft_ifft_out_out));
  m.impl("fft_rfft",
  TORCH_FN(wrapper__fft_rfft));
  m.impl("fft_rfft.out",
  TORCH_FN(wrapper_out_fft_rfft_out_out));
  m.impl("fft_irfft",
  TORCH_FN(wrapper__fft_irfft));
  m.impl("fft_irfft.out",
  TORCH_FN(wrapper_out_fft_irfft_out_out));
  m.impl("fft_hfft",
  TORCH_FN(wrapper__fft_hfft));
  m.impl("fft_hfft.out",
  TORCH_FN(wrapper_out_fft_hfft_out_out));
  m.impl("fft_ihfft",
  TORCH_FN(wrapper__fft_ihfft));
  m.impl("fft_ihfft.out",
  TORCH_FN(wrapper_out_fft_ihfft_out_out));
  m.impl("fft_fft2",
  TORCH_FN(wrapper__fft_fft2));
  m.impl("fft_fft2.out",
  TORCH_FN(wrapper_out_fft_fft2_out_out));
  m.impl("fft_ifft2",
  TORCH_FN(wrapper__fft_ifft2));
  m.impl("fft_ifft2.out",
  TORCH_FN(wrapper_out_fft_ifft2_out_out));
  m.impl("fft_rfft2",
  TORCH_FN(wrapper__fft_rfft2));
  m.impl("fft_rfft2.out",
  TORCH_FN(wrapper_out_fft_rfft2_out_out));
  m.impl("fft_irfft2",
  TORCH_FN(wrapper__fft_irfft2));
  m.impl("fft_irfft2.out",
  TORCH_FN(wrapper_out_fft_irfft2_out_out));
  m.impl("fft_fftn",
  TORCH_FN(wrapper__fft_fftn));
  m.impl("fft_fftn.out",
  TORCH_FN(wrapper_out_fft_fftn_out_out));
  m.impl("fft_ifftn",
  TORCH_FN(wrapper__fft_ifftn));
  m.impl("fft_ifftn.out",
  TORCH_FN(wrapper_out_fft_ifftn_out_out));
  m.impl("fft_rfftn",
  TORCH_FN(wrapper__fft_rfftn));
  m.impl("fft_rfftn.out",
  TORCH_FN(wrapper_out_fft_rfftn_out_out));
  m.impl("fft_irfftn",
  TORCH_FN(wrapper__fft_irfftn));
  m.impl("fft_irfftn.out",
  TORCH_FN(wrapper_out_fft_irfftn_out_out));
  m.impl("fft_fftfreq",
  TORCH_FN(wrapper__fft_fftfreq));
  m.impl("fft_fftfreq.out",
  TORCH_FN(wrapper_out_fft_fftfreq_out_out));
  m.impl("fft_rfftfreq",
  TORCH_FN(wrapper__fft_rfftfreq));
  m.impl("fft_rfftfreq.out",
  TORCH_FN(wrapper_out_fft_rfftfreq_out_out));
  m.impl("fft_fftshift",
  TORCH_FN(wrapper__fft_fftshift));
  m.impl("fft_ifftshift",
  TORCH_FN(wrapper__fft_ifftshift));
  m.impl("linalg_cholesky",
  TORCH_FN(wrapper__linalg_cholesky));
  m.impl("linalg_cholesky.out",
  TORCH_FN(wrapper_out_linalg_cholesky_out_out));
  m.impl("linalg_det",
  TORCH_FN(wrapper__linalg_det));
  m.impl("linalg_det.out",
  TORCH_FN(wrapper_out_linalg_det_out_out));
  m.impl("det",
  TORCH_FN(wrapper__det));
  m.impl("linalg_matmul",
  TORCH_FN(wrapper__linalg_matmul));
  m.impl("linalg_matmul.out",
  TORCH_FN(wrapper_out_linalg_matmul_out_out));
  m.impl("linalg_eigvals",
  TORCH_FN(wrapper__linalg_eigvals));
  m.impl("linalg_eigvals.out",
  TORCH_FN(wrapper_out_linalg_eigvals_out_out));
  m.impl("linalg_eigvalsh",
  TORCH_FN(wrapper__linalg_eigvalsh));
  m.impl("linalg_inv",
  TORCH_FN(wrapper__linalg_inv));
  m.impl("linalg_inv.out",
  TORCH_FN(wrapper_out_linalg_inv_out_out));
  m.impl("inner",
  TORCH_FN(wrapper__inner));
  m.impl("inner.out",
  TORCH_FN(wrapper_out_inner_out_out));
  m.impl("outer",
  TORCH_FN(wrapper__outer));
  m.impl("outer.out",
  TORCH_FN(wrapper_out_outer_out_out));
  m.impl("ger",
  TORCH_FN(wrapper__ger));
  m.impl("ger.out",
  TORCH_FN(wrapper_out_ger_out_out));
  m.impl("linalg_norm",
  TORCH_FN(wrapper__linalg_norm));
  m.impl("linalg_norm.out",
  TORCH_FN(wrapper_out_linalg_norm_out_out));
  m.impl("linalg_norm.ord_str",
  TORCH_FN(wrapper_ord_str_linalg_norm_ord_str));
  m.impl("linalg_norm.ord_str_out",
  TORCH_FN(wrapper_ord_str_out_linalg_norm_out_ord_str_out));
  m.impl("linalg_matrix_norm",
  TORCH_FN(wrapper__linalg_matrix_norm));
  m.impl("linalg_matrix_norm.out",
  TORCH_FN(wrapper_out_linalg_matrix_norm_out_out));
  m.impl("linalg_matrix_norm.str_ord",
  TORCH_FN(wrapper_str_ord_linalg_matrix_norm_str_ord));
  m.impl("linalg_matrix_norm.str_ord_out",
  TORCH_FN(wrapper_str_ord_out_linalg_matrix_norm_out_str_ord_out));
  m.impl("linalg_svd",
  TORCH_FN(wrapper__linalg_svd));
  m.impl("linalg_svd.U",
  TORCH_FN(wrapper_U_linalg_svd_out_U));
  m.impl("linalg_svdvals",
  TORCH_FN(wrapper__linalg_svdvals));
  m.impl("linalg_svdvals.out",
  TORCH_FN(wrapper_out_linalg_svdvals_out_out));
  m.impl("linalg_cond",
  TORCH_FN(wrapper__linalg_cond));
  m.impl("linalg_cond.out",
  TORCH_FN(wrapper_out_linalg_cond_out_out));
  m.impl("linalg_cond.p_str",
  TORCH_FN(wrapper_p_str_linalg_cond_p_str));
  m.impl("linalg_cond.p_str_out",
  TORCH_FN(wrapper_p_str_out_linalg_cond_out_p_str_out));
  m.impl("linalg_pinv",
  TORCH_FN(wrapper__linalg_pinv));
  m.impl("linalg_pinv.out",
  TORCH_FN(wrapper_out_linalg_pinv_out_out));
  m.impl("linalg_pinv.rcond_tensor",
  TORCH_FN(wrapper_rcond_tensor_linalg_pinv_rcond_tensor));
  m.impl("linalg_pinv.out_rcond_tensor",
  TORCH_FN(wrapper_out_rcond_tensor_linalg_pinv_out_out_rcond_tensor));
  m.impl("linalg_tensorinv",
  TORCH_FN(wrapper__linalg_tensorinv));
  m.impl("linalg_tensorinv.out",
  TORCH_FN(wrapper_out_linalg_tensorinv_out_out));
  m.impl("linalg_tensorsolve",
  TORCH_FN(wrapper__linalg_tensorsolve));
  m.impl("linalg_tensorsolve.out",
  TORCH_FN(wrapper_out_linalg_tensorsolve_out_out));
  m.impl("linalg_matrix_power",
  TORCH_FN(wrapper__linalg_matrix_power));
  m.impl("linalg_matrix_power.out",
  TORCH_FN(wrapper_out_linalg_matrix_power_out_out));
  m.impl("linalg_matrix_rank",
  TORCH_FN(wrapper__linalg_matrix_rank));
  m.impl("linalg_matrix_rank.out",
  TORCH_FN(wrapper_out_linalg_matrix_rank_out_out));
  m.impl("linalg_matrix_rank.tol_tensor",
  TORCH_FN(wrapper_tol_tensor_linalg_matrix_rank_tol_tensor));
  m.impl("linalg_matrix_rank.out_tol_tensor",
  TORCH_FN(wrapper_out_tol_tensor_linalg_matrix_rank_out_out_tol_tensor));
  m.impl("linalg_multi_dot",
  TORCH_FN(wrapper__linalg_multi_dot));
  m.impl("linalg_multi_dot.out",
  TORCH_FN(wrapper_out_linalg_multi_dot_out_out));
  m.impl("_test_serialization_subcmul",
  TORCH_FN(wrapper___test_serialization_subcmul));
  m.impl("_test_string_default",
  TORCH_FN(wrapper___test_string_default));
  m.impl("_test_ambiguous_defaults.a",
  TORCH_FN(wrapper_a__test_ambiguous_defaults_a));
  m.impl("_test_ambiguous_defaults.b",
  TORCH_FN(wrapper_b__test_ambiguous_defaults_b));
  m.impl("pad_sequence",
  TORCH_FN(wrapper__pad_sequence));
  m.impl("flatten_dense_tensors",
  TORCH_FN(wrapper__flatten_dense_tensors));
  m.impl("unflatten_dense_tensors",
  TORCH_FN(wrapper__unflatten_dense_tensors));
}

} // anonymous namespace

namespace compositeimplicitautograd {


at::Tensor _cast_Byte(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Byte(self, non_blocking);
}

at::Tensor _cast_Char(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Char(self, non_blocking);
}

at::Tensor _cast_Double(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Double(self, non_blocking);
}

at::Tensor _cast_Float(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Float(self, non_blocking);
}

at::Tensor _cast_Int(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Int(self, non_blocking);
}

at::Tensor _cast_Long(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Long(self, non_blocking);
}

at::Tensor _cast_Short(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Short(self, non_blocking);
}

at::Tensor _cast_Half(const at::Tensor & self, bool non_blocking) {
return wrapper___cast_Half(self, non_blocking);
}

void _backward(const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
return wrapper___backward(self, inputs, gradient, retain_graph, create_graph);
}

void set_data(at::Tensor & self, const at::Tensor & new_data) {
return wrapper__set_data(self, new_data);
}

at::Tensor data(const at::Tensor & self) {
return wrapper__data(self);
}

bool is_leaf(const at::Tensor & self) {
return wrapper__is_leaf(self);
}

int64_t output_nr(const at::Tensor & self) {
return wrapper__output_nr(self);
}

int64_t _version(const at::Tensor & self) {
return wrapper___version(self);
}

at::Tensor & requires_grad_(at::Tensor & self, bool requires_grad) {
return wrapper__requires_grad_(self, requires_grad);
}

void retain_grad(at::Tensor & self) {
return wrapper__retain_grad(self);
}

bool retains_grad(const at::Tensor & self) {
return wrapper__retains_grad(self);
}

at::Tensor _make_dual(const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
return wrapper___make_dual(primal, tangent, level);
}

::std::tuple<at::Tensor,at::Tensor> _unpack_dual(const at::Tensor & dual, int64_t level) {
return wrapper___unpack_dual(dual, level);
}

at::Tensor & rename_(at::Tensor & self, c10::optional<at::DimnameList> names) {
return wrapper__rename_(self, names);
}

at::Tensor rename(const at::Tensor & self, c10::optional<at::DimnameList> names) {
return wrapper__rename(self, names);
}

at::Tensor align_to(const at::Tensor & self, at::DimnameList names) {
return wrapper__align_to(self, names);
}

at::Tensor align_to(const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
return wrapper_ellipsis_idx_align_to_ellipsis_idx(self, order, ellipsis_idx);
}

at::Tensor align_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper__align_as(self, other);
}

::std::vector<at::Tensor> align_tensors(at::TensorList tensors) {
return wrapper__align_tensors(tensors);
}

at::Tensor refine_names(const at::Tensor & self, at::DimnameList names) {
return wrapper__refine_names(self, names);
}

bool _use_cudnn_rnn_flatten_weight() {
return wrapper___use_cudnn_rnn_flatten_weight();
}

int64_t _debug_has_internal_overlap(const at::Tensor & self) {
return wrapper___debug_has_internal_overlap(self);
}

::std::tuple<at::Tensor,at::Tensor> _sobol_engine_draw(const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
return wrapper___sobol_engine_draw(quasi, n, sobolstate, dimension, num_generated, dtype);
}

at::Tensor & _sobol_engine_ff_(at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
return wrapper___sobol_engine_ff_(self, n, sobolstate, dimension, num_generated);
}

at::Tensor & _sobol_engine_scramble_(at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
return wrapper___sobol_engine_scramble_(self, ltm, dimension);
}

at::Tensor & _sobol_engine_initialize_state_(at::Tensor & self, int64_t dimension) {
return wrapper___sobol_engine_initialize_state_(self, dimension);
}

at::Tensor _reshape_from_tensor(const at::Tensor & self, const at::Tensor & shape) {
return wrapper___reshape_from_tensor(self, shape);
}

at::Tensor _shape_as_tensor(const at::Tensor & self) {
return wrapper___shape_as_tensor(self);
}

at::Tensor dropout(const at::Tensor & input, double p, bool train) {
return wrapper__dropout(input, p, train);
}

at::Tensor & dropout_(at::Tensor & self, double p, bool train) {
return wrapper__dropout_(self, p, train);
}

at::Tensor feature_dropout(const at::Tensor & input, double p, bool train) {
return wrapper__feature_dropout(input, p, train);
}

at::Tensor & feature_dropout_(at::Tensor & self, double p, bool train) {
return wrapper__feature_dropout_(self, p, train);
}

at::Tensor alpha_dropout(const at::Tensor & input, double p, bool train) {
return wrapper__alpha_dropout(input, p, train);
}

at::Tensor & alpha_dropout_(at::Tensor & self, double p, bool train) {
return wrapper__alpha_dropout_(self, p, train);
}

at::Tensor feature_alpha_dropout(const at::Tensor & input, double p, bool train) {
return wrapper__feature_alpha_dropout(input, p, train);
}

at::Tensor & feature_alpha_dropout_(at::Tensor & self, double p, bool train) {
return wrapper__feature_alpha_dropout_(self, p, train);
}

at::Tensor absolute(const at::Tensor & self) {
return wrapper__absolute(self);
}

at::Tensor & absolute_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_absolute_out_out(self, out);
}

at::Tensor & absolute_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_absolute_out_out(self, out);
}

at::Tensor & absolute_(at::Tensor & self) {
return wrapper__absolute_(self);
}

at::Tensor real(const at::Tensor & self) {
return wrapper__real(self);
}

at::Tensor imag(const at::Tensor & self) {
return wrapper__imag(self);
}

at::Tensor conj(const at::Tensor & self) {
return wrapper__conj(self);
}

at::Tensor conj_physical(const at::Tensor & self) {
return wrapper__conj_physical(self);
}

at::Tensor resolve_conj(const at::Tensor & self) {
return wrapper__resolve_conj(self);
}

at::Tensor resolve_neg(const at::Tensor & self) {
return wrapper__resolve_neg(self);
}

at::Tensor arccos(const at::Tensor & self) {
return wrapper__arccos(self);
}

at::Tensor & arccos_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_arccos_out_out(self, out);
}

at::Tensor & arccos_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_arccos_out_out(self, out);
}

at::Tensor & arccos_(at::Tensor & self) {
return wrapper__arccos_(self);
}

at::Tensor avg_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
return wrapper__avg_pool1d(self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}

at::Tensor adaptive_avg_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper__adaptive_avg_pool1d(self, output_size);
}

::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool1d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper__adaptive_max_pool1d(self, output_size);
}

at::Tensor addr(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper__addr(self, vec1, vec2, beta, alpha);
}

at::Tensor & addr_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_out_addr_out_out(self, vec1, vec2, beta, alpha, out);
}

at::Tensor & addr_outf(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_addr_out_out(self, vec1, vec2, beta, alpha, out);
}

at::Tensor affine_grid_generator_backward(const at::Tensor & grad, at::IntArrayRef size, bool align_corners) {
return wrapper__affine_grid_generator_backward(grad, size, align_corners);
}

at::Tensor all(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_dimname_all_dimname(self, dim, keepdim);
}

at::Tensor & all_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_dimname_out_all_out_dimname_out(self, dim, keepdim, out);
}

at::Tensor & all_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
return wrapper_dimname_out_all_out_dimname_out(self, dim, keepdim, out);
}

bool allclose(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
return wrapper__allclose(self, other, rtol, atol, equal_nan);
}

at::Tensor any(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_dimname_any_dimname(self, dim, keepdim);
}

at::Tensor & any_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_dimname_out_any_out_dimname_out(self, dim, keepdim, out);
}

at::Tensor & any_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
return wrapper_dimname_out_any_out_dimname_out(self, dim, keepdim, out);
}

at::Tensor arange(const at::Scalar & end, at::TensorOptions options) {
return wrapper__arange(end, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor arange(const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__arange(end, dtype, layout, device, pin_memory);
}

at::Tensor arange(const at::Scalar & start, const at::Scalar & end, at::TensorOptions options) {
return wrapper_start_arange_start(start, end, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor arange(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_start_arange_start(start, end, dtype, layout, device, pin_memory);
}

at::Tensor arange(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::TensorOptions options) {
return wrapper_start_step_arange_start_step(start, end, step, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor arange(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_start_step_arange_start_step(start, end, step, dtype, layout, device, pin_memory);
}

at::Tensor & arange_out(at::Tensor & out, const at::Scalar & end) {
return wrapper_out_arange_out_out(end, out);
}

at::Tensor & arange_outf(const at::Scalar & end, at::Tensor & out) {
return wrapper_out_arange_out_out(end, out);
}

at::Tensor _dim_arange(const at::Tensor & like, int64_t dim) {
return wrapper___dim_arange(like, dim);
}

at::Tensor arccosh(const at::Tensor & self) {
return wrapper__arccosh(self);
}

at::Tensor & arccosh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_arccosh_out_out(self, out);
}

at::Tensor & arccosh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_arccosh_out_out(self, out);
}

at::Tensor & arccosh_(at::Tensor & self) {
return wrapper__arccosh_(self);
}

at::Tensor arcsinh(const at::Tensor & self) {
return wrapper__arcsinh(self);
}

at::Tensor & arcsinh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_arcsinh_out_out(self, out);
}

at::Tensor & arcsinh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_arcsinh_out_out(self, out);
}

at::Tensor & arcsinh_(at::Tensor & self) {
return wrapper__arcsinh_(self);
}

at::Tensor arctanh(const at::Tensor & self) {
return wrapper__arctanh(self);
}

at::Tensor & arctanh_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_arctanh_out_out(self, out);
}

at::Tensor & arctanh_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_arctanh_out_out(self, out);
}

at::Tensor & arctanh_(at::Tensor & self) {
return wrapper__arctanh_(self);
}

at::Tensor arcsin(const at::Tensor & self) {
return wrapper__arcsin(self);
}

at::Tensor & arcsin_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_arcsin_out_out(self, out);
}

at::Tensor & arcsin_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_arcsin_out_out(self, out);
}

at::Tensor & arcsin_(at::Tensor & self) {
return wrapper__arcsin_(self);
}

at::Tensor arctan(const at::Tensor & self) {
return wrapper__arctan(self);
}

at::Tensor & arctan_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_arctan_out_out(self, out);
}

at::Tensor & arctan_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_arctan_out_out(self, out);
}

at::Tensor & arctan_(at::Tensor & self) {
return wrapper__arctan_(self);
}

at::Tensor atleast_1d(const at::Tensor & self) {
return wrapper__atleast_1d(self);
}

::std::vector<at::Tensor> atleast_1d(at::TensorList tensors) {
return wrapper_Sequence_atleast_1d_Sequence(tensors);
}

at::Tensor atleast_2d(const at::Tensor & self) {
return wrapper__atleast_2d(self);
}

::std::vector<at::Tensor> atleast_2d(at::TensorList tensors) {
return wrapper_Sequence_atleast_2d_Sequence(tensors);
}

at::Tensor atleast_3d(const at::Tensor & self) {
return wrapper__atleast_3d(self);
}

::std::vector<at::Tensor> atleast_3d(at::TensorList tensors) {
return wrapper_Sequence_atleast_3d_Sequence(tensors);
}

at::Tensor & _baddbmm_mkl_(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper___baddbmm_mkl_(self, batch1, batch2, beta, alpha);
}

at::Tensor bartlett_window(int64_t window_length, at::TensorOptions options) {
return wrapper__bartlett_window(window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor bartlett_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__bartlett_window(window_length, dtype, layout, device, pin_memory);
}

at::Tensor bartlett_window(int64_t window_length, bool periodic, at::TensorOptions options) {
return wrapper_periodic_bartlett_window_periodic(window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor bartlett_window(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_bartlett_window_periodic(window_length, periodic, dtype, layout, device, pin_memory);
}

at::Tensor batch_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
return wrapper__batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> _batch_norm_impl_index(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
return wrapper___batch_norm_impl_index(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> _batch_norm_impl_index_backward(int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, ::std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
return wrapper___batch_norm_impl_index_backward(impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask, reservedSpace);
}

at::Tensor bernoulli(const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
return wrapper_p_bernoulli_p(self, p, generator);
}

at::Tensor bilinear(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper__bilinear(input1, input2, weight, bias);
}

at::Tensor binary_cross_entropy_with_logits_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
return wrapper__binary_cross_entropy_with_logits_backward(grad_output, self, target, weight, pos_weight, reduction);
}

at::Tensor logical_not(const at::Tensor & self) {
return wrapper__logical_not(self);
}

at::Tensor & logical_not_(at::Tensor & self) {
return wrapper__logical_not_(self);
}

at::Tensor logical_xor(const at::Tensor & self, const at::Tensor & other) {
return wrapper__logical_xor(self, other);
}

at::Tensor & logical_xor_(at::Tensor & self, const at::Tensor & other) {
return wrapper__logical_xor_(self, other);
}

at::Tensor logical_and(const at::Tensor & self, const at::Tensor & other) {
return wrapper__logical_and(self, other);
}

at::Tensor & logical_and_(at::Tensor & self, const at::Tensor & other) {
return wrapper__logical_and_(self, other);
}

at::Tensor logical_or(const at::Tensor & self, const at::Tensor & other) {
return wrapper__logical_or(self, other);
}

at::Tensor & logical_or_(at::Tensor & self, const at::Tensor & other) {
return wrapper__logical_or_(self, other);
}

at::Tensor blackman_window(int64_t window_length, at::TensorOptions options) {
return wrapper__blackman_window(window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor blackman_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__blackman_window(window_length, dtype, layout, device, pin_memory);
}

at::Tensor blackman_window(int64_t window_length, bool periodic, at::TensorOptions options) {
return wrapper_periodic_blackman_window_periodic(window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor blackman_window(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_blackman_window_periodic(window_length, periodic, dtype, layout, device, pin_memory);
}

::std::vector<at::Tensor> broadcast_tensors(at::TensorList tensors) {
return wrapper__broadcast_tensors(tensors);
}

at::Tensor broadcast_to(const at::Tensor & self, at::IntArrayRef size) {
return wrapper__broadcast_to(self, size);
}

at::Tensor cat(at::TensorList tensors, at::Dimname dim) {
return wrapper_names_cat_names(tensors, dim);
}

at::Tensor & cat_out(at::Tensor & out, at::TensorList tensors, at::Dimname dim) {
return wrapper_names_out_cat_out_names_out(tensors, dim, out);
}

at::Tensor & cat_outf(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
return wrapper_names_out_cat_out_names_out(tensors, dim, out);
}

at::Tensor concat(at::TensorList tensors, int64_t dim) {
return wrapper__concat(tensors, dim);
}

at::Tensor & concat_out(at::Tensor & out, at::TensorList tensors, int64_t dim) {
return wrapper_out_concat_out_out(tensors, dim, out);
}

at::Tensor & concat_outf(at::TensorList tensors, int64_t dim, at::Tensor & out) {
return wrapper_out_concat_out_out(tensors, dim, out);
}

at::Tensor concat(at::TensorList tensors, at::Dimname dim) {
return wrapper_names_concat_names(tensors, dim);
}

at::Tensor & concat_out(at::Tensor & out, at::TensorList tensors, at::Dimname dim) {
return wrapper_names_out_concat_out_names_out(tensors, dim, out);
}

at::Tensor & concat_outf(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
return wrapper_names_out_concat_out_names_out(tensors, dim, out);
}

at::Tensor block_diag(at::TensorList tensors) {
return wrapper__block_diag(tensors);
}

at::Tensor chain_matmul(at::TensorList matrices) {
return wrapper__chain_matmul(matrices);
}

at::Tensor & chain_matmul_out(at::Tensor & out, at::TensorList matrices) {
return wrapper_out_chain_matmul_out_out(matrices, out);
}

at::Tensor & chain_matmul_outf(at::TensorList matrices, at::Tensor & out) {
return wrapper_out_chain_matmul_out_out(matrices, out);
}

::std::vector<at::Tensor> unsafe_chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
return wrapper__unsafe_chunk(self, chunks, dim);
}

::std::vector<at::Tensor> chunk(const at::Tensor & self, int64_t chunks, int64_t dim) {
return wrapper__chunk(self, chunks, dim);
}

::std::vector<at::Tensor> tensor_split(const at::Tensor & self, int64_t sections, int64_t dim) {
return wrapper_sections_tensor_split_sections(self, sections, dim);
}

::std::vector<at::Tensor> tensor_split(const at::Tensor & self, at::IntArrayRef indices, int64_t dim) {
return wrapper_indices_tensor_split_indices(self, indices, dim);
}

::std::vector<at::Tensor> tensor_split(const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
return wrapper_tensor_indices_or_sections_tensor_split_tensor_indices_or_sections(self, tensor_indices_or_sections, dim);
}

at::Tensor clip(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper__clip(self, min, max);
}

at::Tensor & clip_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_out_clip_out_out(self, min, max, out);
}

at::Tensor & clip_outf(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
return wrapper_out_clip_out_out(self, min, max, out);
}

at::Tensor & clip_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper__clip_(self, min, max);
}

at::Tensor clip(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_Tensor_clip_Tensor(self, min, max);
}

at::Tensor & clip_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_Tensor_out_clip_out_Tensor_out(self, min, max, out);
}

at::Tensor & clip_outf(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
return wrapper_Tensor_out_clip_out_Tensor_out(self, min, max, out);
}

at::Tensor & clip_(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_Tensor_clip__Tensor(self, min, max);
}

bool cudnn_is_acceptable(const at::Tensor & self) {
return wrapper__cudnn_is_acceptable(self);
}

at::Tensor contiguous(const at::Tensor & self, at::MemoryFormat memory_format) {
return wrapper__contiguous(self, memory_format);
}

at::Tensor convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
return wrapper__convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

at::Tensor _convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
return wrapper___convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
}

at::Tensor _convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
return wrapper_deprecated__convolution_deprecated(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}

at::Tensor _convolution_mode(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper___convolution_mode(input, weight, bias, stride, padding, dilation, groups);
}

at::Tensor _convolution_nogroup(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding) {
return wrapper___convolution_nogroup(input, weight, bias, stride, padding, dilation, transposed, output_padding);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> _convolution_double_backward(const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, ::std::array<bool,3> output_mask) {
return wrapper___convolution_double_backward(ggI, ggW, ggb, gO, weight, self, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, output_mask);
}

at::Tensor conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper__conv1d(input, weight, bias, stride, padding, dilation, groups);
}

at::Tensor conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper__conv2d(input, weight, bias, stride, padding, dilation, groups);
}

at::Tensor conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper__conv3d(input, weight, bias, stride, padding, dilation, groups);
}

at::Tensor conv1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_padding_conv1d_padding(input, weight, bias, stride, padding, dilation, groups);
}

at::Tensor conv2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_padding_conv2d_padding(input, weight, bias, stride, padding, dilation, groups);
}

at::Tensor conv3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
return wrapper_padding_conv3d_padding(input, weight, bias, stride, padding, dilation, groups);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_tbc_backward(const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
return wrapper__conv_tbc_backward(self, input, weight, bias, pad);
}

at::Tensor conv_transpose1d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
return wrapper__conv_transpose1d(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

at::Tensor conv_transpose2d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
return wrapper_input_conv_transpose2d_input(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

at::Tensor conv_transpose3d(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
return wrapper_input_conv_transpose3d_input(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

at::Tensor cosine_embedding_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
return wrapper__cosine_embedding_loss(input1, input2, target, margin, reduction);
}

at::Tensor cov(const at::Tensor & self, int64_t correction, const c10::optional<at::Tensor> & fweights, const c10::optional<at::Tensor> & aweights) {
return wrapper__cov(self, correction, fweights, aweights);
}

at::Tensor corrcoef(const at::Tensor & self) {
return wrapper__corrcoef(self);
}

::std::tuple<at::Tensor,at::Tensor> cummax(const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_cummax_dimname(self, dim);
}

::std::tuple<at::Tensor &,at::Tensor &> cummax_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_out_cummax_out_dimname_out(self, dim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> cummax_outf(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
return wrapper_dimname_out_cummax_out_dimname_out(self, dim, values, indices);
}

::std::tuple<at::Tensor,at::Tensor> cummin(const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_cummin_dimname(self, dim);
}

::std::tuple<at::Tensor &,at::Tensor &> cummin_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_out_cummin_out_dimname_out(self, dim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> cummin_outf(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
return wrapper_dimname_out_cummin_out_dimname_out(self, dim, values, indices);
}

at::Tensor cummaxmin_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
return wrapper__cummaxmin_backward(grad, input, indices, dim);
}

at::Tensor cumprod(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_dimname_cumprod_dimname(self, dim, dtype);
}

at::Tensor & cumprod_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_dimname_out_cumprod_out_dimname_out(self, dim, dtype, out);
}

at::Tensor & cumprod_outf(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_dimname_out_cumprod_out_dimname_out(self, dim, dtype, out);
}

at::Tensor & cumprod_(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_dimname_cumprod__dimname(self, dim, dtype);
}

at::Tensor cumprod_backward(const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
return wrapper__cumprod_backward(grad, input, dim, output);
}

at::Tensor cumsum(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_dimname_cumsum_dimname(self, dim, dtype);
}

at::Tensor & cumsum_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_dimname_out_cumsum_out_dimname_out(self, dim, dtype, out);
}

at::Tensor & cumsum_outf(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_dimname_out_cumsum_out_dimname_out(self, dim, dtype, out);
}

at::Tensor & cumsum_(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_dimname_cumsum__dimname(self, dim, dtype);
}

at::Tensor cumulative_trapezoid(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
return wrapper_x_cumulative_trapezoid_x(y, x, dim);
}

at::Tensor cumulative_trapezoid(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
return wrapper_dx_cumulative_trapezoid_dx(y, dx, dim);
}

at::Tensor ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
return wrapper_IntList_ctc_loss_IntList(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

at::Tensor ctc_loss(const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
return wrapper_Tensor_ctc_loss_Tensor(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

at::Tensor diag_embed(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
return wrapper__diag_embed(self, offset, dim1, dim2);
}

at::Tensor diagflat(const at::Tensor & self, int64_t offset) {
return wrapper__diagflat(self, offset);
}

at::Tensor diagonal(const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
return wrapper_Dimname_diagonal_Dimname(self, outdim, dim1, dim2, offset);
}

at::Tensor & fill_diagonal_(at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
return wrapper__fill_diagonal_(self, fill_value, wrap);
}

at::Tensor diff(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
return wrapper__diff(self, n, dim, prepend, append);
}

at::Tensor & diff_out(at::Tensor & out, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
return wrapper_out_diff_out_out(self, n, dim, prepend, append, out);
}

at::Tensor & diff_outf(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
return wrapper_out_diff_out_out(self, n, dim, prepend, append, out);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
return wrapper_scalarint_gradient_scalarint(self, spacing, dim, edge_order);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_scalararray_gradient_scalararray(self, spacing, dim, edge_order);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_array_gradient_array(self, dim, edge_order);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
return wrapper_scalarrayint_gradient_scalarrayint(self, spacing, dim, edge_order);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_scalarrayarray_gradient_scalarrayarray(self, spacing, dim, edge_order);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
return wrapper_tensorarrayint_gradient_tensorarrayint(self, spacing, dim, edge_order);
}

::std::vector<at::Tensor> gradient(const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
return wrapper_tensorarray_gradient_tensorarray(self, spacing, dim, edge_order);
}

at::Tensor divide(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_divide_Tensor(self, other);
}

at::Tensor & divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_divide_out_out(self, other, out);
}

at::Tensor & divide_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_divide_out_out(self, other, out);
}

at::Tensor & divide_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_divide__Tensor(self, other);
}

at::Tensor divide(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_divide_Scalar(self, other);
}

at::Tensor & divide_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_divide__Scalar(self, other);
}

at::Tensor divide(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Tensor_mode_divide_Tensor_mode(self, other, rounding_mode);
}

at::Tensor & divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_out_mode_divide_out_out_mode(self, other, rounding_mode, out);
}

at::Tensor & divide_outf(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
return wrapper_out_mode_divide_out_out_mode(self, other, rounding_mode, out);
}

at::Tensor & divide_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Tensor_mode_divide__Tensor_mode(self, other, rounding_mode);
}

at::Tensor divide(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Scalar_mode_divide_Scalar_mode(self, other, rounding_mode);
}

at::Tensor & divide_(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Scalar_mode_divide__Scalar_mode(self, other, rounding_mode);
}

at::Tensor true_divide(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_true_divide_Tensor(self, other);
}

at::Tensor & true_divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_true_divide_out_out(self, other, out);
}

at::Tensor & true_divide_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_true_divide_out_out(self, other, out);
}

at::Tensor & true_divide_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_true_divide__Tensor(self, other);
}

at::Tensor true_divide(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_true_divide_Scalar(self, other);
}

at::Tensor & true_divide_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_true_divide__Scalar(self, other);
}

at::Tensor einsum(c10::string_view equation, at::TensorList tensors) {
return wrapper__einsum(equation, tensors);
}

at::Tensor embedding_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
return wrapper__embedding_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}

at::Tensor embedding_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
return wrapper__embedding_sparse_backward(grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}

::std::tuple<at::Tensor,at::Tensor> _rowwise_prune(const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
return wrapper___rowwise_prune(weight, mask, compressed_indices_dtype);
}

at::Tensor row_stack(at::TensorList tensors) {
return wrapper__row_stack(tensors);
}

at::Tensor & row_stack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_out_row_stack_out_out(tensors, out);
}

at::Tensor & row_stack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_out_row_stack_out_out(tensors, out);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
return wrapper__embedding_bag(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
return wrapper_padding_idx_embedding_bag_padding_idx(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

at::Tensor _embedding_bag_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
return wrapper___embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}

at::Tensor _embedding_bag_sparse_backward(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
return wrapper___embedding_bag_sparse_backward(grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}

at::Tensor empty(at::IntArrayRef size, c10::optional<at::DimnameList> names, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_names_empty_names(size, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor empty(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_names_empty_names(size, names, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor new_empty(const at::Tensor & self, at::IntArrayRef size, at::TensorOptions options) {
return wrapper__new_empty(self, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor new_empty(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__new_empty(self, size, dtype, layout, device, pin_memory);
}

at::Tensor new_empty_strided(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, at::TensorOptions options) {
return wrapper__new_empty_strided(self, size, stride, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor new_empty_strided(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__new_empty_strided(self, size, stride, dtype, layout, device, pin_memory);
}

at::Tensor new_full(const at::Tensor & self, at::IntArrayRef size, const at::Scalar & fill_value, at::TensorOptions options) {
return wrapper__new_full(self, size, fill_value, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor new_full(const at::Tensor & self, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__new_full(self, size, fill_value, dtype, layout, device, pin_memory);
}

at::Tensor new_zeros(const at::Tensor & self, at::IntArrayRef size, at::TensorOptions options) {
return wrapper__new_zeros(self, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor new_zeros(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__new_zeros(self, size, dtype, layout, device, pin_memory);
}

at::Tensor new_ones(const at::Tensor & self, at::IntArrayRef size, at::TensorOptions options) {
return wrapper__new_ones(self, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor new_ones(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__new_ones(self, size, dtype, layout, device, pin_memory);
}

at::Tensor & empty_out(at::Tensor & out, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_out_empty_out_out(size, memory_format, out);
}

at::Tensor & empty_outf(at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
return wrapper_out_empty_out_out(size, memory_format, out);
}

at::Tensor empty_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__empty_like(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor empty_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__empty_like(self, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor expand_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper__expand_as(self, other);
}

at::Tensor eye(int64_t n, at::TensorOptions options) {
return wrapper__eye(n, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor eye(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__eye(n, dtype, layout, device, pin_memory);
}

at::Tensor eye(int64_t n, int64_t m, at::TensorOptions options) {
return wrapper_m_eye_m(n, m, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor eye(int64_t n, int64_t m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_m_eye_m(n, m, dtype, layout, device, pin_memory);
}

at::Tensor flatten(const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
return wrapper_using_ints_flatten_using_ints(self, start_dim, end_dim);
}

at::Tensor flatten(const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
return wrapper_named_out_dim_flatten_named_out_dim(self, start_dim, end_dim, out_dim);
}

at::Tensor flatten(const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
return wrapper_using_names_flatten_using_names(self, start_dim, end_dim, out_dim);
}

at::Tensor flatten(const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
return wrapper_DimnameList_flatten_DimnameList(self, dims, out_dim);
}

at::Tensor unflatten(const at::Tensor & self, int64_t dim, at::IntArrayRef sizes, c10::optional<at::DimnameList> names) {
return wrapper_int_unflatten_int(self, dim, sizes, names);
}

at::Tensor unflatten(const at::Tensor & self, at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) {
return wrapper_Dimname_unflatten_Dimname(self, dim, sizes, names);
}

at::Tensor floor_divide(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_floor_divide_Scalar(self, other);
}

at::Tensor & floor_divide_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_floor_divide__Scalar(self, other);
}

at::Tensor full(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_names_full_names(size, fill_value, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor full(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_names_full_names(size, fill_value, names, dtype, layout, device, pin_memory);
}

at::Tensor full(at::IntArrayRef size, const at::Scalar & fill_value, at::TensorOptions options) {
return wrapper__full(size, fill_value, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor full(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__full(size, fill_value, dtype, layout, device, pin_memory);
}

at::Tensor & full_out(at::Tensor & out, at::IntArrayRef size, const at::Scalar & fill_value) {
return wrapper_out_full_out_out(size, fill_value, out);
}

at::Tensor & full_outf(at::IntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
return wrapper_out_full_out_out(size, fill_value, out);
}

at::Tensor full_like(const at::Tensor & self, const at::Scalar & fill_value, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__full_like(self, fill_value, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor full_like(const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__full_like(self, fill_value, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor grid_sampler(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
return wrapper__grid_sampler(input, grid, interpolation_mode, padding_mode, align_corners);
}

::std::tuple<at::Tensor,at::Tensor> _grid_sampler_2d_cpu_fallback_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
return wrapper___grid_sampler_2d_cpu_fallback_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

at::Tensor hann_window(int64_t window_length, at::TensorOptions options) {
return wrapper__hann_window(window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor hann_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__hann_window(window_length, dtype, layout, device, pin_memory);
}

at::Tensor hann_window(int64_t window_length, bool periodic, at::TensorOptions options) {
return wrapper_periodic_hann_window_periodic(window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor hann_window(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_hann_window_periodic(window_length, periodic, dtype, layout, device, pin_memory);
}

at::Tensor hamming_window(int64_t window_length, at::TensorOptions options) {
return wrapper__hamming_window(window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor hamming_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__hamming_window(window_length, dtype, layout, device, pin_memory);
}

at::Tensor hamming_window(int64_t window_length, bool periodic, at::TensorOptions options) {
return wrapper_periodic_hamming_window_periodic(window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor hamming_window(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_hamming_window_periodic(window_length, periodic, dtype, layout, device, pin_memory);
}

at::Tensor hamming_window(int64_t window_length, bool periodic, double alpha, at::TensorOptions options) {
return wrapper_periodic_alpha_hamming_window_periodic_alpha(window_length, periodic, alpha, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor hamming_window(int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_alpha_hamming_window_periodic_alpha(window_length, periodic, alpha, dtype, layout, device, pin_memory);
}

at::Tensor hamming_window(int64_t window_length, bool periodic, double alpha, double beta, at::TensorOptions options) {
return wrapper_periodic_alpha_beta_hamming_window_periodic_alpha_beta(window_length, periodic, alpha, beta, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor hamming_window(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_alpha_beta_hamming_window_periodic_alpha_beta(window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}

at::Tensor kaiser_window(int64_t window_length, at::TensorOptions options) {
return wrapper__kaiser_window(window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor kaiser_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__kaiser_window(window_length, dtype, layout, device, pin_memory);
}

at::Tensor kaiser_window(int64_t window_length, bool periodic, at::TensorOptions options) {
return wrapper_periodic_kaiser_window_periodic(window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor kaiser_window(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_periodic_kaiser_window_periodic(window_length, periodic, dtype, layout, device, pin_memory);
}

at::Tensor kaiser_window(int64_t window_length, bool periodic, double beta, at::TensorOptions options) {
return wrapper_beta_kaiser_window_beta(window_length, periodic, beta, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor kaiser_window(int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_beta_kaiser_window_beta(window_length, periodic, beta, dtype, layout, device, pin_memory);
}

at::Tensor hinge_embedding_loss(const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
return wrapper__hinge_embedding_loss(self, target, margin, reduction);
}

at::Tensor group_norm(const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
return wrapper__group_norm(input, num_groups, weight, bias, eps, cudnn_enabled);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps) {
return wrapper__native_group_norm(input, weight, bias, N, C, HxW, group, eps);
}

int64_t _cufft_get_plan_cache_size(int64_t device_index) {
return wrapper___cufft_get_plan_cache_size(device_index);
}

int64_t _cufft_get_plan_cache_max_size(int64_t device_index) {
return wrapper___cufft_get_plan_cache_max_size(device_index);
}

void _cufft_set_plan_cache_max_size(int64_t device_index, int64_t max_size) {
return wrapper___cufft_set_plan_cache_max_size(device_index, max_size);
}

void _cufft_clear_plan_cache(int64_t device_index) {
return wrapper___cufft_clear_plan_cache(device_index);
}

at::Tensor index_copy(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper__index_copy(self, dim, index, source);
}

at::Tensor & index_copy_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper_dimname_index_copy__dimname(self, dim, index, source);
}

at::Tensor index_copy(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper_dimname_index_copy_dimname(self, dim, index, source);
}

at::Tensor index_put(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
return wrapper__index_put(self, indices, values, accumulate);
}

at::Tensor instance_norm(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
return wrapper__instance_norm(input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}

at::Tensor isclose(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
return wrapper__isclose(self, other, rtol, atol, equal_nan);
}

bool is_distributed(const at::Tensor & self) {
return wrapper__is_distributed(self);
}

bool is_floating_point(const at::Tensor & self) {
return wrapper__is_floating_point(self);
}

bool is_complex(const at::Tensor & self) {
return wrapper__is_complex(self);
}

bool is_conj(const at::Tensor & self) {
return wrapper__is_conj(self);
}

bool is_neg(const at::Tensor & self) {
return wrapper__is_neg(self);
}

at::Tensor isreal(const at::Tensor & self) {
return wrapper__isreal(self);
}

bool is_nonzero(const at::Tensor & self) {
return wrapper__is_nonzero(self);
}

bool is_same_size(const at::Tensor & self, const at::Tensor & other) {
return wrapper__is_same_size(self, other);
}

bool is_signed(const at::Tensor & self) {
return wrapper__is_signed(self);
}

bool is_inference(const at::Tensor & self) {
return wrapper__is_inference(self);
}

at::Tensor kron(const at::Tensor & self, const at::Tensor & other) {
return wrapper__kron(self, other);
}

at::Tensor & kron_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_kron_out_out(self, other, out);
}

at::Tensor & kron_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_kron_out_out(self, other, out);
}

::std::tuple<at::Tensor,at::Tensor> kthvalue(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
return wrapper_dimname_kthvalue_dimname(self, k, dim, keepdim);
}

::std::tuple<at::Tensor &,at::Tensor &> kthvalue_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
return wrapper_dimname_out_kthvalue_out_dimname_out(self, k, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> kthvalue_outf(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_dimname_out_kthvalue_out_dimname_out(self, k, dim, keepdim, values, indices);
}

at::Tensor layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
return wrapper__layer_norm(input, normalized_shape, weight, bias, eps, cudnn_enable);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
return wrapper__native_layer_norm(input, normalized_shape, weight, bias, eps);
}

at::Tensor linear(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper__linear(input, weight, bias);
}

at::Tensor & linear_out(at::Tensor & out, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
return wrapper_out_linear_out_out(input, weight, bias, out);
}

at::Tensor & linear_outf(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
return wrapper_out_linear_out_out(input, weight, bias, out);
}

at::Tensor fbgemm_linear_int8_weight_fp32_activation(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
return wrapper__fbgemm_linear_int8_weight_fp32_activation(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

at::Tensor fbgemm_linear_int8_weight(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
return wrapper__fbgemm_linear_int8_weight(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

::std::tuple<at::Tensor,at::Tensor,double,int64_t> fbgemm_linear_quantize_weight(const at::Tensor & input) {
return wrapper__fbgemm_linear_quantize_weight(input);
}

at::Tensor fbgemm_pack_gemm_matrix_fp16(const at::Tensor & input) {
return wrapper__fbgemm_pack_gemm_matrix_fp16(input);
}

at::Tensor fbgemm_linear_fp16_weight_fp32_activation(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
return wrapper__fbgemm_linear_fp16_weight_fp32_activation(input, packed_weight, bias);
}

at::Tensor fbgemm_linear_fp16_weight(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
return wrapper__fbgemm_linear_fp16_weight(input, packed_weight, bias);
}

at::Tensor fbgemm_pack_quantized_matrix(const at::Tensor & input) {
return wrapper__fbgemm_pack_quantized_matrix(input);
}

at::Tensor fbgemm_pack_quantized_matrix(const at::Tensor & input, int64_t K, int64_t N) {
return wrapper_KN_fbgemm_pack_quantized_matrix_KN(input, K, N);
}

at::Tensor ldexp(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_ldexp_Tensor(self, other);
}

at::Tensor & ldexp_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_ldexp_out_out(self, other, out);
}

at::Tensor & ldexp_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_ldexp_out_out(self, other, out);
}

at::Tensor & ldexp_(at::Tensor & self, const at::Tensor & other) {
return wrapper__ldexp_(self, other);
}

at::Tensor linspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, at::TensorOptions options) {
return wrapper__linspace(start, end, steps, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor linspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__linspace(start, end, steps, dtype, layout, device, pin_memory);
}

at::Tensor logspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, at::TensorOptions options) {
return wrapper__logspace(start, end, steps, base, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor logspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__logspace(start, end, steps, base, dtype, layout, device, pin_memory);
}

at::Tensor log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_int_log_softmax_int(self, dim, dtype);
}

at::Tensor log_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_Dimname_log_softmax_Dimname(self, dim, dtype);
}

at::Tensor logcumsumexp(const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_logcumsumexp_dimname(self, dim);
}

at::Tensor & logcumsumexp_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_out_logcumsumexp_out_dimname_out(self, dim, out);
}

at::Tensor & logcumsumexp_outf(const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
return wrapper_dimname_out_logcumsumexp_out_dimname_out(self, dim, out);
}

at::Tensor logsumexp(const at::Tensor & self, at::DimnameList dim, bool keepdim) {
return wrapper_names_logsumexp_names(self, dim, keepdim);
}

at::Tensor & logsumexp_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool keepdim) {
return wrapper_names_out_logsumexp_out_names_out(self, dim, keepdim, out);
}

at::Tensor & logsumexp_outf(const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
return wrapper_names_out_logsumexp_out_names_out(self, dim, keepdim, out);
}

at::Tensor margin_ranking_loss(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
return wrapper__margin_ranking_loss(input1, input2, target, margin, reduction);
}

at::Tensor matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper__matmul(self, other);
}

at::Tensor & matmul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_matmul_out_out(self, other, out);
}

at::Tensor & matmul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_matmul_out_out(self, other, out);
}

at::Tensor matrix_rank(const at::Tensor & self, double tol, bool symmetric) {
return wrapper_tol_matrix_rank_tol(self, tol, symmetric);
}

at::Tensor matrix_rank(const at::Tensor & self, bool symmetric) {
return wrapper__matrix_rank(self, symmetric);
}

at::Tensor matrix_power(const at::Tensor & self, int64_t n) {
return wrapper__matrix_power(self, n);
}

at::Tensor & matrix_power_out(at::Tensor & out, const at::Tensor & self, int64_t n) {
return wrapper_out_matrix_power_out_out(self, n, out);
}

at::Tensor & matrix_power_outf(const at::Tensor & self, int64_t n, at::Tensor & out) {
return wrapper_out_matrix_power_out_out(self, n, out);
}

at::Tensor matrix_exp_backward(const at::Tensor & self, const at::Tensor & grad) {
return wrapper__matrix_exp_backward(self, grad);
}

::std::tuple<at::Tensor,at::Tensor> max(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_max_names_dim(self, dim, keepdim);
}

::std::tuple<at::Tensor &,at::Tensor &> max_out(at::Tensor & max, at::Tensor & max_values, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_max_max_out_names_dim_max(self, dim, keepdim, max, max_values);
}

::std::tuple<at::Tensor &,at::Tensor &> max_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
return wrapper_names_dim_max_max_out_names_dim_max(self, dim, keepdim, max, max_values);
}

at::Tensor value_selecting_reduction_backward(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, at::IntArrayRef sizes, bool keepdim) {
return wrapper__value_selecting_reduction_backward(grad, dim, indices, sizes, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> max_pool1d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper__max_pool1d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}

at::Tensor max_pool1d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper__max_pool1d(self, kernel_size, stride, padding, dilation, ceil_mode);
}

at::Tensor max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper__max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}

at::Tensor max_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper__max_pool3d(self, kernel_size, stride, padding, dilation, ceil_mode);
}

at::Tensor mean(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_names_dim_mean_names_dim(self, dim, keepdim, dtype);
}

at::Tensor & mean_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_names_out_mean_out_names_out(self, dim, keepdim, dtype, out);
}

at::Tensor & mean_outf(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_names_out_mean_out_names_out(self, dim, keepdim, dtype, out);
}

at::Tensor nanmean(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper__nanmean(self, dim, keepdim, dtype);
}

at::Tensor & nanmean_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_out_nanmean_out_out(self, dim, keepdim, dtype, out);
}

at::Tensor & nanmean_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_out_nanmean_out_out(self, dim, keepdim, dtype, out);
}

::std::tuple<at::Tensor,at::Tensor> median(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_median_names_dim(self, dim, keepdim);
}

::std::tuple<at::Tensor &,at::Tensor &> median_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_values_median_out_names_dim_values(self, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> median_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_names_dim_values_median_out_names_dim_values(self, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor,at::Tensor> nanmedian(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_nanmedian_names_dim(self, dim, keepdim);
}

::std::tuple<at::Tensor &,at::Tensor &> nanmedian_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_values_nanmedian_out_names_dim_values(self, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> nanmedian_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_names_dim_values_nanmedian_out_names_dim_values(self, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor,at::Tensor> min(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_min_names_dim(self, dim, keepdim);
}

::std::tuple<at::Tensor &,at::Tensor &> min_out(at::Tensor & min, at::Tensor & min_indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_names_dim_min_min_out_names_dim_min(self, dim, keepdim, min, min_indices);
}

::std::tuple<at::Tensor &,at::Tensor &> min_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
return wrapper_names_dim_min_min_out_names_dim_min(self, dim, keepdim, min, min_indices);
}

at::Tensor mkldnn_convolution_backward_input(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
return wrapper__mkldnn_convolution_backward_input(self_size, grad_output, weight, padding, stride, dilation, groups, bias_defined);
}

::std::tuple<at::Tensor,at::Tensor> mkldnn_convolution_backward_weights(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
return wrapper__mkldnn_convolution_backward_weights(weight_size, grad_output, self, padding, stride, dilation, groups, bias_defined);
}

at::Tensor _sparse_mm(const at::Tensor & sparse, const at::Tensor & dense) {
return wrapper___sparse_mm(sparse, dense);
}

::std::tuple<at::Tensor,at::Tensor> mode(const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_dimname_mode_dimname(self, dim, keepdim);
}

::std::tuple<at::Tensor &,at::Tensor &> mode_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool keepdim) {
return wrapper_dimname_out_mode_out_dimname_out(self, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> mode_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_dimname_out_mode_out_dimname_out(self, dim, keepdim, values, indices);
}

at::Tensor multiply(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_multiply_Tensor(self, other);
}

at::Tensor & multiply_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_multiply_out_out(self, other, out);
}

at::Tensor & multiply_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_multiply_out_out(self, other, out);
}

at::Tensor & multiply_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_multiply__Tensor(self, other);
}

at::Tensor multiply(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_multiply_Scalar(self, other);
}

at::Tensor & multiply_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_multiply__Scalar(self, other);
}

at::Tensor narrow(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
return wrapper__narrow(self, dim, start, length);
}

at::Tensor narrow(const at::Tensor & self, int64_t dim, const at::Tensor & start, int64_t length) {
return wrapper_Tensor_narrow_Tensor(self, dim, start, length);
}

bool is_vulkan_available() {
return wrapper__is_vulkan_available();
}

bool _nnpack_available() {
return wrapper___nnpack_available();
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> _nnpack_spatial_convolution_backward(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, ::std::array<bool,3> output_mask) {
return wrapper___nnpack_spatial_convolution_backward(input, grad_output, weight, padding, output_mask);
}

at::Tensor _nnpack_spatial_convolution_backward_input(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding) {
return wrapper___nnpack_spatial_convolution_backward_input(input, grad_output, weight, padding);
}

at::Tensor _nnpack_spatial_convolution_backward_weight(const at::Tensor & input, at::IntArrayRef weightsize, const at::Tensor & grad_output, at::IntArrayRef padding) {
return wrapper___nnpack_spatial_convolution_backward_weight(input, weightsize, grad_output, padding);
}

at::Tensor ones(at::IntArrayRef size, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_names_ones_names(size, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor ones(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_names_ones_names(size, names, dtype, layout, device, pin_memory);
}

at::Tensor ones(at::IntArrayRef size, at::TensorOptions options) {
return wrapper__ones(size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor ones(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__ones(size, dtype, layout, device, pin_memory);
}

at::Tensor & ones_out(at::Tensor & out, at::IntArrayRef size) {
return wrapper_out_ones_out_out(size, out);
}

at::Tensor & ones_outf(at::IntArrayRef size, at::Tensor & out) {
return wrapper_out_ones_out_out(size, out);
}

at::Tensor ones_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__ones_like(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor ones_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__ones_like(self, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor pairwise_distance(const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
return wrapper__pairwise_distance(x1, x2, p, eps, keepdim);
}

at::Tensor cdist(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
return wrapper__cdist(x1, x2, p, compute_mode);
}

at::Tensor pdist(const at::Tensor & self, double p) {
return wrapper__pdist(self, p);
}

at::Tensor cosine_similarity(const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
return wrapper__cosine_similarity(x1, x2, dim, eps);
}

at::Tensor movedim(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
return wrapper_intlist_movedim_intlist(self, source, destination);
}

at::Tensor movedim(const at::Tensor & self, int64_t source, int64_t destination) {
return wrapper_int_movedim_int(self, source, destination);
}

at::Tensor moveaxis(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
return wrapper_intlist_moveaxis_intlist(self, source, destination);
}

at::Tensor moveaxis(const at::Tensor & self, int64_t source, int64_t destination) {
return wrapper_int_moveaxis_int(self, source, destination);
}

at::Tensor numpy_T(const at::Tensor & self) {
return wrapper__numpy_T(self);
}

at::Tensor pixel_shuffle(const at::Tensor & self, int64_t upscale_factor) {
return wrapper__pixel_shuffle(self, upscale_factor);
}

at::Tensor pixel_unshuffle(const at::Tensor & self, int64_t downscale_factor) {
return wrapper__pixel_unshuffle(self, downscale_factor);
}

at::Tensor pin_memory(const at::Tensor & self, c10::optional<at::Device> device) {
return wrapper__pin_memory(self, device);
}

at::Tensor pinverse(const at::Tensor & self, double rcond) {
return wrapper__pinverse(self, rcond);
}

at::Tensor poisson_nll_loss(const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
return wrapper__poisson_nll_loss(input, target, log_input, full, eps, reduction);
}

at::Tensor scalar_tensor(const at::Scalar & s, at::TensorOptions options) {
return wrapper__scalar_tensor(s, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor scalar_tensor(const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__scalar_tensor(s, dtype, layout, device, pin_memory);
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_names_rand_names(size, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_names_rand_names(size, names, dtype, layout, device, pin_memory);
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_generator_with_names_rand_generator_with_names(size, generator, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_generator_with_names_rand_generator_with_names(size, generator, names, dtype, layout, device, pin_memory);
}

at::Tensor rand(at::IntArrayRef size, at::TensorOptions options) {
return wrapper__rand(size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__rand(size, dtype, layout, device, pin_memory);
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::Generator> generator, at::TensorOptions options) {
return wrapper_generator_rand_generator(size, generator, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor rand(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_generator_rand_generator(size, generator, dtype, layout, device, pin_memory);
}

at::Tensor & rand_out(at::Tensor & out, at::IntArrayRef size) {
return wrapper_out_rand_out_out(size, out);
}

at::Tensor & rand_outf(at::IntArrayRef size, at::Tensor & out) {
return wrapper_out_rand_out_out(size, out);
}

at::Tensor & rand_out(at::Tensor & out, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_generator_out_rand_out_generator_out(size, generator, out);
}

at::Tensor & rand_outf(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_generator_out_rand_out_generator_out(size, generator, out);
}

at::Tensor rand_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__rand_like(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor rand_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__rand_like(self, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor randint(int64_t high, at::IntArrayRef size, at::TensorOptions options) {
return wrapper__randint(high, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randint(int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__randint(high, size, dtype, layout, device, pin_memory);
}

at::Tensor randint(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::TensorOptions options) {
return wrapper_generator_randint_generator(high, size, generator, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randint(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_generator_randint_generator(high, size, generator, dtype, layout, device, pin_memory);
}

at::Tensor randint(int64_t low, int64_t high, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_low_randint_low(low, high, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randint(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_low_randint_low(low, high, size, dtype, layout, device, pin_memory);
}

at::Tensor randint(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::TensorOptions options) {
return wrapper_low_generator_randint_low_generator(low, high, size, generator, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randint(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_low_generator_randint_low_generator(low, high, size, generator, dtype, layout, device, pin_memory);
}

at::Tensor & randint_out(at::Tensor & out, int64_t high, at::IntArrayRef size) {
return wrapper_out_randint_out_out(high, size, out);
}

at::Tensor & randint_outf(int64_t high, at::IntArrayRef size, at::Tensor & out) {
return wrapper_out_randint_out_out(high, size, out);
}

at::Tensor & randint_out(at::Tensor & out, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_generator_out_randint_out_generator_out(high, size, generator, out);
}

at::Tensor & randint_outf(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_generator_out_randint_out_generator_out(high, size, generator, out);
}

at::Tensor & randint_out(at::Tensor & out, int64_t low, int64_t high, at::IntArrayRef size) {
return wrapper_low_out_randint_out_low_out(low, high, size, out);
}

at::Tensor & randint_outf(int64_t low, int64_t high, at::IntArrayRef size, at::Tensor & out) {
return wrapper_low_out_randint_out_low_out(low, high, size, out);
}

at::Tensor & randint_out(at::Tensor & out, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_low_generator_out_randint_out_low_generator_out(low, high, size, generator, out);
}

at::Tensor & randint_outf(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_low_generator_out_randint_out_low_generator_out(low, high, size, generator, out);
}

at::Tensor randint_like(const at::Tensor & self, int64_t high, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__randint_like(self, high, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor randint_like(const at::Tensor & self, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__randint_like(self, high, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor randint_like(const at::Tensor & self, int64_t low, int64_t high, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_low_dtype_randint_like_low_dtype(self, low, high, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor randint_like(const at::Tensor & self, int64_t low, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_low_dtype_randint_like_low_dtype(self, low, high, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor randn(at::IntArrayRef size, at::TensorOptions options) {
return wrapper__randn(size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__randn(size, dtype, layout, device, pin_memory);
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::Generator> generator, at::TensorOptions options) {
return wrapper_generator_randn_generator(size, generator, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_generator_randn_generator(size, generator, dtype, layout, device, pin_memory);
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_names_randn_names(size, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_names_randn_names(size, names, dtype, layout, device, pin_memory);
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_generator_with_names_randn_generator_with_names(size, generator, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randn(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_generator_with_names_randn_generator_with_names(size, generator, names, dtype, layout, device, pin_memory);
}

at::Tensor & randn_out(at::Tensor & out, at::IntArrayRef size) {
return wrapper_out_randn_out_out(size, out);
}

at::Tensor & randn_outf(at::IntArrayRef size, at::Tensor & out) {
return wrapper_out_randn_out_out(size, out);
}

at::Tensor & randn_out(at::Tensor & out, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_generator_out_randn_out_generator_out(size, generator, out);
}

at::Tensor & randn_outf(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_generator_out_randn_out_generator_out(size, generator, out);
}

at::Tensor randn_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__randn_like(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor randn_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__randn_like(self, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor randperm(int64_t n, at::TensorOptions options) {
return wrapper__randperm(n, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randperm(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__randperm(n, dtype, layout, device, pin_memory);
}

at::Tensor randperm(int64_t n, c10::optional<at::Generator> generator, at::TensorOptions options) {
return wrapper_generator_randperm_generator(n, generator, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor randperm(int64_t n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_generator_randperm_generator(n, generator, dtype, layout, device, pin_memory);
}

at::Tensor & randperm_out(at::Tensor & out, int64_t n) {
return wrapper_out_randperm_out_out(n, out);
}

at::Tensor & randperm_outf(int64_t n, at::Tensor & out) {
return wrapper_out_randperm_out_out(n, out);
}

at::Tensor range(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::TensorOptions options) {
return wrapper_step_range_step(start, end, step, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor range(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_step_range_step(start, end, step, dtype, layout, device, pin_memory);
}

at::Tensor range(const at::Scalar & start, const at::Scalar & end, at::TensorOptions options) {
return wrapper__range(start, end, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor range(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__range(start, end, dtype, layout, device, pin_memory);
}

at::Tensor ravel(const at::Tensor & self) {
return wrapper__ravel(self);
}

at::Tensor negative(const at::Tensor & self) {
return wrapper__negative(self);
}

at::Tensor & negative_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_negative_out_out(self, out);
}

at::Tensor & negative_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_negative_out_out(self, out);
}

at::Tensor & negative_(at::Tensor & self) {
return wrapper__negative_(self);
}

at::Tensor repeat_interleave(const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
return wrapper_self_Tensor_repeat_interleave_self_Tensor(self, repeats, dim, output_size);
}

at::Tensor repeat_interleave(const at::Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
return wrapper_self_int_repeat_interleave_self_int(self, repeats, dim, output_size);
}

at::Tensor reshape(const at::Tensor & self, at::IntArrayRef shape) {
return wrapper__reshape(self, shape);
}

at::Tensor reshape_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper__reshape_as(self, other);
}

at::Tensor rrelu(const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
return wrapper__rrelu(self, lower, upper, training, generator);
}

at::Tensor & rrelu_(at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
return wrapper__rrelu_(self, lower, upper, training, generator);
}

at::Tensor relu6(const at::Tensor & self) {
return wrapper__relu6(self);
}

at::Tensor & relu6_(at::Tensor & self) {
return wrapper__relu6_(self);
}

at::Tensor infinitely_differentiable_gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
return wrapper__infinitely_differentiable_gelu_backward(grad, self);
}

at::Tensor select(const at::Tensor & self, at::Dimname dim, int64_t index) {
return wrapper_Dimname_select_Dimname(self, dim, index);
}

at::Tensor selu(const at::Tensor & self) {
return wrapper__selu(self);
}

at::Tensor & selu_(at::Tensor & self) {
return wrapper__selu_(self);
}

at::Tensor silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper__silu_backward(grad_output, self);
}

at::Tensor mish_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper__mish_backward(grad_output, self);
}

int64_t size(const at::Tensor & self, int64_t dim) {
return wrapper_int_size_int(self, dim);
}

int64_t size(const at::Tensor & self, at::Dimname dim) {
return wrapper_Dimname_size_Dimname(self, dim);
}

at::Tensor smm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper__smm(self, mat2);
}

at::Tensor softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_int_softmax_int(self, dim, dtype);
}

at::Tensor softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_Dimname_softmax_Dimname(self, dim, dtype);
}

::std::vector<at::Tensor> hsplit(const at::Tensor & self, int64_t sections) {
return wrapper_int_hsplit_int(self, sections);
}

::std::vector<at::Tensor> hsplit(const at::Tensor & self, at::IntArrayRef indices) {
return wrapper_array_hsplit_array(self, indices);
}

::std::vector<at::Tensor> vsplit(const at::Tensor & self, int64_t sections) {
return wrapper_int_vsplit_int(self, sections);
}

::std::vector<at::Tensor> vsplit(const at::Tensor & self, at::IntArrayRef indices) {
return wrapper_array_vsplit_array(self, indices);
}

::std::vector<at::Tensor> dsplit(const at::Tensor & self, int64_t sections) {
return wrapper_int_dsplit_int(self, sections);
}

::std::vector<at::Tensor> dsplit(const at::Tensor & self, at::IntArrayRef indices) {
return wrapper_array_dsplit_array(self, indices);
}

at::Tensor squeeze(const at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_squeeze_dimname(self, dim);
}

at::Tensor & squeeze_(at::Tensor & self, at::Dimname dim) {
return wrapper_dimname_squeeze__dimname(self, dim);
}

at::Tensor sspaddmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper__sspaddmm(self, mat1, mat2, beta, alpha);
}

at::Tensor hstack(at::TensorList tensors) {
return wrapper__hstack(tensors);
}

at::Tensor & hstack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_out_hstack_out_out(tensors, out);
}

at::Tensor & hstack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_out_hstack_out_out(tensors, out);
}

at::Tensor vstack(at::TensorList tensors) {
return wrapper__vstack(tensors);
}

at::Tensor & vstack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_out_vstack_out_out(tensors, out);
}

at::Tensor & vstack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_out_vstack_out_out(tensors, out);
}

at::Tensor dstack(at::TensorList tensors) {
return wrapper__dstack(tensors);
}

at::Tensor & dstack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_out_dstack_out_out(tensors, out);
}

at::Tensor & dstack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_out_dstack_out_out(tensors, out);
}

at::Tensor stft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
return wrapper__stft(self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}

at::Tensor istft(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
return wrapper__istft(self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
}

int64_t stride(const at::Tensor & self, int64_t dim) {
return wrapper_int_stride_int(self, dim);
}

int64_t stride(const at::Tensor & self, at::Dimname dim) {
return wrapper_Dimname_stride_Dimname(self, dim);
}

at::Tensor sum(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_dim_DimnameList_sum_dim_DimnameList(self, dim, keepdim, dtype);
}

at::Tensor & sum_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_DimnameList_out_sum_out_DimnameList_out(self, dim, keepdim, dtype, out);
}

at::Tensor & sum_outf(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_DimnameList_out_sum_out_DimnameList_out(self, dim, keepdim, dtype, out);
}

at::Tensor sum_to_size(const at::Tensor & self, at::IntArrayRef size) {
return wrapper__sum_to_size(self, size);
}

at::Tensor square(const at::Tensor & self) {
return wrapper__square(self);
}

at::Tensor & square_(at::Tensor & self) {
return wrapper__square_(self);
}

at::Tensor std(const at::Tensor & self, bool unbiased) {
return wrapper__std(self, unbiased);
}

at::Tensor std(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_dim_std_dim(self, dim, unbiased, keepdim);
}

at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_out_std_out_out(self, dim, unbiased, keepdim, out);
}

at::Tensor & std_outf(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_out_std_out_out(self, dim, unbiased, keepdim, out);
}

::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, bool unbiased) {
return wrapper__std_mean(self, unbiased);
}

::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_dim_std_mean_dim(self, dim, unbiased, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_names_dim_std_mean_names_dim(self, dim, unbiased, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> std_mean(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
return wrapper_correction_names_std_mean_correction_names(self, dim, correction, keepdim);
}

at::Tensor std(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_names_dim_std_names_dim(self, dim, unbiased, keepdim);
}

at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_names_out_std_out_names_out(self, dim, unbiased, keepdim, out);
}

at::Tensor & std_outf(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_names_out_std_out_names_out(self, dim, unbiased, keepdim, out);
}

at::Tensor std(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
return wrapper_correction_names_std_correction_names(self, dim, correction, keepdim);
}

at::Tensor & std_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
return wrapper_correction_names_out_std_out_correction_names_out(self, dim, correction, keepdim, out);
}

at::Tensor & std_outf(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
return wrapper_correction_names_out_std_out_correction_names_out(self, dim, correction, keepdim, out);
}

at::Tensor prod(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_dim_Dimname_prod_dim_Dimname(self, dim, keepdim, dtype);
}

at::Tensor & prod_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_Dimname_out_prod_out_Dimname_out(self, dim, keepdim, dtype, out);
}

at::Tensor & prod_outf(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_Dimname_out_prod_out_Dimname_out(self, dim, keepdim, dtype, out);
}

at::Tensor tensordot(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
return wrapper__tensordot(self, other, dims_self, dims_other);
}

at::Tensor tile(const at::Tensor & self, at::IntArrayRef dims) {
return wrapper__tile(self, dims);
}

at::Tensor transpose(const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
return wrapper_Dimname_transpose_Dimname(self, dim0, dim1);
}

at::Tensor one_hot(const at::Tensor & self, int64_t num_classes) {
return wrapper__one_hot(self, num_classes);
}

at::Tensor fliplr(const at::Tensor & self) {
return wrapper__fliplr(self);
}

at::Tensor flipud(const at::Tensor & self) {
return wrapper__flipud(self);
}

at::Tensor trapezoid(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
return wrapper_x_trapezoid_x(y, x, dim);
}

at::Tensor trapezoid(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
return wrapper_dx_trapezoid_dx(y, dx, dim);
}

at::Tensor trapz(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
return wrapper_x_trapz_x(y, x, dim);
}

at::Tensor trapz(const at::Tensor & y, double dx, int64_t dim) {
return wrapper_dx_trapz_dx(y, dx, dim);
}

at::Tensor triplet_margin_loss(const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
return wrapper__triplet_margin_loss(anchor, positive, negative, margin, p, eps, swap, reduction);
}

at::Tensor fix(const at::Tensor & self) {
return wrapper__fix(self);
}

at::Tensor & fix_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_fix_out_out(self, out);
}

at::Tensor & fix_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_fix_out_out(self, out);
}

at::Tensor & fix_(at::Tensor & self) {
return wrapper__fix_(self);
}

at::Tensor type_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper__type_as(self, other);
}

bool _has_compatible_shallow_copy_type(const at::Tensor & self, const at::Tensor & from) {
return wrapper___has_compatible_shallow_copy_type(self, from);
}

at::Tensor vander(const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
return wrapper__vander(x, N, increasing);
}

at::Tensor var(const at::Tensor & self, bool unbiased) {
return wrapper__var(self, unbiased);
}

at::Tensor var(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_dim_var_dim(self, dim, unbiased, keepdim);
}

at::Tensor & var_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_out_var_out_out(self, dim, unbiased, keepdim, out);
}

at::Tensor & var_outf(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_out_var_out_out(self, dim, unbiased, keepdim, out);
}

at::Tensor var(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_names_dim_var_names_dim(self, dim, unbiased, keepdim);
}

at::Tensor & var_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_names_out_var_out_names_out(self, dim, unbiased, keepdim, out);
}

at::Tensor & var_outf(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
return wrapper_names_out_var_out_names_out(self, dim, unbiased, keepdim, out);
}

at::Tensor var(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
return wrapper_correction_names_var_correction_names(self, dim, correction, keepdim);
}

at::Tensor & var_out(at::Tensor & out, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
return wrapper_correction_names_out_var_out_correction_names_out(self, dim, correction, keepdim, out);
}

at::Tensor & var_outf(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
return wrapper_correction_names_out_var_out_correction_names_out(self, dim, correction, keepdim, out);
}

::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, bool unbiased) {
return wrapper__var_mean(self, unbiased);
}

::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
return wrapper_dim_var_mean_dim(self, dim, unbiased, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
return wrapper_names_dim_var_mean_names_dim(self, dim, unbiased, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> var_mean(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
return wrapper_correction_names_var_mean_correction_names(self, dim, correction, keepdim);
}

at::Tensor view_as(const at::Tensor & self, const at::Tensor & other) {
return wrapper__view_as(self, other);
}

at::Tensor where(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
return wrapper_self_where_self(condition, self, other);
}

at::Tensor where(const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
return wrapper_ScalarSelf_where_ScalarSelf(condition, self, other);
}

at::Tensor where(const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
return wrapper_ScalarOther_where_ScalarOther(condition, self, other);
}

at::Tensor where(const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
return wrapper_Scalar_where_Scalar(condition, self, other);
}

::std::vector<at::Tensor> where(const at::Tensor & condition) {
return wrapper__where(condition);
}

at::Tensor norm_except_dim(const at::Tensor & v, int64_t pow, int64_t dim) {
return wrapper__norm_except_dim(v, pow, dim);
}

at::Tensor _weight_norm(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
return wrapper___weight_norm(v, g, dim);
}

::std::tuple<at::Tensor,at::Tensor> _weight_norm_differentiable_backward(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
return wrapper___weight_norm_differentiable_backward(grad_w, saved_v, saved_g, saved_norms, dim);
}

at::Tensor zeros(at::IntArrayRef size, c10::optional<at::DimnameList> names, at::TensorOptions options) {
return wrapper_names_zeros_names(size, names, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor zeros(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_names_zeros_names(size, names, dtype, layout, device, pin_memory);
}

at::Tensor zeros(at::IntArrayRef size, at::TensorOptions options) {
return wrapper__zeros(size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor zeros(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__zeros(size, dtype, layout, device, pin_memory);
}

at::Tensor & zeros_out(at::Tensor & out, at::IntArrayRef size) {
return wrapper_out_zeros_out_out(size, out);
}

at::Tensor & zeros_outf(at::IntArrayRef size, at::Tensor & out) {
return wrapper_out_zeros_out_out(size, out);
}

at::Tensor zeros_like(const at::Tensor & self, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__zeros_like(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor zeros_like(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__zeros_like(self, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor _sparse_sum(const at::Tensor & self) {
return wrapper___sparse_sum(self);
}

at::Tensor _sparse_sum(const at::Tensor & self, at::ScalarType dtype) {
return wrapper_dtype__sparse_sum_dtype(self, dtype);
}

at::Tensor _sparse_sum(const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
return wrapper_dim_dtype__sparse_sum_dim_dtype(self, dim, dtype);
}

at::Tensor _sparse_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_int__sparse_softmax_int(self, dim, dtype);
}

at::Tensor _sparse_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_Dimname__sparse_softmax_Dimname(self, dim, dtype);
}

at::Tensor _sparse_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_int__sparse_log_softmax_int(self, dim, dtype);
}

at::Tensor _sparse_log_softmax(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
return wrapper_Dimname__sparse_log_softmax_Dimname(self, dim, dtype);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
return wrapper_names_ScalarOpt_dim_dtype_norm_names_ScalarOpt_dim_dtype(self, p, dim, keepdim, dtype);
}

at::Tensor & norm_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
return wrapper_names_dtype_out_norm_out_names_dtype_out(self, p, dim, keepdim, dtype, out);
}

at::Tensor & norm_outf(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
return wrapper_names_dtype_out_norm_out_names_dtype_out(self, p, dim, keepdim, dtype, out);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
return wrapper_names_ScalarOpt_dim_norm_names_ScalarOpt_dim(self, p, dim, keepdim);
}

at::Tensor & norm_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
return wrapper_names_out_norm_out_names_out(self, p, dim, keepdim, out);
}

at::Tensor & norm_outf(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
return wrapper_names_out_norm_out_names_out(self, p, dim, keepdim, out);
}

at::Tensor frobenius_norm(const at::Tensor & self) {
return wrapper__frobenius_norm(self);
}

at::Tensor frobenius_norm(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_dim_frobenius_norm_dim(self, dim, keepdim);
}

at::Tensor & frobenius_norm_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_out_frobenius_norm_out_out(self, dim, keepdim, out);
}

at::Tensor & frobenius_norm_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_out_frobenius_norm_out_out(self, dim, keepdim, out);
}

at::Tensor nuclear_norm(const at::Tensor & self, bool keepdim) {
return wrapper__nuclear_norm(self, keepdim);
}

at::Tensor & nuclear_norm_out(at::Tensor & out, const at::Tensor & self, bool keepdim) {
return wrapper_out_nuclear_norm_out_out(self, keepdim, out);
}

at::Tensor & nuclear_norm_outf(const at::Tensor & self, bool keepdim, at::Tensor & out) {
return wrapper_out_nuclear_norm_out_out(self, keepdim, out);
}

at::Tensor nuclear_norm(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_dim_nuclear_norm_dim(self, dim, keepdim);
}

at::Tensor & nuclear_norm_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_dim_out_nuclear_norm_out_dim_out(self, dim, keepdim, out);
}

at::Tensor & nuclear_norm_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_dim_out_nuclear_norm_out_dim_out(self, dim, keepdim, out);
}

at::Tensor positive(const at::Tensor & self) {
return wrapper__positive(self);
}

at::Tensor subtract(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_subtract_Tensor(self, other, alpha);
}

at::Tensor & subtract_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_out_subtract_out_out(self, other, alpha, out);
}

at::Tensor & subtract_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_subtract_out_out(self, other, alpha, out);
}

at::Tensor & subtract_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_subtract__Tensor(self, other, alpha);
}

at::Tensor subtract(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_subtract_Scalar(self, other, alpha);
}

at::Tensor & subtract_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_subtract__Scalar(self, other, alpha);
}

at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_crow_col_value_size_sparse_csr_tensor_crow_col_value_size(crow_indices, col_indices, values, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_crow_col_value_size_sparse_csr_tensor_crow_col_value_size(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper_crow_col_value_sparse_csr_tensor_crow_col_value(crow_indices, col_indices, values, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor sparse_csr_tensor(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_crow_col_value_sparse_csr_tensor_crow_col_value(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}

at::Tensor _sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper___sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor _sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper___sparse_csr_tensor_unsafe(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

at::Tensor sparse_coo_tensor(at::IntArrayRef size, at::TensorOptions options) {
return wrapper_size_sparse_coo_tensor_size(size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor sparse_coo_tensor(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_size_sparse_coo_tensor_size(size, dtype, layout, device, pin_memory);
}

at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper_indices_sparse_coo_tensor_indices(indices, values, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_indices_sparse_coo_tensor_indices(indices, values, dtype, layout, device, pin_memory);
}

at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper_indices_size_sparse_coo_tensor_indices_size(indices, values, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor sparse_coo_tensor(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_indices_size_sparse_coo_tensor_indices_size(indices, values, size, dtype, layout, device, pin_memory);
}

at::Tensor _sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, at::TensorOptions options) {
return wrapper___sparse_coo_tensor_unsafe(indices, values, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor _sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper___sparse_coo_tensor_unsafe(indices, values, size, dtype, layout, device, pin_memory);
}

void _validate_sparse_coo_tensor_args(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size) {
return wrapper___validate_sparse_coo_tensor_args(indices, values, size);
}

void _validate_sparse_csr_tensor_args(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
return wrapper___validate_sparse_csr_tensor_args(crow_indices, col_indices, values, size);
}

::std::vector<at::Tensor> _to_cpu(at::TensorList tensors) {
return wrapper___to_cpu(tensors);
}

at::Tensor to_dense_backward(const at::Tensor & grad, const at::Tensor & input) {
return wrapper__to_dense_backward(grad, input);
}

at::Tensor coalesce(const at::Tensor & self) {
return wrapper__coalesce(self);
}

::std::vector<at::Tensor> unbind(const at::Tensor & self, at::Dimname dim) {
return wrapper_Dimname_unbind_Dimname(self, dim);
}

at::Tensor to_mkldnn_backward(const at::Tensor & grad, const at::Tensor & input) {
return wrapper__to_mkldnn_backward(grad, input);
}

at::Tensor fake_quantize_per_tensor_affine(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
return wrapper__fake_quantize_per_tensor_affine(self, scale, zero_point, quant_min, quant_max);
}

at::Tensor fake_quantize_per_tensor_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
return wrapper_tensor_qparams_fake_quantize_per_tensor_affine_tensor_qparams(self, scale, zero_point, quant_min, quant_max);
}

at::Tensor fake_quantize_per_tensor_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
return wrapper__fake_quantize_per_tensor_affine_cachemask_backward(grad, mask);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_tensor_affine_backward(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
return wrapper___fake_quantize_learnable_per_tensor_affine_backward(grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

at::Tensor fake_quantize_per_channel_affine(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
return wrapper__fake_quantize_per_channel_affine(self, scale, zero_point, axis, quant_min, quant_max);
}

at::Tensor fake_quantize_per_channel_affine_cachemask_backward(const at::Tensor & grad, const at::Tensor & mask) {
return wrapper__fake_quantize_per_channel_affine_cachemask_backward(grad, mask);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_channel_affine_backward(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
return wrapper___fake_quantize_learnable_per_channel_affine_backward(grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

at::Tensor fused_moving_avg_obs_fake_quant(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
return wrapper__fused_moving_avg_obs_fake_quant(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

::std::tuple<double,int64_t> _choose_qparams_per_tensor(const at::Tensor & self, bool reduce_range) {
return wrapper___choose_qparams_per_tensor(self, reduce_range);
}

at::Tensor _saturate_weight_to_fp16(const at::Tensor & weight) {
return wrapper___saturate_weight_to_fp16(weight);
}

::std::tuple<at::Tensor,at::Tensor> choose_qparams_optimized(const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
return wrapper__choose_qparams_optimized(input, numel, n_bins, ratio, bit_width);
}

at::Tensor to(const at::Tensor & self, at::TensorOptions options, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_dtype_layout_to_dtype_layout(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), non_blocking, copy, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor to(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_dtype_layout_to_dtype_layout(self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}

at::Tensor to(const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_device_to_device(self, device, dtype, non_blocking, copy, memory_format);
}

at::Tensor to(const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_dtype_to_dtype(self, dtype, non_blocking, copy, memory_format);
}

at::Tensor to(const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_other_to_other(self, other, non_blocking, copy, memory_format);
}

::std::vector<at::Tensor> meshgrid(at::TensorList tensors) {
return wrapper__meshgrid(tensors);
}

::std::vector<at::Tensor> meshgrid(at::TensorList tensors, c10::string_view indexing) {
return wrapper_indexing_meshgrid_indexing(tensors, indexing);
}

at::Tensor cartesian_prod(at::TensorList tensors) {
return wrapper__cartesian_prod(tensors);
}

at::Tensor combinations(const at::Tensor & self, int64_t r, bool with_replacement) {
return wrapper__combinations(self, r, with_replacement);
}

at::Scalar item(const at::Tensor & self) {
return wrapper__item(self);
}

at::ScalarType result_type(const at::Tensor & tensor, const at::Tensor & other) {
return wrapper_Tensor_result_type_Tensor(tensor, other);
}

at::ScalarType result_type(const at::Tensor & tensor, const at::Scalar & other) {
return wrapper_Scalar_result_type_Scalar(tensor, other);
}

at::ScalarType result_type(const at::Scalar & scalar, const at::Tensor & tensor) {
return wrapper_Scalar_Tensor_result_type_Scalar_Tensor(scalar, tensor);
}

at::ScalarType result_type(const at::Scalar & scalar1, const at::Scalar & scalar2) {
return wrapper_Scalar_Scalar_result_type_Scalar_Scalar(scalar1, scalar2);
}

bool can_cast(at::ScalarType from, at::ScalarType to) {
return wrapper__can_cast(from, to);
}

at::ScalarType promote_types(at::ScalarType type1, at::ScalarType type2) {
return wrapper__promote_types(type1, type2);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_lstm_cell_backward(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
return wrapper___thnn_differentiable_lstm_cell_backward(grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_gru_cell_backward(const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
return wrapper___thnn_differentiable_gru_cell_backward(grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_input_lstm_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm(const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_data_lstm_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

::std::tuple<at::Tensor,at::Tensor> gru(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_input_gru_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

::std::tuple<at::Tensor,at::Tensor> gru(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_data_gru_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

::std::tuple<at::Tensor,at::Tensor> rnn_tanh(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_input_rnn_tanh_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

::std::tuple<at::Tensor,at::Tensor> rnn_tanh(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_data_rnn_tanh_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

::std::tuple<at::Tensor,at::Tensor> rnn_relu(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
return wrapper_input_rnn_relu_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

::std::tuple<at::Tensor,at::Tensor> rnn_relu(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
return wrapper_data_rnn_relu_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

::std::tuple<at::Tensor,at::Tensor> lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper__lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

at::Tensor gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper__gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

at::Tensor rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper__rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

at::Tensor rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
return wrapper__rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh);
}

::std::tuple<at::Tensor,at::Tensor> quantized_lstm_cell(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper__quantized_lstm_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

at::Tensor quantized_gru_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper__quantized_gru_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

at::Tensor quantized_rnn_relu_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper__quantized_rnn_relu_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

at::Tensor quantized_rnn_tanh_cell(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
return wrapper__quantized_rnn_tanh_cell(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

at::Tensor _pack_padded_sequence_backward(const at::Tensor & grad, at::IntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
return wrapper___pack_padded_sequence_backward(grad, input_size, batch_sizes, batch_first);
}

::std::tuple<at::Tensor,at::Tensor> _pad_packed_sequence(const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
return wrapper___pad_packed_sequence(data, batch_sizes, batch_first, padding_value, total_length);
}

at::Tensor masked_fill(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
return wrapper_Scalar_masked_fill_Scalar(self, mask, value);
}

at::Tensor masked_fill(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
return wrapper_Tensor_masked_fill_Tensor(self, mask, value);
}

at::Tensor masked_scatter(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
return wrapper__masked_scatter(self, mask, source);
}

at::Tensor put(const at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
return wrapper__put(self, index, source, accumulate);
}

at::Tensor & index_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper__index_add_(self, dim, index, source);
}

at::Tensor index_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper__index_add(self, dim, index, source);
}

at::Tensor index_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
return wrapper_alpha_index_add_alpha(self, dim, index, source, alpha);
}

at::Tensor index_add(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
return wrapper_dimname_index_add_dimname(self, dim, index, source, alpha);
}

at::Tensor index_fill(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_int_Scalar_index_fill_int_Scalar(self, dim, index, value);
}

at::Tensor index_fill(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
return wrapper_int_Tensor_index_fill_int_Tensor(self, dim, index, value);
}

at::Tensor & index_fill_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_Dimname_Scalar_index_fill__Dimname_Scalar(self, dim, index, value);
}

at::Tensor index_fill(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_Dimname_Scalar_index_fill_Dimname_Scalar(self, dim, index, value);
}

at::Tensor & index_fill_(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
return wrapper_Dimname_Tensor_index_fill__Dimname_Tensor(self, dim, index, value);
}

at::Tensor index_fill(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
return wrapper_Dimname_Tensor_index_fill_Dimname_Tensor(self, dim, index, value);
}

at::Tensor scatter(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_dimname_src_scatter_dimname_src(self, dim, index, src);
}

at::Tensor scatter(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_dimname_value_scatter_dimname_value(self, dim, index, value);
}

at::Tensor scatter_add(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_dimname_scatter_add_dimname(self, dim, index, src);
}

at::Tensor & bitwise_and_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_bitwise_and__Scalar(self, other);
}

at::Tensor __and__(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar___and___Scalar(self, other);
}

at::Tensor & __iand__(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar___iand___Scalar(self, other);
}

at::Tensor __and__(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor___and___Tensor(self, other);
}

at::Tensor & __iand__(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor___iand___Tensor(self, other);
}

at::Tensor bitwise_or(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_bitwise_or_Scalar(self, other);
}

at::Tensor & bitwise_or_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_bitwise_or__Scalar(self, other);
}

at::Tensor __or__(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar___or___Scalar(self, other);
}

at::Tensor & __ior__(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar___ior___Scalar(self, other);
}

at::Tensor __or__(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor___or___Tensor(self, other);
}

at::Tensor & __ior__(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor___ior___Tensor(self, other);
}

at::Tensor bitwise_xor(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_bitwise_xor_Scalar(self, other);
}

at::Tensor & bitwise_xor_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_bitwise_xor__Scalar(self, other);
}

at::Tensor __xor__(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar___xor___Scalar(self, other);
}

at::Tensor & __ixor__(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar___ixor___Scalar(self, other);
}

at::Tensor __xor__(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor___xor___Tensor(self, other);
}

at::Tensor & __ixor__(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor___ixor___Tensor(self, other);
}

at::Tensor diag_backward(const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t diagonal) {
return wrapper__diag_backward(grad, input_sizes, diagonal);
}

at::Tensor trace_backward(const at::Tensor & grad, at::IntArrayRef sizes) {
return wrapper__trace_backward(grad, sizes);
}

at::Tensor not_equal(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_not_equal_Scalar(self, other);
}

at::Tensor & not_equal_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_not_equal_out_Scalar_out(self, other, out);
}

at::Tensor & not_equal_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_not_equal_out_Scalar_out(self, other, out);
}

at::Tensor & not_equal_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_not_equal__Scalar(self, other);
}

at::Tensor not_equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_not_equal_Tensor(self, other);
}

at::Tensor & not_equal_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_not_equal_out_Tensor_out(self, other, out);
}

at::Tensor & not_equal_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_not_equal_out_Tensor_out(self, other, out);
}

at::Tensor & not_equal_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_not_equal__Tensor(self, other);
}

at::Tensor greater_equal(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_greater_equal_Scalar(self, other);
}

at::Tensor & greater_equal_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_greater_equal_out_Scalar_out(self, other, out);
}

at::Tensor & greater_equal_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_greater_equal_out_Scalar_out(self, other, out);
}

at::Tensor & greater_equal_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_greater_equal__Scalar(self, other);
}

at::Tensor greater_equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_greater_equal_Tensor(self, other);
}

at::Tensor & greater_equal_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_greater_equal_out_Tensor_out(self, other, out);
}

at::Tensor & greater_equal_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_greater_equal_out_Tensor_out(self, other, out);
}

at::Tensor & greater_equal_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_greater_equal__Tensor(self, other);
}

at::Tensor less_equal(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_less_equal_Scalar(self, other);
}

at::Tensor & less_equal_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_less_equal_out_Scalar_out(self, other, out);
}

at::Tensor & less_equal_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_less_equal_out_Scalar_out(self, other, out);
}

at::Tensor & less_equal_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_less_equal__Scalar(self, other);
}

at::Tensor less_equal(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_less_equal_Tensor(self, other);
}

at::Tensor & less_equal_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_less_equal_out_Tensor_out(self, other, out);
}

at::Tensor & less_equal_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_less_equal_out_Tensor_out(self, other, out);
}

at::Tensor & less_equal_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_less_equal__Tensor(self, other);
}

at::Tensor greater(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_greater_Scalar(self, other);
}

at::Tensor & greater_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_greater_out_Scalar_out(self, other, out);
}

at::Tensor & greater_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_greater_out_Scalar_out(self, other, out);
}

at::Tensor & greater_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_greater__Scalar(self, other);
}

at::Tensor greater(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_greater_Tensor(self, other);
}

at::Tensor & greater_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_greater_out_Tensor_out(self, other, out);
}

at::Tensor & greater_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_greater_out_Tensor_out(self, other, out);
}

at::Tensor & greater_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_greater__Tensor(self, other);
}

at::Tensor less(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_less_Scalar(self, other);
}

at::Tensor & less_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_less_out_Scalar_out(self, other, out);
}

at::Tensor & less_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_less_out_Scalar_out(self, other, out);
}

at::Tensor & less_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_less__Scalar(self, other);
}

at::Tensor less(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_less_Tensor(self, other);
}

at::Tensor & less_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_out_less_out_Tensor_out(self, other, out);
}

at::Tensor & less_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_Tensor_out_less_out_Tensor_out(self, other, out);
}

at::Tensor & less_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_less__Tensor(self, other);
}

at::Tensor take_along_dim(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
return wrapper__take_along_dim(self, indices, dim);
}

at::Tensor & take_along_dim_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
return wrapper_out_take_along_dim_out_out(self, indices, dim, out);
}

at::Tensor & take_along_dim_outf(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
return wrapper_out_take_along_dim_out_out(self, indices, dim, out);
}

at::Tensor index_select(const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
return wrapper_dimname_index_select_dimname(self, dim, index);
}

at::Tensor & index_select_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
return wrapper_dimname_out_index_select_out_dimname_out(self, dim, index, out);
}

at::Tensor & index_select_outf(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
return wrapper_dimname_out_index_select_out_dimname_out(self, dim, index, out);
}

at::Tensor index_select_backward(const at::Tensor & grad, at::IntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
return wrapper__index_select_backward(grad, self_sizes, dim, index);
}

at::Tensor masked_select_backward(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
return wrapper__masked_select_backward(grad, input, mask);
}

::std::vector<at::Tensor> nonzero_numpy(const at::Tensor & self) {
return wrapper__nonzero_numpy(self);
}

at::Tensor gather_backward(const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
return wrapper__gather_backward(grad, self, dim, index, sparse_grad);
}

at::Tensor gather(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
return wrapper_dimname_gather_dimname(self, dim, index, sparse_grad);
}

at::Tensor & gather_out(at::Tensor & out, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
return wrapper_dimname_out_gather_out_dimname_out(self, dim, index, sparse_grad, out);
}

at::Tensor & gather_outf(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
return wrapper_dimname_out_gather_out_dimname_out(self, dim, index, sparse_grad, out);
}

at::Tensor _gather_sparse_backward(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
return wrapper___gather_sparse_backward(self, dim, index, grad);
}

at::Tensor cross_entropy_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, double label_smoothing) {
return wrapper__cross_entropy_loss(self, target, weight, reduction, ignore_index, label_smoothing);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> svd(const at::Tensor & self, bool some, bool compute_uv) {
return wrapper__svd(self, some, compute_uv);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_out(at::Tensor & U, at::Tensor & S, at::Tensor & V, const at::Tensor & self, bool some, bool compute_uv) {
return wrapper_U_svd_out_U(self, some, compute_uv, U, S, V);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_outf(const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
return wrapper_U_svd_out_U(self, some, compute_uv, U, S, V);
}

at::Tensor swapaxes(const at::Tensor & self, int64_t axis0, int64_t axis1) {
return wrapper__swapaxes(self, axis0, axis1);
}

at::Tensor & swapaxes_(at::Tensor & self, int64_t axis0, int64_t axis1) {
return wrapper__swapaxes_(self, axis0, axis1);
}

at::Tensor swapdims(const at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper__swapdims(self, dim0, dim1);
}

at::Tensor & swapdims_(at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper__swapdims_(self, dim0, dim1);
}

::std::tuple<at::Tensor,at::Tensor> qr(const at::Tensor & self, bool some) {
return wrapper__qr(self, some);
}

::std::tuple<at::Tensor &,at::Tensor &> qr_out(at::Tensor & Q, at::Tensor & R, const at::Tensor & self, bool some) {
return wrapper_Q_qr_out_Q(self, some, Q, R);
}

::std::tuple<at::Tensor &,at::Tensor &> qr_outf(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
return wrapper_Q_qr_out_Q(self, some, Q, R);
}

at::Tensor orgqr(const at::Tensor & self, const at::Tensor & input2) {
return wrapper__orgqr(self, input2);
}

at::Tensor & orgqr_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & input2) {
return wrapper_out_orgqr_out_out(self, input2, out);
}

at::Tensor & orgqr_outf(const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
return wrapper_out_orgqr_out_out(self, input2, out);
}

at::Tensor max(const at::Tensor & self, const at::Tensor & other) {
return wrapper_other_max_other(self, other);
}

at::Tensor & max_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_max_out_out(self, other, out);
}

at::Tensor & max_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_max_out_out(self, other, out);
}

at::Tensor min(const at::Tensor & self, const at::Tensor & other) {
return wrapper_other_min_other(self, other);
}

at::Tensor & min_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_min_out_out(self, other, out);
}

at::Tensor & min_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_min_out_out(self, other, out);
}

at::Tensor quantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_scalar_quantile_scalar(self, q, dim, keepdim);
}

at::Tensor & quantile_out(at::Tensor & out, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_scalar_out_quantile_out_scalar_out(self, q, dim, keepdim, out);
}

at::Tensor & quantile_outf(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
return wrapper_scalar_out_quantile_out_scalar_out(self, q, dim, keepdim, out);
}

at::Tensor quantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper__quantile(self, q, dim, keepdim);
}

at::Tensor & quantile_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_out_quantile_out_out(self, q, dim, keepdim, out);
}

at::Tensor & quantile_outf(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
return wrapper_out_quantile_out_out(self, q, dim, keepdim, out);
}

at::Tensor nanquantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_scalar_nanquantile_scalar(self, q, dim, keepdim);
}

at::Tensor & nanquantile_out(at::Tensor & out, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_scalar_out_nanquantile_out_scalar_out(self, q, dim, keepdim, out);
}

at::Tensor & nanquantile_outf(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
return wrapper_scalar_out_nanquantile_out_scalar_out(self, q, dim, keepdim, out);
}

at::Tensor nanquantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper__nanquantile(self, q, dim, keepdim);
}

at::Tensor & nanquantile_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_out_nanquantile_out_out(self, q, dim, keepdim, out);
}

at::Tensor & nanquantile_outf(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
return wrapper_out_nanquantile_out_out(self, q, dim, keepdim, out);
}

at::Tensor quantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_scalar_quantile_new_scalar(self, q, dim, keepdim, interpolation);
}

at::Tensor & quantile_out(at::Tensor & out, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_scalar_out_quantile_out_new_scalar_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor & quantile_outf(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_new_scalar_out_quantile_out_new_scalar_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor quantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_quantile_new(self, q, dim, keepdim, interpolation);
}

at::Tensor & quantile_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_out_quantile_out_new_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor & quantile_outf(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_new_out_quantile_out_new_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor nanquantile(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_scalar_nanquantile_new_scalar(self, q, dim, keepdim, interpolation);
}

at::Tensor & nanquantile_out(at::Tensor & out, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_scalar_out_nanquantile_out_new_scalar_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor & nanquantile_outf(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_new_scalar_out_nanquantile_out_new_scalar_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor nanquantile(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_nanquantile_new(self, q, dim, keepdim, interpolation);
}

at::Tensor & nanquantile_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
return wrapper_new_out_nanquantile_out_new_out(self, q, dim, keepdim, interpolation, out);
}

at::Tensor & nanquantile_outf(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
return wrapper_new_out_nanquantile_out_new_out(self, q, dim, keepdim, interpolation, out);
}

::std::tuple<at::Tensor,at::Tensor> sort(const at::Tensor & self, at::Dimname dim, bool descending) {
return wrapper_dimname_sort_dimname(self, dim, descending);
}

::std::tuple<at::Tensor &,at::Tensor &> sort_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, at::Dimname dim, bool descending) {
return wrapper_dimname_values_sort_out_dimname_values(self, dim, descending, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> sort_outf(const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
return wrapper_dimname_values_sort_out_dimname_values(self, dim, descending, values, indices);
}

::std::tuple<at::Tensor,at::Tensor> sort(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
return wrapper_dimname_stable_sort_dimname_stable(self, stable, dim, descending);
}

::std::tuple<at::Tensor &,at::Tensor &> sort_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
return wrapper_dimname_values_stable_sort_out_dimname_values_stable(self, stable, dim, descending, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> sort_outf(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
return wrapper_dimname_values_stable_sort_out_dimname_values_stable(self, stable, dim, descending, values, indices);
}

at::Tensor msort(const at::Tensor & self) {
return wrapper__msort(self);
}

at::Tensor & msort_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_msort_out_out(self, out);
}

at::Tensor & msort_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_msort_out_out(self, out);
}

at::Tensor argsort(const at::Tensor & self, int64_t dim, bool descending) {
return wrapper__argsort(self, dim, descending);
}

at::Tensor argsort(const at::Tensor & self, at::Dimname dim, bool descending) {
return wrapper_dimname_argsort_dimname(self, dim, descending);
}

at::Tensor float_power(const at::Tensor & self, const at::Tensor & exponent) {
return wrapper_Tensor_Tensor_float_power_Tensor_Tensor(self, exponent);
}

at::Tensor & float_power_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & exponent) {
return wrapper_Tensor_Tensor_out_float_power_out_Tensor_Tensor_out(self, exponent, out);
}

at::Tensor & float_power_outf(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
return wrapper_Tensor_Tensor_out_float_power_out_Tensor_Tensor_out(self, exponent, out);
}

at::Tensor & float_power_(at::Tensor & self, const at::Tensor & exponent) {
return wrapper_Tensor_float_power__Tensor(self, exponent);
}

at::Tensor float_power(const at::Scalar & self, const at::Tensor & exponent) {
return wrapper_Scalar_float_power_Scalar(self, exponent);
}

at::Tensor & float_power_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & exponent) {
return wrapper_Scalar_out_float_power_out_Scalar_out(self, exponent, out);
}

at::Tensor & float_power_outf(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
return wrapper_Scalar_out_float_power_out_Scalar_out(self, exponent, out);
}

at::Tensor float_power(const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_Tensor_Scalar_float_power_Tensor_Scalar(self, exponent);
}

at::Tensor & float_power_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_Tensor_Scalar_out_float_power_out_Tensor_Scalar_out(self, exponent, out);
}

at::Tensor & float_power_outf(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
return wrapper_Tensor_Scalar_out_float_power_out_Tensor_Scalar_out(self, exponent, out);
}

at::Tensor & float_power_(at::Tensor & self, const at::Scalar & exponent) {
return wrapper_Scalar_float_power__Scalar(self, exponent);
}

at::Tensor normal(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, at::TensorOptions options) {
return wrapper_float_float_normal_float_float(mean, std, size, generator, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor normal(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper_float_float_normal_float_float(mean, std, size, generator, dtype, layout, device, pin_memory);
}

at::Tensor & normal_out(at::Tensor & out, double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator) {
return wrapper_float_float_out_normal_out_float_float_out(mean, std, size, generator, out);
}

at::Tensor & normal_outf(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
return wrapper_float_float_out_normal_out_float_float_out(mean, std, size, generator, out);
}

at::Tensor multilabel_margin_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper__multilabel_margin_loss(self, target, reduction);
}

at::Tensor & multilabel_margin_loss_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_out_multilabel_margin_loss_out_out(self, target, reduction, out);
}

at::Tensor & multilabel_margin_loss_outf(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
return wrapper_out_multilabel_margin_loss_out_out(self, target, reduction, out);
}

at::Tensor nll_loss(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper__nll_loss(self, target, weight, reduction, ignore_index);
}

at::Tensor & nll_loss_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_out_nll_loss_out_out(self, target, weight, reduction, ignore_index, out);
}

at::Tensor & nll_loss_outf(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
return wrapper_out_nll_loss_out_out(self, target, weight, reduction, ignore_index, out);
}

at::Tensor nll_loss_nd(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper__nll_loss_nd(self, target, weight, reduction, ignore_index);
}

at::Tensor nll_loss2d(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper__nll_loss2d(self, target, weight, reduction, ignore_index);
}

at::Tensor & nll_loss2d_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_out_nll_loss2d_out_out(self, target, weight, reduction, ignore_index, out);
}

at::Tensor & nll_loss2d_outf(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
return wrapper_out_nll_loss2d_out_out(self, target, weight, reduction, ignore_index, out);
}

at::Tensor log_sigmoid(const at::Tensor & self) {
return wrapper__log_sigmoid(self);
}

at::Tensor & log_sigmoid_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_log_sigmoid_out_out(self, out);
}

at::Tensor & log_sigmoid_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_log_sigmoid_out_out(self, out);
}

at::Tensor adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper__adaptive_avg_pool2d(self, output_size);
}

at::Tensor adaptive_avg_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper__adaptive_avg_pool3d(self, output_size);
}

at::Tensor thnn_conv2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper__thnn_conv2d(self, weight, kernel_size, bias, stride, padding);
}

at::Tensor & thnn_conv2d_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper_out_thnn_conv2d_out_out(self, weight, kernel_size, bias, stride, padding, out);
}

at::Tensor & thnn_conv2d_outf(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
return wrapper_out_thnn_conv2d_out_out(self, weight, kernel_size, bias, stride, padding, out);
}

at::Tensor slow_conv3d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper__slow_conv3d(self, weight, kernel_size, bias, stride, padding);
}

at::Tensor & slow_conv3d_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
return wrapper_out_slow_conv3d_out_out(self, weight, kernel_size, bias, stride, padding, out);
}

at::Tensor & slow_conv3d_outf(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
return wrapper_out_slow_conv3d_out_out(self, weight, kernel_size, bias, stride, padding, out);
}

at::Tensor column_stack(at::TensorList tensors) {
return wrapper__column_stack(tensors);
}

at::Tensor & column_stack_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_out_column_stack_out_out(tensors, out);
}

at::Tensor & column_stack_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_out_column_stack_out_out(tensors, out);
}

at::Tensor isfinite(const at::Tensor & self) {
return wrapper__isfinite(self);
}

at::Tensor isinf(const at::Tensor & self) {
return wrapper__isinf(self);
}

at::Tensor _add_batch_dim(const at::Tensor & self, int64_t batch_dim, int64_t level) {
return wrapper___add_batch_dim(self, batch_dim, level);
}

at::Tensor _remove_batch_dim(const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
return wrapper___remove_batch_dim(self, level, batch_size, out_dim);
}

at::Tensor special_expm1(const at::Tensor & self) {
return wrapper__special_expm1(self);
}

at::Tensor & special_expm1_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_expm1_out_out(self, out);
}

at::Tensor & special_expm1_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_expm1_out_out(self, out);
}

at::Tensor special_exp2(const at::Tensor & self) {
return wrapper__special_exp2(self);
}

at::Tensor & special_exp2_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_exp2_out_out(self, out);
}

at::Tensor & special_exp2_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_exp2_out_out(self, out);
}

at::Tensor special_psi(const at::Tensor & self) {
return wrapper__special_psi(self);
}

at::Tensor & special_psi_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_psi_out_out(self, out);
}

at::Tensor & special_psi_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_psi_out_out(self, out);
}

at::Tensor special_digamma(const at::Tensor & self) {
return wrapper__special_digamma(self);
}

at::Tensor & special_digamma_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_digamma_out_out(self, out);
}

at::Tensor & special_digamma_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_digamma_out_out(self, out);
}

at::Tensor special_gammaln(const at::Tensor & self) {
return wrapper__special_gammaln(self);
}

at::Tensor & special_gammaln_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_gammaln_out_out(self, out);
}

at::Tensor & special_gammaln_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_gammaln_out_out(self, out);
}

at::Tensor special_erf(const at::Tensor & self) {
return wrapper__special_erf(self);
}

at::Tensor & special_erf_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_erf_out_out(self, out);
}

at::Tensor & special_erf_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_erf_out_out(self, out);
}

at::Tensor special_erfc(const at::Tensor & self) {
return wrapper__special_erfc(self);
}

at::Tensor & special_erfc_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_erfc_out_out(self, out);
}

at::Tensor & special_erfc_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_erfc_out_out(self, out);
}

at::Tensor special_erfinv(const at::Tensor & self) {
return wrapper__special_erfinv(self);
}

at::Tensor & special_erfinv_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_erfinv_out_out(self, out);
}

at::Tensor & special_erfinv_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_erfinv_out_out(self, out);
}

at::Tensor special_ndtr(const at::Tensor & self) {
return wrapper__special_ndtr(self);
}

at::Tensor & special_ndtr_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_ndtr_out_out(self, out);
}

at::Tensor & special_ndtr_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_ndtr_out_out(self, out);
}

at::Tensor special_xlogy(const at::Tensor & self, const at::Tensor & other) {
return wrapper__special_xlogy(self, other);
}

at::Tensor & special_xlogy_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_special_xlogy_out_out(self, other, out);
}

at::Tensor & special_xlogy_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_special_xlogy_out_out(self, other, out);
}

at::Tensor special_xlogy(const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_special_xlogy_self_scalar(self, other);
}

at::Tensor & special_xlogy_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_out_special_xlogy_out_self_scalar_out(self, other, out);
}

at::Tensor & special_xlogy_outf(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_self_scalar_out_special_xlogy_out_self_scalar_out(self, other, out);
}

at::Tensor special_xlogy(const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_special_xlogy_other_scalar(self, other);
}

at::Tensor & special_xlogy_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_out_special_xlogy_out_other_scalar_out(self, other, out);
}

at::Tensor & special_xlogy_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_other_scalar_out_special_xlogy_out_other_scalar_out(self, other, out);
}

at::Tensor special_i0(const at::Tensor & self) {
return wrapper__special_i0(self);
}

at::Tensor & special_i0_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_i0_out_out(self, out);
}

at::Tensor & special_i0_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_i0_out_out(self, out);
}

at::Tensor special_logit(const at::Tensor & self, c10::optional<double> eps) {
return wrapper__special_logit(self, eps);
}

at::Tensor & special_logit_out(at::Tensor & out, const at::Tensor & self, c10::optional<double> eps) {
return wrapper_out_special_logit_out_out(self, eps, out);
}

at::Tensor & special_logit_outf(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
return wrapper_out_special_logit_out_out(self, eps, out);
}

at::Tensor special_polygamma(int64_t n, const at::Tensor & self) {
return wrapper__special_polygamma(n, self);
}

at::Tensor & special_polygamma_out(at::Tensor & out, int64_t n, const at::Tensor & self) {
return wrapper_out_special_polygamma_out_out(n, self, out);
}

at::Tensor & special_polygamma_outf(int64_t n, const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_polygamma_out_out(n, self, out);
}

at::Tensor special_logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper__special_logsumexp(self, dim, keepdim);
}

at::Tensor & special_logsumexp_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_out_special_logsumexp_out_out(self, dim, keepdim, out);
}

at::Tensor & special_logsumexp_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_out_special_logsumexp_out_out(self, dim, keepdim, out);
}

at::Tensor special_expit(const at::Tensor & self) {
return wrapper__special_expit(self);
}

at::Tensor & special_expit_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_expit_out_out(self, out);
}

at::Tensor & special_expit_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_expit_out_out(self, out);
}

at::Tensor special_sinc(const at::Tensor & self) {
return wrapper__special_sinc(self);
}

at::Tensor & special_sinc_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_sinc_out_out(self, out);
}

at::Tensor & special_sinc_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_sinc_out_out(self, out);
}

at::Tensor special_round(const at::Tensor & self) {
return wrapper__special_round(self);
}

at::Tensor & special_round_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_round_out_out(self, out);
}

at::Tensor & special_round_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_round_out_out(self, out);
}

at::Tensor special_log1p(const at::Tensor & self) {
return wrapper__special_log1p(self);
}

at::Tensor & special_log1p_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_special_log1p_out_out(self, out);
}

at::Tensor & special_log1p_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_special_log1p_out_out(self, out);
}

at::Tensor special_log_softmax(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper__special_log_softmax(self, dim, dtype);
}

at::Tensor special_gammainc(const at::Tensor & self, const at::Tensor & other) {
return wrapper__special_gammainc(self, other);
}

at::Tensor & special_gammainc_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_special_gammainc_out_out(self, other, out);
}

at::Tensor & special_gammainc_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_special_gammainc_out_out(self, other, out);
}

at::Tensor special_gammaincc(const at::Tensor & self, const at::Tensor & other) {
return wrapper__special_gammaincc(self, other);
}

at::Tensor & special_gammaincc_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_special_gammaincc_out_out(self, other, out);
}

at::Tensor & special_gammaincc_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_special_gammaincc_out_out(self, other, out);
}

at::Tensor special_multigammaln(const at::Tensor & self, int64_t p) {
return wrapper__special_multigammaln(self, p);
}

at::Tensor & special_multigammaln_out(at::Tensor & out, const at::Tensor & self, int64_t p) {
return wrapper_out_special_multigammaln_out_out(self, p, out);
}

at::Tensor & special_multigammaln_outf(const at::Tensor & self, int64_t p, at::Tensor & out) {
return wrapper_out_special_multigammaln_out_out(self, p, out);
}

at::Tensor fft_fft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_fft(self, n, dim, norm);
}

at::Tensor & fft_fft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_fft_out_out(self, n, dim, norm, out);
}

at::Tensor & fft_fft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_fft_out_out(self, n, dim, norm, out);
}

at::Tensor fft_ifft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_ifft(self, n, dim, norm);
}

at::Tensor & fft_ifft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_ifft_out_out(self, n, dim, norm, out);
}

at::Tensor & fft_ifft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_ifft_out_out(self, n, dim, norm, out);
}

at::Tensor fft_rfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_rfft(self, n, dim, norm);
}

at::Tensor & fft_rfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_rfft_out_out(self, n, dim, norm, out);
}

at::Tensor & fft_rfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_rfft_out_out(self, n, dim, norm, out);
}

at::Tensor fft_irfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_irfft(self, n, dim, norm);
}

at::Tensor & fft_irfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_irfft_out_out(self, n, dim, norm, out);
}

at::Tensor & fft_irfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_irfft_out_out(self, n, dim, norm, out);
}

at::Tensor fft_hfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_hfft(self, n, dim, norm);
}

at::Tensor & fft_hfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_hfft_out_out(self, n, dim, norm, out);
}

at::Tensor & fft_hfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_hfft_out_out(self, n, dim, norm, out);
}

at::Tensor fft_ihfft(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_ihfft(self, n, dim, norm);
}

at::Tensor & fft_ihfft_out(at::Tensor & out, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_ihfft_out_out(self, n, dim, norm, out);
}

at::Tensor & fft_ihfft_outf(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_ihfft_out_out(self, n, dim, norm, out);
}

at::Tensor fft_fft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_fft2(self, s, dim, norm);
}

at::Tensor & fft_fft2_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_fft2_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_fft2_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_fft2_out_out(self, s, dim, norm, out);
}

at::Tensor fft_ifft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_ifft2(self, s, dim, norm);
}

at::Tensor & fft_ifft2_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_ifft2_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_ifft2_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_ifft2_out_out(self, s, dim, norm, out);
}

at::Tensor fft_rfft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_rfft2(self, s, dim, norm);
}

at::Tensor & fft_rfft2_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_rfft2_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_rfft2_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_rfft2_out_out(self, s, dim, norm, out);
}

at::Tensor fft_irfft2(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_irfft2(self, s, dim, norm);
}

at::Tensor & fft_irfft2_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_irfft2_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_irfft2_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_irfft2_out_out(self, s, dim, norm, out);
}

at::Tensor fft_fftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_fftn(self, s, dim, norm);
}

at::Tensor & fft_fftn_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_fftn_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_fftn_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_fftn_out_out(self, s, dim, norm, out);
}

at::Tensor fft_ifftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_ifftn(self, s, dim, norm);
}

at::Tensor & fft_ifftn_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_ifftn_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_ifftn_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_ifftn_out_out(self, s, dim, norm, out);
}

at::Tensor fft_rfftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_rfftn(self, s, dim, norm);
}

at::Tensor & fft_rfftn_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_rfftn_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_rfftn_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_rfftn_out_out(self, s, dim, norm, out);
}

at::Tensor fft_irfftn(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper__fft_irfftn(self, s, dim, norm);
}

at::Tensor & fft_irfftn_out(at::Tensor & out, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
return wrapper_out_fft_irfftn_out_out(self, s, dim, norm, out);
}

at::Tensor & fft_irfftn_outf(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
return wrapper_out_fft_irfftn_out_out(self, s, dim, norm, out);
}

at::Tensor fft_fftfreq(int64_t n, double d, at::TensorOptions options) {
return wrapper__fft_fftfreq(n, d, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor fft_fftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__fft_fftfreq(n, d, dtype, layout, device, pin_memory);
}

at::Tensor & fft_fftfreq_out(at::Tensor & out, int64_t n, double d) {
return wrapper_out_fft_fftfreq_out_out(n, d, out);
}

at::Tensor & fft_fftfreq_outf(int64_t n, double d, at::Tensor & out) {
return wrapper_out_fft_fftfreq_out_out(n, d, out);
}

at::Tensor fft_rfftfreq(int64_t n, double d, at::TensorOptions options) {
return wrapper__fft_rfftfreq(n, d, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor fft_rfftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper__fft_rfftfreq(n, d, dtype, layout, device, pin_memory);
}

at::Tensor & fft_rfftfreq_out(at::Tensor & out, int64_t n, double d) {
return wrapper_out_fft_rfftfreq_out_out(n, d, out);
}

at::Tensor & fft_rfftfreq_outf(int64_t n, double d, at::Tensor & out) {
return wrapper_out_fft_rfftfreq_out_out(n, d, out);
}

at::Tensor fft_fftshift(const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
return wrapper__fft_fftshift(self, dim);
}

at::Tensor fft_ifftshift(const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
return wrapper__fft_ifftshift(self, dim);
}

at::Tensor linalg_cholesky(const at::Tensor & self, bool upper) {
return wrapper__linalg_cholesky(self, upper);
}

at::Tensor & linalg_cholesky_out(at::Tensor & out, const at::Tensor & self, bool upper) {
return wrapper_out_linalg_cholesky_out_out(self, upper, out);
}

at::Tensor & linalg_cholesky_outf(const at::Tensor & self, bool upper, at::Tensor & out) {
return wrapper_out_linalg_cholesky_out_out(self, upper, out);
}

at::Tensor linalg_det(const at::Tensor & self) {
return wrapper__linalg_det(self);
}

at::Tensor & linalg_det_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_linalg_det_out_out(self, out);
}

at::Tensor & linalg_det_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_linalg_det_out_out(self, out);
}

at::Tensor det(const at::Tensor & self) {
return wrapper__det(self);
}

at::Tensor linalg_matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper__linalg_matmul(self, other);
}

at::Tensor & linalg_matmul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_linalg_matmul_out_out(self, other, out);
}

at::Tensor & linalg_matmul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_linalg_matmul_out_out(self, other, out);
}

at::Tensor linalg_eigvals(const at::Tensor & self) {
return wrapper__linalg_eigvals(self);
}

at::Tensor & linalg_eigvals_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_linalg_eigvals_out_out(self, out);
}

at::Tensor & linalg_eigvals_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_linalg_eigvals_out_out(self, out);
}

at::Tensor linalg_eigvalsh(const at::Tensor & self, c10::string_view UPLO) {
return wrapper__linalg_eigvalsh(self, UPLO);
}

at::Tensor linalg_inv(const at::Tensor & self) {
return wrapper__linalg_inv(self);
}

at::Tensor & linalg_inv_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_linalg_inv_out_out(self, out);
}

at::Tensor & linalg_inv_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_linalg_inv_out_out(self, out);
}

at::Tensor inner(const at::Tensor & self, const at::Tensor & other) {
return wrapper__inner(self, other);
}

at::Tensor & inner_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_inner_out_out(self, other, out);
}

at::Tensor & inner_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_inner_out_out(self, other, out);
}

at::Tensor outer(const at::Tensor & self, const at::Tensor & vec2) {
return wrapper__outer(self, vec2);
}

at::Tensor & outer_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & vec2) {
return wrapper_out_outer_out_out(self, vec2, out);
}

at::Tensor & outer_outf(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
return wrapper_out_outer_out_out(self, vec2, out);
}

at::Tensor ger(const at::Tensor & self, const at::Tensor & vec2) {
return wrapper__ger(self, vec2);
}

at::Tensor & ger_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & vec2) {
return wrapper_out_ger_out_out(self, vec2, out);
}

at::Tensor & ger_outf(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
return wrapper_out_ger_out_out(self, vec2, out);
}

at::Tensor linalg_norm(const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper__linalg_norm(self, ord, dim, keepdim, dtype);
}

at::Tensor & linalg_norm_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_out_linalg_norm_out_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor & linalg_norm_outf(const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_out_linalg_norm_out_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor linalg_norm(const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_ord_str_linalg_norm_ord_str(self, ord, dim, keepdim, dtype);
}

at::Tensor & linalg_norm_out(at::Tensor & out, const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_ord_str_out_linalg_norm_out_ord_str_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor & linalg_norm_outf(const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_ord_str_out_linalg_norm_out_ord_str_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor linalg_matrix_norm(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper__linalg_matrix_norm(self, ord, dim, keepdim, dtype);
}

at::Tensor & linalg_matrix_norm_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_out_linalg_matrix_norm_out_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor & linalg_matrix_norm_outf(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_out_linalg_matrix_norm_out_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor linalg_matrix_norm(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_str_ord_linalg_matrix_norm_str_ord(self, ord, dim, keepdim, dtype);
}

at::Tensor & linalg_matrix_norm_out(at::Tensor & out, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_str_ord_out_linalg_matrix_norm_out_str_ord_out(self, ord, dim, keepdim, dtype, out);
}

at::Tensor & linalg_matrix_norm_outf(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
return wrapper_str_ord_out_linalg_matrix_norm_out_str_ord_out(self, ord, dim, keepdim, dtype, out);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_svd(const at::Tensor & self, bool full_matrices) {
return wrapper__linalg_svd(self, full_matrices);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_out(at::Tensor & U, at::Tensor & S, at::Tensor & Vh, const at::Tensor & self, bool full_matrices) {
return wrapper_U_linalg_svd_out_U(self, full_matrices, U, S, Vh);
}

::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_outf(const at::Tensor & self, bool full_matrices, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
return wrapper_U_linalg_svd_out_U(self, full_matrices, U, S, Vh);
}

at::Tensor linalg_svdvals(const at::Tensor & input) {
return wrapper__linalg_svdvals(input);
}

at::Tensor & linalg_svdvals_out(at::Tensor & out, const at::Tensor & input) {
return wrapper_out_linalg_svdvals_out_out(input, out);
}

at::Tensor & linalg_svdvals_outf(const at::Tensor & input, at::Tensor & out) {
return wrapper_out_linalg_svdvals_out_out(input, out);
}

at::Tensor linalg_cond(const at::Tensor & self, const c10::optional<at::Scalar> & p) {
return wrapper__linalg_cond(self, p);
}

at::Tensor & linalg_cond_out(at::Tensor & out, const at::Tensor & self, const c10::optional<at::Scalar> & p) {
return wrapper_out_linalg_cond_out_out(self, p, out);
}

at::Tensor & linalg_cond_outf(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
return wrapper_out_linalg_cond_out_out(self, p, out);
}

at::Tensor linalg_cond(const at::Tensor & self, c10::string_view p) {
return wrapper_p_str_linalg_cond_p_str(self, p);
}

at::Tensor & linalg_cond_out(at::Tensor & out, const at::Tensor & self, c10::string_view p) {
return wrapper_p_str_out_linalg_cond_out_p_str_out(self, p, out);
}

at::Tensor & linalg_cond_outf(const at::Tensor & self, c10::string_view p, at::Tensor & out) {
return wrapper_p_str_out_linalg_cond_out_p_str_out(self, p, out);
}

at::Tensor linalg_pinv(const at::Tensor & self, double rcond, bool hermitian) {
return wrapper__linalg_pinv(self, rcond, hermitian);
}

at::Tensor & linalg_pinv_out(at::Tensor & out, const at::Tensor & self, double rcond, bool hermitian) {
return wrapper_out_linalg_pinv_out_out(self, rcond, hermitian, out);
}

at::Tensor & linalg_pinv_outf(const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
return wrapper_out_linalg_pinv_out_out(self, rcond, hermitian, out);
}

at::Tensor linalg_pinv(const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
return wrapper_rcond_tensor_linalg_pinv_rcond_tensor(self, rcond, hermitian);
}

at::Tensor & linalg_pinv_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
return wrapper_out_rcond_tensor_linalg_pinv_out_out_rcond_tensor(self, rcond, hermitian, out);
}

at::Tensor & linalg_pinv_outf(const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
return wrapper_out_rcond_tensor_linalg_pinv_out_out_rcond_tensor(self, rcond, hermitian, out);
}

at::Tensor linalg_tensorinv(const at::Tensor & self, int64_t ind) {
return wrapper__linalg_tensorinv(self, ind);
}

at::Tensor & linalg_tensorinv_out(at::Tensor & out, const at::Tensor & self, int64_t ind) {
return wrapper_out_linalg_tensorinv_out_out(self, ind, out);
}

at::Tensor & linalg_tensorinv_outf(const at::Tensor & self, int64_t ind, at::Tensor & out) {
return wrapper_out_linalg_tensorinv_out_out(self, ind, out);
}

at::Tensor linalg_tensorsolve(const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims) {
return wrapper__linalg_tensorsolve(self, other, dims);
}

at::Tensor & linalg_tensorsolve_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims) {
return wrapper_out_linalg_tensorsolve_out_out(self, other, dims, out);
}

at::Tensor & linalg_tensorsolve_outf(const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims, at::Tensor & out) {
return wrapper_out_linalg_tensorsolve_out_out(self, other, dims, out);
}

at::Tensor linalg_matrix_power(const at::Tensor & self, int64_t n) {
return wrapper__linalg_matrix_power(self, n);
}

at::Tensor & linalg_matrix_power_out(at::Tensor & out, const at::Tensor & self, int64_t n) {
return wrapper_out_linalg_matrix_power_out_out(self, n, out);
}

at::Tensor & linalg_matrix_power_outf(const at::Tensor & self, int64_t n, at::Tensor & out) {
return wrapper_out_linalg_matrix_power_out_out(self, n, out);
}

at::Tensor linalg_matrix_rank(const at::Tensor & self, c10::optional<double> tol, bool hermitian) {
return wrapper__linalg_matrix_rank(self, tol, hermitian);
}

at::Tensor & linalg_matrix_rank_out(at::Tensor & out, const at::Tensor & self, c10::optional<double> tol, bool hermitian) {
return wrapper_out_linalg_matrix_rank_out_out(self, tol, hermitian, out);
}

at::Tensor & linalg_matrix_rank_outf(const at::Tensor & self, c10::optional<double> tol, bool hermitian, at::Tensor & out) {
return wrapper_out_linalg_matrix_rank_out_out(self, tol, hermitian, out);
}

at::Tensor linalg_matrix_rank(const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
return wrapper_tol_tensor_linalg_matrix_rank_tol_tensor(input, tol, hermitian);
}

at::Tensor & linalg_matrix_rank_out(at::Tensor & out, const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
return wrapper_out_tol_tensor_linalg_matrix_rank_out_out_tol_tensor(input, tol, hermitian, out);
}

at::Tensor & linalg_matrix_rank_outf(const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
return wrapper_out_tol_tensor_linalg_matrix_rank_out_out_tol_tensor(input, tol, hermitian, out);
}

at::Tensor linalg_multi_dot(at::TensorList tensors) {
return wrapper__linalg_multi_dot(tensors);
}

at::Tensor & linalg_multi_dot_out(at::Tensor & out, at::TensorList tensors) {
return wrapper_out_linalg_multi_dot_out_out(tensors, out);
}

at::Tensor & linalg_multi_dot_outf(at::TensorList tensors, at::Tensor & out) {
return wrapper_out_linalg_multi_dot_out_out(tensors, out);
}

at::Tensor _test_serialization_subcmul(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper___test_serialization_subcmul(self, other, alpha);
}

at::Tensor _test_string_default(const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
return wrapper___test_string_default(dummy, a, b);
}

at::Tensor _test_ambiguous_defaults(const at::Tensor & dummy, int64_t a, int64_t b) {
return wrapper_a__test_ambiguous_defaults_a(dummy, a, b);
}

at::Tensor _test_ambiguous_defaults(const at::Tensor & dummy, int64_t a, c10::string_view b) {
return wrapper_b__test_ambiguous_defaults_b(dummy, a, b);
}

at::Tensor pad_sequence(at::TensorList sequences, bool batch_first, double padding_value) {
return wrapper__pad_sequence(sequences, batch_first, padding_value);
}

at::Tensor flatten_dense_tensors(at::TensorList tensors) {
return wrapper__flatten_dense_tensors(tensors);
}

::std::vector<at::Tensor> unflatten_dense_tensors(const at::Tensor & flat, at::TensorList tensors) {
return wrapper__unflatten_dense_tensors(flat, tensors);
}

} // namespace compositeimplicitautograd

} // namespace at
