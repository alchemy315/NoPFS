// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// @generated by tools/codegen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/Functions.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>



namespace at {

// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {


void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      at::native::as_strided_(out, sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

namespace {

at::Tensor & wrapper_out_conj_physical_out_out(const at::Tensor & self, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_conj_physical_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_conj_physical_out_out", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::conj_physical_out_sparse(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_add_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_sparse(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_add_out_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_out_sparse_cuda(self, other, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_add__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_sparse_(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__asin(const at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asin_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_asin_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asin_out_sparse(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__asin_(at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::asin_sparse_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__bmm(const at::Tensor & self, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__bmm", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__bmm", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::bmm_sparse_cuda(self, mat2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_bmm_out_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_bmm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_bmm_out_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_bmm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::bmm_out_sparse_cuda(self, mat2, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_div_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_div_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_out_sparse_zerodim(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_div__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_mode_div_Tensor_mode(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_mode_div_out_out_mode(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_out_sparse_zerodim(self, other, rounding_mode, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_mode_div__Tensor_mode(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::div_sparse_(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_memory_format_empty_memory_format(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning


  globalContext().lazyInitCUDA();
  const DeviceGuard device_guard(device_or_default(device));
  return at::native::empty_sparse(size, dtype, layout, device, pin_memory, memory_format);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__floor_divide(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_divide_sparse(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_floor_divide_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_divide_out_sparse_zerodim(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_floor_divide__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::floor_divide_sparse_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__isnan(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::isnan_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__log1p(const at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log1p_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_log1p_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log1p_out_sparse(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__log1p_(at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log1p_sparse_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mm(const at::Tensor & self, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__mm", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__mm", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_mm(self, mat2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_mm_out_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_mm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_mm_out_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_mm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_mm_out(self, mat2, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_sparse_matmul(const at::Tensor & self, const at::Tensor & other) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper___sparse_sparse_matmul", "self");
  c10::impl::check_and_update_common_device(common_device, other, "wrapper___sparse_sparse_matmul", "other");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_sparse_matmul_cuda(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_mask_helper(const at::Tensor & t, const at::Tensor & mask_indices) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, t, "wrapper___sparse_mask_helper", "t");
  c10::impl::check_and_update_common_device(common_device, mask_indices, "wrapper___sparse_mask_helper", "mask_indices");

  const OptionalDeviceGuard device_guard(device_of(t));
  return at::native::sparse_mask_helper_cuda(t, mask_indices);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_mul_Tensor(const at::Tensor & self, const at::Tensor & other) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mul_sparse(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_mul_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mul_out_sparse_cuda(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_mul__Tensor(at::Tensor & self, const at::Tensor & other) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mul_sparse_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mv(const at::Tensor & self, const at::Tensor & vec) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__mv", "self");
  c10::impl::check_and_update_common_device(common_device, vec, "wrapper__mv", "vec");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mv_sparse(self, vec);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__narrow_copy", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::narrow_copy_sparse(self, dim, start, length);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__neg(const at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::neg_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_neg_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::neg_out_sparse(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__neg_(at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::neg_sparse_(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_sspaddmm_out_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_sspaddmm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_sspaddmm_out_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_out_sspaddmm_out_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_sspaddmm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sspaddmm_out_cuda(self, mat1, mat2, beta, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__sqrt(const at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sqrt_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_sqrt_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sqrt_out_sparse(self, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__native_norm(const at::Tensor & self, const at::Scalar & p) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__native_norm", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::norm_sparse(self, p);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ScalarOpt_dim_dtype_native_norm_ScalarOpt_dim_dtype(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper_ScalarOpt_dim_dtype_native_norm_ScalarOpt_dim_dtype", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::norm_sparse(self, p, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, grad, "wrapper___sparse_sum_backward", "grad");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper___sparse_sum_backward", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_sum_backward_cuda(grad, self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper___sparse_softmax", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::softmax_sparse_cuda(self, dim, half_to_float);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, grad_output, "wrapper___sparse_softmax_backward_data", "grad_output");
  c10::impl::check_and_update_common_device(common_device, output, "wrapper___sparse_softmax_backward_data", "output");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper___sparse_softmax_backward_data", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::softmax_backward_sparse_cuda(grad_output, output, dim, self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper___sparse_log_softmax", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log_softmax_sparse_cuda(self, dim, half_to_float);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, grad_output, "wrapper___sparse_log_softmax_backward_data", "grad_output");
  c10::impl::check_and_update_common_device(common_device, output, "wrapper___sparse_log_softmax_backward_data", "output");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper___sparse_log_softmax_backward_data", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::log_softmax_backward_sparse_cuda(grad_output, output, dim, self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ScalarOpt_dim_dtype_norm_ScalarOpt_dim_dtype(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_dtype_norm(self, p, dim, keepdim, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ScalarOpt_dim_norm_ScalarOpt_dim(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_norm(self, p, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__clone", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::clone_sparse(self, memory_format);
}

} // anonymous namespace
namespace {

const at::Tensor & wrapper__resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__resize_as_sparse_", "self");
  c10::impl::check_and_update_common_device(common_device, the_template, "wrapper__resize_as_sparse_", "the_template");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::resize_as_sparse_(self, the_template);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__zero_(at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::zero_sparse_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_sub_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sub_sparse(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_sub_out_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sub_out_sparse(self, other, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_sub__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sub_sparse_(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__addmm", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper__addmm", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__addmm", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::addmm_sparse_dense_cuda(self, mat1, mat2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_addmm_out_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_addmm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_addmm_out_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_out_addmm_out_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_addmm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::addmm_out_sparse_dense_cuda(self, mat1, mat2, beta, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__addmm_", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper__addmm_", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__addmm_", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::s_addmm_sparse_dense_cuda_(self, mat1, mat2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning


  globalContext().lazyInitCUDA();
  const DeviceGuard device_guard(device_or_default(device));
  return at::native::new_with_dims_sparse(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, indices, "wrapper___sparse_coo_tensor_with_dims_and_tensors", "indices");
  c10::impl::check_and_update_common_device(common_device, values, "wrapper___sparse_coo_tensor_with_dims_and_tensors", "values");

  globalContext().lazyInitCUDA();
  const DeviceGuard device_guard(device_or_default(device));
  return at::native::new_with_dims_and_tensor_sparse(sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory);
}

} // anonymous namespace
namespace {

const at::Tensor & wrapper__sparse_resize_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__sparse_resize_", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_resize_(self, size, sparse_dim, dense_dim);
}

} // anonymous namespace
namespace {

const at::Tensor & wrapper__sparse_resize_and_clear_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__sparse_resize_and_clear_", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__sparse_mask", "self");
  c10::impl::check_and_update_common_device(common_device, mask, "wrapper__sparse_mask", "mask");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_mask_cuda(self, mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__to_dense", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_to_dense(self, dtype);
}

} // anonymous namespace
namespace {

int64_t wrapper__sparse_dim(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_dim_sparse(self);
}

} // anonymous namespace
namespace {

int64_t wrapper___dimI(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::sparse_dim_sparse(self);
}

} // anonymous namespace
namespace {

int64_t wrapper__dense_dim(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::dense_dim_sparse(self);
}

} // anonymous namespace
namespace {

int64_t wrapper___dimV(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::dense_dim_sparse(self);
}

} // anonymous namespace
namespace {

int64_t wrapper___nnz(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnz_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___coalesce(const at::Tensor & self) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper___coalesce", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_coalesce_sparse_cuda(self);
}

} // anonymous namespace
namespace {

bool wrapper__is_coalesced(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_coalesced_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___indices(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_indices_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___values(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_values_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper___coalesced_(at::Tensor & self, bool coalesced) {
    // No device check


  // DeviceGuard omitted
  return at::native::_coalesced_sparse_(self, coalesced);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__indices(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::indices_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__values(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::values_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper__hspmm", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__hspmm", "mat2");

  const OptionalDeviceGuard device_guard(device_of(mat1));
  return at::native::hspmm_sparse_cuda(mat1, mat2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_hspmm_out_out(const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_hspmm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_out_hspmm_out_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_hspmm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(out));
  return at::native::hspmm_out_sparse_cuda(mat1, mat2, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::copy_sparse_(self, src, non_blocking);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__index_select", "self");
  c10::impl::check_and_update_common_device(common_device, index, "wrapper__index_select", "index");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::index_select_sparse(self, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__any(const at::Tensor & self) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::any_sparse(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_Scalar_pow_Tensor_Scalar(const at::Tensor & self, const at::Scalar & exponent) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::pow_sparse_scalar(self, exponent);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_Scalar_out_pow_out_Tensor_Scalar_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::pow_out_sparse_scalar(self, exponent, out);
}

} // anonymous namespace

TORCH_LIBRARY_IMPL(aten, SparseCUDA, m) {
  m.impl("conj_physical.out",
  TORCH_FN(wrapper_out_conj_physical_out_out));
  m.impl("add.Tensor",
  TORCH_FN(wrapper_Tensor_add_Tensor));
  m.impl("add.out",
  TORCH_FN(wrapper_out_add_out_out));
  m.impl("add_.Tensor",
  TORCH_FN(wrapper_Tensor_add__Tensor));
  m.impl("asin",
  TORCH_FN(wrapper__asin));
  m.impl("asin.out",
  TORCH_FN(wrapper_out_asin_out_out));
  m.impl("asin_",
  TORCH_FN(wrapper__asin_));
  m.impl("bmm",
  TORCH_FN(wrapper__bmm));
  m.impl("bmm.out",
  TORCH_FN(wrapper_out_bmm_out_out));
  m.impl("div.Tensor",
  TORCH_FN(wrapper_Tensor_div_Tensor));
  m.impl("div.out",
  TORCH_FN(wrapper_out_div_out_out));
  m.impl("div_.Tensor",
  TORCH_FN(wrapper_Tensor_div__Tensor));
  m.impl("div.Tensor_mode",
  TORCH_FN(wrapper_Tensor_mode_div_Tensor_mode));
  m.impl("div.out_mode",
  TORCH_FN(wrapper_out_mode_div_out_out_mode));
  m.impl("div_.Tensor_mode",
  TORCH_FN(wrapper_Tensor_mode_div__Tensor_mode));
  m.impl("empty.memory_format",
  TORCH_FN(wrapper_memory_format_empty_memory_format));
  m.impl("floor_divide",
  TORCH_FN(wrapper__floor_divide));
  m.impl("floor_divide.out",
  TORCH_FN(wrapper_out_floor_divide_out_out));
  m.impl("floor_divide_.Tensor",
  TORCH_FN(wrapper_Tensor_floor_divide__Tensor));
  m.impl("isnan",
  TORCH_FN(wrapper__isnan));
  m.impl("log1p",
  TORCH_FN(wrapper__log1p));
  m.impl("log1p.out",
  TORCH_FN(wrapper_out_log1p_out_out));
  m.impl("log1p_",
  TORCH_FN(wrapper__log1p_));
  m.impl("mm",
  TORCH_FN(wrapper__mm));
  m.impl("mm.out",
  TORCH_FN(wrapper_out_mm_out_out));
  m.impl("_sparse_sparse_matmul",
  TORCH_FN(wrapper___sparse_sparse_matmul));
  m.impl("_sparse_mask_helper",
  TORCH_FN(wrapper___sparse_mask_helper));
  m.impl("mul.Tensor",
  TORCH_FN(wrapper_Tensor_mul_Tensor));
  m.impl("mul.out",
  TORCH_FN(wrapper_out_mul_out_out));
  m.impl("mul_.Tensor",
  TORCH_FN(wrapper_Tensor_mul__Tensor));
  m.impl("mv",
  TORCH_FN(wrapper__mv));
  m.impl("narrow_copy",
  TORCH_FN(wrapper__narrow_copy));
  m.impl("neg",
  TORCH_FN(wrapper__neg));
  m.impl("neg.out",
  TORCH_FN(wrapper_out_neg_out_out));
  m.impl("neg_",
  TORCH_FN(wrapper__neg_));
  m.impl("sspaddmm.out",
  TORCH_FN(wrapper_out_sspaddmm_out_out));
  m.impl("sqrt",
  TORCH_FN(wrapper__sqrt));
  m.impl("sqrt.out",
  TORCH_FN(wrapper_out_sqrt_out_out));
  m.impl("native_norm",
  TORCH_FN(wrapper__native_norm));
  m.impl("native_norm.ScalarOpt_dim_dtype",
  TORCH_FN(wrapper_ScalarOpt_dim_dtype_native_norm_ScalarOpt_dim_dtype));
  m.impl("_sparse_sum_backward",
  TORCH_FN(wrapper___sparse_sum_backward));
  m.impl("_sparse_softmax",
  TORCH_FN(wrapper___sparse_softmax));
  m.impl("_sparse_softmax_backward_data",
  TORCH_FN(wrapper___sparse_softmax_backward_data));
  m.impl("_sparse_log_softmax",
  TORCH_FN(wrapper___sparse_log_softmax));
  m.impl("_sparse_log_softmax_backward_data",
  TORCH_FN(wrapper___sparse_log_softmax_backward_data));
  m.impl("norm.ScalarOpt_dim_dtype",
  TORCH_FN(wrapper_ScalarOpt_dim_dtype_norm_ScalarOpt_dim_dtype));
  m.impl("norm.ScalarOpt_dim",
  TORCH_FN(wrapper_ScalarOpt_dim_norm_ScalarOpt_dim));
  m.impl("clone",
  TORCH_FN(wrapper__clone));
  m.impl("resize_as_sparse_",
  TORCH_FN(wrapper__resize_as_sparse_));
  m.impl("zero_",
  TORCH_FN(wrapper__zero_));
  m.impl("sub.Tensor",
  TORCH_FN(wrapper_Tensor_sub_Tensor));
  m.impl("sub.out",
  TORCH_FN(wrapper_out_sub_out_out));
  m.impl("sub_.Tensor",
  TORCH_FN(wrapper_Tensor_sub__Tensor));
  m.impl("addmm",
  TORCH_FN(wrapper__addmm));
  m.impl("addmm.out",
  TORCH_FN(wrapper_out_addmm_out_out));
  m.impl("addmm_",
  TORCH_FN(wrapper__addmm_));
  m.impl("_sparse_coo_tensor_with_dims",
  TORCH_FN(wrapper___sparse_coo_tensor_with_dims));
  m.impl("_sparse_coo_tensor_with_dims_and_tensors",
  TORCH_FN(wrapper___sparse_coo_tensor_with_dims_and_tensors));
  m.impl("sparse_resize_",
  TORCH_FN(wrapper__sparse_resize_));
  m.impl("sparse_resize_and_clear_",
  TORCH_FN(wrapper__sparse_resize_and_clear_));
  m.impl("sparse_mask",
  TORCH_FN(wrapper__sparse_mask));
  m.impl("to_dense",
  TORCH_FN(wrapper__to_dense));
  m.impl("sparse_dim",
  TORCH_FN(wrapper__sparse_dim));
  m.impl("_dimI",
  TORCH_FN(wrapper___dimI));
  m.impl("dense_dim",
  TORCH_FN(wrapper__dense_dim));
  m.impl("_dimV",
  TORCH_FN(wrapper___dimV));
  m.impl("_nnz",
  TORCH_FN(wrapper___nnz));
  m.impl("_coalesce",
  TORCH_FN(wrapper___coalesce));
  m.impl("is_coalesced",
  TORCH_FN(wrapper__is_coalesced));
  m.impl("_indices",
  TORCH_FN(wrapper___indices));
  m.impl("_values",
  TORCH_FN(wrapper___values));
  m.impl("_coalesced_",
  TORCH_FN(wrapper___coalesced_));
  m.impl("indices",
  TORCH_FN(wrapper__indices));
  m.impl("values",
  TORCH_FN(wrapper__values));
  m.impl("hspmm",
  TORCH_FN(wrapper__hspmm));
  m.impl("hspmm.out",
  TORCH_FN(wrapper_out_hspmm_out_out));
  m.impl("copy_sparse_to_sparse_",
  TORCH_FN(wrapper__copy_sparse_to_sparse_));
  m.impl("index_select",
  TORCH_FN(wrapper__index_select));
  m.impl("any",
  TORCH_FN(wrapper__any));
  m.impl("pow.Tensor_Scalar",
  TORCH_FN(wrapper_Tensor_Scalar_pow_Tensor_Scalar));
  m.impl("pow.Tensor_Scalar_out",
  TORCH_FN(wrapper_Tensor_Scalar_out_pow_out_Tensor_Scalar_out));
}

} // anonymous namespace

namespace sparsecuda {


at::Tensor & conj_physical_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_conj_physical_out_out(self, out);
}

at::Tensor & conj_physical_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_conj_physical_out_out(self, out);
}

at::Tensor add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_add_Tensor(self, other, alpha);
}

at::Tensor & add_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_out_add_out_out(self, other, alpha, out);
}

at::Tensor & add_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_add_out_out(self, other, alpha, out);
}

at::Tensor & add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_add__Tensor(self, other, alpha);
}

at::Tensor asin(const at::Tensor & self) {
return wrapper__asin(self);
}

at::Tensor & asin_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_asin_out_out(self, out);
}

at::Tensor & asin_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_asin_out_out(self, out);
}

at::Tensor & asin_(at::Tensor & self) {
return wrapper__asin_(self);
}

at::Tensor bmm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper__bmm(self, mat2);
}

at::Tensor & bmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_out_bmm_out_out(self, mat2, out);
}

at::Tensor & bmm_outf(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_out_bmm_out_out(self, mat2, out);
}

at::Tensor div(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_div_Tensor(self, other);
}

at::Tensor & div_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_div_out_out(self, other, out);
}

at::Tensor & div_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_div_out_out(self, other, out);
}

at::Tensor & div_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_div__Tensor(self, other);
}

at::Tensor div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Tensor_mode_div_Tensor_mode(self, other, rounding_mode);
}

at::Tensor & div_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_out_mode_div_out_out_mode(self, other, rounding_mode, out);
}

at::Tensor & div_outf(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
return wrapper_out_mode_div_out_out_mode(self, other, rounding_mode, out);
}

at::Tensor & div_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Tensor_mode_div__Tensor_mode(self, other, rounding_mode);
}

at::Tensor empty(at::IntArrayRef size, at::TensorOptions options, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_memory_format_empty_memory_format(size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor empty(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
return wrapper_memory_format_empty_memory_format(size, dtype, layout, device, pin_memory, memory_format);
}

at::Tensor floor_divide(const at::Tensor & self, const at::Tensor & other) {
return wrapper__floor_divide(self, other);
}

at::Tensor & floor_divide_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_floor_divide_out_out(self, other, out);
}

at::Tensor & floor_divide_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_floor_divide_out_out(self, other, out);
}

at::Tensor & floor_divide_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_floor_divide__Tensor(self, other);
}

at::Tensor isnan(const at::Tensor & self) {
return wrapper__isnan(self);
}

at::Tensor log1p(const at::Tensor & self) {
return wrapper__log1p(self);
}

at::Tensor & log1p_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_log1p_out_out(self, out);
}

at::Tensor & log1p_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_log1p_out_out(self, out);
}

at::Tensor & log1p_(at::Tensor & self) {
return wrapper__log1p_(self);
}

at::Tensor mm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper__mm(self, mat2);
}

at::Tensor & mm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_out_mm_out_out(self, mat2, out);
}

at::Tensor & mm_outf(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_out_mm_out_out(self, mat2, out);
}

at::Tensor _sparse_sparse_matmul(const at::Tensor & self, const at::Tensor & other) {
return wrapper___sparse_sparse_matmul(self, other);
}

at::Tensor _sparse_mask_helper(const at::Tensor & t, const at::Tensor & mask_indices) {
return wrapper___sparse_mask_helper(t, mask_indices);
}

at::Tensor mul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_mul_Tensor(self, other);
}

at::Tensor & mul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_mul_out_out(self, other, out);
}

at::Tensor & mul_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_mul_out_out(self, other, out);
}

at::Tensor & mul_(at::Tensor & self, const at::Tensor & other) {
return wrapper_Tensor_mul__Tensor(self, other);
}

at::Tensor mv(const at::Tensor & self, const at::Tensor & vec) {
return wrapper__mv(self, vec);
}

at::Tensor narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
return wrapper__narrow_copy(self, dim, start, length);
}

at::Tensor neg(const at::Tensor & self) {
return wrapper__neg(self);
}

at::Tensor & neg_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_neg_out_out(self, out);
}

at::Tensor & neg_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_neg_out_out(self, out);
}

at::Tensor & neg_(at::Tensor & self) {
return wrapper__neg_(self);
}

at::Tensor & sspaddmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_out_sspaddmm_out_out(self, mat1, mat2, beta, alpha, out);
}

at::Tensor & sspaddmm_outf(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_sspaddmm_out_out(self, mat1, mat2, beta, alpha, out);
}

at::Tensor sqrt(const at::Tensor & self) {
return wrapper__sqrt(self);
}

at::Tensor & sqrt_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_sqrt_out_out(self, out);
}

at::Tensor & sqrt_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_sqrt_out_out(self, out);
}

at::Tensor native_norm(const at::Tensor & self, const at::Scalar & p) {
return wrapper__native_norm(self, p);
}

at::Tensor native_norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_ScalarOpt_dim_dtype_native_norm_ScalarOpt_dim_dtype(self, p, dim, keepdim, dtype);
}

at::Tensor _sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
return wrapper___sparse_sum_backward(grad, self, dim);
}

at::Tensor _sparse_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper___sparse_softmax(self, dim, half_to_float);
}

at::Tensor _sparse_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
return wrapper___sparse_softmax_backward_data(grad_output, output, dim, self);
}

at::Tensor _sparse_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper___sparse_log_softmax(self, dim, half_to_float);
}

at::Tensor _sparse_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
return wrapper___sparse_log_softmax_backward_data(grad_output, output, dim, self);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
return wrapper_ScalarOpt_dim_dtype_norm_ScalarOpt_dim_dtype(self, p, dim, keepdim, dtype);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
return wrapper_ScalarOpt_dim_norm_ScalarOpt_dim(self, p, dim, keepdim);
}

at::Tensor clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__clone(self, memory_format);
}

const at::Tensor & resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
return wrapper__resize_as_sparse_(self, the_template);
}

at::Tensor & zero_(at::Tensor & self) {
return wrapper__zero_(self);
}

at::Tensor sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_sub_Tensor(self, other, alpha);
}

at::Tensor & sub_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_out_sub_out_out(self, other, alpha, out);
}

at::Tensor & sub_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_sub_out_out(self, other, alpha, out);
}

at::Tensor & sub_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_sub__Tensor(self, other, alpha);
}

at::Tensor addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper__addmm(self, mat1, mat2, beta, alpha);
}

at::Tensor & addmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_out_addmm_out_out(self, mat1, mat2, beta, alpha, out);
}

at::Tensor & addmm_outf(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_addmm_out_out(self, mat1, mat2, beta, alpha, out);
}

at::Tensor & addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper__addmm_(self, mat1, mat2, beta, alpha);
}

at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, at::TensorOptions options) {
return wrapper___sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper___sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}

at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, at::TensorOptions options) {
return wrapper___sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, indices, values, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}

at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
return wrapper___sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory);
}

const at::Tensor & sparse_resize_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
return wrapper__sparse_resize_(self, size, sparse_dim, dense_dim);
}

const at::Tensor & sparse_resize_and_clear_(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
return wrapper__sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}

at::Tensor sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
return wrapper__sparse_mask(self, mask);
}

at::Tensor to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
return wrapper__to_dense(self, dtype);
}

int64_t sparse_dim(const at::Tensor & self) {
return wrapper__sparse_dim(self);
}

int64_t _dimI(const at::Tensor & self) {
return wrapper___dimI(self);
}

int64_t dense_dim(const at::Tensor & self) {
return wrapper__dense_dim(self);
}

int64_t _dimV(const at::Tensor & self) {
return wrapper___dimV(self);
}

int64_t _nnz(const at::Tensor & self) {
return wrapper___nnz(self);
}

at::Tensor _coalesce(const at::Tensor & self) {
return wrapper___coalesce(self);
}

bool is_coalesced(const at::Tensor & self) {
return wrapper__is_coalesced(self);
}

at::Tensor _indices(const at::Tensor & self) {
return wrapper___indices(self);
}

at::Tensor _values(const at::Tensor & self) {
return wrapper___values(self);
}

at::Tensor & _coalesced_(at::Tensor & self, bool coalesced) {
return wrapper___coalesced_(self, coalesced);
}

at::Tensor indices(const at::Tensor & self) {
return wrapper__indices(self);
}

at::Tensor values(const at::Tensor & self) {
return wrapper__values(self);
}

at::Tensor hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
return wrapper__hspmm(mat1, mat2);
}

at::Tensor & hspmm_out(at::Tensor & out, const at::Tensor & mat1, const at::Tensor & mat2) {
return wrapper_out_hspmm_out_out(mat1, mat2, out);
}

at::Tensor & hspmm_outf(const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_out_hspmm_out_out(mat1, mat2, out);
}

at::Tensor & copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
return wrapper__copy_sparse_to_sparse_(self, src, non_blocking);
}

at::Tensor index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
return wrapper__index_select(self, dim, index);
}

at::Tensor any(const at::Tensor & self) {
return wrapper__any(self);
}

at::Tensor pow(const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_Tensor_Scalar_pow_Tensor_Scalar(self, exponent);
}

at::Tensor & pow_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_Tensor_Scalar_out_pow_out_Tensor_Scalar_out(self, exponent, out);
}

at::Tensor & pow_outf(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
return wrapper_Tensor_Scalar_out_pow_out_Tensor_Scalar_out(self, exponent, out);
}

} // namespace sparsecuda

} // namespace at
