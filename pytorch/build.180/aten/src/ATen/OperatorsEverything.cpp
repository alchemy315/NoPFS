#include <ATen/Operators.h>
#include <ATen/Tensor.h>
#include <ATen/core/dispatch/Dispatcher.h>

// NOTE See [Sharded File] comment in VariableType

namespace at { namespace _ops {


STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Byte, name, "aten::_cast_Byte")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Byte, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Byte, schema_str, "_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Byte::schema> create__cast_Byte_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Byte::name, _cast_Byte::overload_name)
      .typed<_cast_Byte::schema>();
}

// aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Byte::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Byte_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Byte::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Byte_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Char, name, "aten::_cast_Char")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Char, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Char, schema_str, "_cast_Char(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Char::schema> create__cast_Char_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Char::name, _cast_Char::overload_name)
      .typed<_cast_Char::schema>();
}

// aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Char::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Char_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Char(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Char::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Char_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Double, name, "aten::_cast_Double")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Double, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Double, schema_str, "_cast_Double(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Double::schema> create__cast_Double_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Double::name, _cast_Double::overload_name)
      .typed<_cast_Double::schema>();
}

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Double::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Double_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Double(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Double::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Double_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Float, name, "aten::_cast_Float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Float, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Float, schema_str, "_cast_Float(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Float::schema> create__cast_Float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Float::name, _cast_Float::overload_name)
      .typed<_cast_Float::schema>();
}

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Float::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Float_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Float(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Float::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Float_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Int, name, "aten::_cast_Int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Int, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Int, schema_str, "_cast_Int(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Int::schema> create__cast_Int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Int::name, _cast_Int::overload_name)
      .typed<_cast_Int::schema>();
}

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Int::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Int_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Int(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Int_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Long, name, "aten::_cast_Long")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Long, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Long, schema_str, "_cast_Long(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Long::schema> create__cast_Long_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Long::name, _cast_Long::overload_name)
      .typed<_cast_Long::schema>();
}

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Long::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Long_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Long(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Long::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Long_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Short, name, "aten::_cast_Short")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Short, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Short, schema_str, "_cast_Short(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Short::schema> create__cast_Short_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Short::name, _cast_Short::overload_name)
      .typed<_cast_Short::schema>();
}

// aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Short::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Short_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Short(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Short::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Short_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Half, name, "aten::_cast_Half")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Half, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cast_Half, schema_str, "_cast_Half(Tensor self, bool non_blocking=False) -> Tensor")

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cast_Half::schema> create__cast_Half_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cast_Half::name, _cast_Half::overload_name)
      .typed<_cast_Half::schema>();
}

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Half::call(const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Half_typed_handle();
    return op.call(self, non_blocking);
}

// aten::_cast_Half(Tensor self, bool non_blocking=False) -> Tensor
at::Tensor _cast_Half::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool non_blocking) {
    static auto op = create__cast_Half_typed_handle();
    return op.redispatch(dispatchKeySet, self, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_backward, name, "aten::_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_backward, schema_str, "_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()")

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_backward::schema> create__backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_backward::name, _backward::overload_name)
      .typed<_backward::schema>();
}

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()
void _backward::call(const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
    static auto op = create__backward_typed_handle();
    return op.call(self, inputs, gradient, retain_graph, create_graph);
}

// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -> ()
void _backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList inputs, const c10::optional<at::Tensor> & gradient, c10::optional<bool> retain_graph, bool create_graph) {
    static auto op = create__backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, inputs, gradient, retain_graph, create_graph);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set_data, name, "aten::set_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set_data, schema_str, "set_data(Tensor(a!) self, Tensor new_data) -> ()")

// aten::set_data(Tensor(a!) self, Tensor new_data) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<set_data::schema> create_set_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(set_data::name, set_data::overload_name)
      .typed<set_data::schema>();
}

// aten::set_data(Tensor(a!) self, Tensor new_data) -> ()
void set_data::call(at::Tensor & self, const at::Tensor & new_data) {
    static auto op = create_set_data_typed_handle();
    return op.call(self, new_data);
}

// aten::set_data(Tensor(a!) self, Tensor new_data) -> ()
void set_data::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & new_data) {
    static auto op = create_set_data_typed_handle();
    return op.redispatch(dispatchKeySet, self, new_data);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(data, name, "aten::data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(data, schema_str, "data(Tensor self) -> Tensor")

// aten::data(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<data::schema> create_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(data::name, data::overload_name)
      .typed<data::schema>();
}

// aten::data(Tensor self) -> Tensor
at::Tensor data::call(const at::Tensor & self) {
    static auto op = create_data_typed_handle();
    return op.call(self);
}

// aten::data(Tensor self) -> Tensor
at::Tensor data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_data_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_leaf, name, "aten::is_leaf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_leaf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_leaf, schema_str, "is_leaf(Tensor self) -> bool")

// aten::is_leaf(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_leaf::schema> create_is_leaf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_leaf::name, is_leaf::overload_name)
      .typed<is_leaf::schema>();
}

// aten::is_leaf(Tensor self) -> bool
bool is_leaf::call(const at::Tensor & self) {
    static auto op = create_is_leaf_typed_handle();
    return op.call(self);
}

// aten::is_leaf(Tensor self) -> bool
bool is_leaf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_leaf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(output_nr, name, "aten::output_nr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(output_nr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(output_nr, schema_str, "output_nr(Tensor self) -> int")

// aten::output_nr(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<output_nr::schema> create_output_nr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(output_nr::name, output_nr::overload_name)
      .typed<output_nr::schema>();
}

// aten::output_nr(Tensor self) -> int
int64_t output_nr::call(const at::Tensor & self) {
    static auto op = create_output_nr_typed_handle();
    return op.call(self);
}

// aten::output_nr(Tensor self) -> int
int64_t output_nr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_output_nr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_version, name, "aten::_version")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_version, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_version, schema_str, "_version(Tensor self) -> int")

// aten::_version(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_version::schema> create__version_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_version::name, _version::overload_name)
      .typed<_version::schema>();
}

// aten::_version(Tensor self) -> int
int64_t _version::call(const at::Tensor & self) {
    static auto op = create__version_typed_handle();
    return op.call(self);
}

// aten::_version(Tensor self) -> int
int64_t _version::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__version_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(requires_grad_, name, "aten::requires_grad_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(requires_grad_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(requires_grad_, schema_str, "requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)")

// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<requires_grad_::schema> create_requires_grad__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(requires_grad_::name, requires_grad_::overload_name)
      .typed<requires_grad_::schema>();
}

// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)
at::Tensor & requires_grad_::call(at::Tensor & self, bool requires_grad) {
    static auto op = create_requires_grad__typed_handle();
    return op.call(self, requires_grad);
}

// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -> Tensor(a!)
at::Tensor & requires_grad_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, bool requires_grad) {
    static auto op = create_requires_grad__typed_handle();
    return op.redispatch(dispatchKeySet, self, requires_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retain_grad, name, "aten::retain_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retain_grad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retain_grad, schema_str, "retain_grad(Tensor(a!) self) -> ()")

// aten::retain_grad(Tensor(a!) self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<retain_grad::schema> create_retain_grad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(retain_grad::name, retain_grad::overload_name)
      .typed<retain_grad::schema>();
}

// aten::retain_grad(Tensor(a!) self) -> ()
void retain_grad::call(at::Tensor & self) {
    static auto op = create_retain_grad_typed_handle();
    return op.call(self);
}

// aten::retain_grad(Tensor(a!) self) -> ()
void retain_grad::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_retain_grad_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retains_grad, name, "aten::retains_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retains_grad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(retains_grad, schema_str, "retains_grad(Tensor self) -> bool")

// aten::retains_grad(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<retains_grad::schema> create_retains_grad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(retains_grad::name, retains_grad::overload_name)
      .typed<retains_grad::schema>();
}

// aten::retains_grad(Tensor self) -> bool
bool retains_grad::call(const at::Tensor & self) {
    static auto op = create_retains_grad_typed_handle();
    return op.call(self);
}

// aten::retains_grad(Tensor self) -> bool
bool retains_grad::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_retains_grad_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal, name, "aten::_fw_primal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fw_primal, schema_str, "_fw_primal(Tensor(a) self, int level) -> Tensor(a)")

// aten::_fw_primal(Tensor(a) self, int level) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_fw_primal::schema> create__fw_primal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fw_primal::name, _fw_primal::overload_name)
      .typed<_fw_primal::schema>();
}

// aten::_fw_primal(Tensor(a) self, int level) -> Tensor(a)
at::Tensor _fw_primal::call(const at::Tensor & self, int64_t level) {
    static auto op = create__fw_primal_typed_handle();
    return op.call(self, level);
}

// aten::_fw_primal(Tensor(a) self, int level) -> Tensor(a)
at::Tensor _fw_primal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level) {
    static auto op = create__fw_primal_typed_handle();
    return op.redispatch(dispatchKeySet, self, level);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_dual, name, "aten::_make_dual")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_dual, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_dual, schema_str, "_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)")

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_make_dual::schema> create__make_dual_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_make_dual::name, _make_dual::overload_name)
      .typed<_make_dual::schema>();
}

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
at::Tensor _make_dual::call(const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
    static auto op = create__make_dual_typed_handle();
    return op.call(primal, tangent, level);
}

// aten::_make_dual(Tensor(a) primal, Tensor tangent, int level) -> Tensor(a)
at::Tensor _make_dual::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & primal, const at::Tensor & tangent, int64_t level) {
    static auto op = create__make_dual_typed_handle();
    return op.redispatch(dispatchKeySet, primal, tangent, level);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unpack_dual, name, "aten::_unpack_dual")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unpack_dual, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unpack_dual, schema_str, "_unpack_dual(Tensor(a) dual, int level) -> (Tensor(a) primal, Tensor tangent)")

// aten::_unpack_dual(Tensor(a) dual, int level) -> (Tensor(a) primal, Tensor tangent)
static C10_NOINLINE c10::TypedOperatorHandle<_unpack_dual::schema> create__unpack_dual_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unpack_dual::name, _unpack_dual::overload_name)
      .typed<_unpack_dual::schema>();
}

// aten::_unpack_dual(Tensor(a) dual, int level) -> (Tensor(a) primal, Tensor tangent)
::std::tuple<at::Tensor,at::Tensor> _unpack_dual::call(const at::Tensor & dual, int64_t level) {
    static auto op = create__unpack_dual_typed_handle();
    return op.call(dual, level);
}

// aten::_unpack_dual(Tensor(a) dual, int level) -> (Tensor(a) primal, Tensor tangent)
::std::tuple<at::Tensor,at::Tensor> _unpack_dual::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dual, int64_t level) {
    static auto op = create__unpack_dual_typed_handle();
    return op.redispatch(dispatchKeySet, dual, level);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename_, name, "aten::rename_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename_, schema_str, "rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)")

// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rename_::schema> create_rename__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rename_::name, rename_::overload_name)
      .typed<rename_::schema>();
}

// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)
at::Tensor & rename_::call(at::Tensor & self, c10::optional<at::DimnameList> names) {
    static auto op = create_rename__typed_handle();
    return op.call(self, names);
}

// aten::rename_(Tensor(a!) self, Dimname[]? names) -> Tensor(a!)
at::Tensor & rename_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::DimnameList> names) {
    static auto op = create_rename__typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename, name, "aten::rename")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rename, schema_str, "rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)")

// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<rename::schema> create_rename_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rename::name, rename::overload_name)
      .typed<rename::schema>();
}

// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)
at::Tensor rename::call(const at::Tensor & self, c10::optional<at::DimnameList> names) {
    static auto op = create_rename_typed_handle();
    return op.call(self, names);
}

// aten::rename(Tensor(a) self, Dimname[]? names) -> Tensor(a)
at::Tensor rename::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::DimnameList> names) {
    static auto op = create_rename_typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_to, name, "aten::align_to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_to, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_to, schema_str, "align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)")

// aten::align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<align_to::schema> create_align_to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(align_to::name, align_to::overload_name)
      .typed<align_to::schema>();
}

// aten::align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)
at::Tensor align_to::call(const at::Tensor & self, at::DimnameList names) {
    static auto op = create_align_to_typed_handle();
    return op.call(self, names);
}

// aten::align_to(Tensor(a) self, Dimname[] names) -> Tensor(a)
at::Tensor align_to::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList names) {
    static auto op = create_align_to_typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_to_ellipsis_idx, name, "aten::align_to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_to_ellipsis_idx, overload_name, "ellipsis_idx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_to_ellipsis_idx, schema_str, "align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)")

// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<align_to_ellipsis_idx::schema> create_align_to_ellipsis_idx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(align_to_ellipsis_idx::name, align_to_ellipsis_idx::overload_name)
      .typed<align_to_ellipsis_idx::schema>();
}

// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)
at::Tensor align_to_ellipsis_idx::call(const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
    static auto op = create_align_to_ellipsis_idx_typed_handle();
    return op.call(self, order, ellipsis_idx);
}

// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -> Tensor(a)
at::Tensor align_to_ellipsis_idx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList order, int64_t ellipsis_idx) {
    static auto op = create_align_to_ellipsis_idx_typed_handle();
    return op.redispatch(dispatchKeySet, self, order, ellipsis_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_as, name, "aten::align_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_as, schema_str, "align_as(Tensor self, Tensor other) -> Tensor")

// aten::align_as(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<align_as::schema> create_align_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(align_as::name, align_as::overload_name)
      .typed<align_as::schema>();
}

// aten::align_as(Tensor self, Tensor other) -> Tensor
at::Tensor align_as::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_align_as_typed_handle();
    return op.call(self, other);
}

// aten::align_as(Tensor self, Tensor other) -> Tensor
at::Tensor align_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_align_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_tensors, name, "aten::align_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(align_tensors, schema_str, "align_tensors(Tensor[] tensors) -> Tensor[]")

// aten::align_tensors(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<align_tensors::schema> create_align_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(align_tensors::name, align_tensors::overload_name)
      .typed<align_tensors::schema>();
}

// aten::align_tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> align_tensors::call(at::TensorList tensors) {
    static auto op = create_align_tensors_typed_handle();
    return op.call(tensors);
}

// aten::align_tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> align_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_align_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_assert_async, name, "aten::_assert_async")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_assert_async, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_assert_async, schema_str, "_assert_async(Tensor self) -> ()")

// aten::_assert_async(Tensor self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_assert_async::schema> create__assert_async_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_assert_async::name, _assert_async::overload_name)
      .typed<_assert_async::schema>();
}

// aten::_assert_async(Tensor self) -> ()
void _assert_async::call(const at::Tensor & self) {
    static auto op = create__assert_async_typed_handle();
    return op.call(self);
}

// aten::_assert_async(Tensor self) -> ()
void _assert_async::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__assert_async_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(refine_names, name, "aten::refine_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(refine_names, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(refine_names, schema_str, "refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)")

// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<refine_names::schema> create_refine_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(refine_names::name, refine_names::overload_name)
      .typed<refine_names::schema>();
}

// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)
at::Tensor refine_names::call(const at::Tensor & self, at::DimnameList names) {
    static auto op = create_refine_names_typed_handle();
    return op.call(self, names);
}

// aten::refine_names(Tensor(a) self, Dimname[] names) -> Tensor(a)
at::Tensor refine_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList names) {
    static auto op = create_refine_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_ctc_loss, name, "aten::_use_cudnn_ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_ctc_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_ctc_loss, schema_str, "_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool")

// aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<_use_cudnn_ctc_loss::schema> create__use_cudnn_ctc_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_use_cudnn_ctc_loss::name, _use_cudnn_ctc_loss::overload_name)
      .typed<_use_cudnn_ctc_loss::schema>();
}

// aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool
bool _use_cudnn_ctc_loss::call(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank) {
    static auto op = create__use_cudnn_ctc_loss_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank);
}

// aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool
bool _use_cudnn_ctc_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank) {
    static auto op = create__use_cudnn_ctc_loss_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_ctc_loss, name, "aten::_cudnn_ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_ctc_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_ctc_loss, schema_str, "_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)")

// aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_ctc_loss::schema> create__cudnn_ctc_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_ctc_loss::name, _cudnn_ctc_loss::overload_name)
      .typed<_cudnn_ctc_loss::schema>();
}

// aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _cudnn_ctc_loss::call(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) {
    static auto op = create__cudnn_ctc_loss_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank, deterministic, zero_infinity);
}

// aten::_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _cudnn_ctc_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) {
    static auto op = create__cudnn_ctc_loss_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, deterministic, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_rnn_flatten_weight, name, "aten::_use_cudnn_rnn_flatten_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_rnn_flatten_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_use_cudnn_rnn_flatten_weight, schema_str, "_use_cudnn_rnn_flatten_weight() -> bool")

// aten::_use_cudnn_rnn_flatten_weight() -> bool
static C10_NOINLINE c10::TypedOperatorHandle<_use_cudnn_rnn_flatten_weight::schema> create__use_cudnn_rnn_flatten_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_use_cudnn_rnn_flatten_weight::name, _use_cudnn_rnn_flatten_weight::overload_name)
      .typed<_use_cudnn_rnn_flatten_weight::schema>();
}

// aten::_use_cudnn_rnn_flatten_weight() -> bool
bool _use_cudnn_rnn_flatten_weight::call() {
    static auto op = create__use_cudnn_rnn_flatten_weight_typed_handle();
    return op.call();
}

// aten::_use_cudnn_rnn_flatten_weight() -> bool
bool _use_cudnn_rnn_flatten_weight::redispatch(c10::DispatchKeySet dispatchKeySet) {
    static auto op = create__use_cudnn_rnn_flatten_weight_typed_handle();
    return op.redispatch(dispatchKeySet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight, name, "aten::_cudnn_rnn_flatten_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_flatten_weight, schema_str, "_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, int input_size, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor")

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, int input_size, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn_flatten_weight::schema> create__cudnn_rnn_flatten_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn_flatten_weight::name, _cudnn_rnn_flatten_weight::overload_name)
      .typed<_cudnn_rnn_flatten_weight::schema>();
}

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, int input_size, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
at::Tensor _cudnn_rnn_flatten_weight::call(at::TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, bool bidirectional) {
    static auto op = create__cudnn_rnn_flatten_weight_typed_handle();
    return op.call(weight_arr, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional);
}

// aten::_cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, int input_size, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, bool bidirectional) -> Tensor
at::Tensor _cudnn_rnn_flatten_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, bool bidirectional) {
    static auto op = create__cudnn_rnn_flatten_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_arr, weight_stride0, input_size, mode, hidden_size, proj_size, num_layers, batch_first, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn, name, "aten::_cudnn_rnn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn, schema_str, "_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn::schema> create__cudnn_rnn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn::name, _cudnn_rnn::overload_name)
      .typed<_cudnn_rnn::schema>();
}

// aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _cudnn_rnn::call(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const c10::optional<at::Tensor> & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
    static auto op = create__cudnn_rnn_typed_handle();
    return op.call(input, weight, weight_stride0, weight_buf, hx, cx, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
}

// aten::_cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _cudnn_rnn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const c10::optional<at::Tensor> & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
    static auto op = create__cudnn_rnn_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, weight_stride0, weight_buf, hx, cx, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward, name, "aten::_cudnn_rnn_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_rnn_backward, schema_str, "_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])")

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_rnn_backward::schema> create__cudnn_rnn_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_rnn_backward::name, _cudnn_rnn_backward::overload_name)
      .typed<_cudnn_rnn_backward::schema>();
}

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> _cudnn_rnn_backward::call(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
    static auto op = create__cudnn_rnn_backward_typed_handle();
    return op.call(input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}

// aten::_cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> _cudnn_rnn_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t proj_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
    static auto op = create__cudnn_rnn_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_init_dropout_state, name, "aten::_cudnn_init_dropout_state")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_init_dropout_state, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cudnn_init_dropout_state, schema_str, "_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cudnn_init_dropout_state::schema> create__cudnn_init_dropout_state_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cudnn_init_dropout_state::name, _cudnn_init_dropout_state::overload_name)
      .typed<_cudnn_init_dropout_state::schema>();
}

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _cudnn_init_dropout_state::call(double dropout, bool train, int64_t dropout_seed, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__cudnn_init_dropout_state_typed_handle();
    return op.call(dropout, train, dropout_seed, dtype, layout, device, pin_memory);
}

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _cudnn_init_dropout_state::redispatch(c10::DispatchKeySet dispatchKeySet, double dropout, bool train, int64_t dropout_seed, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__cudnn_init_dropout_state_typed_handle();
    return op.redispatch(dispatchKeySet, dropout, train, dropout_seed, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_debug_has_internal_overlap, name, "aten::_debug_has_internal_overlap")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_debug_has_internal_overlap, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_debug_has_internal_overlap, schema_str, "_debug_has_internal_overlap(Tensor self) -> int")

// aten::_debug_has_internal_overlap(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_debug_has_internal_overlap::schema> create__debug_has_internal_overlap_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_debug_has_internal_overlap::name, _debug_has_internal_overlap::overload_name)
      .typed<_debug_has_internal_overlap::schema>();
}

// aten::_debug_has_internal_overlap(Tensor self) -> int
int64_t _debug_has_internal_overlap::call(const at::Tensor & self) {
    static auto op = create__debug_has_internal_overlap_typed_handle();
    return op.call(self);
}

// aten::_debug_has_internal_overlap(Tensor self) -> int
int64_t _debug_has_internal_overlap::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__debug_has_internal_overlap_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_dropout, name, "aten::_fused_dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_dropout, schema_str, "_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)")

// aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_fused_dropout::schema> create__fused_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_dropout::name, _fused_dropout::overload_name)
      .typed<_fused_dropout::schema>();
}

// aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _fused_dropout::call(const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create__fused_dropout_typed_handle();
    return op.call(self, p, generator);
}

// aten::_fused_dropout(Tensor self, float p, Generator? generator=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _fused_dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create__fused_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_scale, name, "aten::_masked_scale")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_scale, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_masked_scale, schema_str, "_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor")

// aten::_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_masked_scale::schema> create__masked_scale_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_masked_scale::name, _masked_scale::overload_name)
      .typed<_masked_scale::schema>();
}

// aten::_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor
at::Tensor _masked_scale::call(const at::Tensor & self, const at::Tensor & mask, double scale) {
    static auto op = create__masked_scale_typed_handle();
    return op.call(self, mask, scale);
}

// aten::_masked_scale(Tensor self, Tensor mask, float scale) -> Tensor
at::Tensor _masked_scale::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, double scale) {
    static auto op = create__masked_scale_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_draw, name, "aten::_sobol_engine_draw")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_draw, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_draw, schema_str, "_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)")

// aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_sobol_engine_draw::schema> create__sobol_engine_draw_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sobol_engine_draw::name, _sobol_engine_draw::overload_name)
      .typed<_sobol_engine_draw::schema>();
}

// aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _sobol_engine_draw::call(const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sobol_engine_draw_typed_handle();
    return op.call(quasi, n, sobolstate, dimension, num_generated, dtype);
}

// aten::_sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _sobol_engine_draw::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & quasi, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sobol_engine_draw_typed_handle();
    return op.redispatch(dispatchKeySet, quasi, n, sobolstate, dimension, num_generated, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_ff_, name, "aten::_sobol_engine_ff_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_ff_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_ff_, schema_str, "_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)")

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sobol_engine_ff_::schema> create__sobol_engine_ff__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sobol_engine_ff_::name, _sobol_engine_ff_::overload_name)
      .typed<_sobol_engine_ff_::schema>();
}

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
at::Tensor & _sobol_engine_ff_::call(at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
    static auto op = create__sobol_engine_ff__typed_handle();
    return op.call(self, n, sobolstate, dimension, num_generated);
}

// aten::_sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -> Tensor(a!)
at::Tensor & _sobol_engine_ff_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t n, const at::Tensor & sobolstate, int64_t dimension, int64_t num_generated) {
    static auto op = create__sobol_engine_ff__typed_handle();
    return op.redispatch(dispatchKeySet, self, n, sobolstate, dimension, num_generated);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_scramble_, name, "aten::_sobol_engine_scramble_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_scramble_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_scramble_, schema_str, "_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)")

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sobol_engine_scramble_::schema> create__sobol_engine_scramble__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sobol_engine_scramble_::name, _sobol_engine_scramble_::overload_name)
      .typed<_sobol_engine_scramble_::schema>();
}

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
at::Tensor & _sobol_engine_scramble_::call(at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
    static auto op = create__sobol_engine_scramble__typed_handle();
    return op.call(self, ltm, dimension);
}

// aten::_sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -> Tensor(a!)
at::Tensor & _sobol_engine_scramble_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & ltm, int64_t dimension) {
    static auto op = create__sobol_engine_scramble__typed_handle();
    return op.redispatch(dispatchKeySet, self, ltm, dimension);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_initialize_state_, name, "aten::_sobol_engine_initialize_state_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_initialize_state_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sobol_engine_initialize_state_, schema_str, "_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)")

// aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_sobol_engine_initialize_state_::schema> create__sobol_engine_initialize_state__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sobol_engine_initialize_state_::name, _sobol_engine_initialize_state_::overload_name)
      .typed<_sobol_engine_initialize_state_::schema>();
}

// aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)
at::Tensor & _sobol_engine_initialize_state_::call(at::Tensor & self, int64_t dimension) {
    static auto op = create__sobol_engine_initialize_state__typed_handle();
    return op.call(self, dimension);
}

// aten::_sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -> Tensor(a!)
at::Tensor & _sobol_engine_initialize_state_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dimension) {
    static auto op = create__sobol_engine_initialize_state__typed_handle();
    return op.redispatch(dispatchKeySet, self, dimension);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_from_tensor, name, "aten::_reshape_from_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_from_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_from_tensor, schema_str, "_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor")

// aten::_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_reshape_from_tensor::schema> create__reshape_from_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_reshape_from_tensor::name, _reshape_from_tensor::overload_name)
      .typed<_reshape_from_tensor::schema>();
}

// aten::_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor
at::Tensor _reshape_from_tensor::call(const at::Tensor & self, const at::Tensor & shape) {
    static auto op = create__reshape_from_tensor_typed_handle();
    return op.call(self, shape);
}

// aten::_reshape_from_tensor(Tensor self, Tensor shape) -> Tensor
at::Tensor _reshape_from_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & shape) {
    static auto op = create__reshape_from_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, shape);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_shape_as_tensor, name, "aten::_shape_as_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_shape_as_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_shape_as_tensor, schema_str, "_shape_as_tensor(Tensor self) -> Tensor")

// aten::_shape_as_tensor(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_shape_as_tensor::schema> create__shape_as_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_shape_as_tensor::name, _shape_as_tensor::overload_name)
      .typed<_shape_as_tensor::schema>();
}

// aten::_shape_as_tensor(Tensor self) -> Tensor
at::Tensor _shape_as_tensor::call(const at::Tensor & self) {
    static auto op = create__shape_as_tensor_typed_handle();
    return op.call(self);
}

// aten::_shape_as_tensor(Tensor self) -> Tensor
at::Tensor _shape_as_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__shape_as_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dropout, name, "aten::dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dropout, schema_str, "dropout(Tensor input, float p, bool train) -> Tensor")

// aten::dropout(Tensor input, float p, bool train) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<dropout::schema> create_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dropout::name, dropout::overload_name)
      .typed<dropout::schema>();
}

// aten::dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor dropout::call(const at::Tensor & input, double p, bool train) {
    static auto op = create_dropout_typed_handle();
    return op.call(input, p, train);
}

// aten::dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
    static auto op = create_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, input, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dropout_, name, "aten::dropout_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dropout_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dropout_, schema_str, "dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)")

// aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<dropout_::schema> create_dropout__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dropout_::name, dropout_::overload_name)
      .typed<dropout_::schema>();
}

// aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & dropout_::call(at::Tensor & self, double p, bool train) {
    static auto op = create_dropout__typed_handle();
    return op.call(self, p, train);
}

// aten::dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & dropout_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
    static auto op = create_dropout__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout, name, "aten::feature_dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout, schema_str, "feature_dropout(Tensor input, float p, bool train) -> Tensor")

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<feature_dropout::schema> create_feature_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_dropout::name, feature_dropout::overload_name)
      .typed<feature_dropout::schema>();
}

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_dropout::call(const at::Tensor & input, double p, bool train) {
    static auto op = create_feature_dropout_typed_handle();
    return op.call(input, p, train);
}

// aten::feature_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
    static auto op = create_feature_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, input, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout_, name, "aten::feature_dropout_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_dropout_, schema_str, "feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)")

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<feature_dropout_::schema> create_feature_dropout__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_dropout_::name, feature_dropout_::overload_name)
      .typed<feature_dropout_::schema>();
}

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_dropout_::call(at::Tensor & self, double p, bool train) {
    static auto op = create_feature_dropout__typed_handle();
    return op.call(self, p, train);
}

// aten::feature_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_dropout_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
    static auto op = create_feature_dropout__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alpha_dropout, name, "aten::alpha_dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alpha_dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alpha_dropout, schema_str, "alpha_dropout(Tensor input, float p, bool train) -> Tensor")

// aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<alpha_dropout::schema> create_alpha_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(alpha_dropout::name, alpha_dropout::overload_name)
      .typed<alpha_dropout::schema>();
}

// aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor alpha_dropout::call(const at::Tensor & input, double p, bool train) {
    static auto op = create_alpha_dropout_typed_handle();
    return op.call(input, p, train);
}

// aten::alpha_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor alpha_dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
    static auto op = create_alpha_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, input, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alpha_dropout_, name, "aten::alpha_dropout_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alpha_dropout_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alpha_dropout_, schema_str, "alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)")

// aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<alpha_dropout_::schema> create_alpha_dropout__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(alpha_dropout_::name, alpha_dropout_::overload_name)
      .typed<alpha_dropout_::schema>();
}

// aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & alpha_dropout_::call(at::Tensor & self, double p, bool train) {
    static auto op = create_alpha_dropout__typed_handle();
    return op.call(self, p, train);
}

// aten::alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & alpha_dropout_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
    static auto op = create_alpha_dropout__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout, name, "aten::feature_alpha_dropout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout, schema_str, "feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor")

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<feature_alpha_dropout::schema> create_feature_alpha_dropout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_alpha_dropout::name, feature_alpha_dropout::overload_name)
      .typed<feature_alpha_dropout::schema>();
}

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_alpha_dropout::call(const at::Tensor & input, double p, bool train) {
    static auto op = create_feature_alpha_dropout_typed_handle();
    return op.call(input, p, train);
}

// aten::feature_alpha_dropout(Tensor input, float p, bool train) -> Tensor
at::Tensor feature_alpha_dropout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double p, bool train) {
    static auto op = create_feature_alpha_dropout_typed_handle();
    return op.redispatch(dispatchKeySet, input, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout_, name, "aten::feature_alpha_dropout_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(feature_alpha_dropout_, schema_str, "feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)")

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<feature_alpha_dropout_::schema> create_feature_alpha_dropout__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(feature_alpha_dropout_::name, feature_alpha_dropout_::overload_name)
      .typed<feature_alpha_dropout_::schema>();
}

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_alpha_dropout_::call(at::Tensor & self, double p, bool train) {
    static auto op = create_feature_alpha_dropout__typed_handle();
    return op.call(self, p, train);
}

// aten::feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
at::Tensor & feature_alpha_dropout_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, bool train) {
    static auto op = create_feature_alpha_dropout__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, train);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs, name, "aten::abs")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs, schema_str, "abs(Tensor self) -> Tensor")

// aten::abs(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<abs::schema> create_abs_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(abs::name, abs::overload_name)
      .typed<abs::schema>();
}

// aten::abs(Tensor self) -> Tensor
at::Tensor abs::call(const at::Tensor & self) {
    static auto op = create_abs_typed_handle();
    return op.call(self);
}

// aten::abs(Tensor self) -> Tensor
at::Tensor abs::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_abs_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_, name, "aten::abs_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_, schema_str, "abs_(Tensor(a!) self) -> Tensor(a!)")

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<abs_::schema> create_abs__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(abs_::name, abs_::overload_name)
      .typed<abs_::schema>();
}

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & abs_::call(at::Tensor & self) {
    static auto op = create_abs__typed_handle();
    return op.call(self);
}

// aten::abs_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & abs_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_abs__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_out, name, "aten::abs")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(abs_out, schema_str, "abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<abs_out::schema> create_abs_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(abs_out::name, abs_out::overload_name)
      .typed<abs_out::schema>();
}

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & abs_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_abs_out_typed_handle();
    return op.call(self, out);
}

// aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & abs_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_abs_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute, name, "aten::absolute")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute, schema_str, "absolute(Tensor self) -> Tensor")

// aten::absolute(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<absolute::schema> create_absolute_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(absolute::name, absolute::overload_name)
      .typed<absolute::schema>();
}

// aten::absolute(Tensor self) -> Tensor
at::Tensor absolute::call(const at::Tensor & self) {
    static auto op = create_absolute_typed_handle();
    return op.call(self);
}

// aten::absolute(Tensor self) -> Tensor
at::Tensor absolute::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_absolute_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute_, name, "aten::absolute_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute_, schema_str, "absolute_(Tensor(a!) self) -> Tensor(a!)")

// aten::absolute_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<absolute_::schema> create_absolute__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(absolute_::name, absolute_::overload_name)
      .typed<absolute_::schema>();
}

// aten::absolute_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & absolute_::call(at::Tensor & self) {
    static auto op = create_absolute__typed_handle();
    return op.call(self);
}

// aten::absolute_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & absolute_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_absolute__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute_out, name, "aten::absolute")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(absolute_out, schema_str, "absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<absolute_out::schema> create_absolute_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(absolute_out::name, absolute_out::overload_name)
      .typed<absolute_out::schema>();
}

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & absolute_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_absolute_out_typed_handle();
    return op.call(self, out);
}

// aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & absolute_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_absolute_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(angle, name, "aten::angle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(angle, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(angle, schema_str, "angle(Tensor self) -> Tensor")

// aten::angle(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<angle::schema> create_angle_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(angle::name, angle::overload_name)
      .typed<angle::schema>();
}

// aten::angle(Tensor self) -> Tensor
at::Tensor angle::call(const at::Tensor & self) {
    static auto op = create_angle_typed_handle();
    return op.call(self);
}

// aten::angle(Tensor self) -> Tensor
at::Tensor angle::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_angle_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(angle_out, name, "aten::angle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(angle_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(angle_out, schema_str, "angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<angle_out::schema> create_angle_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(angle_out::name, angle_out::overload_name)
      .typed<angle_out::schema>();
}

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & angle_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_angle_out_typed_handle();
    return op.call(self, out);
}

// aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & angle_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_angle_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real, name, "aten::view_as_real")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_real, schema_str, "view_as_real(Tensor(a) self) -> Tensor(a)")

// aten::view_as_real(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view_as_real::schema> create_view_as_real_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_as_real::name, view_as_real::overload_name)
      .typed<view_as_real::schema>();
}

// aten::view_as_real(Tensor(a) self) -> Tensor(a)
at::Tensor view_as_real::call(const at::Tensor & self) {
    static auto op = create_view_as_real_typed_handle();
    return op.call(self);
}

// aten::view_as_real(Tensor(a) self) -> Tensor(a)
at::Tensor view_as_real::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_view_as_real_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_complex, name, "aten::view_as_complex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_complex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as_complex, schema_str, "view_as_complex(Tensor(a) self) -> Tensor(a)")

// aten::view_as_complex(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view_as_complex::schema> create_view_as_complex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_as_complex::name, view_as_complex::overload_name)
      .typed<view_as_complex::schema>();
}

// aten::view_as_complex(Tensor(a) self) -> Tensor(a)
at::Tensor view_as_complex::call(const at::Tensor & self) {
    static auto op = create_view_as_complex_typed_handle();
    return op.call(self);
}

// aten::view_as_complex(Tensor(a) self) -> Tensor(a)
at::Tensor view_as_complex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_view_as_complex_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn, name, "aten::sgn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn, schema_str, "sgn(Tensor self) -> Tensor")

// aten::sgn(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sgn::schema> create_sgn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sgn::name, sgn::overload_name)
      .typed<sgn::schema>();
}

// aten::sgn(Tensor self) -> Tensor
at::Tensor sgn::call(const at::Tensor & self) {
    static auto op = create_sgn_typed_handle();
    return op.call(self);
}

// aten::sgn(Tensor self) -> Tensor
at::Tensor sgn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sgn_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn_, name, "aten::sgn_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn_, schema_str, "sgn_(Tensor(a!) self) -> Tensor(a!)")

// aten::sgn_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sgn_::schema> create_sgn__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sgn_::name, sgn_::overload_name)
      .typed<sgn_::schema>();
}

// aten::sgn_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sgn_::call(at::Tensor & self) {
    static auto op = create_sgn__typed_handle();
    return op.call(self);
}

// aten::sgn_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sgn_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sgn__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn_out, name, "aten::sgn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sgn_out, schema_str, "sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sgn_out::schema> create_sgn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sgn_out::name, sgn_out::overload_name)
      .typed<sgn_out::schema>();
}

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sgn_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sgn_out_typed_handle();
    return op.call(self, out);
}

// aten::sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sgn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sgn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(real, name, "aten::real")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(real, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(real, schema_str, "real(Tensor(a) self) -> Tensor(a)")

// aten::real(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<real::schema> create_real_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(real::name, real::overload_name)
      .typed<real::schema>();
}

// aten::real(Tensor(a) self) -> Tensor(a)
at::Tensor real::call(const at::Tensor & self) {
    static auto op = create_real_typed_handle();
    return op.call(self);
}

// aten::real(Tensor(a) self) -> Tensor(a)
at::Tensor real::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_real_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(imag, name, "aten::imag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(imag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(imag, schema_str, "imag(Tensor(a) self) -> Tensor(a)")

// aten::imag(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<imag::schema> create_imag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(imag::name, imag::overload_name)
      .typed<imag::schema>();
}

// aten::imag(Tensor(a) self) -> Tensor(a)
at::Tensor imag::call(const at::Tensor & self) {
    static auto op = create_imag_typed_handle();
    return op.call(self);
}

// aten::imag(Tensor(a) self) -> Tensor(a)
at::Tensor imag::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_imag_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj, name, "aten::_conj")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj, schema_str, "_conj(Tensor(a) self) -> Tensor(a)")

// aten::_conj(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_conj::schema> create__conj_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conj::name, _conj::overload_name)
      .typed<_conj::schema>();
}

// aten::_conj(Tensor(a) self) -> Tensor(a)
at::Tensor _conj::call(const at::Tensor & self) {
    static auto op = create__conj_typed_handle();
    return op.call(self);
}

// aten::_conj(Tensor(a) self) -> Tensor(a)
at::Tensor _conj::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__conj_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj, name, "aten::conj")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj, schema_str, "conj(Tensor(a) self) -> Tensor(a)")

// aten::conj(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<conj::schema> create_conj_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conj::name, conj::overload_name)
      .typed<conj::schema>();
}

// aten::conj(Tensor(a) self) -> Tensor(a)
at::Tensor conj::call(const at::Tensor & self) {
    static auto op = create_conj_typed_handle();
    return op.call(self);
}

// aten::conj(Tensor(a) self) -> Tensor(a)
at::Tensor conj::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_conj_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_physical, name, "aten::_conj_physical")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_physical, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conj_physical, schema_str, "_conj_physical(Tensor self) -> Tensor")

// aten::_conj_physical(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_conj_physical::schema> create__conj_physical_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conj_physical::name, _conj_physical::overload_name)
      .typed<_conj_physical::schema>();
}

// aten::_conj_physical(Tensor self) -> Tensor
at::Tensor _conj_physical::call(const at::Tensor & self) {
    static auto op = create__conj_physical_typed_handle();
    return op.call(self);
}

// aten::_conj_physical(Tensor self) -> Tensor
at::Tensor _conj_physical::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__conj_physical_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical, name, "aten::conj_physical")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical, schema_str, "conj_physical(Tensor self) -> Tensor")

// aten::conj_physical(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conj_physical::schema> create_conj_physical_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conj_physical::name, conj_physical::overload_name)
      .typed<conj_physical::schema>();
}

// aten::conj_physical(Tensor self) -> Tensor
at::Tensor conj_physical::call(const at::Tensor & self) {
    static auto op = create_conj_physical_typed_handle();
    return op.call(self);
}

// aten::conj_physical(Tensor self) -> Tensor
at::Tensor conj_physical::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_conj_physical_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical_out, name, "aten::conj_physical")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical_out, schema_str, "conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<conj_physical_out::schema> create_conj_physical_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conj_physical_out::name, conj_physical_out::overload_name)
      .typed<conj_physical_out::schema>();
}

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & conj_physical_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_conj_physical_out_typed_handle();
    return op.call(self, out);
}

// aten::conj_physical.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & conj_physical_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_conj_physical_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical_, name, "aten::conj_physical_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conj_physical_, schema_str, "conj_physical_(Tensor(a!) self) -> Tensor(a!)")

// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<conj_physical_::schema> create_conj_physical__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conj_physical_::name, conj_physical_::overload_name)
      .typed<conj_physical_::schema>();
}

// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & conj_physical_::call(at::Tensor & self) {
    static auto op = create_conj_physical__typed_handle();
    return op.call(self);
}

// aten::conj_physical_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & conj_physical_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_conj_physical__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_conj, name, "aten::resolve_conj")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_conj, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_conj, schema_str, "resolve_conj(Tensor(a) self) -> Tensor(a)")

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<resolve_conj::schema> create_resolve_conj_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resolve_conj::name, resolve_conj::overload_name)
      .typed<resolve_conj::schema>();
}

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_conj::call(const at::Tensor & self) {
    static auto op = create_resolve_conj_typed_handle();
    return op.call(self);
}

// aten::resolve_conj(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_conj::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_resolve_conj_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_neg, name, "aten::resolve_neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_neg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resolve_neg, schema_str, "resolve_neg(Tensor(a) self) -> Tensor(a)")

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<resolve_neg::schema> create_resolve_neg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resolve_neg::name, resolve_neg::overload_name)
      .typed<resolve_neg::schema>();
}

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_neg::call(const at::Tensor & self) {
    static auto op = create_resolve_neg_typed_handle();
    return op.call(self);
}

// aten::resolve_neg(Tensor(a) self) -> Tensor(a)
at::Tensor resolve_neg::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_resolve_neg_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_neg_view, name, "aten::_neg_view")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_neg_view, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_neg_view, schema_str, "_neg_view(Tensor(a) self) -> Tensor(a)")

// aten::_neg_view(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_neg_view::schema> create__neg_view_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_neg_view::name, _neg_view::overload_name)
      .typed<_neg_view::schema>();
}

// aten::_neg_view(Tensor(a) self) -> Tensor(a)
at::Tensor _neg_view::call(const at::Tensor & self) {
    static auto op = create__neg_view_typed_handle();
    return op.call(self);
}

// aten::_neg_view(Tensor(a) self) -> Tensor(a)
at::Tensor _neg_view::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__neg_view_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos, name, "aten::acos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos, schema_str, "acos(Tensor self) -> Tensor")

// aten::acos(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<acos::schema> create_acos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(acos::name, acos::overload_name)
      .typed<acos::schema>();
}

// aten::acos(Tensor self) -> Tensor
at::Tensor acos::call(const at::Tensor & self) {
    static auto op = create_acos_typed_handle();
    return op.call(self);
}

// aten::acos(Tensor self) -> Tensor
at::Tensor acos::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_acos_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos_, name, "aten::acos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos_, schema_str, "acos_(Tensor(a!) self) -> Tensor(a!)")

// aten::acos_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<acos_::schema> create_acos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(acos_::name, acos_::overload_name)
      .typed<acos_::schema>();
}

// aten::acos_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & acos_::call(at::Tensor & self) {
    static auto op = create_acos__typed_handle();
    return op.call(self);
}

// aten::acos_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & acos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_acos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos_out, name, "aten::acos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acos_out, schema_str, "acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<acos_out::schema> create_acos_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(acos_out::name, acos_out::overload_name)
      .typed<acos_out::schema>();
}

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & acos_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_acos_out_typed_handle();
    return op.call(self, out);
}

// aten::acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & acos_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_acos_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos, name, "aten::arccos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos, schema_str, "arccos(Tensor self) -> Tensor")

// aten::arccos(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arccos::schema> create_arccos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arccos::name, arccos::overload_name)
      .typed<arccos::schema>();
}

// aten::arccos(Tensor self) -> Tensor
at::Tensor arccos::call(const at::Tensor & self) {
    static auto op = create_arccos_typed_handle();
    return op.call(self);
}

// aten::arccos(Tensor self) -> Tensor
at::Tensor arccos::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_arccos_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos_, name, "aten::arccos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos_, schema_str, "arccos_(Tensor(a!) self) -> Tensor(a!)")

// aten::arccos_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arccos_::schema> create_arccos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arccos_::name, arccos_::overload_name)
      .typed<arccos_::schema>();
}

// aten::arccos_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arccos_::call(at::Tensor & self) {
    static auto op = create_arccos__typed_handle();
    return op.call(self);
}

// aten::arccos_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arccos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_arccos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos_out, name, "aten::arccos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccos_out, schema_str, "arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arccos_out::schema> create_arccos_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arccos_out::name, arccos_out::overload_name)
      .typed<arccos_out::schema>();
}

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arccos_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arccos_out_typed_handle();
    return op.call(self, out);
}

// aten::arccos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arccos_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arccos_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool1d, name, "aten::avg_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool1d, schema_str, "avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor")

// aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool1d::schema> create_avg_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool1d::name, avg_pool1d::overload_name)
      .typed<avg_pool1d::schema>();
}

// aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor
at::Tensor avg_pool1d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
    static auto op = create_avg_pool1d_typed_handle();
    return op.call(self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}

// aten::avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor
at::Tensor avg_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad) {
    static auto op = create_avg_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool1d, name, "aten::adaptive_avg_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool1d, schema_str, "adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor")

// aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool1d::schema> create_adaptive_avg_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool1d::name, adaptive_avg_pool1d::overload_name)
      .typed<adaptive_avg_pool1d::schema>();
}

// aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
at::Tensor adaptive_avg_pool1d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_avg_pool1d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
at::Tensor adaptive_avg_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_avg_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool1d, name, "aten::adaptive_max_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool1d, schema_str, "adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)")

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool1d::schema> create_adaptive_max_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool1d::name, adaptive_max_pool1d::overload_name)
      .typed<adaptive_max_pool1d::schema>();
}

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool1d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_max_pool1d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_max_pool1d(Tensor self, int[1] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_max_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_Tensor, name, "aten::add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_Tensor, schema_str, "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<add_Tensor::schema> create_add_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(add_Tensor::name, add_Tensor::overload_name)
      .typed<add_Tensor::schema>();
}

// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor add_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_add_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor add_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_add_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add__Tensor, name, "aten::add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add__Tensor, schema_str, "add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")

// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<add__Tensor::schema> create_add__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(add__Tensor::name, add__Tensor::overload_name)
      .typed<add__Tensor::schema>();
}

// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & add__Tensor::call(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_add__Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & add__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_add__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_out, name, "aten::add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_out, schema_str, "add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<add_out::schema> create_add_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(add_out::name, add_out::overload_name)
      .typed<add_out::schema>();
}

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & add_out::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_add_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & add_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_add_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Tensor, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Tensor, schema_str, "_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_Tensor::schema> create__add_relu_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_Tensor::name, _add_relu_Tensor::overload_name)
      .typed<_add_relu_Tensor::schema>();
}

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create__add_relu_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create__add_relu_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Tensor, name, "aten::_add_relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Tensor, schema_str, "_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu__Tensor::schema> create__add_relu__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu__Tensor::name, _add_relu__Tensor::overload_name)
      .typed<_add_relu__Tensor::schema>();
}

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Tensor::call(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create__add_relu__Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create__add_relu__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_out, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_out, schema_str, "_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_out::schema> create__add_relu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_out::name, _add_relu_out::overload_name)
      .typed<_add_relu_out::schema>();
}

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _add_relu_out::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create__add_relu_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::_add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _add_relu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create__add_relu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar, name, "aten::_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu_Scalar, schema_str, "_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu_Scalar::schema> create__add_relu_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu_Scalar::name, _add_relu_Scalar::overload_name)
      .typed<_add_relu_Scalar::schema>();
}

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create__add_relu_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor _add_relu_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create__add_relu_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Scalar, name, "aten::_add_relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_relu__Scalar, schema_str, "_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)")

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_add_relu__Scalar::schema> create__add_relu__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_relu__Scalar::name, _add_relu__Scalar::overload_name)
      .typed<_add_relu__Scalar::schema>();
}

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Scalar::call(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create__add_relu__Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _add_relu__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create__add_relu__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_Scalar, name, "aten::add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add_Scalar, schema_str, "add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<add_Scalar::schema> create_add_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(add_Scalar::name, add_Scalar::overload_name)
      .typed<add_Scalar::schema>();
}

// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor add_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_add_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor add_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_add_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add__Scalar, name, "aten::add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(add__Scalar, schema_str, "add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)")

// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<add__Scalar::schema> create_add__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(add__Scalar::name, add__Scalar::overload_name)
      .typed<add__Scalar::schema>();
}

// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & add__Scalar::call(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_add__Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & add__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_add__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv, name, "aten::addmv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv, schema_str, "addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addmv::schema> create_addmv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmv::name, addmv::overload_name)
      .typed<addmv::schema>();
}

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addmv::call(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmv_typed_handle();
    return op.call(self, mat, vec, beta, alpha);
}

// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addmv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmv_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat, vec, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_, name, "aten::addmv_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_, schema_str, "addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addmv_::schema> create_addmv__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmv_::name, addmv_::overload_name)
      .typed<addmv_::schema>();
}

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addmv_::call(at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmv__typed_handle();
    return op.call(self, mat, vec, beta, alpha);
}

// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addmv_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmv__typed_handle();
    return op.redispatch(dispatchKeySet, self, mat, vec, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_out, name, "aten::addmv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmv_out, schema_str, "addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addmv_out::schema> create_addmv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmv_out::name, addmv_out::overload_name)
      .typed<addmv_out::schema>();
}

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addmv_out::call(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addmv_out_typed_handle();
    return op.call(self, mat, vec, beta, alpha, out);
}

// aten::addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addmv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addmv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat, vec, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr, name, "aten::addr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr, schema_str, "addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addr::schema> create_addr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addr::name, addr::overload_name)
      .typed<addr::schema>();
}

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addr::call(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addr_typed_handle();
    return op.call(self, vec1, vec2, beta, alpha);
}

// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addr_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec1, vec2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_, name, "aten::addr_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_, schema_str, "addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addr_::schema> create_addr__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addr_::name, addr_::overload_name)
      .typed<addr_::schema>();
}

// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addr_::call(at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addr__typed_handle();
    return op.call(self, vec1, vec2, beta, alpha);
}

// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addr_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addr__typed_handle();
    return op.redispatch(dispatchKeySet, self, vec1, vec2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_out, name, "aten::addr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addr_out, schema_str, "addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addr_out::schema> create_addr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addr_out::name, addr_out::overload_name)
      .typed<addr_out::schema>();
}

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addr_out::call(const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addr_out_typed_handle();
    return op.call(self, vec1, vec2, beta, alpha, out);
}

// aten::addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec1, vec2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator, name, "aten::affine_grid_generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator, schema_str, "affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor")

// aten::affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<affine_grid_generator::schema> create_affine_grid_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(affine_grid_generator::name, affine_grid_generator::overload_name)
      .typed<affine_grid_generator::schema>();
}

// aten::affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator::call(const at::Tensor & theta, at::IntArrayRef size, bool align_corners) {
    static auto op = create_affine_grid_generator_typed_handle();
    return op.call(theta, size, align_corners);
}

// aten::affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, at::IntArrayRef size, bool align_corners) {
    static auto op = create_affine_grid_generator_typed_handle();
    return op.redispatch(dispatchKeySet, theta, size, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_backward, name, "aten::affine_grid_generator_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(affine_grid_generator_backward, schema_str, "affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor")

// aten::affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<affine_grid_generator_backward::schema> create_affine_grid_generator_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(affine_grid_generator_backward::name, affine_grid_generator_backward::overload_name)
      .typed<affine_grid_generator_backward::schema>();
}

// aten::affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator_backward::call(const at::Tensor & grad, at::IntArrayRef size, bool align_corners) {
    static auto op = create_affine_grid_generator_backward_typed_handle();
    return op.call(grad, size, align_corners);
}

// aten::affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor
at::Tensor affine_grid_generator_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef size, bool align_corners) {
    static auto op = create_affine_grid_generator_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, size, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dim, name, "aten::all")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dim, schema_str, "all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor")

// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<all_dim::schema> create_all_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(all_dim::name, all_dim::overload_name)
      .typed<all_dim::schema>();
}

// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
at::Tensor all_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_all_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
at::Tensor all_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_all_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_out, name, "aten::all")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_out, schema_str, "all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<all_out::schema> create_all_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(all_out::name, all_out::overload_name)
      .typed<all_out::schema>();
}

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & all_out::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
    static auto op = create_all_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & all_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
    static auto op = create_all_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dimname, name, "aten::all")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dimname, schema_str, "all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor")

// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<all_dimname::schema> create_all_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(all_dimname::name, all_dimname::overload_name)
      .typed<all_dimname::schema>();
}

// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
at::Tensor all_dimname::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_all_dimname_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
at::Tensor all_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_all_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dimname_out, name, "aten::all")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_dimname_out, schema_str, "all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<all_dimname_out::schema> create_all_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(all_dimname_out::name, all_dimname_out::overload_name)
      .typed<all_dimname_out::schema>();
}

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & all_dimname_out::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    static auto op = create_all_dimname_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & all_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    static auto op = create_all_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(allclose, name, "aten::allclose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(allclose, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(allclose, schema_str, "allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool")

// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<allclose::schema> create_allclose_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(allclose::name, allclose::overload_name)
      .typed<allclose::schema>();
}

// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool
bool allclose::call(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    static auto op = create_allclose_typed_handle();
    return op.call(self, other, rtol, atol, equal_nan);
}

// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> bool
bool allclose::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    static auto op = create_allclose_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rtol, atol, equal_nan);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dim, name, "aten::any")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dim, schema_str, "any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor")

// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<any_dim::schema> create_any_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(any_dim::name, any_dim::overload_name)
      .typed<any_dim::schema>();
}

// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
at::Tensor any_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_any_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
at::Tensor any_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_any_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_out, name, "aten::any")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_out, schema_str, "any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<any_out::schema> create_any_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(any_out::name, any_out::overload_name)
      .typed<any_out::schema>();
}

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & any_out::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
    static auto op = create_any_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & any_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & out) {
    static auto op = create_any_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dimname, name, "aten::any")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dimname, schema_str, "any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor")

// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<any_dimname::schema> create_any_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(any_dimname::name, any_dimname::overload_name)
      .typed<any_dimname::schema>();
}

// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
at::Tensor any_dimname::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_any_dimname_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -> Tensor
at::Tensor any_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_any_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dimname_out, name, "aten::any")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_dimname_out, schema_str, "any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<any_dimname_out::schema> create_any_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(any_dimname_out::name, any_dimname_out::overload_name)
      .typed<any_dimname_out::schema>();
}

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & any_dimname_out::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    static auto op = create_any_dimname_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & any_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & out) {
    static auto op = create_any_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange, schema_str, "arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arange::schema> create_arange_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange::name, arange::overload_name)
      .typed<arange::schema>();
}

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange::call(const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_arange_typed_handle();
    return op.call(end, dtype, layout, device, pin_memory);
}

// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_arange_typed_handle();
    return op.redispatch(dispatchKeySet, end, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start, overload_name, "start")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start, schema_str, "arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arange_start::schema> create_arange_start_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_start::name, arange_start::overload_name)
      .typed<arange_start::schema>();
}

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start::call(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_arange_start_typed_handle();
    return op.call(start, end, dtype, layout, device, pin_memory);
}

// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_arange_start_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_step, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_step, overload_name, "start_step")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_step, schema_str, "arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arange_start_step::schema> create_arange_start_step_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_start_step::name, arange_start_step::overload_name)
      .typed<arange_start_step::schema>();
}

// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start_step::call(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_arange_start_step_typed_handle();
    return op.call(start, end, step, dtype, layout, device, pin_memory);
}

// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor arange_start_step::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_arange_start_step_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, step, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_out, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_out, schema_str, "arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arange_out::schema> create_arange_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_out::name, arange_out::overload_name)
      .typed<arange_out::schema>();
}

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_out::call(const at::Scalar & end, at::Tensor & out) {
    static auto op = create_arange_out_typed_handle();
    return op.call(end, out);
}

// aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & end, at::Tensor & out) {
    static auto op = create_arange_out_typed_handle();
    return op.redispatch(dispatchKeySet, end, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_out, name, "aten::arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_out, overload_name, "start_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arange_start_out, schema_str, "arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arange_start_out::schema> create_arange_start_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arange_start_out::name, arange_start_out::overload_name)
      .typed<arange_start_out::schema>();
}

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_start_out::call(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    static auto op = create_arange_start_out_typed_handle();
    return op.call(start, end, step, out);
}

// aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arange_start_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    static auto op = create_arange_start_out_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, step, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dim_arange, name, "aten::_dim_arange")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dim_arange, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dim_arange, schema_str, "_dim_arange(Tensor like, int dim) -> Tensor")

// aten::_dim_arange(Tensor like, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_dim_arange::schema> create__dim_arange_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dim_arange::name, _dim_arange::overload_name)
      .typed<_dim_arange::schema>();
}

// aten::_dim_arange(Tensor like, int dim) -> Tensor
at::Tensor _dim_arange::call(const at::Tensor & like, int64_t dim) {
    static auto op = create__dim_arange_typed_handle();
    return op.call(like, dim);
}

// aten::_dim_arange(Tensor like, int dim) -> Tensor
at::Tensor _dim_arange::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & like, int64_t dim) {
    static auto op = create__dim_arange_typed_handle();
    return op.redispatch(dispatchKeySet, like, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmax, name, "aten::argmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmax, schema_str, "argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor")

// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argmax::schema> create_argmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argmax::name, argmax::overload_name)
      .typed<argmax::schema>();
}

// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor argmax::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_argmax_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor argmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_argmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmax_out, name, "aten::argmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmax_out, schema_str, "argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<argmax_out::schema> create_argmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argmax_out::name, argmax_out::overload_name)
      .typed<argmax_out::schema>();
}

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argmax_out::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_argmax_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_argmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin, name, "aten::argmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin, schema_str, "argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor")

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argmin::schema> create_argmin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argmin::name, argmin::overload_name)
      .typed<argmin::schema>();
}

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor argmin::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_argmin_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor argmin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_argmin_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin_out, name, "aten::argmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argmin_out, schema_str, "argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<argmin_out::schema> create_argmin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argmin_out::name, argmin_out::overload_name)
      .typed<argmin_out::schema>();
}

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argmin_out::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_argmin_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & argmin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_argmin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh, name, "aten::acosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh, schema_str, "acosh(Tensor self) -> Tensor")

// aten::acosh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<acosh::schema> create_acosh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(acosh::name, acosh::overload_name)
      .typed<acosh::schema>();
}

// aten::acosh(Tensor self) -> Tensor
at::Tensor acosh::call(const at::Tensor & self) {
    static auto op = create_acosh_typed_handle();
    return op.call(self);
}

// aten::acosh(Tensor self) -> Tensor
at::Tensor acosh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_acosh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh_, name, "aten::acosh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh_, schema_str, "acosh_(Tensor(a!) self) -> Tensor(a!)")

// aten::acosh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<acosh_::schema> create_acosh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(acosh_::name, acosh_::overload_name)
      .typed<acosh_::schema>();
}

// aten::acosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & acosh_::call(at::Tensor & self) {
    static auto op = create_acosh__typed_handle();
    return op.call(self);
}

// aten::acosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & acosh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_acosh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh_out, name, "aten::acosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(acosh_out, schema_str, "acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<acosh_out::schema> create_acosh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(acosh_out::name, acosh_out::overload_name)
      .typed<acosh_out::schema>();
}

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & acosh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_acosh_out_typed_handle();
    return op.call(self, out);
}

// aten::acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & acosh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_acosh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh, name, "aten::arccosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh, schema_str, "arccosh(Tensor self) -> Tensor")

// aten::arccosh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arccosh::schema> create_arccosh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arccosh::name, arccosh::overload_name)
      .typed<arccosh::schema>();
}

// aten::arccosh(Tensor self) -> Tensor
at::Tensor arccosh::call(const at::Tensor & self) {
    static auto op = create_arccosh_typed_handle();
    return op.call(self);
}

// aten::arccosh(Tensor self) -> Tensor
at::Tensor arccosh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_arccosh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh_, name, "aten::arccosh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh_, schema_str, "arccosh_(Tensor(a!) self) -> Tensor(a!)")

// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arccosh_::schema> create_arccosh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arccosh_::name, arccosh_::overload_name)
      .typed<arccosh_::schema>();
}

// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arccosh_::call(at::Tensor & self) {
    static auto op = create_arccosh__typed_handle();
    return op.call(self);
}

// aten::arccosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arccosh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_arccosh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh_out, name, "aten::arccosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arccosh_out, schema_str, "arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arccosh_out::schema> create_arccosh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arccosh_out::name, arccosh_out::overload_name)
      .typed<arccosh_out::schema>();
}

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arccosh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arccosh_out_typed_handle();
    return op.call(self, out);
}

// aten::arccosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arccosh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arccosh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh, name, "aten::asinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh, schema_str, "asinh(Tensor self) -> Tensor")

// aten::asinh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<asinh::schema> create_asinh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(asinh::name, asinh::overload_name)
      .typed<asinh::schema>();
}

// aten::asinh(Tensor self) -> Tensor
at::Tensor asinh::call(const at::Tensor & self) {
    static auto op = create_asinh_typed_handle();
    return op.call(self);
}

// aten::asinh(Tensor self) -> Tensor
at::Tensor asinh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_asinh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh_, name, "aten::asinh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh_, schema_str, "asinh_(Tensor(a!) self) -> Tensor(a!)")

// aten::asinh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<asinh_::schema> create_asinh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(asinh_::name, asinh_::overload_name)
      .typed<asinh_::schema>();
}

// aten::asinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & asinh_::call(at::Tensor & self) {
    static auto op = create_asinh__typed_handle();
    return op.call(self);
}

// aten::asinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & asinh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_asinh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh_out, name, "aten::asinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asinh_out, schema_str, "asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<asinh_out::schema> create_asinh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(asinh_out::name, asinh_out::overload_name)
      .typed<asinh_out::schema>();
}

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & asinh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_asinh_out_typed_handle();
    return op.call(self, out);
}

// aten::asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & asinh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_asinh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh, name, "aten::arcsinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh, schema_str, "arcsinh(Tensor self) -> Tensor")

// aten::arcsinh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arcsinh::schema> create_arcsinh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsinh::name, arcsinh::overload_name)
      .typed<arcsinh::schema>();
}

// aten::arcsinh(Tensor self) -> Tensor
at::Tensor arcsinh::call(const at::Tensor & self) {
    static auto op = create_arcsinh_typed_handle();
    return op.call(self);
}

// aten::arcsinh(Tensor self) -> Tensor
at::Tensor arcsinh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_arcsinh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_, name, "aten::arcsinh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_, schema_str, "arcsinh_(Tensor(a!) self) -> Tensor(a!)")

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsinh_::schema> create_arcsinh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsinh_::name, arcsinh_::overload_name)
      .typed<arcsinh_::schema>();
}

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsinh_::call(at::Tensor & self) {
    static auto op = create_arcsinh__typed_handle();
    return op.call(self);
}

// aten::arcsinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsinh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_arcsinh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_out, name, "aten::arcsinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsinh_out, schema_str, "arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsinh_out::schema> create_arcsinh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsinh_out::name, arcsinh_out::overload_name)
      .typed<arcsinh_out::schema>();
}

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsinh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arcsinh_out_typed_handle();
    return op.call(self, out);
}

// aten::arcsinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsinh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arcsinh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh, name, "aten::atanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh, schema_str, "atanh(Tensor self) -> Tensor")

// aten::atanh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atanh::schema> create_atanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atanh::name, atanh::overload_name)
      .typed<atanh::schema>();
}

// aten::atanh(Tensor self) -> Tensor
at::Tensor atanh::call(const at::Tensor & self) {
    static auto op = create_atanh_typed_handle();
    return op.call(self);
}

// aten::atanh(Tensor self) -> Tensor
at::Tensor atanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_atanh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_, name, "aten::atanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_, schema_str, "atanh_(Tensor(a!) self) -> Tensor(a!)")

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atanh_::schema> create_atanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atanh_::name, atanh_::overload_name)
      .typed<atanh_::schema>();
}

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atanh_::call(at::Tensor & self) {
    static auto op = create_atanh__typed_handle();
    return op.call(self);
}

// aten::atanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_atanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_out, name, "aten::atanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atanh_out, schema_str, "atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atanh_out::schema> create_atanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atanh_out::name, atanh_out::overload_name)
      .typed<atanh_out::schema>();
}

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atanh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_atanh_out_typed_handle();
    return op.call(self, out);
}

// aten::atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_atanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh, name, "aten::arctanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh, schema_str, "arctanh(Tensor self) -> Tensor")

// aten::arctanh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arctanh::schema> create_arctanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctanh::name, arctanh::overload_name)
      .typed<arctanh::schema>();
}

// aten::arctanh(Tensor self) -> Tensor
at::Tensor arctanh::call(const at::Tensor & self) {
    static auto op = create_arctanh_typed_handle();
    return op.call(self);
}

// aten::arctanh(Tensor self) -> Tensor
at::Tensor arctanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_arctanh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh_, name, "aten::arctanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh_, schema_str, "arctanh_(Tensor(a!) self) -> Tensor(a!)")

// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arctanh_::schema> create_arctanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctanh_::name, arctanh_::overload_name)
      .typed<arctanh_::schema>();
}

// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arctanh_::call(at::Tensor & self) {
    static auto op = create_arctanh__typed_handle();
    return op.call(self);
}

// aten::arctanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arctanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_arctanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh_out, name, "aten::arctanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctanh_out, schema_str, "arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arctanh_out::schema> create_arctanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctanh_out::name, arctanh_out::overload_name)
      .typed<arctanh_out::schema>();
}

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arctanh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arctanh_out_typed_handle();
    return op.call(self, out);
}

// aten::arctanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arctanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arctanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided, name, "aten::as_strided")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided, schema_str, "as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)")

// aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<as_strided::schema> create_as_strided_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(as_strided::name, as_strided::overload_name)
      .typed<as_strided::schema>();
}

// aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)
at::Tensor as_strided::call(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    static auto op = create_as_strided_typed_handle();
    return op.call(self, size, stride, storage_offset);
}

// aten::as_strided(Tensor(a) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a)
at::Tensor as_strided::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    static auto op = create_as_strided_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride, storage_offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_, name, "aten::as_strided_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(as_strided_, schema_str, "as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)")

// aten::as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<as_strided_::schema> create_as_strided__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(as_strided_::name, as_strided_::overload_name)
      .typed<as_strided_::schema>();
}

// aten::as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)
const at::Tensor & as_strided_::call(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    static auto op = create_as_strided__typed_handle();
    return op.call(self, size, stride, storage_offset);
}

// aten::as_strided_(Tensor(a!) self, int[] size, int[] stride, int? storage_offset=None) -> Tensor(a!)
const at::Tensor & as_strided_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    static auto op = create_as_strided__typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride, storage_offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin, name, "aten::asin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin, schema_str, "asin(Tensor self) -> Tensor")

// aten::asin(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<asin::schema> create_asin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(asin::name, asin::overload_name)
      .typed<asin::schema>();
}

// aten::asin(Tensor self) -> Tensor
at::Tensor asin::call(const at::Tensor & self) {
    static auto op = create_asin_typed_handle();
    return op.call(self);
}

// aten::asin(Tensor self) -> Tensor
at::Tensor asin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_asin_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin_, name, "aten::asin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin_, schema_str, "asin_(Tensor(a!) self) -> Tensor(a!)")

// aten::asin_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<asin_::schema> create_asin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(asin_::name, asin_::overload_name)
      .typed<asin_::schema>();
}

// aten::asin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & asin_::call(at::Tensor & self) {
    static auto op = create_asin__typed_handle();
    return op.call(self);
}

// aten::asin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & asin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_asin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin_out, name, "aten::asin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(asin_out, schema_str, "asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<asin_out::schema> create_asin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(asin_out::name, asin_out::overload_name)
      .typed<asin_out::schema>();
}

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & asin_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_asin_out_typed_handle();
    return op.call(self, out);
}

// aten::asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & asin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_asin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin, name, "aten::arcsin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin, schema_str, "arcsin(Tensor self) -> Tensor")

// aten::arcsin(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arcsin::schema> create_arcsin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsin::name, arcsin::overload_name)
      .typed<arcsin::schema>();
}

// aten::arcsin(Tensor self) -> Tensor
at::Tensor arcsin::call(const at::Tensor & self) {
    static auto op = create_arcsin_typed_handle();
    return op.call(self);
}

// aten::arcsin(Tensor self) -> Tensor
at::Tensor arcsin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_arcsin_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_, name, "aten::arcsin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_, schema_str, "arcsin_(Tensor(a!) self) -> Tensor(a!)")

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsin_::schema> create_arcsin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsin_::name, arcsin_::overload_name)
      .typed<arcsin_::schema>();
}

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsin_::call(at::Tensor & self) {
    static auto op = create_arcsin__typed_handle();
    return op.call(self);
}

// aten::arcsin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arcsin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_arcsin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_out, name, "aten::arcsin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arcsin_out, schema_str, "arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arcsin_out::schema> create_arcsin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arcsin_out::name, arcsin_out::overload_name)
      .typed<arcsin_out::schema>();
}

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsin_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arcsin_out_typed_handle();
    return op.call(self, out);
}

// aten::arcsin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arcsin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arcsin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan, name, "aten::atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan, schema_str, "atan(Tensor self) -> Tensor")

// aten::atan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atan::schema> create_atan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan::name, atan::overload_name)
      .typed<atan::schema>();
}

// aten::atan(Tensor self) -> Tensor
at::Tensor atan::call(const at::Tensor & self) {
    static auto op = create_atan_typed_handle();
    return op.call(self);
}

// aten::atan(Tensor self) -> Tensor
at::Tensor atan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_atan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_, name, "aten::atan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_, schema_str, "atan_(Tensor(a!) self) -> Tensor(a!)")

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atan_::schema> create_atan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan_::name, atan_::overload_name)
      .typed<atan_::schema>();
}

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atan_::call(at::Tensor & self) {
    static auto op = create_atan__typed_handle();
    return op.call(self);
}

// aten::atan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & atan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_atan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_out, name, "aten::atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan_out, schema_str, "atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atan_out::schema> create_atan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan_out::name, atan_out::overload_name)
      .typed<atan_out::schema>();
}

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atan_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_atan_out_typed_handle();
    return op.call(self, out);
}

// aten::atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atan_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_atan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan, name, "aten::arctan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan, schema_str, "arctan(Tensor self) -> Tensor")

// aten::arctan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<arctan::schema> create_arctan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctan::name, arctan::overload_name)
      .typed<arctan::schema>();
}

// aten::arctan(Tensor self) -> Tensor
at::Tensor arctan::call(const at::Tensor & self) {
    static auto op = create_arctan_typed_handle();
    return op.call(self);
}

// aten::arctan(Tensor self) -> Tensor
at::Tensor arctan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_arctan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_, name, "aten::arctan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_, schema_str, "arctan_(Tensor(a!) self) -> Tensor(a!)")

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arctan_::schema> create_arctan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctan_::name, arctan_::overload_name)
      .typed<arctan_::schema>();
}

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arctan_::call(at::Tensor & self) {
    static auto op = create_arctan__typed_handle();
    return op.call(self);
}

// aten::arctan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & arctan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_arctan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_out, name, "aten::arctan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(arctan_out, schema_str, "arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<arctan_out::schema> create_arctan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(arctan_out::name, arctan_out::overload_name)
      .typed<arctan_out::schema>();
}

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arctan_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arctan_out_typed_handle();
    return op.call(self, out);
}

// aten::arctan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & arctan_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_arctan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_1d, name, "aten::atleast_1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_1d, schema_str, "atleast_1d(Tensor self) -> Tensor")

// aten::atleast_1d(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atleast_1d::schema> create_atleast_1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atleast_1d::name, atleast_1d::overload_name)
      .typed<atleast_1d::schema>();
}

// aten::atleast_1d(Tensor self) -> Tensor
at::Tensor atleast_1d::call(const at::Tensor & self) {
    static auto op = create_atleast_1d_typed_handle();
    return op.call(self);
}

// aten::atleast_1d(Tensor self) -> Tensor
at::Tensor atleast_1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_atleast_1d_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_1d_Sequence, name, "aten::atleast_1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_1d_Sequence, overload_name, "Sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_1d_Sequence, schema_str, "atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]")

// aten::atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<atleast_1d_Sequence::schema> create_atleast_1d_Sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atleast_1d_Sequence::name, atleast_1d_Sequence::overload_name)
      .typed<atleast_1d_Sequence::schema>();
}

// aten::atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> atleast_1d_Sequence::call(at::TensorList tensors) {
    static auto op = create_atleast_1d_Sequence_typed_handle();
    return op.call(tensors);
}

// aten::atleast_1d.Sequence(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> atleast_1d_Sequence::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_atleast_1d_Sequence_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_2d, name, "aten::atleast_2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_2d, schema_str, "atleast_2d(Tensor self) -> Tensor")

// aten::atleast_2d(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atleast_2d::schema> create_atleast_2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atleast_2d::name, atleast_2d::overload_name)
      .typed<atleast_2d::schema>();
}

// aten::atleast_2d(Tensor self) -> Tensor
at::Tensor atleast_2d::call(const at::Tensor & self) {
    static auto op = create_atleast_2d_typed_handle();
    return op.call(self);
}

// aten::atleast_2d(Tensor self) -> Tensor
at::Tensor atleast_2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_atleast_2d_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_2d_Sequence, name, "aten::atleast_2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_2d_Sequence, overload_name, "Sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_2d_Sequence, schema_str, "atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]")

// aten::atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<atleast_2d_Sequence::schema> create_atleast_2d_Sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atleast_2d_Sequence::name, atleast_2d_Sequence::overload_name)
      .typed<atleast_2d_Sequence::schema>();
}

// aten::atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> atleast_2d_Sequence::call(at::TensorList tensors) {
    static auto op = create_atleast_2d_Sequence_typed_handle();
    return op.call(tensors);
}

// aten::atleast_2d.Sequence(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> atleast_2d_Sequence::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_atleast_2d_Sequence_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_3d, name, "aten::atleast_3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_3d, schema_str, "atleast_3d(Tensor self) -> Tensor")

// aten::atleast_3d(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atleast_3d::schema> create_atleast_3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atleast_3d::name, atleast_3d::overload_name)
      .typed<atleast_3d::schema>();
}

// aten::atleast_3d(Tensor self) -> Tensor
at::Tensor atleast_3d::call(const at::Tensor & self) {
    static auto op = create_atleast_3d_typed_handle();
    return op.call(self);
}

// aten::atleast_3d(Tensor self) -> Tensor
at::Tensor atleast_3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_atleast_3d_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_3d_Sequence, name, "aten::atleast_3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_3d_Sequence, overload_name, "Sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atleast_3d_Sequence, schema_str, "atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]")

// aten::atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<atleast_3d_Sequence::schema> create_atleast_3d_Sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atleast_3d_Sequence::name, atleast_3d_Sequence::overload_name)
      .typed<atleast_3d_Sequence::schema>();
}

// aten::atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> atleast_3d_Sequence::call(at::TensorList tensors) {
    static auto op = create_atleast_3d_Sequence_typed_handle();
    return op.call(tensors);
}

// aten::atleast_3d.Sequence(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> atleast_3d_Sequence::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_atleast_3d_Sequence_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm, name, "aten::baddbmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm, schema_str, "baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<baddbmm::schema> create_baddbmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(baddbmm::name, baddbmm::overload_name)
      .typed<baddbmm::schema>();
}

// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor baddbmm::call(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_baddbmm_typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor baddbmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_baddbmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm_, name, "aten::baddbmm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm_, schema_str, "baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<baddbmm_::schema> create_baddbmm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(baddbmm_::name, baddbmm_::overload_name)
      .typed<baddbmm_::schema>();
}

// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & baddbmm_::call(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_baddbmm__typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & baddbmm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_baddbmm__typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_baddbmm_mkl_, name, "aten::_baddbmm_mkl_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_baddbmm_mkl_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_baddbmm_mkl_, schema_str, "_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_baddbmm_mkl_::schema> create__baddbmm_mkl__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_baddbmm_mkl_::name, _baddbmm_mkl_::overload_name)
      .typed<_baddbmm_mkl_::schema>();
}

// aten::_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _baddbmm_mkl_::call(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create__baddbmm_mkl__typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::_baddbmm_mkl_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & _baddbmm_mkl_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create__baddbmm_mkl__typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm_out, name, "aten::baddbmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(baddbmm_out, schema_str, "baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<baddbmm_out::schema> create_baddbmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(baddbmm_out::name, baddbmm_out::overload_name)
      .typed<baddbmm_out::schema>();
}

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & baddbmm_out::call(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_baddbmm_out_typed_handle();
    return op.call(self, batch1, batch2, beta, alpha, out);
}

// aten::baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & baddbmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_baddbmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window, name, "aten::bartlett_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window, schema_str, "bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bartlett_window::schema> create_bartlett_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bartlett_window::name, bartlett_window::overload_name)
      .typed<bartlett_window::schema>();
}

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_bartlett_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_bartlett_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic, name, "aten::bartlett_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bartlett_window_periodic, schema_str, "bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bartlett_window_periodic::schema> create_bartlett_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bartlett_window_periodic::name, bartlett_window_periodic::overload_name)
      .typed<bartlett_window_periodic::schema>();
}

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_bartlett_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor bartlett_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_bartlett_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm, name, "aten::batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm, schema_str, "batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor")

// aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm::schema> create_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm::name, batch_norm::overload_name)
      .typed<batch_norm::schema>();
}

// aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
at::Tensor batch_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    static auto op = create_batch_norm_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

// aten::batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
at::Tensor batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    static auto op = create_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm, name, "aten::quantized_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_batch_norm, schema_str, "quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor")

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_batch_norm::schema> create_quantized_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_batch_norm::name, quantized_batch_norm::overload_name)
      .typed<quantized_batch_norm::schema>();
}

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
at::Tensor quantized_batch_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
    static auto op = create_quantized_batch_norm_typed_handle();
    return op.call(input, weight, bias, mean, var, eps, output_scale, output_zero_point);
}

// aten::quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -> Tensor
at::Tensor quantized_batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & var, double eps, double output_scale, int64_t output_zero_point) {
    static auto op = create_quantized_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, mean, var, eps, output_scale, output_zero_point);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_batch_norm_impl_index, name, "aten::_batch_norm_impl_index")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_batch_norm_impl_index, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_batch_norm_impl_index, schema_str, "_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)")

// aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)
static C10_NOINLINE c10::TypedOperatorHandle<_batch_norm_impl_index::schema> create__batch_norm_impl_index_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_batch_norm_impl_index::name, _batch_norm_impl_index::overload_name)
      .typed<_batch_norm_impl_index::schema>();
}

// aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> _batch_norm_impl_index::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    static auto op = create__batch_norm_impl_index_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

// aten::_batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,int64_t> _batch_norm_impl_index::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, bool cudnn_enabled) {
    static auto op = create__batch_norm_impl_index_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_batch_norm_impl_index_backward, name, "aten::_batch_norm_impl_index_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_batch_norm_impl_index_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_batch_norm_impl_index_backward, schema_str, "_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)")

// aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_batch_norm_impl_index_backward::schema> create__batch_norm_impl_index_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_batch_norm_impl_index_backward::name, _batch_norm_impl_index_backward::overload_name)
      .typed<_batch_norm_impl_index_backward::schema>();
}

// aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _batch_norm_impl_index_backward::call(int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, ::std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
    static auto op = create__batch_norm_impl_index_backward_typed_handle();
    return op.call(impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask, reservedSpace);
}

// aten::_batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _batch_norm_impl_index_backward::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t impl_index, const at::Tensor & input, const at::Tensor & grad_output, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var_transform, bool train, double eps, ::std::array<bool,3> output_mask, const at::Tensor & reservedSpace) {
    static auto op = create__batch_norm_impl_index_backward_typed_handle();
    return op.redispatch(dispatchKeySet, impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask, reservedSpace);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli, name, "aten::bernoulli")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli, schema_str, "bernoulli(Tensor self, *, Generator? generator=None) -> Tensor")

// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bernoulli::schema> create_bernoulli_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bernoulli::name, bernoulli::overload_name)
      .typed<bernoulli::schema>();
}

// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
at::Tensor bernoulli::call(const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli_typed_handle();
    return op.call(self, generator);
}

// aten::bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
at::Tensor bernoulli::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli_out, name, "aten::bernoulli")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli_out, schema_str, "bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bernoulli_out::schema> create_bernoulli_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bernoulli_out::name, bernoulli_out::overload_name)
      .typed<bernoulli_out::schema>();
}

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bernoulli_out::call(const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_bernoulli_out_typed_handle();
    return op.call(self, generator, out);
}

// aten::bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bernoulli_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_bernoulli_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli__Tensor, name, "aten::bernoulli_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli__Tensor, schema_str, "bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)")

// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bernoulli__Tensor::schema> create_bernoulli__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bernoulli__Tensor::name, bernoulli__Tensor::overload_name)
      .typed<bernoulli__Tensor::schema>();
}

// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & bernoulli__Tensor::call(at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli__Tensor_typed_handle();
    return op.call(self, p, generator);
}

// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & bernoulli__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & p, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli__float, name, "aten::bernoulli_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli__float, overload_name, "float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli__float, schema_str, "bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)")

// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bernoulli__float::schema> create_bernoulli__float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bernoulli__float::name, bernoulli__float::overload_name)
      .typed<bernoulli__float::schema>();
}

// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & bernoulli__float::call(at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli__float_typed_handle();
    return op.call(self, p, generator);
}

// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & bernoulli__float::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli__float_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli_p, name, "aten::bernoulli")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli_p, overload_name, "p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bernoulli_p, schema_str, "bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor")

// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bernoulli_p::schema> create_bernoulli_p_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bernoulli_p::name, bernoulli_p::overload_name)
      .typed<bernoulli_p::schema>();
}

// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
at::Tensor bernoulli_p::call(const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli_p_typed_handle();
    return op.call(self, p, generator);
}

// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
at::Tensor bernoulli_p::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create_bernoulli_p_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bilinear, name, "aten::bilinear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bilinear, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bilinear, schema_str, "bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor")

// aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bilinear::schema> create_bilinear_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bilinear::name, bilinear::overload_name)
      .typed<bilinear::schema>();
}

// aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor
at::Tensor bilinear::call(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    static auto op = create_bilinear_typed_handle();
    return op.call(input1, input2, weight, bias);
}

// aten::bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias) -> Tensor
at::Tensor bilinear::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    static auto op = create_bilinear_typed_handle();
    return op.redispatch(dispatchKeySet, input1, input2, weight, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy, name, "aten::binary_cross_entropy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy, schema_str, "binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor")

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy::schema> create_binary_cross_entropy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy::name, binary_cross_entropy::overload_name)
      .typed<binary_cross_entropy::schema>();
}

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_typed_handle();
    return op.call(self, target, weight, reduction);
}

// aten::binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_out, name, "aten::binary_cross_entropy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_out, schema_str, "binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_out::schema> create_binary_cross_entropy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_out::name, binary_cross_entropy_out::overload_name)
      .typed<binary_cross_entropy_out::schema>();
}

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & binary_cross_entropy_out::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
    static auto op = create_binary_cross_entropy_out_typed_handle();
    return op.call(self, target, weight, reduction, out);
}

// aten::binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & binary_cross_entropy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
    static auto op = create_binary_cross_entropy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward, name, "aten::binary_cross_entropy_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward, schema_str, "binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor")

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_backward::schema> create_binary_cross_entropy_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_backward::name, binary_cross_entropy_backward::overload_name)
      .typed<binary_cross_entropy_backward::schema>();
}

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_backward_typed_handle();
    return op.call(grad_output, self, target, weight, reduction);
}

// aten::binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward_grad_input, name, "aten::binary_cross_entropy_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_backward_grad_input, schema_str, "binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_backward_grad_input::schema> create_binary_cross_entropy_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_backward_grad_input::name, binary_cross_entropy_backward_grad_input::overload_name)
      .typed<binary_cross_entropy_backward_grad_input::schema>();
}

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & binary_cross_entropy_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_binary_cross_entropy_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, grad_input);
}

// aten::binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & binary_cross_entropy_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_binary_cross_entropy_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_with_logits, name, "aten::binary_cross_entropy_with_logits")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_with_logits, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_with_logits, schema_str, "binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor")

// aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_with_logits::schema> create_binary_cross_entropy_with_logits_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_with_logits::name, binary_cross_entropy_with_logits::overload_name)
      .typed<binary_cross_entropy_with_logits::schema>();
}

// aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_with_logits::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_with_logits_typed_handle();
    return op.call(self, target, weight, pos_weight, reduction);
}

// aten::binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_with_logits::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_with_logits_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, pos_weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_with_logits_backward, name, "aten::binary_cross_entropy_with_logits_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_with_logits_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binary_cross_entropy_with_logits_backward, schema_str, "binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor")

// aten::binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binary_cross_entropy_with_logits_backward::schema> create_binary_cross_entropy_with_logits_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binary_cross_entropy_with_logits_backward::name, binary_cross_entropy_with_logits_backward::overload_name)
      .typed<binary_cross_entropy_with_logits_backward::schema>();
}

// aten::binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_with_logits_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_with_logits_backward_typed_handle();
    return op.call(grad_output, self, target, weight, pos_weight, reduction);
}

// aten::binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
at::Tensor binary_cross_entropy_with_logits_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
    static auto op = create_binary_cross_entropy_with_logits_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, pos_weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bincount, name, "aten::bincount")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bincount, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bincount, schema_str, "bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor")

// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bincount::schema> create_bincount_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bincount::name, bincount::overload_name)
      .typed<bincount::schema>();
}

// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
at::Tensor bincount::call(const at::Tensor & self, const c10::optional<at::Tensor> & weights, int64_t minlength) {
    static auto op = create_bincount_typed_handle();
    return op.call(self, weights, minlength);
}

// aten::bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
at::Tensor bincount::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & weights, int64_t minlength) {
    static auto op = create_bincount_typed_handle();
    return op.redispatch(dispatchKeySet, self, weights, minlength);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not, name, "aten::bitwise_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not, schema_str, "bitwise_not(Tensor self) -> Tensor")

// aten::bitwise_not(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_not::schema> create_bitwise_not_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_not::name, bitwise_not::overload_name)
      .typed<bitwise_not::schema>();
}

// aten::bitwise_not(Tensor self) -> Tensor
at::Tensor bitwise_not::call(const at::Tensor & self) {
    static auto op = create_bitwise_not_typed_handle();
    return op.call(self);
}

// aten::bitwise_not(Tensor self) -> Tensor
at::Tensor bitwise_not::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_bitwise_not_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_, name, "aten::bitwise_not_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_, schema_str, "bitwise_not_(Tensor(a!) self) -> Tensor(a!)")

// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_not_::schema> create_bitwise_not__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_not_::name, bitwise_not_::overload_name)
      .typed<bitwise_not_::schema>();
}

// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & bitwise_not_::call(at::Tensor & self) {
    static auto op = create_bitwise_not__typed_handle();
    return op.call(self);
}

// aten::bitwise_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & bitwise_not_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_bitwise_not__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_out, name, "aten::bitwise_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_not_out, schema_str, "bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_not_out::schema> create_bitwise_not_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_not_out::name, bitwise_not_out::overload_name)
      .typed<bitwise_not_out::schema>();
}

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_not_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_bitwise_not_out_typed_handle();
    return op.call(self, out);
}

// aten::bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_not_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_bitwise_not_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_out, name, "aten::copysign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_out, schema_str, "copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copysign_out::schema> create_copysign_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copysign_out::name, copysign_out::overload_name)
      .typed<copysign_out::schema>();
}

// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & copysign_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_copysign_out_typed_handle();
    return op.call(self, other, out);
}

// aten::copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & copysign_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_copysign_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Tensor, name, "aten::copysign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Tensor, schema_str, "copysign.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<copysign_Tensor::schema> create_copysign_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copysign_Tensor::name, copysign_Tensor::overload_name)
      .typed<copysign_Tensor::schema>();
}

// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor copysign_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_copysign_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::copysign.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor copysign_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_copysign_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign__Tensor, name, "aten::copysign_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign__Tensor, schema_str, "copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copysign__Tensor::schema> create_copysign__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copysign__Tensor::name, copysign__Tensor::overload_name)
      .typed<copysign__Tensor::schema>();
}

// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & copysign__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_copysign__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & copysign__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_copysign__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Scalar, name, "aten::copysign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Scalar, schema_str, "copysign.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<copysign_Scalar::schema> create_copysign_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copysign_Scalar::name, copysign_Scalar::overload_name)
      .typed<copysign_Scalar::schema>();
}

// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor copysign_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_copysign_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::copysign.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor copysign_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_copysign_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign__Scalar, name, "aten::copysign_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign__Scalar, schema_str, "copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copysign__Scalar::schema> create_copysign__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copysign__Scalar::name, copysign__Scalar::overload_name)
      .typed<copysign__Scalar::schema>();
}

// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & copysign__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_copysign__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & copysign__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_copysign__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Scalar_out, name, "aten::copysign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copysign_Scalar_out, schema_str, "copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copysign_Scalar_out::schema> create_copysign_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copysign_Scalar_out::name, copysign_Scalar_out::overload_name)
      .typed<copysign_Scalar_out::schema>();
}

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & copysign_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_copysign_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & copysign_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_copysign_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not, name, "aten::logical_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not, schema_str, "logical_not(Tensor self) -> Tensor")

// aten::logical_not(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logical_not::schema> create_logical_not_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_not::name, logical_not::overload_name)
      .typed<logical_not::schema>();
}

// aten::logical_not(Tensor self) -> Tensor
at::Tensor logical_not::call(const at::Tensor & self) {
    static auto op = create_logical_not_typed_handle();
    return op.call(self);
}

// aten::logical_not(Tensor self) -> Tensor
at::Tensor logical_not::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_logical_not_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_, name, "aten::logical_not_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_, schema_str, "logical_not_(Tensor(a!) self) -> Tensor(a!)")

// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_not_::schema> create_logical_not__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_not_::name, logical_not_::overload_name)
      .typed<logical_not_::schema>();
}

// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & logical_not_::call(at::Tensor & self) {
    static auto op = create_logical_not__typed_handle();
    return op.call(self);
}

// aten::logical_not_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & logical_not_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_logical_not__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_out, name, "aten::logical_not")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_not_out, schema_str, "logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_not_out::schema> create_logical_not_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_not_out::name, logical_not_out::overload_name)
      .typed<logical_not_out::schema>();
}

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_not_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_logical_not_out_typed_handle();
    return op.call(self, out);
}

// aten::logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_not_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_logical_not_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor, name, "aten::logical_xor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor, schema_str, "logical_xor(Tensor self, Tensor other) -> Tensor")

// aten::logical_xor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logical_xor::schema> create_logical_xor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_xor::name, logical_xor::overload_name)
      .typed<logical_xor::schema>();
}

// aten::logical_xor(Tensor self, Tensor other) -> Tensor
at::Tensor logical_xor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_xor_typed_handle();
    return op.call(self, other);
}

// aten::logical_xor(Tensor self, Tensor other) -> Tensor
at::Tensor logical_xor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_xor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor_, name, "aten::logical_xor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor_, schema_str, "logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_xor_::schema> create_logical_xor__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_xor_::name, logical_xor_::overload_name)
      .typed<logical_xor_::schema>();
}

// aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & logical_xor_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_xor__typed_handle();
    return op.call(self, other);
}

// aten::logical_xor_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & logical_xor_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_xor__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor_out, name, "aten::logical_xor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_xor_out, schema_str, "logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_xor_out::schema> create_logical_xor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_xor_out::name, logical_xor_out::overload_name)
      .typed<logical_xor_out::schema>();
}

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_xor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logical_xor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_xor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logical_xor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and, name, "aten::logical_and")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and, schema_str, "logical_and(Tensor self, Tensor other) -> Tensor")

// aten::logical_and(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logical_and::schema> create_logical_and_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_and::name, logical_and::overload_name)
      .typed<logical_and::schema>();
}

// aten::logical_and(Tensor self, Tensor other) -> Tensor
at::Tensor logical_and::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_and_typed_handle();
    return op.call(self, other);
}

// aten::logical_and(Tensor self, Tensor other) -> Tensor
at::Tensor logical_and::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_and_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and_, name, "aten::logical_and_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and_, schema_str, "logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_and_::schema> create_logical_and__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_and_::name, logical_and_::overload_name)
      .typed<logical_and_::schema>();
}

// aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & logical_and_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_and__typed_handle();
    return op.call(self, other);
}

// aten::logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & logical_and_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_and__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and_out, name, "aten::logical_and")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_and_out, schema_str, "logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_and_out::schema> create_logical_and_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_and_out::name, logical_and_out::overload_name)
      .typed<logical_and_out::schema>();
}

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_and_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logical_and_out_typed_handle();
    return op.call(self, other, out);
}

// aten::logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_and_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logical_and_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or, name, "aten::logical_or")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or, schema_str, "logical_or(Tensor self, Tensor other) -> Tensor")

// aten::logical_or(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logical_or::schema> create_logical_or_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_or::name, logical_or::overload_name)
      .typed<logical_or::schema>();
}

// aten::logical_or(Tensor self, Tensor other) -> Tensor
at::Tensor logical_or::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_or_typed_handle();
    return op.call(self, other);
}

// aten::logical_or(Tensor self, Tensor other) -> Tensor
at::Tensor logical_or::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_or_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or_, name, "aten::logical_or_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or_, schema_str, "logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_or_::schema> create_logical_or__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_or_::name, logical_or_::overload_name)
      .typed<logical_or_::schema>();
}

// aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & logical_or_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_or__typed_handle();
    return op.call(self, other);
}

// aten::logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & logical_or_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logical_or__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or_out, name, "aten::logical_or")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logical_or_out, schema_str, "logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logical_or_out::schema> create_logical_or_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logical_or_out::name, logical_or_out::overload_name)
      .typed<logical_or_out::schema>();
}

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_or_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logical_or_out_typed_handle();
    return op.call(self, other, out);
}

// aten::logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logical_or_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logical_or_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(blackman_window, name, "aten::blackman_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(blackman_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(blackman_window, schema_str, "blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<blackman_window::schema> create_blackman_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(blackman_window::name, blackman_window::overload_name)
      .typed<blackman_window::schema>();
}

// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor blackman_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_blackman_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor blackman_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_blackman_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(blackman_window_periodic, name, "aten::blackman_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(blackman_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(blackman_window_periodic, schema_str, "blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<blackman_window_periodic::schema> create_blackman_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(blackman_window_periodic::name, blackman_window_periodic::overload_name)
      .typed<blackman_window_periodic::schema>();
}

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor blackman_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_blackman_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor blackman_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_blackman_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm, name, "aten::bmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm, schema_str, "bmm(Tensor self, Tensor mat2) -> Tensor")

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bmm::schema> create_bmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bmm::name, bmm::overload_name)
      .typed<bmm::schema>();
}

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
at::Tensor bmm::call(const at::Tensor & self, const at::Tensor & mat2) {
    static auto op = create_bmm_typed_handle();
    return op.call(self, mat2);
}

// aten::bmm(Tensor self, Tensor mat2) -> Tensor
at::Tensor bmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
    static auto op = create_bmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm_out, name, "aten::bmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bmm_out, schema_str, "bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bmm_out::schema> create_bmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bmm_out::name, bmm_out::overload_name)
      .typed<bmm_out::schema>();
}

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bmm_out::call(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    static auto op = create_bmm_out_typed_handle();
    return op.call(self, mat2, out);
}

// aten::bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    static auto op = create_bmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(broadcast_tensors, name, "aten::broadcast_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(broadcast_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(broadcast_tensors, schema_str, "broadcast_tensors(Tensor[] tensors) -> Tensor[]")

// aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<broadcast_tensors::schema> create_broadcast_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(broadcast_tensors::name, broadcast_tensors::overload_name)
      .typed<broadcast_tensors::schema>();
}

// aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> broadcast_tensors::call(at::TensorList tensors) {
    static auto op = create_broadcast_tensors_typed_handle();
    return op.call(tensors);
}

// aten::broadcast_tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> broadcast_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_broadcast_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(broadcast_to, name, "aten::broadcast_to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(broadcast_to, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(broadcast_to, schema_str, "broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)")

// aten::broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<broadcast_to::schema> create_broadcast_to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(broadcast_to::name, broadcast_to::overload_name)
      .typed<broadcast_to::schema>();
}

// aten::broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
at::Tensor broadcast_to::call(const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create_broadcast_to_typed_handle();
    return op.call(self, size);
}

// aten::broadcast_to(Tensor(a) self, int[] size) -> Tensor(a)
at::Tensor broadcast_to::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create_broadcast_to_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat, name, "aten::cat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat, schema_str, "cat(Tensor[] tensors, int dim=0) -> Tensor")

// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cat::schema> create_cat_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cat::name, cat::overload_name)
      .typed<cat::schema>();
}

// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor cat::call(at::TensorList tensors, int64_t dim) {
    static auto op = create_cat_typed_handle();
    return op.call(tensors, dim);
}

// aten::cat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor cat::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    static auto op = create_cat_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_out, name, "aten::cat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_out, schema_str, "cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cat_out::schema> create_cat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cat_out::name, cat_out::overload_name)
      .typed<cat_out::schema>();
}

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cat_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create_cat_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cat_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create_cat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_names, name, "aten::cat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_names, schema_str, "cat.names(Tensor[] tensors, Dimname dim) -> Tensor")

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cat_names::schema> create_cat_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cat_names::name, cat_names::overload_name)
      .typed<cat_names::schema>();
}

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor cat_names::call(at::TensorList tensors, at::Dimname dim) {
    static auto op = create_cat_names_typed_handle();
    return op.call(tensors, dim);
}

// aten::cat.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor cat_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim) {
    static auto op = create_cat_names_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_names_out, name, "aten::cat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cat_names_out, schema_str, "cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cat_names_out::schema> create_cat_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cat_names_out::name, cat_names_out::overload_name)
      .typed<cat_names_out::schema>();
}

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cat_names_out::call(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    static auto op = create_cat_names_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cat_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    static auto op = create_cat_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat, schema_str, "concat(Tensor[] tensors, int dim=0) -> Tensor")

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<concat::schema> create_concat_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat::name, concat::overload_name)
      .typed<concat::schema>();
}

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor concat::call(at::TensorList tensors, int64_t dim) {
    static auto op = create_concat_typed_handle();
    return op.call(tensors, dim);
}

// aten::concat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor concat::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    static auto op = create_concat_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_out, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_out, schema_str, "concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<concat_out::schema> create_concat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat_out::name, concat_out::overload_name)
      .typed<concat_out::schema>();
}

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create_concat_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create_concat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names, schema_str, "concat.names(Tensor[] tensors, Dimname dim) -> Tensor")

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<concat_names::schema> create_concat_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat_names::name, concat_names::overload_name)
      .typed<concat_names::schema>();
}

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor concat_names::call(at::TensorList tensors, at::Dimname dim) {
    static auto op = create_concat_names_typed_handle();
    return op.call(tensors, dim);
}

// aten::concat.names(Tensor[] tensors, Dimname dim) -> Tensor
at::Tensor concat_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim) {
    static auto op = create_concat_names_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names_out, name, "aten::concat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(concat_names_out, schema_str, "concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<concat_names_out::schema> create_concat_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(concat_names_out::name, concat_names_out::overload_name)
      .typed<concat_names_out::schema>();
}

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_names_out::call(at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    static auto op = create_concat_names_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & concat_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Dimname dim, at::Tensor & out) {
    static auto op = create_concat_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(block_diag, name, "aten::block_diag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(block_diag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(block_diag, schema_str, "block_diag(Tensor[] tensors) -> Tensor")

// aten::block_diag(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<block_diag::schema> create_block_diag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(block_diag::name, block_diag::overload_name)
      .typed<block_diag::schema>();
}

// aten::block_diag(Tensor[] tensors) -> Tensor
at::Tensor block_diag::call(at::TensorList tensors) {
    static auto op = create_block_diag_typed_handle();
    return op.call(tensors);
}

// aten::block_diag(Tensor[] tensors) -> Tensor
at::Tensor block_diag::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_block_diag_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil, name, "aten::ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil, schema_str, "ceil(Tensor self) -> Tensor")

// aten::ceil(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ceil::schema> create_ceil_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ceil::name, ceil::overload_name)
      .typed<ceil::schema>();
}

// aten::ceil(Tensor self) -> Tensor
at::Tensor ceil::call(const at::Tensor & self) {
    static auto op = create_ceil_typed_handle();
    return op.call(self);
}

// aten::ceil(Tensor self) -> Tensor
at::Tensor ceil::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_ceil_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_, name, "aten::ceil_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_, schema_str, "ceil_(Tensor(a!) self) -> Tensor(a!)")

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ceil_::schema> create_ceil__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ceil_::name, ceil_::overload_name)
      .typed<ceil_::schema>();
}

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & ceil_::call(at::Tensor & self) {
    static auto op = create_ceil__typed_handle();
    return op.call(self);
}

// aten::ceil_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & ceil_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_ceil__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_out, name, "aten::ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ceil_out, schema_str, "ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ceil_out::schema> create_ceil_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ceil_out::name, ceil_out::overload_name)
      .typed<ceil_out::schema>();
}

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ceil_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_ceil_out_typed_handle();
    return op.call(self, out);
}

// aten::ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ceil_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_ceil_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul, name, "aten::chain_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul, schema_str, "chain_matmul(Tensor[] matrices) -> Tensor")

// aten::chain_matmul(Tensor[] matrices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<chain_matmul::schema> create_chain_matmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(chain_matmul::name, chain_matmul::overload_name)
      .typed<chain_matmul::schema>();
}

// aten::chain_matmul(Tensor[] matrices) -> Tensor
at::Tensor chain_matmul::call(at::TensorList matrices) {
    static auto op = create_chain_matmul_typed_handle();
    return op.call(matrices);
}

// aten::chain_matmul(Tensor[] matrices) -> Tensor
at::Tensor chain_matmul::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices) {
    static auto op = create_chain_matmul_typed_handle();
    return op.redispatch(dispatchKeySet, matrices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul_out, name, "aten::chain_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chain_matmul_out, schema_str, "chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)")

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<chain_matmul_out::schema> create_chain_matmul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(chain_matmul_out::name, chain_matmul_out::overload_name)
      .typed<chain_matmul_out::schema>();
}

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & chain_matmul_out::call(at::TensorList matrices, at::Tensor & out) {
    static auto op = create_chain_matmul_out_typed_handle();
    return op.call(matrices, out);
}

// aten::chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & chain_matmul_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList matrices, at::Tensor & out) {
    static auto op = create_chain_matmul_out_typed_handle();
    return op.redispatch(dispatchKeySet, matrices, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_chunk, name, "aten::unsafe_chunk")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_chunk, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_chunk, schema_str, "unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]")

// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<unsafe_chunk::schema> create_unsafe_chunk_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsafe_chunk::name, unsafe_chunk::overload_name)
      .typed<unsafe_chunk::schema>();
}

// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_chunk::call(const at::Tensor & self, int64_t chunks, int64_t dim) {
    static auto op = create_unsafe_chunk_typed_handle();
    return op.call(self, chunks, dim);
}

// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_chunk::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t chunks, int64_t dim) {
    static auto op = create_unsafe_chunk_typed_handle();
    return op.redispatch(dispatchKeySet, self, chunks, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chunk, name, "aten::chunk")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chunk, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(chunk, schema_str, "chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]")

// aten::chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<chunk::schema> create_chunk_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(chunk::name, chunk::overload_name)
      .typed<chunk::schema>();
}

// aten::chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> chunk::call(const at::Tensor & self, int64_t chunks, int64_t dim) {
    static auto op = create_chunk_typed_handle();
    return op.call(self, chunks, dim);
}

// aten::chunk(Tensor(a) self, int chunks, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> chunk::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t chunks, int64_t dim) {
    static auto op = create_chunk_typed_handle();
    return op.redispatch(dispatchKeySet, self, chunks, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_sections, name, "aten::tensor_split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_sections, overload_name, "sections")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_sections, schema_str, "tensor_split.sections(Tensor(a) self, int sections, int dim=0) -> Tensor(a)[]")

// aten::tensor_split.sections(Tensor(a) self, int sections, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<tensor_split_sections::schema> create_tensor_split_sections_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensor_split_sections::name, tensor_split_sections::overload_name)
      .typed<tensor_split_sections::schema>();
}

// aten::tensor_split.sections(Tensor(a) self, int sections, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> tensor_split_sections::call(const at::Tensor & self, int64_t sections, int64_t dim) {
    static auto op = create_tensor_split_sections_typed_handle();
    return op.call(self, sections, dim);
}

// aten::tensor_split.sections(Tensor(a) self, int sections, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> tensor_split_sections::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections, int64_t dim) {
    static auto op = create_tensor_split_sections_typed_handle();
    return op.redispatch(dispatchKeySet, self, sections, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_indices, name, "aten::tensor_split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_indices, overload_name, "indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_indices, schema_str, "tensor_split.indices(Tensor(a) self, int[] indices, int dim=0) -> Tensor(a)[]")

// aten::tensor_split.indices(Tensor(a) self, int[] indices, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<tensor_split_indices::schema> create_tensor_split_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensor_split_indices::name, tensor_split_indices::overload_name)
      .typed<tensor_split_indices::schema>();
}

// aten::tensor_split.indices(Tensor(a) self, int[] indices, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> tensor_split_indices::call(const at::Tensor & self, at::IntArrayRef indices, int64_t dim) {
    static auto op = create_tensor_split_indices_typed_handle();
    return op.call(self, indices, dim);
}

// aten::tensor_split.indices(Tensor(a) self, int[] indices, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> tensor_split_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices, int64_t dim) {
    static auto op = create_tensor_split_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_tensor_indices_or_sections, name, "aten::tensor_split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_tensor_indices_or_sections, overload_name, "tensor_indices_or_sections")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensor_split_tensor_indices_or_sections, schema_str, "tensor_split.tensor_indices_or_sections(Tensor(a) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]")

// aten::tensor_split.tensor_indices_or_sections(Tensor(a) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<tensor_split_tensor_indices_or_sections::schema> create_tensor_split_tensor_indices_or_sections_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensor_split_tensor_indices_or_sections::name, tensor_split_tensor_indices_or_sections::overload_name)
      .typed<tensor_split_tensor_indices_or_sections::schema>();
}

// aten::tensor_split.tensor_indices_or_sections(Tensor(a) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> tensor_split_tensor_indices_or_sections::call(const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
    static auto op = create_tensor_split_tensor_indices_or_sections_typed_handle();
    return op.call(self, tensor_indices_or_sections, dim);
}

// aten::tensor_split.tensor_indices_or_sections(Tensor(a) self, Tensor tensor_indices_or_sections, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> tensor_split_tensor_indices_or_sections::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor_indices_or_sections, int64_t dim) {
    static auto op = create_tensor_split_tensor_indices_or_sections_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor_indices_or_sections, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp, name, "aten::clamp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp, schema_str, "clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor")

// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp::schema> create_clamp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp::name, clamp::overload_name)
      .typed<clamp::schema>();
}

// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
at::Tensor clamp::call(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clamp_typed_handle();
    return op.call(self, min, max);
}

// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
at::Tensor clamp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clamp_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_Tensor, name, "aten::clamp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_Tensor, schema_str, "clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor")

// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_Tensor::schema> create_clamp_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_Tensor::name, clamp_Tensor::overload_name)
      .typed<clamp_Tensor::schema>();
}

// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
at::Tensor clamp_Tensor::call(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clamp_Tensor_typed_handle();
    return op.call(self, min, max);
}

// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
at::Tensor clamp_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clamp_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_, name, "aten::clamp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_, schema_str, "clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)")

// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_::schema> create_clamp__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_::name, clamp_::overload_name)
      .typed<clamp_::schema>();
}

// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
at::Tensor & clamp_::call(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clamp__typed_handle();
    return op.call(self, min, max);
}

// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
at::Tensor & clamp_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clamp__typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp__Tensor, name, "aten::clamp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp__Tensor, schema_str, "clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)")

// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp__Tensor::schema> create_clamp__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp__Tensor::name, clamp__Tensor::overload_name)
      .typed<clamp__Tensor::schema>();
}

// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
at::Tensor & clamp__Tensor::call(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clamp__Tensor_typed_handle();
    return op.call(self, min, max);
}

// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
at::Tensor & clamp__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clamp__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_out, name, "aten::clamp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_out, schema_str, "clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_out::schema> create_clamp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_out::name, clamp_out::overload_name)
      .typed<clamp_out::schema>();
}

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
    static auto op = create_clamp_out_typed_handle();
    return op.call(self, min, max, out);
}

// aten::clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
    static auto op = create_clamp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_Tensor_out, name, "aten::clamp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_Tensor_out, schema_str, "clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_Tensor_out::schema> create_clamp_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_Tensor_out::name, clamp_Tensor_out::overload_name)
      .typed<clamp_Tensor_out::schema>();
}

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_Tensor_out::call(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
    static auto op = create_clamp_Tensor_out_typed_handle();
    return op.call(self, min, max, out);
}

// aten::clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
    static auto op = create_clamp_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max, name, "aten::clamp_max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max, schema_str, "clamp_max(Tensor self, Scalar max) -> Tensor")

// aten::clamp_max(Tensor self, Scalar max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_max::schema> create_clamp_max_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_max::name, clamp_max::overload_name)
      .typed<clamp_max::schema>();
}

// aten::clamp_max(Tensor self, Scalar max) -> Tensor
at::Tensor clamp_max::call(const at::Tensor & self, const at::Scalar & max) {
    static auto op = create_clamp_max_typed_handle();
    return op.call(self, max);
}

// aten::clamp_max(Tensor self, Scalar max) -> Tensor
at::Tensor clamp_max::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & max) {
    static auto op = create_clamp_max_typed_handle();
    return op.redispatch(dispatchKeySet, self, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_Tensor, name, "aten::clamp_max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_Tensor, schema_str, "clamp_max.Tensor(Tensor self, Tensor max) -> Tensor")

// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_max_Tensor::schema> create_clamp_max_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_max_Tensor::name, clamp_max_Tensor::overload_name)
      .typed<clamp_max_Tensor::schema>();
}

// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
at::Tensor clamp_max_Tensor::call(const at::Tensor & self, const at::Tensor & max) {
    static auto op = create_clamp_max_Tensor_typed_handle();
    return op.call(self, max);
}

// aten::clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
at::Tensor clamp_max_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & max) {
    static auto op = create_clamp_max_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_, name, "aten::clamp_max_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_, schema_str, "clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)")

// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_max_::schema> create_clamp_max__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_max_::name, clamp_max_::overload_name)
      .typed<clamp_max_::schema>();
}

// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
at::Tensor & clamp_max_::call(at::Tensor & self, const at::Scalar & max) {
    static auto op = create_clamp_max__typed_handle();
    return op.call(self, max);
}

// aten::clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
at::Tensor & clamp_max_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & max) {
    static auto op = create_clamp_max__typed_handle();
    return op.redispatch(dispatchKeySet, self, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max__Tensor, name, "aten::clamp_max_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max__Tensor, schema_str, "clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)")

// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_max__Tensor::schema> create_clamp_max__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_max__Tensor::name, clamp_max__Tensor::overload_name)
      .typed<clamp_max__Tensor::schema>();
}

// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
at::Tensor & clamp_max__Tensor::call(at::Tensor & self, const at::Tensor & max) {
    static auto op = create_clamp_max__Tensor_typed_handle();
    return op.call(self, max);
}

// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
at::Tensor & clamp_max__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & max) {
    static auto op = create_clamp_max__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_out, name, "aten::clamp_max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_out, schema_str, "clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_max_out::schema> create_clamp_max_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_max_out::name, clamp_max_out::overload_name)
      .typed<clamp_max_out::schema>();
}

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_max_out::call(const at::Tensor & self, const at::Scalar & max, at::Tensor & out) {
    static auto op = create_clamp_max_out_typed_handle();
    return op.call(self, max, out);
}

// aten::clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_max_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & max, at::Tensor & out) {
    static auto op = create_clamp_max_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_Tensor_out, name, "aten::clamp_max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_max_Tensor_out, schema_str, "clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_max_Tensor_out::schema> create_clamp_max_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_max_Tensor_out::name, clamp_max_Tensor_out::overload_name)
      .typed<clamp_max_Tensor_out::schema>();
}

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_max_Tensor_out::call(const at::Tensor & self, const at::Tensor & max, at::Tensor & out) {
    static auto op = create_clamp_max_Tensor_out_typed_handle();
    return op.call(self, max, out);
}

// aten::clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_max_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & max, at::Tensor & out) {
    static auto op = create_clamp_max_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min, schema_str, "clamp_min(Tensor self, Scalar min) -> Tensor")

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min::schema> create_clamp_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min::name, clamp_min::overload_name)
      .typed<clamp_min::schema>();
}

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
at::Tensor clamp_min::call(const at::Tensor & self, const at::Scalar & min) {
    static auto op = create_clamp_min_typed_handle();
    return op.call(self, min);
}

// aten::clamp_min(Tensor self, Scalar min) -> Tensor
at::Tensor clamp_min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min) {
    static auto op = create_clamp_min_typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor, schema_str, "clamp_min.Tensor(Tensor self, Tensor min) -> Tensor")

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_Tensor::schema> create_clamp_min_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_Tensor::name, clamp_min_Tensor::overload_name)
      .typed<clamp_min_Tensor::schema>();
}

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
at::Tensor clamp_min_Tensor::call(const at::Tensor & self, const at::Tensor & min) {
    static auto op = create_clamp_min_Tensor_typed_handle();
    return op.call(self, min);
}

// aten::clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
at::Tensor clamp_min_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min) {
    static auto op = create_clamp_min_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_, name, "aten::clamp_min_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_, schema_str, "clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)")

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_::schema> create_clamp_min__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_::name, clamp_min_::overload_name)
      .typed<clamp_min_::schema>();
}

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
at::Tensor & clamp_min_::call(at::Tensor & self, const at::Scalar & min) {
    static auto op = create_clamp_min__typed_handle();
    return op.call(self, min);
}

// aten::clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
at::Tensor & clamp_min_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min) {
    static auto op = create_clamp_min__typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min__Tensor, name, "aten::clamp_min_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min__Tensor, schema_str, "clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)")

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min__Tensor::schema> create_clamp_min__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min__Tensor::name, clamp_min__Tensor::overload_name)
      .typed<clamp_min__Tensor::schema>();
}

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
at::Tensor & clamp_min__Tensor::call(at::Tensor & self, const at::Tensor & min) {
    static auto op = create_clamp_min__Tensor_typed_handle();
    return op.call(self, min);
}

// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
at::Tensor & clamp_min__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & min) {
    static auto op = create_clamp_min__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_out, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_out, schema_str, "clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_out::schema> create_clamp_min_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_out::name, clamp_min_out::overload_name)
      .typed<clamp_min_out::schema>();
}

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_out::call(const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
    static auto op = create_clamp_min_out_typed_handle();
    return op.call(self, min, out);
}

// aten::clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min, at::Tensor & out) {
    static auto op = create_clamp_min_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor_out, name, "aten::clamp_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clamp_min_Tensor_out, schema_str, "clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clamp_min_Tensor_out::schema> create_clamp_min_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clamp_min_Tensor_out::name, clamp_min_Tensor_out::overload_name)
      .typed<clamp_min_Tensor_out::schema>();
}

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_Tensor_out::call(const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
    static auto op = create_clamp_min_Tensor_out_typed_handle();
    return op.call(self, min, out);
}

// aten::clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clamp_min_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & min, at::Tensor & out) {
    static auto op = create_clamp_min_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip, name, "aten::clip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip, schema_str, "clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor")

// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clip::schema> create_clip_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clip::name, clip::overload_name)
      .typed<clip::schema>();
}

// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
at::Tensor clip::call(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clip_typed_handle();
    return op.call(self, min, max);
}

// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
at::Tensor clip::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clip_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_Tensor, name, "aten::clip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_Tensor, schema_str, "clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor")

// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clip_Tensor::schema> create_clip_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clip_Tensor::name, clip_Tensor::overload_name)
      .typed<clip_Tensor::schema>();
}

// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
at::Tensor clip_Tensor::call(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clip_Tensor_typed_handle();
    return op.call(self, min, max);
}

// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
at::Tensor clip_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clip_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_, name, "aten::clip_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_, schema_str, "clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)")

// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clip_::schema> create_clip__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clip_::name, clip_::overload_name)
      .typed<clip_::schema>();
}

// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
at::Tensor & clip_::call(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clip__typed_handle();
    return op.call(self, min, max);
}

// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
at::Tensor & clip_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
    static auto op = create_clip__typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip__Tensor, name, "aten::clip_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip__Tensor, schema_str, "clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)")

// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clip__Tensor::schema> create_clip__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clip__Tensor::name, clip__Tensor::overload_name)
      .typed<clip__Tensor::schema>();
}

// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
at::Tensor & clip__Tensor::call(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clip__Tensor_typed_handle();
    return op.call(self, min, max);
}

// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
at::Tensor & clip__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    static auto op = create_clip__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_out, name, "aten::clip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_out, schema_str, "clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clip_out::schema> create_clip_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clip_out::name, clip_out::overload_name)
      .typed<clip_out::schema>();
}

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clip_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
    static auto op = create_clip_out_typed_handle();
    return op.call(self, min, max, out);
}

// aten::clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clip_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max, at::Tensor & out) {
    static auto op = create_clip_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_Tensor_out, name, "aten::clip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clip_Tensor_out, schema_str, "clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<clip_Tensor_out::schema> create_clip_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clip_Tensor_out::name, clip_Tensor_out::overload_name)
      .typed<clip_Tensor_out::schema>();
}

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clip_Tensor_out::call(const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
    static auto op = create_clip_Tensor_out_typed_handle();
    return op.call(self, min, max, out);
}

// aten::clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & clip_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max, at::Tensor & out) {
    static auto op = create_clip_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_is_acceptable, name, "aten::cudnn_is_acceptable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_is_acceptable, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_is_acceptable, schema_str, "cudnn_is_acceptable(Tensor self) -> bool")

// aten::cudnn_is_acceptable(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_is_acceptable::schema> create_cudnn_is_acceptable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_is_acceptable::name, cudnn_is_acceptable::overload_name)
      .typed<cudnn_is_acceptable::schema>();
}

// aten::cudnn_is_acceptable(Tensor self) -> bool
bool cudnn_is_acceptable::call(const at::Tensor & self) {
    static auto op = create_cudnn_is_acceptable_typed_handle();
    return op.call(self);
}

// aten::cudnn_is_acceptable(Tensor self) -> bool
bool cudnn_is_acceptable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_cudnn_is_acceptable_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(complex, name, "aten::complex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(complex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(complex, schema_str, "complex(Tensor real, Tensor imag) -> Tensor")

// aten::complex(Tensor real, Tensor imag) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<complex::schema> create_complex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(complex::name, complex::overload_name)
      .typed<complex::schema>();
}

// aten::complex(Tensor real, Tensor imag) -> Tensor
at::Tensor complex::call(const at::Tensor & real, const at::Tensor & imag) {
    static auto op = create_complex_typed_handle();
    return op.call(real, imag);
}

// aten::complex(Tensor real, Tensor imag) -> Tensor
at::Tensor complex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & real, const at::Tensor & imag) {
    static auto op = create_complex_typed_handle();
    return op.redispatch(dispatchKeySet, real, imag);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(complex_out, name, "aten::complex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(complex_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(complex_out, schema_str, "complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)")

// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<complex_out::schema> create_complex_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(complex_out::name, complex_out::overload_name)
      .typed<complex_out::schema>();
}

// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & complex_out::call(const at::Tensor & real, const at::Tensor & imag, at::Tensor & out) {
    static auto op = create_complex_out_typed_handle();
    return op.call(real, imag, out);
}

// aten::complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & complex_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & real, const at::Tensor & imag, at::Tensor & out) {
    static auto op = create_complex_out_typed_handle();
    return op.redispatch(dispatchKeySet, real, imag, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polar, name, "aten::polar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polar, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polar, schema_str, "polar(Tensor abs, Tensor angle) -> Tensor")

// aten::polar(Tensor abs, Tensor angle) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<polar::schema> create_polar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polar::name, polar::overload_name)
      .typed<polar::schema>();
}

// aten::polar(Tensor abs, Tensor angle) -> Tensor
at::Tensor polar::call(const at::Tensor & abs, const at::Tensor & angle) {
    static auto op = create_polar_typed_handle();
    return op.call(abs, angle);
}

// aten::polar(Tensor abs, Tensor angle) -> Tensor
at::Tensor polar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & abs, const at::Tensor & angle) {
    static auto op = create_polar_typed_handle();
    return op.redispatch(dispatchKeySet, abs, angle);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polar_out, name, "aten::polar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polar_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polar_out, schema_str, "polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)")

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<polar_out::schema> create_polar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polar_out::name, polar_out::overload_name)
      .typed<polar_out::schema>();
}

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & polar_out::call(const at::Tensor & abs, const at::Tensor & angle, at::Tensor & out) {
    static auto op = create_polar_out_typed_handle();
    return op.call(abs, angle, out);
}

// aten::polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & polar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & abs, const at::Tensor & angle, at::Tensor & out) {
    static auto op = create_polar_out_typed_handle();
    return op.redispatch(dispatchKeySet, abs, angle, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(constant_pad_nd, name, "aten::constant_pad_nd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(constant_pad_nd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(constant_pad_nd, schema_str, "constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor")

// aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<constant_pad_nd::schema> create_constant_pad_nd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(constant_pad_nd::name, constant_pad_nd::overload_name)
      .typed<constant_pad_nd::schema>();
}

// aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor
at::Tensor constant_pad_nd::call(const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
    static auto op = create_constant_pad_nd_typed_handle();
    return op.call(self, pad, value);
}

// aten::constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor
at::Tensor constant_pad_nd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
    static auto op = create_constant_pad_nd_typed_handle();
    return op.redispatch(dispatchKeySet, self, pad, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(contiguous, name, "aten::contiguous")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(contiguous, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(contiguous, schema_str, "contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)")

// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<contiguous::schema> create_contiguous_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(contiguous::name, contiguous::overload_name)
      .typed<contiguous::schema>();
}

// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)
at::Tensor contiguous::call(const at::Tensor & self, at::MemoryFormat memory_format) {
    static auto op = create_contiguous_typed_handle();
    return op.call(self, memory_format);
}

// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -> Tensor(a)
at::Tensor contiguous::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::MemoryFormat memory_format) {
    static auto op = create_contiguous_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution, name, "aten::convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution, schema_str, "convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor")

// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<convolution::schema> create_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(convolution::name, convolution::overload_name)
      .typed<convolution::schema>();
}

// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
at::Tensor convolution::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    static auto op = create_convolution_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

// aten::convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
at::Tensor convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    static auto op = create_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution_overrideable, name, "aten::convolution_overrideable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution_overrideable, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution_overrideable, schema_str, "convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor")

// aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<convolution_overrideable::schema> create_convolution_overrideable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(convolution_overrideable::name, convolution_overrideable::overload_name)
      .typed<convolution_overrideable::schema>();
}

// aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
at::Tensor convolution_overrideable::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    static auto op = create_convolution_overrideable_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

// aten::convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
at::Tensor convolution_overrideable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    static auto op = create_convolution_overrideable_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution_backward_overrideable, name, "aten::convolution_backward_overrideable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution_backward_overrideable, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(convolution_backward_overrideable, schema_str, "convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<convolution_backward_overrideable::schema> create_convolution_backward_overrideable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(convolution_backward_overrideable::name, convolution_backward_overrideable::overload_name)
      .typed<convolution_backward_overrideable::schema>();
}

// aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> convolution_backward_overrideable::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
    static auto op = create_convolution_backward_overrideable_typed_handle();
    return op.call(grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}

// aten::convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> convolution_backward_overrideable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
    static auto op = create_convolution_backward_overrideable_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution, name, "aten::_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution, schema_str, "_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor")

// aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convolution::schema> create__convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convolution::name, _convolution::overload_name)
      .typed<_convolution::schema>();
}

// aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
at::Tensor _convolution::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
    static auto op = create__convolution_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
}

// aten::_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
at::Tensor _convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) {
    static auto op = create__convolution_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_deprecated, name, "aten::_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_deprecated, overload_name, "deprecated")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_deprecated, schema_str, "_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor")

// aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convolution_deprecated::schema> create__convolution_deprecated_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convolution_deprecated::name, _convolution_deprecated::overload_name)
      .typed<_convolution_deprecated::schema>();
}

// aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor
at::Tensor _convolution_deprecated::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
    static auto op = create__convolution_deprecated_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}

// aten::_convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled) -> Tensor
at::Tensor _convolution_deprecated::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) {
    static auto op = create__convolution_deprecated_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_mode, name, "aten::_convolution_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_mode, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_mode, schema_str, "_convolution_mode(Tensor input, Tensor weight, Tensor? bias, int[] stride, str padding, int[] dilation, int groups) -> Tensor")

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, int[] stride, str padding, int[] dilation, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convolution_mode::schema> create__convolution_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convolution_mode::name, _convolution_mode::overload_name)
      .typed<_convolution_mode::schema>();
}

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, int[] stride, str padding, int[] dilation, int groups) -> Tensor
at::Tensor _convolution_mode::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create__convolution_mode_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::_convolution_mode(Tensor input, Tensor weight, Tensor? bias, int[] stride, str padding, int[] dilation, int groups) -> Tensor
at::Tensor _convolution_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create__convolution_mode_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_nogroup, name, "aten::_convolution_nogroup")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_nogroup, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_nogroup, schema_str, "_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor")

// aten::_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convolution_nogroup::schema> create__convolution_nogroup_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convolution_nogroup::name, _convolution_nogroup::overload_name)
      .typed<_convolution_nogroup::schema>();
}

// aten::_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor
at::Tensor _convolution_nogroup::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding) {
    static auto op = create__convolution_nogroup_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, transposed, output_padding);
}

// aten::_convolution_nogroup(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding) -> Tensor
at::Tensor _convolution_nogroup::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding) {
    static auto op = create__convolution_nogroup_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, transposed, output_padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_double_backward, name, "aten::_convolution_double_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_double_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convolution_double_backward, schema_str, "_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_convolution_double_backward::schema> create__convolution_double_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convolution_double_backward::name, _convolution_double_backward::overload_name)
      .typed<_convolution_double_backward::schema>();
}

// aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _convolution_double_backward::call(const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, ::std::array<bool,3> output_mask) {
    static auto op = create__convolution_double_backward_typed_handle();
    return op.call(ggI, ggW, ggb, gO, weight, self, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, output_mask);
}

// aten::_convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _convolution_double_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & ggI, const c10::optional<at::Tensor> & ggW, const c10::optional<at::Tensor> & ggb, const at::Tensor & gO, const at::Tensor & weight, const at::Tensor & self, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32, ::std::array<bool,3> output_mask) {
    static auto op = create__convolution_double_backward_typed_handle();
    return op.redispatch(dispatchKeySet, ggI, ggW, ggb, gO, weight, self, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d, name, "aten::conv1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d, schema_str, "conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor")

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv1d::schema> create_conv1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv1d::name, conv1d::overload_name)
      .typed<conv1d::schema>();
}

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
at::Tensor conv1d::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv1d_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] dilation=1, int groups=1) -> Tensor
at::Tensor conv1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv1d_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv2d, name, "aten::conv2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv2d, schema_str, "conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor")

// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv2d::schema> create_conv2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv2d::name, conv2d::overload_name)
      .typed<conv2d::schema>();
}

// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
at::Tensor conv2d::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv2d_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor
at::Tensor conv2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv2d_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d, name, "aten::conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d, schema_str, "conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor")

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv3d::schema> create_conv3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv3d::name, conv3d::overload_name)
      .typed<conv3d::schema>();
}

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
at::Tensor conv3d::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv3d_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv3d(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1, int groups=1) -> Tensor
at::Tensor conv3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv3d_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d_padding, name, "aten::conv1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d_padding, overload_name, "padding")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv1d_padding, schema_str, "conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding=\"valid\", int[1] dilation=1, int groups=1) -> Tensor")

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding="valid", int[1] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv1d_padding::schema> create_conv1d_padding_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv1d_padding::name, conv1d_padding::overload_name)
      .typed<conv1d_padding::schema>();
}

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding="valid", int[1] dilation=1, int groups=1) -> Tensor
at::Tensor conv1d_padding::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv1d_padding_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, str padding="valid", int[1] dilation=1, int groups=1) -> Tensor
at::Tensor conv1d_padding::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv1d_padding_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv2d_padding, name, "aten::conv2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv2d_padding, overload_name, "padding")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv2d_padding, schema_str, "conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding=\"valid\", int[2] dilation=1, int groups=1) -> Tensor")

// aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding="valid", int[2] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv2d_padding::schema> create_conv2d_padding_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv2d_padding::name, conv2d_padding::overload_name)
      .typed<conv2d_padding::schema>();
}

// aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding="valid", int[2] dilation=1, int groups=1) -> Tensor
at::Tensor conv2d_padding::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv2d_padding_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, str padding="valid", int[2] dilation=1, int groups=1) -> Tensor
at::Tensor conv2d_padding::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv2d_padding_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d_padding, name, "aten::conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d_padding, overload_name, "padding")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv3d_padding, schema_str, "conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding=\"valid\", int[3] dilation=1, int groups=1) -> Tensor")

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding="valid", int[3] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv3d_padding::schema> create_conv3d_padding_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv3d_padding::name, conv3d_padding::overload_name)
      .typed<conv3d_padding::schema>();
}

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding="valid", int[3] dilation=1, int groups=1) -> Tensor
at::Tensor conv3d_padding::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv3d_padding_typed_handle();
    return op.call(input, weight, bias, stride, padding, dilation, groups);
}

// aten::conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, str padding="valid", int[3] dilation=1, int groups=1) -> Tensor
at::Tensor conv3d_padding::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, c10::string_view padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_conv3d_padding_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc, name, "aten::conv_tbc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc, schema_str, "conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor")

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_tbc::schema> create_conv_tbc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_tbc::name, conv_tbc::overload_name)
      .typed<conv_tbc::schema>();
}

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
at::Tensor conv_tbc::call(const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    static auto op = create_conv_tbc_typed_handle();
    return op.call(self, weight, bias, pad);
}

// aten::conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
at::Tensor conv_tbc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    static auto op = create_conv_tbc_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, pad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_backward, name, "aten::conv_tbc_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_tbc_backward, schema_str, "conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)")

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<conv_tbc_backward::schema> create_conv_tbc_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_tbc_backward::name, conv_tbc_backward::overload_name)
      .typed<conv_tbc_backward::schema>();
}

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_tbc_backward::call(const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    static auto op = create_conv_tbc_backward_typed_handle();
    return op.call(self, input, weight, bias, pad);
}

// aten::conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_tbc_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    static auto op = create_conv_tbc_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, input, weight, bias, pad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose1d, name, "aten::conv_transpose1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose1d, schema_str, "conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor")

// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_transpose1d::schema> create_conv_transpose1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_transpose1d::name, conv_transpose1d::overload_name)
      .typed<conv_transpose1d::schema>();
}

// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
at::Tensor conv_transpose1d::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    static auto op = create_conv_transpose1d_typed_handle();
    return op.call(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

// aten::conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, int[1] stride=1, int[1] padding=0, int[1] output_padding=0, int groups=1, int[1] dilation=1) -> Tensor
at::Tensor conv_transpose1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    static auto op = create_conv_transpose1d_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, output_padding, groups, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose2d_input, name, "aten::conv_transpose2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose2d_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose2d_input, schema_str, "conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor")

// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_transpose2d_input::schema> create_conv_transpose2d_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_transpose2d_input::name, conv_transpose2d_input::overload_name)
      .typed<conv_transpose2d_input::schema>();
}

// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
at::Tensor conv_transpose2d_input::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    static auto op = create_conv_transpose2d_input_typed_handle();
    return op.call(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

// aten::conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
at::Tensor conv_transpose2d_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    static auto op = create_conv_transpose2d_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, output_padding, groups, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose3d_input, name, "aten::conv_transpose3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose3d_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_transpose3d_input, schema_str, "conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor")

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_transpose3d_input::schema> create_conv_transpose3d_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_transpose3d_input::name, conv_transpose3d_input::overload_name)
      .typed<conv_transpose3d_input::schema>();
}

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
at::Tensor conv_transpose3d_input::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    static auto op = create_conv_transpose3d_input_typed_handle();
    return op.call(input, weight, bias, stride, padding, output_padding, groups, dilation);
}

// aten::conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
at::Tensor conv_transpose3d_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, int64_t groups, at::IntArrayRef dilation) {
    static auto op = create_conv_transpose3d_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, stride, padding, output_padding, groups, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_, name, "aten::copy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_, schema_str, "copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)")

// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copy_::schema> create_copy__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copy_::name, copy_::overload_name)
      .typed<copy_::schema>();
}

// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
at::Tensor & copy_::call(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    static auto op = create_copy__typed_handle();
    return op.call(self, src, non_blocking);
}

// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
at::Tensor & copy_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    static auto op = create_copy__typed_handle();
    return op.redispatch(dispatchKeySet, self, src, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from, name, "aten::_copy_from")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from, schema_str, "_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor")

// aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_copy_from::schema> create__copy_from_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_copy_from::name, _copy_from::overload_name)
      .typed<_copy_from::schema>();
}

// aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor
at::Tensor _copy_from::call(const at::Tensor & self, const at::Tensor & dst, bool non_blocking) {
    static auto op = create__copy_from_typed_handle();
    return op.call(self, dst, non_blocking);
}

// aten::_copy_from(Tensor self, Tensor dst, bool non_blocking=False) -> Tensor
at::Tensor _copy_from::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & dst, bool non_blocking) {
    static auto op = create__copy_from_typed_handle();
    return op.redispatch(dispatchKeySet, self, dst, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize, name, "aten::_copy_from_and_resize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_copy_from_and_resize, schema_str, "_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor")

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_copy_from_and_resize::schema> create__copy_from_and_resize_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_copy_from_and_resize::name, _copy_from_and_resize::overload_name)
      .typed<_copy_from_and_resize::schema>();
}

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
at::Tensor _copy_from_and_resize::call(const at::Tensor & self, const at::Tensor & dst) {
    static auto op = create__copy_from_and_resize_typed_handle();
    return op.call(self, dst);
}

// aten::_copy_from_and_resize(Tensor self, Tensor dst) -> Tensor
at::Tensor _copy_from_and_resize::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & dst) {
    static auto op = create__copy_from_and_resize_typed_handle();
    return op.redispatch(dispatchKeySet, self, dst);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos, name, "aten::cos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos, schema_str, "cos(Tensor self) -> Tensor")

// aten::cos(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cos::schema> create_cos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cos::name, cos::overload_name)
      .typed<cos::schema>();
}

// aten::cos(Tensor self) -> Tensor
at::Tensor cos::call(const at::Tensor & self) {
    static auto op = create_cos_typed_handle();
    return op.call(self);
}

// aten::cos(Tensor self) -> Tensor
at::Tensor cos::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_cos_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos_, name, "aten::cos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos_, schema_str, "cos_(Tensor(a!) self) -> Tensor(a!)")

// aten::cos_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cos_::schema> create_cos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cos_::name, cos_::overload_name)
      .typed<cos_::schema>();
}

// aten::cos_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & cos_::call(at::Tensor & self) {
    static auto op = create_cos__typed_handle();
    return op.call(self);
}

// aten::cos_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & cos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_cos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos_out, name, "aten::cos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cos_out, schema_str, "cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cos_out::schema> create_cos_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cos_out::name, cos_out::overload_name)
      .typed<cos_out::schema>();
}

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cos_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_cos_out_typed_handle();
    return op.call(self, out);
}

// aten::cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cos_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_cos_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh, name, "aten::cosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh, schema_str, "cosh(Tensor self) -> Tensor")

// aten::cosh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cosh::schema> create_cosh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosh::name, cosh::overload_name)
      .typed<cosh::schema>();
}

// aten::cosh(Tensor self) -> Tensor
at::Tensor cosh::call(const at::Tensor & self) {
    static auto op = create_cosh_typed_handle();
    return op.call(self);
}

// aten::cosh(Tensor self) -> Tensor
at::Tensor cosh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_cosh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_, name, "aten::cosh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_, schema_str, "cosh_(Tensor(a!) self) -> Tensor(a!)")

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cosh_::schema> create_cosh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosh_::name, cosh_::overload_name)
      .typed<cosh_::schema>();
}

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & cosh_::call(at::Tensor & self) {
    static auto op = create_cosh__typed_handle();
    return op.call(self);
}

// aten::cosh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & cosh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_cosh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_out, name, "aten::cosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosh_out, schema_str, "cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cosh_out::schema> create_cosh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosh_out::name, cosh_out::overload_name)
      .typed<cosh_out::schema>();
}

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cosh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_cosh_out_typed_handle();
    return op.call(self, out);
}

// aten::cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cosh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_cosh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_embedding_loss, name, "aten::cosine_embedding_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_embedding_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_embedding_loss, schema_str, "cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor")

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cosine_embedding_loss::schema> create_cosine_embedding_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosine_embedding_loss::name, cosine_embedding_loss::overload_name)
      .typed<cosine_embedding_loss::schema>();
}

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
at::Tensor cosine_embedding_loss::call(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    static auto op = create_cosine_embedding_loss_typed_handle();
    return op.call(input1, input2, target, margin, reduction);
}

// aten::cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
at::Tensor cosine_embedding_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    static auto op = create_cosine_embedding_loss_typed_handle();
    return op.redispatch(dispatchKeySet, input1, input2, target, margin, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(count_nonzero_dim_IntList, name, "aten::count_nonzero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(count_nonzero_dim_IntList, overload_name, "dim_IntList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(count_nonzero_dim_IntList, schema_str, "count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor")

// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<count_nonzero_dim_IntList::schema> create_count_nonzero_dim_IntList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(count_nonzero_dim_IntList::name, count_nonzero_dim_IntList::overload_name)
      .typed<count_nonzero_dim_IntList::schema>();
}

// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
at::Tensor count_nonzero_dim_IntList::call(const at::Tensor & self, at::IntArrayRef dim) {
    static auto op = create_count_nonzero_dim_IntList_typed_handle();
    return op.call(self, dim);
}

// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
at::Tensor count_nonzero_dim_IntList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim) {
    static auto op = create_count_nonzero_dim_IntList_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(count_nonzero, name, "aten::count_nonzero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(count_nonzero, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(count_nonzero, schema_str, "count_nonzero(Tensor self, int? dim=None) -> Tensor")

// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<count_nonzero::schema> create_count_nonzero_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(count_nonzero::name, count_nonzero::overload_name)
      .typed<count_nonzero::schema>();
}

// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor
at::Tensor count_nonzero::call(const at::Tensor & self, c10::optional<int64_t> dim) {
    static auto op = create_count_nonzero_typed_handle();
    return op.call(self, dim);
}

// aten::count_nonzero(Tensor self, int? dim=None) -> Tensor
at::Tensor count_nonzero::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim) {
    static auto op = create_count_nonzero_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cov, name, "aten::cov")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cov, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cov, schema_str, "cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor")

// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cov::schema> create_cov_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cov::name, cov::overload_name)
      .typed<cov::schema>();
}

// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor
at::Tensor cov::call(const at::Tensor & self, int64_t correction, const c10::optional<at::Tensor> & fweights, const c10::optional<at::Tensor> & aweights) {
    static auto op = create_cov_typed_handle();
    return op.call(self, correction, fweights, aweights);
}

// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -> Tensor
at::Tensor cov::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t correction, const c10::optional<at::Tensor> & fweights, const c10::optional<at::Tensor> & aweights) {
    static auto op = create_cov_typed_handle();
    return op.redispatch(dispatchKeySet, self, correction, fweights, aweights);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(corrcoef, name, "aten::corrcoef")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(corrcoef, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(corrcoef, schema_str, "corrcoef(Tensor self) -> Tensor")

// aten::corrcoef(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<corrcoef::schema> create_corrcoef_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(corrcoef::name, corrcoef::overload_name)
      .typed<corrcoef::schema>();
}

// aten::corrcoef(Tensor self) -> Tensor
at::Tensor corrcoef::call(const at::Tensor & self) {
    static auto op = create_corrcoef_typed_handle();
    return op.call(self);
}

// aten::corrcoef(Tensor self) -> Tensor
at::Tensor corrcoef::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_corrcoef_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator, name, "aten::cudnn_affine_grid_generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator, schema_str, "cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid")

// aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_affine_grid_generator::schema> create_cudnn_affine_grid_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_affine_grid_generator::name, cudnn_affine_grid_generator::overload_name)
      .typed<cudnn_affine_grid_generator::schema>();
}

// aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid
at::Tensor cudnn_affine_grid_generator::call(const at::Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) {
    static auto op = create_cudnn_affine_grid_generator_typed_handle();
    return op.call(theta, N, C, H, W);
}

// aten::cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -> Tensor grid
at::Tensor cudnn_affine_grid_generator::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) {
    static auto op = create_cudnn_affine_grid_generator_typed_handle();
    return op.redispatch(dispatchKeySet, theta, N, C, H, W);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward, name, "aten::cudnn_affine_grid_generator_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_affine_grid_generator_backward, schema_str, "cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta")

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_affine_grid_generator_backward::schema> create_cudnn_affine_grid_generator_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_affine_grid_generator_backward::name, cudnn_affine_grid_generator_backward::overload_name)
      .typed<cudnn_affine_grid_generator_backward::schema>();
}

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
at::Tensor cudnn_affine_grid_generator_backward::call(const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) {
    static auto op = create_cudnn_affine_grid_generator_backward_typed_handle();
    return op.call(grad, N, C, H, W);
}

// aten::cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -> Tensor grad_theta
at::Tensor cudnn_affine_grid_generator_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) {
    static auto op = create_cudnn_affine_grid_generator_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, N, C, H, W);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_batch_norm, name, "aten::cudnn_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_batch_norm, schema_str, "cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_batch_norm::schema> create_cudnn_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_batch_norm::name, cudnn_batch_norm::overload_name)
      .typed<cudnn_batch_norm::schema>();
}

// aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> cudnn_batch_norm::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
    static auto op = create_cudnn_batch_norm_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}

// aten::cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> cudnn_batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
    static auto op = create_cudnn_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_batch_norm_backward, name, "aten::cudnn_batch_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_batch_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_batch_norm_backward, schema_str, "cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)")

// aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_batch_norm_backward::schema> create_cudnn_batch_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_batch_norm_backward::name, cudnn_batch_norm_backward::overload_name)
      .typed<cudnn_batch_norm_backward::schema>();
}

// aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> cudnn_batch_norm_backward::call(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon, const at::Tensor & reserveSpace) {
    static auto op = create_cudnn_batch_norm_backward_typed_handle();
    return op.call(input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon, reserveSpace);
}

// aten::cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> cudnn_batch_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon, const at::Tensor & reserveSpace) {
    static auto op = create_cudnn_batch_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon, reserveSpace);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_deprecated, name, "aten::cudnn_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_deprecated, overload_name, "deprecated")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_deprecated, schema_str, "cudnn_convolution.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::cudnn_convolution.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_deprecated::schema> create_cudnn_convolution_deprecated_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_deprecated::name, cudnn_convolution_deprecated::overload_name)
      .typed<cudnn_convolution_deprecated::schema>();
}

// aten::cudnn_convolution.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_deprecated::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_deprecated_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::cudnn_convolution.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_deprecated::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_deprecated_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_deprecated2, name, "aten::cudnn_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_deprecated2, overload_name, "deprecated2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_deprecated2, schema_str, "cudnn_convolution.deprecated2(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::cudnn_convolution.deprecated2(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_deprecated2::schema> create_cudnn_convolution_deprecated2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_deprecated2::name, cudnn_convolution_deprecated2::overload_name)
      .typed<cudnn_convolution_deprecated2::schema>();
}

// aten::cudnn_convolution.deprecated2(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_deprecated2::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_deprecated2_typed_handle();
    return op.call(self, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::cudnn_convolution.deprecated2(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_deprecated2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_deprecated2_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution, name, "aten::cudnn_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution, schema_str, "cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution::schema> create_cudnn_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution::name, cudnn_convolution::overload_name)
      .typed<cudnn_convolution::schema>();
}

// aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_typed_handle();
    return op.call(self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution(Tensor self, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward_input, name, "aten::cudnn_convolution_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward_input, schema_str, "cudnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_backward_input::schema> create_cudnn_convolution_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_backward_input::name, cudnn_convolution_backward_input::overload_name)
      .typed<cudnn_convolution_backward_input::schema>();
}

// aten::cudnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_backward_input::call(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_backward_input_typed_handle();
    return op.call(self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward, name, "aten::cudnn_convolution_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward, schema_str, "cudnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)")

// aten::cudnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_backward::schema> create_cudnn_convolution_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_backward::name, cudnn_convolution_backward::overload_name)
      .typed<cudnn_convolution_backward::schema>();
}

// aten::cudnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> cudnn_convolution_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, ::std::array<bool,2> output_mask) {
    static auto op = create_cudnn_convolution_backward_typed_handle();
    return op.call(self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask);
}

// aten::cudnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> cudnn_convolution_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, ::std::array<bool,2> output_mask) {
    static auto op = create_cudnn_convolution_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward_weight, name, "aten::cudnn_convolution_backward_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_backward_weight, schema_str, "cudnn_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_backward_weight::schema> create_cudnn_convolution_backward_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_backward_weight::name, cudnn_convolution_backward_weight::overload_name)
      .typed<cudnn_convolution_backward_weight::schema>();
}

// aten::cudnn_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_backward_weight::call(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_backward_weight_typed_handle();
    return op.call(weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_backward_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_backward_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_deprecated, name, "aten::cudnn_convolution_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_deprecated, overload_name, "deprecated")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_deprecated, schema_str, "cudnn_convolution_transpose.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::cudnn_convolution_transpose.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_transpose_deprecated::schema> create_cudnn_convolution_transpose_deprecated_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_transpose_deprecated::name, cudnn_convolution_transpose_deprecated::overload_name)
      .typed<cudnn_convolution_transpose_deprecated::schema>();
}

// aten::cudnn_convolution_transpose.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_transpose_deprecated::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_transpose_deprecated_typed_handle();
    return op.call(self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::cudnn_convolution_transpose.deprecated(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_transpose_deprecated::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_transpose_deprecated_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_deprecated2, name, "aten::cudnn_convolution_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_deprecated2, overload_name, "deprecated2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_deprecated2, schema_str, "cudnn_convolution_transpose.deprecated2(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::cudnn_convolution_transpose.deprecated2(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_transpose_deprecated2::schema> create_cudnn_convolution_transpose_deprecated2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_transpose_deprecated2::name, cudnn_convolution_transpose_deprecated2::overload_name)
      .typed<cudnn_convolution_transpose_deprecated2::schema>();
}

// aten::cudnn_convolution_transpose.deprecated2(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_transpose_deprecated2::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_transpose_deprecated2_typed_handle();
    return op.call(self, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::cudnn_convolution_transpose.deprecated2(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor cudnn_convolution_transpose_deprecated2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_cudnn_convolution_transpose_deprecated2_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose, name, "aten::cudnn_convolution_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose, schema_str, "cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_transpose::schema> create_cudnn_convolution_transpose_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_transpose::name, cudnn_convolution_transpose::overload_name)
      .typed<cudnn_convolution_transpose::schema>();
}

// aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_transpose::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_transpose_typed_handle();
    return op.call(self, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution_transpose(Tensor self, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_transpose::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_transpose_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward, name, "aten::cudnn_convolution_transpose_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward, schema_str, "cudnn_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)")

// aten::cudnn_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_transpose_backward::schema> create_cudnn_convolution_transpose_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_transpose_backward::name, cudnn_convolution_transpose_backward::overload_name)
      .typed<cudnn_convolution_transpose_backward::schema>();
}

// aten::cudnn_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> cudnn_convolution_transpose_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, ::std::array<bool,2> output_mask) {
    static auto op = create_cudnn_convolution_transpose_backward_typed_handle();
    return op.call(self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask);
}

// aten::cudnn_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32, bool[2] output_mask) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> cudnn_convolution_transpose_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32, ::std::array<bool,2> output_mask) {
    static auto op = create_cudnn_convolution_transpose_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, allow_tf32, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward_input, name, "aten::cudnn_convolution_transpose_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward_input, schema_str, "cudnn_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_transpose_backward_input::schema> create_cudnn_convolution_transpose_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_transpose_backward_input::name, cudnn_convolution_transpose_backward_input::overload_name)
      .typed<cudnn_convolution_transpose_backward_input::schema>();
}

// aten::cudnn_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_transpose_backward_input::call(const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_transpose_backward_input_typed_handle();
    return op.call(grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_transpose_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_transpose_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward_weight, name, "aten::cudnn_convolution_transpose_backward_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_transpose_backward_weight, schema_str, "cudnn_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor")

// aten::cudnn_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_transpose_backward_weight::schema> create_cudnn_convolution_transpose_backward_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_transpose_backward_weight::name, cudnn_convolution_transpose_backward_weight::overload_name)
      .typed<cudnn_convolution_transpose_backward_weight::schema>();
}

// aten::cudnn_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_transpose_backward_weight::call(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_transpose_backward_weight_typed_handle();
    return op.call(weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

// aten::cudnn_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool allow_tf32) -> Tensor
at::Tensor cudnn_convolution_transpose_backward_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, bool allow_tf32) {
    static auto op = create_cudnn_convolution_transpose_backward_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic, allow_tf32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu, name, "aten::cudnn_convolution_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_relu, schema_str, "cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor")

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_relu::schema> create_cudnn_convolution_relu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_relu::name, cudnn_convolution_relu::overload_name)
      .typed<cudnn_convolution_relu::schema>();
}

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
at::Tensor cudnn_convolution_relu::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_cudnn_convolution_relu_typed_handle();
    return op.call(self, weight, bias, stride, padding, dilation, groups);
}

// aten::cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
at::Tensor cudnn_convolution_relu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_cudnn_convolution_relu_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_add_relu, name, "aten::cudnn_convolution_add_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_add_relu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_convolution_add_relu, schema_str, "cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor")

// aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_convolution_add_relu::schema> create_cudnn_convolution_add_relu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_convolution_add_relu::name, cudnn_convolution_add_relu::overload_name)
      .typed<cudnn_convolution_add_relu::schema>();
}

// aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
at::Tensor cudnn_convolution_add_relu::call(const at::Tensor & self, const at::Tensor & weight, const at::Tensor & z, const c10::optional<at::Scalar> & alpha, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_cudnn_convolution_add_relu_typed_handle();
    return op.call(self, weight, z, alpha, bias, stride, padding, dilation, groups);
}

// aten::cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
at::Tensor cudnn_convolution_add_relu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const at::Tensor & z, const c10::optional<at::Scalar> & alpha, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_cudnn_convolution_add_relu_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, z, alpha, bias, stride, padding, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler, name, "aten::cudnn_grid_sampler")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler, schema_str, "cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output")

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_grid_sampler::schema> create_cudnn_grid_sampler_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_grid_sampler::name, cudnn_grid_sampler::overload_name)
      .typed<cudnn_grid_sampler::schema>();
}

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
at::Tensor cudnn_grid_sampler::call(const at::Tensor & self, const at::Tensor & grid) {
    static auto op = create_cudnn_grid_sampler_typed_handle();
    return op.call(self, grid);
}

// aten::cudnn_grid_sampler(Tensor self, Tensor grid) -> Tensor output
at::Tensor cudnn_grid_sampler::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid) {
    static auto op = create_cudnn_grid_sampler_typed_handle();
    return op.redispatch(dispatchKeySet, self, grid);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler_backward, name, "aten::cudnn_grid_sampler_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cudnn_grid_sampler_backward, schema_str, "cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)")

// aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)
static C10_NOINLINE c10::TypedOperatorHandle<cudnn_grid_sampler_backward::schema> create_cudnn_grid_sampler_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cudnn_grid_sampler_backward::name, cudnn_grid_sampler_backward::overload_name)
      .typed<cudnn_grid_sampler_backward::schema>();
}

// aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)
::std::tuple<at::Tensor,at::Tensor> cudnn_grid_sampler_backward::call(const at::Tensor & self, const at::Tensor & grid, const at::Tensor & grad_output) {
    static auto op = create_cudnn_grid_sampler_backward_typed_handle();
    return op.call(self, grid, grad_output);
}

// aten::cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -> (Tensor grad_self, Tensor grad_grid)
::std::tuple<at::Tensor,at::Tensor> cudnn_grid_sampler_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grid, const at::Tensor & grad_output) {
    static auto op = create_cudnn_grid_sampler_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grid, grad_output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax, name, "aten::cummax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax, schema_str, "cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)")

// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummax::schema> create_cummax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummax::name, cummax::overload_name)
      .typed<cummax::schema>();
}

// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummax::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_cummax_typed_handle();
    return op.call(self, dim);
}

// aten::cummax(Tensor self, int dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_cummax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_out, name, "aten::cummax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_out, schema_str, "cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummax_out::schema> create_cummax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummax_out::name, cummax_out::overload_name)
      .typed<cummax_out::schema>();
}

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummax_out::call(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummax_out_typed_handle();
    return op.call(self, dim, values, indices);
}

// aten::cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_dimname, name, "aten::cummax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_dimname, schema_str, "cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)")

// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummax_dimname::schema> create_cummax_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummax_dimname::name, cummax_dimname::overload_name)
      .typed<cummax_dimname::schema>();
}

// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummax_dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_cummax_dimname_typed_handle();
    return op.call(self, dim);
}

// aten::cummax.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummax_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_cummax_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_dimname_out, name, "aten::cummax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummax_dimname_out, schema_str, "cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummax_dimname_out::schema> create_cummax_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummax_dimname_out::name, cummax_dimname_out::overload_name)
      .typed<cummax_dimname_out::schema>();
}

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummax_dimname_out::call(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummax_dimname_out_typed_handle();
    return op.call(self, dim, values, indices);
}

// aten::cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummax_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummax_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummax_helper, name, "aten::_cummax_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummax_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummax_helper, schema_str, "_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()")

// aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_cummax_helper::schema> create__cummax_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cummax_helper::name, _cummax_helper::overload_name)
      .typed<_cummax_helper::schema>();
}

// aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
void _cummax_helper::call(const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
    static auto op = create__cummax_helper_typed_handle();
    return op.call(self, values, indices, dim);
}

// aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
void _cummax_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
    static auto op = create__cummax_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, values, indices, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin, schema_str, "cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)")

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin::schema> create_cummin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin::name, cummin::overload_name)
      .typed<cummin::schema>();
}

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_cummin_typed_handle();
    return op.call(self, dim);
}

// aten::cummin(Tensor self, int dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_cummin_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_out, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_out, schema_str, "cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin_out::schema> create_cummin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin_out::name, cummin_out::overload_name)
      .typed<cummin_out::schema>();
}

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_out::call(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummin_out_typed_handle();
    return op.call(self, dim, values, indices);
}

// aten::cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname, schema_str, "cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)")

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin_dimname::schema> create_cummin_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin_dimname::name, cummin_dimname::overload_name)
      .typed<cummin_dimname::schema>();
}

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin_dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_cummin_dimname_typed_handle();
    return op.call(self, dim);
}

// aten::cummin.dimname(Tensor self, Dimname dim) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> cummin_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_cummin_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname_out, name, "aten::cummin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummin_dimname_out, schema_str, "cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<cummin_dimname_out::schema> create_cummin_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummin_dimname_out::name, cummin_dimname_out::overload_name)
      .typed<cummin_dimname_out::schema>();
}

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_dimname_out::call(const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummin_dimname_out_typed_handle();
    return op.call(self, dim, values, indices);
}

// aten::cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> cummin_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_cummin_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummin_helper, name, "aten::_cummin_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummin_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cummin_helper, schema_str, "_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()")

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_cummin_helper::schema> create__cummin_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cummin_helper::name, _cummin_helper::overload_name)
      .typed<_cummin_helper::schema>();
}

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
void _cummin_helper::call(const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
    static auto op = create__cummin_helper_typed_handle();
    return op.call(self, values, indices, dim);
}

// aten::_cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
void _cummin_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & values, at::Tensor & indices, int64_t dim) {
    static auto op = create__cummin_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, values, indices, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummaxmin_backward, name, "aten::cummaxmin_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummaxmin_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cummaxmin_backward, schema_str, "cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor")

// aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cummaxmin_backward::schema> create_cummaxmin_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cummaxmin_backward::name, cummaxmin_backward::overload_name)
      .typed<cummaxmin_backward::schema>();
}

// aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor
at::Tensor cummaxmin_backward::call(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
    static auto op = create_cummaxmin_backward_typed_handle();
    return op.call(grad, input, indices, dim);
}

// aten::cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -> Tensor
at::Tensor cummaxmin_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, const at::Tensor & indices, int64_t dim) {
    static auto op = create_cummaxmin_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input, indices, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod, schema_str, "cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor")

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumprod::schema> create_cumprod_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod::name, cumprod::overload_name)
      .typed<cumprod::schema>();
}

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_, name, "aten::cumprod_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_, schema_str, "cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)")

// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_::schema> create_cumprod__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_::name, cumprod_::overload_name)
      .typed<cumprod_::schema>();
}

// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod_::call(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod__typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_out, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_out, schema_str, "cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_out::schema> create_cumprod_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_out::name, cumprod_out::overload_name)
      .typed<cumprod_out::schema>();
}

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_out::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumprod_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumprod_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname, schema_str, "cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_dimname::schema> create_cumprod_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_dimname::name, cumprod_dimname::overload_name)
      .typed<cumprod_dimname::schema>();
}

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod_dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod_dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumprod_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod__dimname, name, "aten::cumprod_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod__dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod__dimname, schema_str, "cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)")

// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod__dimname::schema> create_cumprod__dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod__dimname::name, cumprod__dimname::overload_name)
      .typed<cumprod__dimname::schema>();
}

// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod__dimname::call(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod__dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumprod__dimname::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumprod__dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname_out, name, "aten::cumprod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_dimname_out, schema_str, "cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_dimname_out::schema> create_cumprod_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_dimname_out::name, cumprod_dimname_out::overload_name)
      .typed<cumprod_dimname_out::schema>();
}

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_dimname_out::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumprod_dimname_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumprod_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumprod_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_backward, name, "aten::cumprod_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumprod_backward, schema_str, "cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor")

// aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumprod_backward::schema> create_cumprod_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumprod_backward::name, cumprod_backward::overload_name)
      .typed<cumprod_backward::schema>();
}

// aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor
at::Tensor cumprod_backward::call(const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
    static auto op = create_cumprod_backward_typed_handle();
    return op.call(grad, input, dim, output);
}

// aten::cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -> Tensor
at::Tensor cumprod_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, int64_t dim, const at::Tensor & output) {
    static auto op = create_cumprod_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input, dim, output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum, name, "aten::cumsum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum, schema_str, "cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor")

// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumsum::schema> create_cumsum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumsum::name, cumsum::overload_name)
      .typed<cumsum::schema>();
}

// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumsum::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumsum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_, name, "aten::cumsum_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_, schema_str, "cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)")

// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumsum_::schema> create_cumsum__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumsum_::name, cumsum_::overload_name)
      .typed<cumsum_::schema>();
}

// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumsum_::call(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum__typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumsum_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_out, name, "aten::cumsum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_out, schema_str, "cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumsum_out::schema> create_cumsum_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumsum_out::name, cumsum_out::overload_name)
      .typed<cumsum_out::schema>();
}

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumsum_out::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumsum_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumsum_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumsum_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_dimname, name, "aten::cumsum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_dimname, schema_str, "cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumsum_dimname::schema> create_cumsum_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumsum_dimname::name, cumsum_dimname::overload_name)
      .typed<cumsum_dimname::schema>();
}

// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumsum_dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum_dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor cumsum_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum__dimname, name, "aten::cumsum_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum__dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum__dimname, schema_str, "cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)")

// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumsum__dimname::schema> create_cumsum__dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumsum__dimname::name, cumsum__dimname::overload_name)
      .typed<cumsum__dimname::schema>();
}

// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumsum__dimname::call(at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum__dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
at::Tensor & cumsum__dimname::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_cumsum__dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_dimname_out, name, "aten::cumsum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumsum_dimname_out, schema_str, "cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cumsum_dimname_out::schema> create_cumsum_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumsum_dimname_out::name, cumsum_dimname_out::overload_name)
      .typed<cumsum_dimname_out::schema>();
}

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumsum_dimname_out::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumsum_dimname_out_typed_handle();
    return op.call(self, dim, dtype, out);
}

// aten::cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cumsum_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_cumsum_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_x, name, "aten::cumulative_trapezoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_x, overload_name, "x")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_x, schema_str, "cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor")

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumulative_trapezoid_x::schema> create_cumulative_trapezoid_x_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumulative_trapezoid_x::name, cumulative_trapezoid_x::overload_name)
      .typed<cumulative_trapezoid_x::schema>();
}

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_x::call(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    static auto op = create_cumulative_trapezoid_x_typed_handle();
    return op.call(y, x, dim);
}

// aten::cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_x::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    static auto op = create_cumulative_trapezoid_x_typed_handle();
    return op.redispatch(dispatchKeySet, y, x, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_dx, name, "aten::cumulative_trapezoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_dx, overload_name, "dx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cumulative_trapezoid_dx, schema_str, "cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor")

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cumulative_trapezoid_dx::schema> create_cumulative_trapezoid_dx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cumulative_trapezoid_dx::name, cumulative_trapezoid_dx::overload_name)
      .typed<cumulative_trapezoid_dx::schema>();
}

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_dx::call(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    static auto op = create_cumulative_trapezoid_dx_typed_handle();
    return op.call(y, dx, dim);
}

// aten::cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
at::Tensor cumulative_trapezoid_dx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    static auto op = create_cumulative_trapezoid_dx_typed_handle();
    return op.redispatch(dispatchKeySet, y, dx, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_IntList, name, "aten::ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_IntList, overload_name, "IntList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_IntList, schema_str, "ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor")

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ctc_loss_IntList::schema> create_ctc_loss_IntList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ctc_loss_IntList::name, ctc_loss_IntList::overload_name)
      .typed<ctc_loss_IntList::schema>();
}

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_IntList::call(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    static auto op = create_ctc_loss_IntList_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

// aten::ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_IntList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    static auto op = create_ctc_loss_IntList_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_Tensor, name, "aten::ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ctc_loss_Tensor, schema_str, "ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor")

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ctc_loss_Tensor::schema> create_ctc_loss_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ctc_loss_Tensor::name, ctc_loss_Tensor::overload_name)
      .typed<ctc_loss_Tensor::schema>();
}

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_Tensor::call(const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    static auto op = create_ctc_loss_Tensor_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

// aten::ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
at::Tensor ctc_loss_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, const at::Tensor & input_lengths, const at::Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) {
    static auto op = create_ctc_loss_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_ctc_loss, name, "aten::_ctc_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_ctc_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_ctc_loss, schema_str, "_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)")

// aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_ctc_loss::schema> create__ctc_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_ctc_loss::name, _ctc_loss::overload_name)
      .typed<_ctc_loss::schema>();
}

// aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _ctc_loss::call(const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool zero_infinity) {
    static auto op = create__ctc_loss_typed_handle();
    return op.call(log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
}

// aten::_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _ctc_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, int64_t blank, bool zero_infinity) {
    static auto op = create__ctc_loss_typed_handle();
    return op.redispatch(dispatchKeySet, log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_ctc_loss_backward, name, "aten::_ctc_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_ctc_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_ctc_loss_backward, schema_str, "_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor")

// aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_ctc_loss_backward::schema> create__ctc_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_ctc_loss_backward::name, _ctc_loss_backward::overload_name)
      .typed<_ctc_loss_backward::schema>();
}

// aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
at::Tensor _ctc_loss_backward::call(const at::Tensor & grad, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, const at::Tensor & neg_log_likelihood, const at::Tensor & log_alpha, int64_t blank, bool zero_infinity) {
    static auto op = create__ctc_loss_backward_typed_handle();
    return op.call(grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, zero_infinity);
}

// aten::_ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
at::Tensor _ctc_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & log_probs, const at::Tensor & targets, at::IntArrayRef input_lengths, at::IntArrayRef target_lengths, const at::Tensor & neg_log_likelihood, const at::Tensor & log_alpha, int64_t blank, bool zero_infinity) {
    static auto op = create__ctc_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, zero_infinity);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed, name, "aten::diag_embed")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_embed, schema_str, "diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor")

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diag_embed::schema> create_diag_embed_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diag_embed::name, diag_embed::overload_name)
      .typed<diag_embed::schema>();
}

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
at::Tensor diag_embed::call(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    static auto op = create_diag_embed_typed_handle();
    return op.call(self, offset, dim1, dim2);
}

// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -> Tensor
at::Tensor diag_embed::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    static auto op = create_diag_embed_typed_handle();
    return op.redispatch(dispatchKeySet, self, offset, dim1, dim2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagflat, name, "aten::diagflat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagflat, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagflat, schema_str, "diagflat(Tensor self, int offset=0) -> Tensor")

// aten::diagflat(Tensor self, int offset=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diagflat::schema> create_diagflat_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diagflat::name, diagflat::overload_name)
      .typed<diagflat::schema>();
}

// aten::diagflat(Tensor self, int offset=0) -> Tensor
at::Tensor diagflat::call(const at::Tensor & self, int64_t offset) {
    static auto op = create_diagflat_typed_handle();
    return op.call(self, offset);
}

// aten::diagflat(Tensor self, int offset=0) -> Tensor
at::Tensor diagflat::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset) {
    static auto op = create_diagflat_typed_handle();
    return op.redispatch(dispatchKeySet, self, offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal, name, "aten::diagonal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal, schema_str, "diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)")

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<diagonal::schema> create_diagonal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diagonal::name, diagonal::overload_name)
      .typed<diagonal::schema>();
}

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
at::Tensor diagonal::call(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    static auto op = create_diagonal_typed_handle();
    return op.call(self, offset, dim1, dim2);
}

// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)
at::Tensor diagonal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    static auto op = create_diagonal_typed_handle();
    return op.redispatch(dispatchKeySet, self, offset, dim1, dim2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_Dimname, name, "aten::diagonal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_Dimname, schema_str, "diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)")

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<diagonal_Dimname::schema> create_diagonal_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diagonal_Dimname::name, diagonal_Dimname::overload_name)
      .typed<diagonal_Dimname::schema>();
}

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
at::Tensor diagonal_Dimname::call(const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
    static auto op = create_diagonal_Dimname_typed_handle();
    return op.call(self, outdim, dim1, dim2, offset);
}

// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -> Tensor(a)
at::Tensor diagonal_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) {
    static auto op = create_diagonal_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, outdim, dim1, dim2, offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_backward, name, "aten::diagonal_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diagonal_backward, schema_str, "diagonal_backward(Tensor grad_output, int[] input_sizes, int offset, int dim1, int dim2) -> Tensor")

// aten::diagonal_backward(Tensor grad_output, int[] input_sizes, int offset, int dim1, int dim2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diagonal_backward::schema> create_diagonal_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diagonal_backward::name, diagonal_backward::overload_name)
      .typed<diagonal_backward::schema>();
}

// aten::diagonal_backward(Tensor grad_output, int[] input_sizes, int offset, int dim1, int dim2) -> Tensor
at::Tensor diagonal_backward::call(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
    static auto op = create_diagonal_backward_typed_handle();
    return op.call(grad_output, input_sizes, offset, dim1, dim2);
}

// aten::diagonal_backward(Tensor grad_output, int[] input_sizes, int offset, int dim1, int dim2) -> Tensor
at::Tensor diagonal_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
    static auto op = create_diagonal_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, offset, dim1, dim2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill_diagonal_, name, "aten::fill_diagonal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill_diagonal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill_diagonal_, schema_str, "fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)")

// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fill_diagonal_::schema> create_fill_diagonal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fill_diagonal_::name, fill_diagonal_::overload_name)
      .typed<fill_diagonal_::schema>();
}

// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)
at::Tensor & fill_diagonal_::call(at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
    static auto op = create_fill_diagonal__typed_handle();
    return op.call(self, fill_value, wrap);
}

// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)
at::Tensor & fill_diagonal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & fill_value, bool wrap) {
    static auto op = create_fill_diagonal__typed_handle();
    return op.redispatch(dispatchKeySet, self, fill_value, wrap);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diff, name, "aten::diff")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diff, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diff, schema_str, "diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor")

// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diff::schema> create_diff_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diff::name, diff::overload_name)
      .typed<diff::schema>();
}

// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor
at::Tensor diff::call(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
    static auto op = create_diff_typed_handle();
    return op.call(self, n, dim, prepend, append);
}

// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -> Tensor
at::Tensor diff::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append) {
    static auto op = create_diff_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, prepend, append);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diff_out, name, "aten::diff")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diff_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diff_out, schema_str, "diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<diff_out::schema> create_diff_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diff_out::name, diff_out::overload_name)
      .typed<diff_out::schema>();
}

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & diff_out::call(const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
    static auto op = create_diff_out_typed_handle();
    return op.call(self, n, dim, prepend, append, out);
}

// aten::diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & diff_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, int64_t dim, const c10::optional<at::Tensor> & prepend, const c10::optional<at::Tensor> & append, at::Tensor & out) {
    static auto op = create_diff_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, prepend, append, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarint, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarint, overload_name, "scalarint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarint, schema_str, "gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]")

// aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_scalarint::schema> create_gradient_scalarint_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_scalarint::name, gradient_scalarint::overload_name)
      .typed<gradient_scalarint::schema>();
}

// aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalarint::call(const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    static auto op = create_gradient_scalarint_typed_handle();
    return op.call(self, spacing, dim, edge_order);
}

// aten::gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalarint::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    static auto op = create_gradient_scalarint_typed_handle();
    return op.redispatch(dispatchKeySet, self, spacing, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalararray, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalararray, overload_name, "scalararray")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalararray, schema_str, "gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]")

// aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_scalararray::schema> create_gradient_scalararray_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_scalararray::name, gradient_scalararray::overload_name)
      .typed<gradient_scalararray::schema>();
}

// aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalararray::call(const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_scalararray_typed_handle();
    return op.call(self, spacing, dim, edge_order);
}

// aten::gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalararray::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & spacing, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_scalararray_typed_handle();
    return op.redispatch(dispatchKeySet, self, spacing, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_array, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_array, overload_name, "array")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_array, schema_str, "gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]")

// aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_array::schema> create_gradient_array_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_array::name, gradient_array::overload_name)
      .typed<gradient_array::schema>();
}

// aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_array::call(const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_array_typed_handle();
    return op.call(self, dim, edge_order);
}

// aten::gradient.array(Tensor self, *, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_array::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_array_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarrayint, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarrayint, overload_name, "scalarrayint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarrayint, schema_str, "gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]")

// aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_scalarrayint::schema> create_gradient_scalarrayint_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_scalarrayint::name, gradient_scalarrayint::overload_name)
      .typed<gradient_scalarrayint::schema>();
}

// aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalarrayint::call(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    static auto op = create_gradient_scalarrayint_typed_handle();
    return op.call(self, spacing, dim, edge_order);
}

// aten::gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalarrayint::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    static auto op = create_gradient_scalarrayint_typed_handle();
    return op.redispatch(dispatchKeySet, self, spacing, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarrayarray, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarrayarray, overload_name, "scalarrayarray")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_scalarrayarray, schema_str, "gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]")

// aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_scalarrayarray::schema> create_gradient_scalarrayarray_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_scalarrayarray::name, gradient_scalarrayarray::overload_name)
      .typed<gradient_scalarrayarray::schema>();
}

// aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalarrayarray::call(const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_scalarrayarray_typed_handle();
    return op.call(self, spacing, dim, edge_order);
}

// aten::gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_scalarrayarray::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ArrayRef<at::Scalar> spacing, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_scalarrayarray_typed_handle();
    return op.redispatch(dispatchKeySet, self, spacing, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_tensorarrayint, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_tensorarrayint, overload_name, "tensorarrayint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_tensorarrayint, schema_str, "gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]")

// aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_tensorarrayint::schema> create_gradient_tensorarrayint_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_tensorarrayint::name, gradient_tensorarrayint::overload_name)
      .typed<gradient_tensorarrayint::schema>();
}

// aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_tensorarrayint::call(const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    static auto op = create_gradient_tensorarrayint_typed_handle();
    return op.call(self, spacing, dim, edge_order);
}

// aten::gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_tensorarrayint::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList spacing, c10::optional<int64_t> dim, int64_t edge_order) {
    static auto op = create_gradient_tensorarrayint_typed_handle();
    return op.redispatch(dispatchKeySet, self, spacing, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_tensorarray, name, "aten::gradient")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_tensorarray, overload_name, "tensorarray")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gradient_tensorarray, schema_str, "gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]")

// aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<gradient_tensorarray::schema> create_gradient_tensorarray_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gradient_tensorarray::name, gradient_tensorarray::overload_name)
      .typed<gradient_tensorarray::schema>();
}

// aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_tensorarray::call(const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_tensorarray_typed_handle();
    return op.call(self, spacing, dim, edge_order);
}

// aten::gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -> Tensor[]
::std::vector<at::Tensor> gradient_tensorarray::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::TensorList spacing, at::IntArrayRef dim, int64_t edge_order) {
    static auto op = create_gradient_tensorarray_typed_handle();
    return op.redispatch(dispatchKeySet, self, spacing, dim, edge_order);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor, schema_str, "div.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Tensor::schema> create_div_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Tensor::name, div_Tensor::overload_name)
      .typed<div_Tensor::schema>();
}

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor div_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_div_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::div.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor div_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_div_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor, schema_str, "div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Tensor::schema> create_div__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Tensor::name, div__Tensor::overload_name)
      .typed<div__Tensor::schema>();
}

// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & div__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_div__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & div__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_div__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out, schema_str, "div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div_out::schema> create_div_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_out::name, div_out::overload_name)
      .typed<div_out::schema>();
}

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_div_out_typed_handle();
    return op.call(self, other, out);
}

// aten::div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_div_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor_mode, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Tensor_mode, schema_str, "div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor")

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Tensor_mode::schema> create_div_Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Tensor_mode::name, div_Tensor_mode::overload_name)
      .typed<div_Tensor_mode::schema>();
}

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor div_Tensor_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div_Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor div_Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div_Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor_mode, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Tensor_mode, schema_str, "div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)")

// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Tensor_mode::schema> create_div__Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Tensor_mode::name, div__Tensor_mode::overload_name)
      .typed<div__Tensor_mode::schema>();
}

// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Tensor_mode::call(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div__Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div__Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out_mode, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out_mode, overload_name, "out_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_out_mode, schema_str, "div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)")

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div_out_mode::schema> create_div_out_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_out_mode::name, div_out_mode::overload_name)
      .typed<div_out_mode::schema>();
}

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    static auto op = create_div_out_mode_typed_handle();
    return op.call(self, other, rounding_mode, out);
}

// aten::div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & div_out_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    static auto op = create_div_out_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar, schema_str, "div.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Scalar::schema> create_div_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Scalar::name, div_Scalar::overload_name)
      .typed<div_Scalar::schema>();
}

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor div_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_div_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::div.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor div_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_div_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar, schema_str, "div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Scalar::schema> create_div__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Scalar::name, div__Scalar::overload_name)
      .typed<div__Scalar::schema>();
}

// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & div__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_div__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & div__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_div__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode, name, "aten::div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div_Scalar_mode, schema_str, "div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor")

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<div_Scalar_mode::schema> create_div_Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div_Scalar_mode::name, div_Scalar_mode::overload_name)
      .typed<div_Scalar_mode::schema>();
}

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor div_Scalar_mode::call(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div_Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor div_Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div_Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar_mode, name, "aten::div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(div__Scalar_mode, schema_str, "div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)")

// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<div__Scalar_mode::schema> create_div__Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(div__Scalar_mode::name, div__Scalar_mode::overload_name)
      .typed<div__Scalar_mode::schema>();
}

// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Scalar_mode::call(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div__Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & div__Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_div__Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor, schema_str, "divide.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Tensor::schema> create_divide_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Tensor::name, divide_Tensor::overload_name)
      .typed<divide_Tensor::schema>();
}

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor divide_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_divide_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::divide.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor divide_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_divide_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor, schema_str, "divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Tensor::schema> create_divide__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Tensor::name, divide__Tensor::overload_name)
      .typed<divide__Tensor::schema>();
}

// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & divide__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_divide__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & divide__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_divide__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out, schema_str, "divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide_out::schema> create_divide_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_out::name, divide_out::overload_name)
      .typed<divide_out::schema>();
}

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_divide_out_typed_handle();
    return op.call(self, other, out);
}

// aten::divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_divide_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar, schema_str, "divide.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Scalar::schema> create_divide_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Scalar::name, divide_Scalar::overload_name)
      .typed<divide_Scalar::schema>();
}

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor divide_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_divide_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor divide_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_divide_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar, schema_str, "divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Scalar::schema> create_divide__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Scalar::name, divide__Scalar::overload_name)
      .typed<divide__Scalar::schema>();
}

// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & divide__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_divide__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & divide__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_divide__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor_mode, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Tensor_mode, schema_str, "divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor")

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Tensor_mode::schema> create_divide_Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Tensor_mode::name, divide_Tensor_mode::overload_name)
      .typed<divide_Tensor_mode::schema>();
}

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Tensor_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide_Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide_Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor_mode, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor_mode, overload_name, "Tensor_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Tensor_mode, schema_str, "divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)")

// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Tensor_mode::schema> create_divide__Tensor_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Tensor_mode::name, divide__Tensor_mode::overload_name)
      .typed<divide__Tensor_mode::schema>();
}

// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Tensor_mode::call(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide__Tensor_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Tensor_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide__Tensor_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out_mode, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out_mode, overload_name, "out_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_out_mode, schema_str, "divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)")

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide_out_mode::schema> create_divide_out_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_out_mode::name, divide_out_mode::overload_name)
      .typed<divide_out_mode::schema>();
}

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out_mode::call(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    static auto op = create_divide_out_mode_typed_handle();
    return op.call(self, other, rounding_mode, out);
}

// aten::divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
at::Tensor & divide_out_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    static auto op = create_divide_out_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar_mode, name, "aten::divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide_Scalar_mode, schema_str, "divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor")

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<divide_Scalar_mode::schema> create_divide_Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide_Scalar_mode::name, divide_Scalar_mode::overload_name)
      .typed<divide_Scalar_mode::schema>();
}

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Scalar_mode::call(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide_Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
at::Tensor divide_Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide_Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar_mode, name, "aten::divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar_mode, overload_name, "Scalar_mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(divide__Scalar_mode, schema_str, "divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)")

// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<divide__Scalar_mode::schema> create_divide__Scalar_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(divide__Scalar_mode::name, divide__Scalar_mode::overload_name)
      .typed<divide__Scalar_mode::schema>();
}

// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Scalar_mode::call(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide__Scalar_mode_typed_handle();
    return op.call(self, other, rounding_mode);
}

// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
at::Tensor & divide__Scalar_mode::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    static auto op = create_divide__Scalar_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rounding_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_Tensor, name, "aten::true_divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_Tensor, schema_str, "true_divide.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<true_divide_Tensor::schema> create_true_divide_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(true_divide_Tensor::name, true_divide_Tensor::overload_name)
      .typed<true_divide_Tensor::schema>();
}

// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor true_divide_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_true_divide_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::true_divide.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor true_divide_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_true_divide_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide__Tensor, name, "aten::true_divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide__Tensor, schema_str, "true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<true_divide__Tensor::schema> create_true_divide__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(true_divide__Tensor::name, true_divide__Tensor::overload_name)
      .typed<true_divide__Tensor::schema>();
}

// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & true_divide__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_true_divide__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & true_divide__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_true_divide__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_out, name, "aten::true_divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_out, schema_str, "true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<true_divide_out::schema> create_true_divide_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(true_divide_out::name, true_divide_out::overload_name)
      .typed<true_divide_out::schema>();
}

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & true_divide_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_true_divide_out_typed_handle();
    return op.call(self, other, out);
}

// aten::true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & true_divide_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_true_divide_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_Scalar, name, "aten::true_divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide_Scalar, schema_str, "true_divide.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<true_divide_Scalar::schema> create_true_divide_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(true_divide_Scalar::name, true_divide_Scalar::overload_name)
      .typed<true_divide_Scalar::schema>();
}

// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor true_divide_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_true_divide_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::true_divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor true_divide_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_true_divide_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide__Scalar, name, "aten::true_divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(true_divide__Scalar, schema_str, "true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<true_divide__Scalar::schema> create_true_divide__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(true_divide__Scalar::name, true_divide__Scalar::overload_name)
      .typed<true_divide__Scalar::schema>();
}

// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & true_divide__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_true_divide__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & true_divide__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_true_divide__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dot, name, "aten::dot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dot, schema_str, "dot(Tensor self, Tensor tensor) -> Tensor")

// aten::dot(Tensor self, Tensor tensor) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<dot::schema> create_dot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dot::name, dot::overload_name)
      .typed<dot::schema>();
}

// aten::dot(Tensor self, Tensor tensor) -> Tensor
at::Tensor dot::call(const at::Tensor & self, const at::Tensor & tensor) {
    static auto op = create_dot_typed_handle();
    return op.call(self, tensor);
}

// aten::dot(Tensor self, Tensor tensor) -> Tensor
at::Tensor dot::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor) {
    static auto op = create_dot_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dot_out, name, "aten::dot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dot_out, schema_str, "dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)")

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<dot_out::schema> create_dot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dot_out::name, dot_out::overload_name)
      .typed<dot_out::schema>();
}

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & dot_out::call(const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
    static auto op = create_dot_out_typed_handle();
    return op.call(self, tensor, out);
}

// aten::dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & dot_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
    static auto op = create_dot_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vdot, name, "aten::vdot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vdot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vdot, schema_str, "vdot(Tensor self, Tensor other) -> Tensor")

// aten::vdot(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<vdot::schema> create_vdot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vdot::name, vdot::overload_name)
      .typed<vdot::schema>();
}

// aten::vdot(Tensor self, Tensor other) -> Tensor
at::Tensor vdot::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_vdot_typed_handle();
    return op.call(self, other);
}

// aten::vdot(Tensor self, Tensor other) -> Tensor
at::Tensor vdot::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_vdot_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vdot_out, name, "aten::vdot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vdot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vdot_out, schema_str, "vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<vdot_out::schema> create_vdot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vdot_out::name, vdot_out::overload_name)
      .typed<vdot_out::schema>();
}

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & vdot_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_vdot_out_typed_handle();
    return op.call(self, other, out);
}

// aten::vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & vdot_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_vdot_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(einsum, name, "aten::einsum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(einsum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(einsum, schema_str, "einsum(str equation, Tensor[] tensors) -> Tensor")

// aten::einsum(str equation, Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<einsum::schema> create_einsum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(einsum::name, einsum::overload_name)
      .typed<einsum::schema>();
}

// aten::einsum(str equation, Tensor[] tensors) -> Tensor
at::Tensor einsum::call(c10::string_view equation, at::TensorList tensors) {
    static auto op = create_einsum_typed_handle();
    return op.call(equation, tensors);
}

// aten::einsum(str equation, Tensor[] tensors) -> Tensor
at::Tensor einsum::redispatch(c10::DispatchKeySet dispatchKeySet, c10::string_view equation, at::TensorList tensors) {
    static auto op = create_einsum_typed_handle();
    return op.redispatch(dispatchKeySet, equation, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding, name, "aten::embedding")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding, schema_str, "embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor")

// aten::embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<embedding::schema> create_embedding_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding::name, embedding::overload_name)
      .typed<embedding::schema>();
}

// aten::embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
at::Tensor embedding::call(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    static auto op = create_embedding_typed_handle();
    return op.call(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}

// aten::embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
at::Tensor embedding::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    static auto op = create_embedding_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, padding_idx, scale_grad_by_freq, sparse);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_backward, name, "aten::embedding_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_backward, schema_str, "embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor")

// aten::embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<embedding_backward::schema> create_embedding_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_backward::name, embedding_backward::overload_name)
      .typed<embedding_backward::schema>();
}

// aten::embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
at::Tensor embedding_backward::call(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    static auto op = create_embedding_backward_typed_handle();
    return op.call(grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}

// aten::embedding_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq, bool sparse) -> Tensor
at::Tensor embedding_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    static auto op = create_embedding_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_dense_backward, name, "aten::embedding_dense_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_dense_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_dense_backward, schema_str, "embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor")

// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<embedding_dense_backward::schema> create_embedding_dense_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_dense_backward::name, embedding_dense_backward::overload_name)
      .typed<embedding_dense_backward::schema>();
}

// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
at::Tensor embedding_dense_backward::call(const at::Tensor & grad_output, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    static auto op = create_embedding_dense_backward_typed_handle();
    return op.call(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
}

// aten::embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
at::Tensor embedding_dense_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    static auto op = create_embedding_dense_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_renorm_, name, "aten::embedding_renorm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_renorm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_renorm_, schema_str, "embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)")

// aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<embedding_renorm_::schema> create_embedding_renorm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_renorm_::name, embedding_renorm_::overload_name)
      .typed<embedding_renorm_::schema>();
}

// aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
at::Tensor & embedding_renorm_::call(at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type) {
    static auto op = create_embedding_renorm__typed_handle();
    return op.call(self, indices, max_norm, norm_type);
}

// aten::embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
at::Tensor & embedding_renorm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & indices, double max_norm, double norm_type) {
    static auto op = create_embedding_renorm__typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, max_norm, norm_type);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_sparse_backward, name, "aten::embedding_sparse_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_sparse_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_sparse_backward, schema_str, "embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor")

// aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<embedding_sparse_backward::schema> create_embedding_sparse_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_sparse_backward::name, embedding_sparse_backward::overload_name)
      .typed<embedding_sparse_backward::schema>();
}

// aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
at::Tensor embedding_sparse_backward::call(const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    static auto op = create_embedding_sparse_backward_typed_handle();
    return op.call(grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}

// aten::embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
at::Tensor embedding_sparse_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
    static auto op = create_embedding_sparse_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only, name, "aten::_embedding_bag_forward_only")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_forward_only, schema_str, "_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_forward_only::schema> create__embedding_bag_forward_only_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_forward_only::name, _embedding_bag_forward_only::overload_name)
      .typed<_embedding_bag_forward_only::schema>();
}

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag_forward_only::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    static auto op = create__embedding_bag_forward_only_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

// aten::_embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag_forward_only::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    static auto op = create__embedding_bag_forward_only_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_rowwise_prune, name, "aten::_rowwise_prune")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_rowwise_prune, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_rowwise_prune, schema_str, "_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -> (Tensor, Tensor)")

// aten::_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_rowwise_prune::schema> create__rowwise_prune_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_rowwise_prune::name, _rowwise_prune::overload_name)
      .typed<_rowwise_prune::schema>();
}

// aten::_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _rowwise_prune::call(const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
    static auto op = create__rowwise_prune_typed_handle();
    return op.call(weight, mask, compressed_indices_dtype);
}

// aten::_rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _rowwise_prune::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & mask, at::ScalarType compressed_indices_dtype) {
    static auto op = create__rowwise_prune_typed_handle();
    return op.redispatch(dispatchKeySet, weight, mask, compressed_indices_dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_stack, name, "aten::row_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_stack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_stack, schema_str, "row_stack(Tensor[] tensors) -> Tensor")

// aten::row_stack(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<row_stack::schema> create_row_stack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(row_stack::name, row_stack::overload_name)
      .typed<row_stack::schema>();
}

// aten::row_stack(Tensor[] tensors) -> Tensor
at::Tensor row_stack::call(at::TensorList tensors) {
    static auto op = create_row_stack_typed_handle();
    return op.call(tensors);
}

// aten::row_stack(Tensor[] tensors) -> Tensor
at::Tensor row_stack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_row_stack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_stack_out, name, "aten::row_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_stack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(row_stack_out, schema_str, "row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<row_stack_out::schema> create_row_stack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(row_stack_out::name, row_stack_out::overload_name)
      .typed<row_stack_out::schema>();
}

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & row_stack_out::call(at::TensorList tensors, at::Tensor & out) {
    static auto op = create_row_stack_out_typed_handle();
    return op.call(tensors, out);
}

// aten::row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & row_stack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    static auto op = create_row_stack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag, name, "aten::embedding_bag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag, schema_str, "embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<embedding_bag::schema> create_embedding_bag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_bag::name, embedding_bag::overload_name)
      .typed<embedding_bag::schema>();
}

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
    static auto op = create_embedding_bag_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

// aten::embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset) {
    static auto op = create_embedding_bag_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag_padding_idx, name, "aten::embedding_bag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag_padding_idx, overload_name, "padding_idx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(embedding_bag_padding_idx, schema_str, "embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<embedding_bag_padding_idx::schema> create_embedding_bag_padding_idx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(embedding_bag_padding_idx::name, embedding_bag_padding_idx::overload_name)
      .typed<embedding_bag_padding_idx::schema>();
}

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag_padding_idx::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
    static auto op = create_embedding_bag_padding_idx_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

// aten::embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> embedding_bag_padding_idx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, c10::optional<int64_t> padding_idx) {
    static auto op = create_embedding_bag_padding_idx_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag, name, "aten::_embedding_bag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag, schema_str, "_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag::schema> create__embedding_bag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag::name, _embedding_bag::overload_name)
      .typed<_embedding_bag::schema>();
}

// aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag::call(const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    static auto op = create__embedding_bag_typed_handle();
    return op.call(weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

// aten::_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> _embedding_bag::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, bool include_last_offset, int64_t padding_idx) {
    static auto op = create__embedding_bag_typed_handle();
    return op.redispatch(dispatchKeySet, weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_backward, name, "aten::_embedding_bag_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_backward, schema_str, "_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor")

// aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_backward::schema> create__embedding_bag_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_backward::name, _embedding_bag_backward::overload_name)
      .typed<_embedding_bag_backward::schema>();
}

// aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_backward::call(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    static auto op = create__embedding_bag_backward_typed_handle();
    return op.call(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}

// aten::_embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    static auto op = create__embedding_bag_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_sparse_backward, name, "aten::_embedding_bag_sparse_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_sparse_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_sparse_backward, schema_str, "_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor")

// aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_sparse_backward::schema> create__embedding_bag_sparse_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_sparse_backward::name, _embedding_bag_sparse_backward::overload_name)
      .typed<_embedding_bag_sparse_backward::schema>();
}

// aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_sparse_backward::call(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    static auto op = create__embedding_bag_sparse_backward_typed_handle();
    return op.call(grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}

// aten::_embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_sparse_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, const at::Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    static auto op = create__embedding_bag_sparse_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_dense_backward, name, "aten::_embedding_bag_dense_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_dense_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_dense_backward, schema_str, "_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor")

// aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_dense_backward::schema> create__embedding_bag_dense_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_dense_backward::name, _embedding_bag_dense_backward::overload_name)
      .typed<_embedding_bag_dense_backward::schema>();
}

// aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_dense_backward::call(const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    static auto op = create__embedding_bag_dense_backward_typed_handle();
    return op.call(grad, indices, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}

// aten::_embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_dense_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & indices, const at::Tensor & offset2bag, const at::Tensor & bag_size, const at::Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<at::Tensor> & per_sample_weights, int64_t padding_idx) {
    static auto op = create__embedding_bag_dense_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, indices, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_per_sample_weights_backward, name, "aten::_embedding_bag_per_sample_weights_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_per_sample_weights_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_embedding_bag_per_sample_weights_backward, schema_str, "_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor")

// aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_embedding_bag_per_sample_weights_backward::schema> create__embedding_bag_per_sample_weights_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_embedding_bag_per_sample_weights_backward::name, _embedding_bag_per_sample_weights_backward::overload_name)
      .typed<_embedding_bag_per_sample_weights_backward::schema>();
}

// aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_per_sample_weights_backward::call(const at::Tensor & grad, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, int64_t mode, int64_t padding_idx) {
    static auto op = create__embedding_bag_per_sample_weights_backward_typed_handle();
    return op.call(grad, weight, indices, offsets, offset2bag, mode, padding_idx);
}

// aten::_embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor
at::Tensor _embedding_bag_per_sample_weights_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & weight, const at::Tensor & indices, const at::Tensor & offsets, const at::Tensor & offset2bag, int64_t mode, int64_t padding_idx) {
    static auto op = create__embedding_bag_per_sample_weights_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, weight, indices, offsets, offset2bag, mode, padding_idx);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_names, name, "aten::empty")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_names, schema_str, "empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_names::schema> create_empty_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_names::name, empty_names::overload_name)
      .typed<empty_names::schema>();
}

// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_names::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_names_typed_handle();
    return op.call(size, names, dtype, layout, device, pin_memory, memory_format);
}

// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_memory_format, name, "aten::empty")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_memory_format, overload_name, "memory_format")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_memory_format, schema_str, "empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_memory_format::schema> create_empty_memory_format_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_memory_format::name, empty_memory_format::overload_name)
      .typed<empty_memory_format::schema>();
}

// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_memory_format::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_memory_format_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory, memory_format);
}

// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_memory_format::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_memory_format_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_empty, name, "aten::new_empty")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_empty, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_empty, schema_str, "new_empty(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::new_empty(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<new_empty::schema> create_new_empty_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_empty::name, new_empty::overload_name)
      .typed<new_empty::schema>();
}

// aten::new_empty(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_empty::call(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_empty_typed_handle();
    return op.call(self, size, dtype, layout, device, pin_memory);
}

// aten::new_empty(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_empty::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_empty_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_empty_strided, name, "aten::new_empty_strided")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_empty_strided, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_empty_strided, schema_str, "new_empty_strided(Tensor self, int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::new_empty_strided(Tensor self, int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<new_empty_strided::schema> create_new_empty_strided_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_empty_strided::name, new_empty_strided::overload_name)
      .typed<new_empty_strided::schema>();
}

// aten::new_empty_strided(Tensor self, int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_empty_strided::call(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_empty_strided_typed_handle();
    return op.call(self, size, stride, dtype, layout, device, pin_memory);
}

// aten::new_empty_strided(Tensor self, int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_empty_strided::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_empty_strided_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_full, name, "aten::new_full")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_full, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_full, schema_str, "new_full(Tensor self, int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::new_full(Tensor self, int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<new_full::schema> create_new_full_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_full::name, new_full::overload_name)
      .typed<new_full::schema>();
}

// aten::new_full(Tensor self, int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_full::call(const at::Tensor & self, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_full_typed_handle();
    return op.call(self, size, fill_value, dtype, layout, device, pin_memory);
}

// aten::new_full(Tensor self, int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_full::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_full_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, fill_value, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros, name, "aten::new_zeros")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_zeros, schema_str, "new_zeros(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::new_zeros(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<new_zeros::schema> create_new_zeros_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_zeros::name, new_zeros::overload_name)
      .typed<new_zeros::schema>();
}

// aten::new_zeros(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_zeros::call(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_zeros_typed_handle();
    return op.call(self, size, dtype, layout, device, pin_memory);
}

// aten::new_zeros(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_zeros::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_zeros_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_ones, name, "aten::new_ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_ones, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(new_ones, schema_str, "new_ones(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::new_ones(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<new_ones::schema> create_new_ones_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(new_ones::name, new_ones::overload_name)
      .typed<new_ones::schema>();
}

// aten::new_ones(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_ones::call(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_ones_typed_handle();
    return op.call(self, size, dtype, layout, device, pin_memory);
}

// aten::new_ones(Tensor self, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor new_ones::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_new_ones_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized, name, "aten::_empty_affine_quantized")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_affine_quantized, schema_str, "_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor")

// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_empty_affine_quantized::schema> create__empty_affine_quantized_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_empty_affine_quantized::name, _empty_affine_quantized::overload_name)
      .typed<_empty_affine_quantized::schema>();
}

// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
at::Tensor _empty_affine_quantized::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create__empty_affine_quantized_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}

// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
at::Tensor _empty_affine_quantized::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create__empty_affine_quantized_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_per_channel_affine_quantized, name, "aten::_empty_per_channel_affine_quantized")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_per_channel_affine_quantized, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_empty_per_channel_affine_quantized, schema_str, "_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor")

// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_empty_per_channel_affine_quantized::schema> create__empty_per_channel_affine_quantized_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_empty_per_channel_affine_quantized::name, _empty_per_channel_affine_quantized::overload_name)
      .typed<_empty_per_channel_affine_quantized::schema>();
}

// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
at::Tensor _empty_per_channel_affine_quantized::call(at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create__empty_per_channel_affine_quantized_typed_handle();
    return op.call(size, scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}

// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
at::Tensor _empty_per_channel_affine_quantized::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create__empty_per_channel_affine_quantized_typed_handle();
    return op.redispatch(dispatchKeySet, size, scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_, name, "aten::resize_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_, schema_str, "resize_(Tensor(a!) self, int[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)")

// aten::resize_(Tensor(a!) self, int[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_::schema> create_resize__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_::name, resize_::overload_name)
      .typed<resize_::schema>();
}

// aten::resize_(Tensor(a!) self, int[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)
const at::Tensor & resize_::call(const at::Tensor & self, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_resize__typed_handle();
    return op.call(self, size, memory_format);
}

// aten::resize_(Tensor(a!) self, int[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)
const at::Tensor & resize_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_resize__typed_handle();
    return op.redispatch(dispatchKeySet, self, size, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_quantized, name, "aten::empty_quantized")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_quantized, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_quantized, schema_str, "empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_quantized::schema> create_empty_quantized_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_quantized::name, empty_quantized::overload_name)
      .typed<empty_quantized::schema>();
}

// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_quantized::call(at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_quantized_typed_handle();
    return op.call(size, qtensor, dtype, layout, device, pin_memory, memory_format);
}

// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_quantized::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_quantized_typed_handle();
    return op.redispatch(dispatchKeySet, size, qtensor, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_out, name, "aten::empty")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_out, schema_str, "empty.out(int[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)")

// aten::empty.out(int[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<empty_out::schema> create_empty_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_out::name, empty_out::overload_name)
      .typed<empty_out::schema>();
}

// aten::empty.out(int[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & empty_out::call(at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    static auto op = create_empty_out_typed_handle();
    return op.call(size, memory_format, out);
}

// aten::empty.out(int[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & empty_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::MemoryFormat> memory_format, at::Tensor & out) {
    static auto op = create_empty_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, memory_format, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like, name, "aten::empty_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_like, schema_str, "empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_like::schema> create_empty_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_like::name, empty_like::overload_name)
      .typed<empty_like::schema>();
}

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor empty_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_empty_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_strided, name, "aten::empty_strided")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_strided, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(empty_strided, schema_str, "empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<empty_strided::schema> create_empty_strided_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(empty_strided::name, empty_strided::overload_name)
      .typed<empty_strided::schema>();
}

// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor empty_strided::call(at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_empty_strided_typed_handle();
    return op.call(size, stride, dtype, layout, device, pin_memory);
}

// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor empty_strided::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_empty_strided_typed_handle();
    return op.redispatch(dispatchKeySet, size, stride, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf, name, "aten::erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf, schema_str, "erf(Tensor self) -> Tensor")

// aten::erf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<erf::schema> create_erf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erf::name, erf::overload_name)
      .typed<erf::schema>();
}

// aten::erf(Tensor self) -> Tensor
at::Tensor erf::call(const at::Tensor & self) {
    static auto op = create_erf_typed_handle();
    return op.call(self);
}

// aten::erf(Tensor self) -> Tensor
at::Tensor erf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_erf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_, name, "aten::erf_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_, schema_str, "erf_(Tensor(a!) self) -> Tensor(a!)")

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erf_::schema> create_erf__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erf_::name, erf_::overload_name)
      .typed<erf_::schema>();
}

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erf_::call(at::Tensor & self) {
    static auto op = create_erf__typed_handle();
    return op.call(self);
}

// aten::erf_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erf_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_erf__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_out, name, "aten::erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erf_out, schema_str, "erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erf_out::schema> create_erf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erf_out::name, erf_out::overload_name)
      .typed<erf_out::schema>();
}

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erf_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_erf_out_typed_handle();
    return op.call(self, out);
}

// aten::erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_erf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc, name, "aten::erfc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc, schema_str, "erfc(Tensor self) -> Tensor")

// aten::erfc(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<erfc::schema> create_erfc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erfc::name, erfc::overload_name)
      .typed<erfc::schema>();
}

// aten::erfc(Tensor self) -> Tensor
at::Tensor erfc::call(const at::Tensor & self) {
    static auto op = create_erfc_typed_handle();
    return op.call(self);
}

// aten::erfc(Tensor self) -> Tensor
at::Tensor erfc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_erfc_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc_, name, "aten::erfc_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc_, schema_str, "erfc_(Tensor(a!) self) -> Tensor(a!)")

// aten::erfc_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erfc_::schema> create_erfc__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erfc_::name, erfc_::overload_name)
      .typed<erfc_::schema>();
}

// aten::erfc_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erfc_::call(at::Tensor & self) {
    static auto op = create_erfc__typed_handle();
    return op.call(self);
}

// aten::erfc_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erfc_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_erfc__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc_out, name, "aten::erfc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfc_out, schema_str, "erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erfc_out::schema> create_erfc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erfc_out::name, erfc_out::overload_name)
      .typed<erfc_out::schema>();
}

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erfc_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_erfc_out_typed_handle();
    return op.call(self, out);
}

// aten::erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erfc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_erfc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp, name, "aten::exp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp, schema_str, "exp(Tensor self) -> Tensor")

// aten::exp(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<exp::schema> create_exp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exp::name, exp::overload_name)
      .typed<exp::schema>();
}

// aten::exp(Tensor self) -> Tensor
at::Tensor exp::call(const at::Tensor & self) {
    static auto op = create_exp_typed_handle();
    return op.call(self);
}

// aten::exp(Tensor self) -> Tensor
at::Tensor exp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_exp_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp_, name, "aten::exp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp_, schema_str, "exp_(Tensor(a!) self) -> Tensor(a!)")

// aten::exp_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<exp_::schema> create_exp__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exp_::name, exp_::overload_name)
      .typed<exp_::schema>();
}

// aten::exp_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & exp_::call(at::Tensor & self) {
    static auto op = create_exp__typed_handle();
    return op.call(self);
}

// aten::exp_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & exp_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_exp__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp_out, name, "aten::exp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp_out, schema_str, "exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<exp_out::schema> create_exp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exp_out::name, exp_out::overload_name)
      .typed<exp_out::schema>();
}

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & exp_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_exp_out_typed_handle();
    return op.call(self, out);
}

// aten::exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & exp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_exp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2, name, "aten::exp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2, schema_str, "exp2(Tensor self) -> Tensor")

// aten::exp2(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<exp2::schema> create_exp2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exp2::name, exp2::overload_name)
      .typed<exp2::schema>();
}

// aten::exp2(Tensor self) -> Tensor
at::Tensor exp2::call(const at::Tensor & self) {
    static auto op = create_exp2_typed_handle();
    return op.call(self);
}

// aten::exp2(Tensor self) -> Tensor
at::Tensor exp2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_exp2_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2_, name, "aten::exp2_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2_, schema_str, "exp2_(Tensor(a!) self) -> Tensor(a!)")

// aten::exp2_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<exp2_::schema> create_exp2__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exp2_::name, exp2_::overload_name)
      .typed<exp2_::schema>();
}

// aten::exp2_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & exp2_::call(at::Tensor & self) {
    static auto op = create_exp2__typed_handle();
    return op.call(self);
}

// aten::exp2_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & exp2_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_exp2__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2_out, name, "aten::exp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exp2_out, schema_str, "exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<exp2_out::schema> create_exp2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exp2_out::name, exp2_out::overload_name)
      .typed<exp2_out::schema>();
}

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & exp2_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_exp2_out_typed_handle();
    return op.call(self, out);
}

// aten::exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & exp2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_exp2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1, name, "aten::expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1, schema_str, "expm1(Tensor self) -> Tensor")

// aten::expm1(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<expm1::schema> create_expm1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(expm1::name, expm1::overload_name)
      .typed<expm1::schema>();
}

// aten::expm1(Tensor self) -> Tensor
at::Tensor expm1::call(const at::Tensor & self) {
    static auto op = create_expm1_typed_handle();
    return op.call(self);
}

// aten::expm1(Tensor self) -> Tensor
at::Tensor expm1::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_expm1_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1_, name, "aten::expm1_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1_, schema_str, "expm1_(Tensor(a!) self) -> Tensor(a!)")

// aten::expm1_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<expm1_::schema> create_expm1__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(expm1_::name, expm1_::overload_name)
      .typed<expm1_::schema>();
}

// aten::expm1_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & expm1_::call(at::Tensor & self) {
    static auto op = create_expm1__typed_handle();
    return op.call(self);
}

// aten::expm1_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & expm1_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_expm1__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1_out, name, "aten::expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expm1_out, schema_str, "expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<expm1_out::schema> create_expm1_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(expm1_out::name, expm1_out::overload_name)
      .typed<expm1_out::schema>();
}

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & expm1_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_expm1_out_typed_handle();
    return op.call(self, out);
}

// aten::expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & expm1_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_expm1_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand, name, "aten::expand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand, schema_str, "expand(Tensor(a) self, int[] size, *, bool implicit=False) -> Tensor(a)")

// aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<expand::schema> create_expand_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(expand::name, expand::overload_name)
      .typed<expand::schema>();
}

// aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> Tensor(a)
at::Tensor expand::call(const at::Tensor & self, at::IntArrayRef size, bool implicit) {
    static auto op = create_expand_typed_handle();
    return op.call(self, size, implicit);
}

// aten::expand(Tensor(a) self, int[] size, *, bool implicit=False) -> Tensor(a)
at::Tensor expand::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, bool implicit) {
    static auto op = create_expand_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, implicit);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand_as, name, "aten::expand_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(expand_as, schema_str, "expand_as(Tensor(a) self, Tensor other) -> Tensor(a)")

// aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<expand_as::schema> create_expand_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(expand_as::name, expand_as::overload_name)
      .typed<expand_as::schema>();
}

// aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor expand_as::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_expand_as_typed_handle();
    return op.call(self, other);
}

// aten::expand_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor expand_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_expand_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye, name, "aten::eye")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye, schema_str, "eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<eye::schema> create_eye_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eye::name, eye::overload_name)
      .typed<eye::schema>();
}

// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor eye::call(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_eye_typed_handle();
    return op.call(n, dtype, layout, device, pin_memory);
}

// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor eye::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_eye_typed_handle();
    return op.redispatch(dispatchKeySet, n, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_m, name, "aten::eye")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_m, overload_name, "m")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_m, schema_str, "eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<eye_m::schema> create_eye_m_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eye_m::name, eye_m::overload_name)
      .typed<eye_m::schema>();
}

// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor eye_m::call(int64_t n, int64_t m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_eye_m_typed_handle();
    return op.call(n, m, dtype, layout, device, pin_memory);
}

// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor eye_m::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, int64_t m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_eye_m_typed_handle();
    return op.redispatch(dispatchKeySet, n, m, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_out, name, "aten::eye")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_out, schema_str, "eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<eye_out::schema> create_eye_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eye_out::name, eye_out::overload_name)
      .typed<eye_out::schema>();
}

// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eye_out::call(int64_t n, at::Tensor & out) {
    static auto op = create_eye_out_typed_handle();
    return op.call(n, out);
}

// aten::eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eye_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, at::Tensor & out) {
    static auto op = create_eye_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_m_out, name, "aten::eye")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_m_out, overload_name, "m_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eye_m_out, schema_str, "eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)")

// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<eye_m_out::schema> create_eye_m_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eye_m_out::name, eye_m_out::overload_name)
      .typed<eye_m_out::schema>();
}

// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eye_m_out::call(int64_t n, int64_t m, at::Tensor & out) {
    static auto op = create_eye_m_out_typed_handle();
    return op.call(n, m, out);
}

// aten::eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eye_m_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, int64_t m, at::Tensor & out) {
    static auto op = create_eye_m_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, m, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_ints, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_ints, overload_name, "using_ints")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_ints, schema_str, "flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)")

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_using_ints::schema> create_flatten_using_ints_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_using_ints::name, flatten_using_ints::overload_name)
      .typed<flatten_using_ints::schema>();
}

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
at::Tensor flatten_using_ints::call(const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
    static auto op = create_flatten_using_ints_typed_handle();
    return op.call(self, start_dim, end_dim);
}

// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -> Tensor(a)
at::Tensor flatten_using_ints::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t start_dim, int64_t end_dim) {
    static auto op = create_flatten_using_ints_typed_handle();
    return op.redispatch(dispatchKeySet, self, start_dim, end_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_named_out_dim, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_named_out_dim, overload_name, "named_out_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_named_out_dim, schema_str, "flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)")

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_named_out_dim::schema> create_flatten_named_out_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_named_out_dim::name, flatten_named_out_dim::overload_name)
      .typed<flatten_named_out_dim::schema>();
}

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_named_out_dim::call(const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
    static auto op = create_flatten_named_out_dim_typed_handle();
    return op.call(self, start_dim, end_dim, out_dim);
}

// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_named_out_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t start_dim, int64_t end_dim, at::Dimname out_dim) {
    static auto op = create_flatten_named_out_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, start_dim, end_dim, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_names, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_names, overload_name, "using_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_using_names, schema_str, "flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)")

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_using_names::schema> create_flatten_using_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_using_names::name, flatten_using_names::overload_name)
      .typed<flatten_using_names::schema>();
}

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_using_names::call(const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
    static auto op = create_flatten_using_names_typed_handle();
    return op.call(self, start_dim, end_dim, out_dim);
}

// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_using_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) {
    static auto op = create_flatten_using_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, start_dim, end_dim, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_DimnameList, name, "aten::flatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_DimnameList, overload_name, "DimnameList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_DimnameList, schema_str, "flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)")

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<flatten_DimnameList::schema> create_flatten_DimnameList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_DimnameList::name, flatten_DimnameList::overload_name)
      .typed<flatten_DimnameList::schema>();
}

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_DimnameList::call(const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
    static auto op = create_flatten_DimnameList_typed_handle();
    return op.call(self, dims, out_dim);
}

// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -> Tensor(a)
at::Tensor flatten_DimnameList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dims, at::Dimname out_dim) {
    static auto op = create_flatten_DimnameList_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_int, name, "aten::unflatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_int, schema_str, "unflatten.int(Tensor(a) self, int dim, int[] sizes, Dimname[]? names=None) -> Tensor(a)")

// aten::unflatten.int(Tensor(a) self, int dim, int[] sizes, Dimname[]? names=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<unflatten_int::schema> create_unflatten_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unflatten_int::name, unflatten_int::overload_name)
      .typed<unflatten_int::schema>();
}

// aten::unflatten.int(Tensor(a) self, int dim, int[] sizes, Dimname[]? names=None) -> Tensor(a)
at::Tensor unflatten_int::call(const at::Tensor & self, int64_t dim, at::IntArrayRef sizes, c10::optional<at::DimnameList> names) {
    static auto op = create_unflatten_int_typed_handle();
    return op.call(self, dim, sizes, names);
}

// aten::unflatten.int(Tensor(a) self, int dim, int[] sizes, Dimname[]? names=None) -> Tensor(a)
at::Tensor unflatten_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::IntArrayRef sizes, c10::optional<at::DimnameList> names) {
    static auto op = create_unflatten_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, sizes, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_Dimname, name, "aten::unflatten")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_Dimname, schema_str, "unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)")

// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<unflatten_Dimname::schema> create_unflatten_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unflatten_Dimname::name, unflatten_Dimname::overload_name)
      .typed<unflatten_Dimname::schema>();
}

// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)
at::Tensor unflatten_Dimname::call(const at::Tensor & self, at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) {
    static auto op = create_unflatten_Dimname_typed_handle();
    return op.call(self, dim, sizes, names);
}

// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, int[] sizes, Dimname[] names) -> Tensor(a)
at::Tensor unflatten_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) {
    static auto op = create_unflatten_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, sizes, names);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill__Scalar, name, "aten::fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill__Scalar, schema_str, "fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)")

// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fill__Scalar::schema> create_fill__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fill__Scalar::name, fill__Scalar::overload_name)
      .typed<fill__Scalar::schema>();
}

// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
at::Tensor & fill__Scalar::call(at::Tensor & self, const at::Scalar & value) {
    static auto op = create_fill__Scalar_typed_handle();
    return op.call(self, value);
}

// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
at::Tensor & fill__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & value) {
    static auto op = create_fill__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill__Tensor, name, "aten::fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fill__Tensor, schema_str, "fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)")

// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fill__Tensor::schema> create_fill__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fill__Tensor::name, fill__Tensor::overload_name)
      .typed<fill__Tensor::schema>();
}

// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
at::Tensor & fill__Tensor::call(at::Tensor & self, const at::Tensor & value) {
    static auto op = create_fill__Tensor_typed_handle();
    return op.call(self, value);
}

// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
at::Tensor & fill__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & value) {
    static auto op = create_fill__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor, name, "aten::floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor, schema_str, "floor(Tensor self) -> Tensor")

// aten::floor(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<floor::schema> create_floor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor::name, floor::overload_name)
      .typed<floor::schema>();
}

// aten::floor(Tensor self) -> Tensor
at::Tensor floor::call(const at::Tensor & self) {
    static auto op = create_floor_typed_handle();
    return op.call(self);
}

// aten::floor(Tensor self) -> Tensor
at::Tensor floor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_floor_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_, name, "aten::floor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_, schema_str, "floor_(Tensor(a!) self) -> Tensor(a!)")

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_::schema> create_floor__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_::name, floor_::overload_name)
      .typed<floor_::schema>();
}

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & floor_::call(at::Tensor & self) {
    static auto op = create_floor__typed_handle();
    return op.call(self);
}

// aten::floor_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & floor_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_floor__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_out, name, "aten::floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_out, schema_str, "floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_out::schema> create_floor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_out::name, floor_out::overload_name)
      .typed<floor_out::schema>();
}

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & floor_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_floor_out_typed_handle();
    return op.call(self, out);
}

// aten::floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & floor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_floor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide, name, "aten::floor_divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide, schema_str, "floor_divide(Tensor self, Tensor other) -> Tensor")

// aten::floor_divide(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<floor_divide::schema> create_floor_divide_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_divide::name, floor_divide::overload_name)
      .typed<floor_divide::schema>();
}

// aten::floor_divide(Tensor self, Tensor other) -> Tensor
at::Tensor floor_divide::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_floor_divide_typed_handle();
    return op.call(self, other);
}

// aten::floor_divide(Tensor self, Tensor other) -> Tensor
at::Tensor floor_divide::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_floor_divide_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide__Tensor, name, "aten::floor_divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide__Tensor, schema_str, "floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_divide__Tensor::schema> create_floor_divide__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_divide__Tensor::name, floor_divide__Tensor::overload_name)
      .typed<floor_divide__Tensor::schema>();
}

// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & floor_divide__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_floor_divide__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & floor_divide__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_floor_divide__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide_out, name, "aten::floor_divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide_out, schema_str, "floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_divide_out::schema> create_floor_divide_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_divide_out::name, floor_divide_out::overload_name)
      .typed<floor_divide_out::schema>();
}

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & floor_divide_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_floor_divide_out_typed_handle();
    return op.call(self, other, out);
}

// aten::floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & floor_divide_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_floor_divide_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide_Scalar, name, "aten::floor_divide")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide_Scalar, schema_str, "floor_divide.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<floor_divide_Scalar::schema> create_floor_divide_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_divide_Scalar::name, floor_divide_Scalar::overload_name)
      .typed<floor_divide_Scalar::schema>();
}

// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor floor_divide_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_floor_divide_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor floor_divide_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_floor_divide_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide__Scalar, name, "aten::floor_divide_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(floor_divide__Scalar, schema_str, "floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<floor_divide__Scalar::schema> create_floor_divide__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(floor_divide__Scalar::name, floor_divide__Scalar::overload_name)
      .typed<floor_divide__Scalar::schema>();
}

// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & floor_divide__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_floor_divide__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & floor_divide__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_floor_divide__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac, name, "aten::frac")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac, schema_str, "frac(Tensor self) -> Tensor")

// aten::frac(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<frac::schema> create_frac_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frac::name, frac::overload_name)
      .typed<frac::schema>();
}

// aten::frac(Tensor self) -> Tensor
at::Tensor frac::call(const at::Tensor & self) {
    static auto op = create_frac_typed_handle();
    return op.call(self);
}

// aten::frac(Tensor self) -> Tensor
at::Tensor frac::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_frac_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac_, name, "aten::frac_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac_, schema_str, "frac_(Tensor(a!) self) -> Tensor(a!)")

// aten::frac_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<frac_::schema> create_frac__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frac_::name, frac_::overload_name)
      .typed<frac_::schema>();
}

// aten::frac_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & frac_::call(at::Tensor & self) {
    static auto op = create_frac__typed_handle();
    return op.call(self);
}

// aten::frac_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & frac_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_frac__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac_out, name, "aten::frac")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frac_out, schema_str, "frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<frac_out::schema> create_frac_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frac_out::name, frac_out::overload_name)
      .typed<frac_out::schema>();
}

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & frac_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_frac_out_typed_handle();
    return op.call(self, out);
}

// aten::frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & frac_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_frac_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_names, name, "aten::full")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_names, schema_str, "full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<full_names::schema> create_full_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(full_names::name, full_names::overload_name)
      .typed<full_names::schema>();
}

// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor full_names::call(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_full_names_typed_handle();
    return op.call(size, fill_value, names, dtype, layout, device, pin_memory);
}

// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor full_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_full_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, fill_value, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full, name, "aten::full")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full, schema_str, "full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<full::schema> create_full_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(full::name, full::overload_name)
      .typed<full::schema>();
}

// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor full::call(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_full_typed_handle();
    return op.call(size, fill_value, dtype, layout, device, pin_memory);
}

// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor full::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_full_typed_handle();
    return op.redispatch(dispatchKeySet, size, fill_value, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_out, name, "aten::full")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_out, schema_str, "full.out(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)")

// aten::full.out(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<full_out::schema> create_full_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(full_out::name, full_out::overload_name)
      .typed<full_out::schema>();
}

// aten::full.out(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & full_out::call(at::IntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
    static auto op = create_full_out_typed_handle();
    return op.call(size, fill_value, out);
}

// aten::full.out(int[] size, Scalar fill_value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & full_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, const at::Scalar & fill_value, at::Tensor & out) {
    static auto op = create_full_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, fill_value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_like, name, "aten::full_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(full_like, schema_str, "full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<full_like::schema> create_full_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(full_like::name, full_like::overload_name)
      .typed<full_like::schema>();
}

// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor full_like::call(const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_full_like_typed_handle();
    return op.call(self, fill_value, dtype, layout, device, pin_memory, memory_format);
}

// aten::full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor full_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_full_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, fill_value, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(from_file, name, "aten::from_file")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(from_file, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(from_file, schema_str, "from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<from_file::schema> create_from_file_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(from_file::name, from_file::overload_name)
      .typed<from_file::schema>();
}

// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor from_file::call(c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_from_file_typed_handle();
    return op.call(filename, shared, size, dtype, layout, device, pin_memory);
}

// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor from_file::redispatch(c10::DispatchKeySet dispatchKeySet, c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_from_file_typed_handle();
    return op.redispatch(dispatchKeySet, filename, shared, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd_out, name, "aten::gcd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd_out, schema_str, "gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gcd_out::schema> create_gcd_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gcd_out::name, gcd_out::overload_name)
      .typed<gcd_out::schema>();
}

// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gcd_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_gcd_out_typed_handle();
    return op.call(self, other, out);
}

// aten::gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gcd_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_gcd_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd, name, "aten::gcd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd, schema_str, "gcd(Tensor self, Tensor other) -> Tensor")

// aten::gcd(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gcd::schema> create_gcd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gcd::name, gcd::overload_name)
      .typed<gcd::schema>();
}

// aten::gcd(Tensor self, Tensor other) -> Tensor
at::Tensor gcd::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gcd_typed_handle();
    return op.call(self, other);
}

// aten::gcd(Tensor self, Tensor other) -> Tensor
at::Tensor gcd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gcd_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd_, name, "aten::gcd_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gcd_, schema_str, "gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gcd_::schema> create_gcd__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gcd_::name, gcd_::overload_name)
      .typed<gcd_::schema>();
}

// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & gcd_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gcd__typed_handle();
    return op.call(self, other);
}

// aten::gcd_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & gcd_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gcd__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm_out, name, "aten::lcm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm_out, schema_str, "lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lcm_out::schema> create_lcm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lcm_out::name, lcm_out::overload_name)
      .typed<lcm_out::schema>();
}

// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lcm_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_lcm_out_typed_handle();
    return op.call(self, other, out);
}

// aten::lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lcm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_lcm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm, name, "aten::lcm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm, schema_str, "lcm(Tensor self, Tensor other) -> Tensor")

// aten::lcm(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lcm::schema> create_lcm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lcm::name, lcm::overload_name)
      .typed<lcm::schema>();
}

// aten::lcm(Tensor self, Tensor other) -> Tensor
at::Tensor lcm::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lcm_typed_handle();
    return op.call(self, other);
}

// aten::lcm(Tensor self, Tensor other) -> Tensor
at::Tensor lcm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lcm_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm_, name, "aten::lcm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lcm_, schema_str, "lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lcm_::schema> create_lcm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lcm_::name, lcm_::overload_name)
      .typed<lcm_::schema>();
}

// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & lcm_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lcm__typed_handle();
    return op.call(self, other);
}

// aten::lcm_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & lcm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lcm__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler, name, "aten::grid_sampler")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler, schema_str, "grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler::schema> create_grid_sampler_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler::name, grid_sampler::overload_name)
      .typed<grid_sampler::schema>();
}

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_2d, name, "aten::grid_sampler_2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_2d, schema_str, "grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_2d::schema> create_grid_sampler_2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_2d::name, grid_sampler_2d::overload_name)
      .typed<grid_sampler_2d::schema>();
}

// aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler_2d::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_2d_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler_2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_2d_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_2d_backward, name, "aten::grid_sampler_2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_2d_backward, schema_str, "grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)")

// aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_2d_backward::schema> create_grid_sampler_2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_2d_backward::name, grid_sampler_2d_backward::overload_name)
      .typed<grid_sampler_2d_backward::schema>();
}

// aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> grid_sampler_2d_backward::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_2d_backward_typed_handle();
    return op.call(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> grid_sampler_2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback, name, "aten::_grid_sampler_2d_cpu_fallback")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback, schema_str, "_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_grid_sampler_2d_cpu_fallback::schema> create__grid_sampler_2d_cpu_fallback_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_grid_sampler_2d_cpu_fallback::name, _grid_sampler_2d_cpu_fallback::overload_name)
      .typed<_grid_sampler_2d_cpu_fallback::schema>();
}

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor _grid_sampler_2d_cpu_fallback::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create__grid_sampler_2d_cpu_fallback_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::_grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor _grid_sampler_2d_cpu_fallback::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create__grid_sampler_2d_cpu_fallback_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback_backward, name, "aten::_grid_sampler_2d_cpu_fallback_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_grid_sampler_2d_cpu_fallback_backward, schema_str, "_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)")

// aten::_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_grid_sampler_2d_cpu_fallback_backward::schema> create__grid_sampler_2d_cpu_fallback_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_grid_sampler_2d_cpu_fallback_backward::name, _grid_sampler_2d_cpu_fallback_backward::overload_name)
      .typed<_grid_sampler_2d_cpu_fallback_backward::schema>();
}

// aten::_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _grid_sampler_2d_cpu_fallback_backward::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create__grid_sampler_2d_cpu_fallback_backward_typed_handle();
    return op.call(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::_grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _grid_sampler_2d_cpu_fallback_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create__grid_sampler_2d_cpu_fallback_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d, name, "aten::grid_sampler_3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d, schema_str, "grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor")

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_3d::schema> create_grid_sampler_3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_3d::name, grid_sampler_3d::overload_name)
      .typed<grid_sampler_3d::schema>();
}

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler_3d::call(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_3d_typed_handle();
    return op.call(input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
at::Tensor grid_sampler_3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_3d_typed_handle();
    return op.redispatch(dispatchKeySet, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward, name, "aten::grid_sampler_3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(grid_sampler_3d_backward, schema_str, "grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)")

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<grid_sampler_3d_backward::schema> create_grid_sampler_3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(grid_sampler_3d_backward::name, grid_sampler_3d_backward::overload_name)
      .typed<grid_sampler_3d_backward::schema>();
}

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> grid_sampler_3d_backward::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_3d_backward_typed_handle();
    return op.call(grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

// aten::grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> grid_sampler_3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    static auto op = create_grid_sampler_3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, grid, interpolation_mode, padding_mode, align_corners);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window, name, "aten::hann_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window, schema_str, "hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hann_window::schema> create_hann_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hann_window::name, hann_window::overload_name)
      .typed<hann_window::schema>();
}

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hann_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hann_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic, name, "aten::hann_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hann_window_periodic, schema_str, "hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hann_window_periodic::schema> create_hann_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hann_window_periodic::name, hann_window_periodic::overload_name)
      .typed<hann_window_periodic::schema>();
}

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hann_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hann_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hann_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window, schema_str, "hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window::schema> create_hamming_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window::name, hamming_window::overload_name)
      .typed<hamming_window::schema>();
}

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic, schema_str, "hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic::schema> create_hamming_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic::name, hamming_window_periodic::overload_name)
      .typed<hamming_window_periodic::schema>();
}

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha, overload_name, "periodic_alpha")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha, schema_str, "hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_alpha::schema> create_hamming_window_periodic_alpha_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_alpha::name, hamming_window_periodic_alpha::overload_name)
      .typed<hamming_window_periodic_alpha::schema>();
}

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha::call(int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_periodic_alpha_typed_handle();
    return op.call(window_length, periodic, alpha, dtype, layout, device, pin_memory);
}

// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_periodic_alpha_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, alpha, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta, name, "aten::hamming_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta, overload_name, "periodic_alpha_beta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hamming_window_periodic_alpha_beta, schema_str, "hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hamming_window_periodic_alpha_beta::schema> create_hamming_window_periodic_alpha_beta_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hamming_window_periodic_alpha_beta::name, hamming_window_periodic_alpha_beta::overload_name)
      .typed<hamming_window_periodic_alpha_beta::schema>();
}

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha_beta::call(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_periodic_alpha_beta_typed_handle();
    return op.call(window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}

// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor hamming_window_periodic_alpha_beta::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_hamming_window_periodic_alpha_beta_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window, name, "aten::kaiser_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window, schema_str, "kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<kaiser_window::schema> create_kaiser_window_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kaiser_window::name, kaiser_window::overload_name)
      .typed<kaiser_window::schema>();
}

// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor kaiser_window::call(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_kaiser_window_typed_handle();
    return op.call(window_length, dtype, layout, device, pin_memory);
}

// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor kaiser_window::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_kaiser_window_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window_periodic, name, "aten::kaiser_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window_periodic, overload_name, "periodic")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window_periodic, schema_str, "kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<kaiser_window_periodic::schema> create_kaiser_window_periodic_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kaiser_window_periodic::name, kaiser_window_periodic::overload_name)
      .typed<kaiser_window_periodic::schema>();
}

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor kaiser_window_periodic::call(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_kaiser_window_periodic_typed_handle();
    return op.call(window_length, periodic, dtype, layout, device, pin_memory);
}

// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor kaiser_window_periodic::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_kaiser_window_periodic_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window_beta, name, "aten::kaiser_window")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window_beta, overload_name, "beta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kaiser_window_beta, schema_str, "kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<kaiser_window_beta::schema> create_kaiser_window_beta_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kaiser_window_beta::name, kaiser_window_beta::overload_name)
      .typed<kaiser_window_beta::schema>();
}

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor kaiser_window_beta::call(int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_kaiser_window_beta_typed_handle();
    return op.call(window_length, periodic, beta, dtype, layout, device, pin_memory);
}

// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor kaiser_window_beta::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_kaiser_window_beta_typed_handle();
    return op.redispatch(dispatchKeySet, window_length, periodic, beta, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hinge_embedding_loss, name, "aten::hinge_embedding_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hinge_embedding_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hinge_embedding_loss, schema_str, "hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor")

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hinge_embedding_loss::schema> create_hinge_embedding_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hinge_embedding_loss::name, hinge_embedding_loss::overload_name)
      .typed<hinge_embedding_loss::schema>();
}

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
at::Tensor hinge_embedding_loss::call(const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
    static auto op = create_hinge_embedding_loss_typed_handle();
    return op.call(self, target, margin, reduction);
}

// aten::hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -> Tensor
at::Tensor hinge_embedding_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, double margin, int64_t reduction) {
    static auto op = create_hinge_embedding_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, margin, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(group_norm, name, "aten::group_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(group_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(group_norm, schema_str, "group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor")

// aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<group_norm::schema> create_group_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(group_norm::name, group_norm::overload_name)
      .typed<group_norm::schema>();
}

// aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor
at::Tensor group_norm::call(const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
    static auto op = create_group_norm_typed_handle();
    return op.call(input, num_groups, weight, bias, eps, cudnn_enabled);
}

// aten::group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -> Tensor
at::Tensor group_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, int64_t num_groups, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enabled) {
    static auto op = create_group_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, num_groups, weight, bias, eps, cudnn_enabled);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm, name, "aten::native_group_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm, schema_str, "native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)")

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_group_norm::schema> create_native_group_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_group_norm::name, native_group_norm::overload_name)
      .typed<native_group_norm::schema>();
}

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps) {
    static auto op = create_native_group_norm_typed_handle();
    return op.call(input, weight, bias, N, C, HxW, group, eps);
}

// aten::native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps) {
    static auto op = create_native_group_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, N, C, HxW, group, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward, name, "aten::native_group_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_group_norm_backward, schema_str, "native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_group_norm_backward::schema> create_native_group_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_group_norm_backward::name, native_group_norm_backward::overload_name)
      .typed<native_group_norm_backward::schema>();
}

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm_backward::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, int64_t N, int64_t C, int64_t HxW, int64_t group, ::std::array<bool,3> output_mask) {
    static auto op = create_native_group_norm_backward_typed_handle();
    return op.call(grad_out, input, mean, rstd, weight, N, C, HxW, group, output_mask);
}

// aten::native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_group_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, int64_t N, int64_t C, int64_t HxW, int64_t group, ::std::array<bool,3> output_mask) {
    static auto op = create_native_group_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, rstd, weight, N, C, HxW, group, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c, name, "aten::_fft_r2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c, schema_str, "_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor")

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fft_r2c::schema> create__fft_r2c_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_r2c::name, _fft_r2c::overload_name)
      .typed<_fft_r2c::schema>();
}

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
at::Tensor _fft_r2c::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
    static auto op = create__fft_r2c_typed_handle();
    return op.call(self, dim, normalization, onesided);
}

// aten::_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
at::Tensor _fft_r2c::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided) {
    static auto op = create__fft_r2c_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, onesided);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c_out, name, "aten::_fft_r2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_r2c_out, schema_str, "_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fft_r2c_out::schema> create__fft_r2c_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_r2c_out::name, _fft_r2c_out::overload_name)
      .typed<_fft_r2c_out::schema>();
}

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_r2c_out::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
    static auto op = create__fft_r2c_out_typed_handle();
    return op.call(self, dim, normalization, onesided, out);
}

// aten::_fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_r2c_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool onesided, at::Tensor & out) {
    static auto op = create__fft_r2c_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, onesided, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2r, name, "aten::_fft_c2r")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2r, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2r, schema_str, "_fft_c2r(Tensor self, int[] dim, int normalization, int last_dim_size) -> Tensor")

// aten::_fft_c2r(Tensor self, int[] dim, int normalization, int last_dim_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fft_c2r::schema> create__fft_c2r_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_c2r::name, _fft_c2r::overload_name)
      .typed<_fft_c2r::schema>();
}

// aten::_fft_c2r(Tensor self, int[] dim, int normalization, int last_dim_size) -> Tensor
at::Tensor _fft_c2r::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
    static auto op = create__fft_c2r_typed_handle();
    return op.call(self, dim, normalization, last_dim_size);
}

// aten::_fft_c2r(Tensor self, int[] dim, int normalization, int last_dim_size) -> Tensor
at::Tensor _fft_c2r::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size) {
    static auto op = create__fft_c2r_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, last_dim_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2r_out, name, "aten::_fft_c2r")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2r_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2r_out, schema_str, "_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fft_c2r_out::schema> create__fft_c2r_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_c2r_out::name, _fft_c2r_out::overload_name)
      .typed<_fft_c2r_out::schema>();
}

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_c2r_out::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size, at::Tensor & out) {
    static auto op = create__fft_c2r_out_typed_handle();
    return op.call(self, dim, normalization, last_dim_size, out);
}

// aten::_fft_c2r.out(Tensor self, int[] dim, int normalization, int last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_c2r_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, int64_t last_dim_size, at::Tensor & out) {
    static auto op = create__fft_c2r_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, last_dim_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c, name, "aten::_fft_c2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c, schema_str, "_fft_c2c(Tensor self, int[] dim, int normalization, bool forward) -> Tensor")

// aten::_fft_c2c(Tensor self, int[] dim, int normalization, bool forward) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fft_c2c::schema> create__fft_c2c_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_c2c::name, _fft_c2c::overload_name)
      .typed<_fft_c2c::schema>();
}

// aten::_fft_c2c(Tensor self, int[] dim, int normalization, bool forward) -> Tensor
at::Tensor _fft_c2c::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward) {
    static auto op = create__fft_c2c_typed_handle();
    return op.call(self, dim, normalization, forward);
}

// aten::_fft_c2c(Tensor self, int[] dim, int normalization, bool forward) -> Tensor
at::Tensor _fft_c2c::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward) {
    static auto op = create__fft_c2c_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, forward);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c_out, name, "aten::_fft_c2c")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fft_c2c_out, schema_str, "_fft_c2c.out(Tensor self, int[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_fft_c2c.out(Tensor self, int[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_fft_c2c_out::schema> create__fft_c2c_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fft_c2c_out::name, _fft_c2c_out::overload_name)
      .typed<_fft_c2c_out::schema>();
}

// aten::_fft_c2c.out(Tensor self, int[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_c2c_out::call(const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
    static auto op = create__fft_c2c_out_typed_handle();
    return op.call(self, dim, normalization, forward, out);
}

// aten::_fft_c2c.out(Tensor self, int[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _fft_c2c_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, int64_t normalization, bool forward, at::Tensor & out) {
    static auto op = create__fft_c2c_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, normalization, forward, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_size, name, "aten::_cufft_get_plan_cache_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_size, schema_str, "_cufft_get_plan_cache_size(int device_index) -> int")

// aten::_cufft_get_plan_cache_size(int device_index) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_cufft_get_plan_cache_size::schema> create__cufft_get_plan_cache_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cufft_get_plan_cache_size::name, _cufft_get_plan_cache_size::overload_name)
      .typed<_cufft_get_plan_cache_size::schema>();
}

// aten::_cufft_get_plan_cache_size(int device_index) -> int
int64_t _cufft_get_plan_cache_size::call(int64_t device_index) {
    static auto op = create__cufft_get_plan_cache_size_typed_handle();
    return op.call(device_index);
}

// aten::_cufft_get_plan_cache_size(int device_index) -> int
int64_t _cufft_get_plan_cache_size::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t device_index) {
    static auto op = create__cufft_get_plan_cache_size_typed_handle();
    return op.redispatch(dispatchKeySet, device_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_max_size, name, "aten::_cufft_get_plan_cache_max_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_max_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_get_plan_cache_max_size, schema_str, "_cufft_get_plan_cache_max_size(int device_index) -> int")

// aten::_cufft_get_plan_cache_max_size(int device_index) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_cufft_get_plan_cache_max_size::schema> create__cufft_get_plan_cache_max_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cufft_get_plan_cache_max_size::name, _cufft_get_plan_cache_max_size::overload_name)
      .typed<_cufft_get_plan_cache_max_size::schema>();
}

// aten::_cufft_get_plan_cache_max_size(int device_index) -> int
int64_t _cufft_get_plan_cache_max_size::call(int64_t device_index) {
    static auto op = create__cufft_get_plan_cache_max_size_typed_handle();
    return op.call(device_index);
}

// aten::_cufft_get_plan_cache_max_size(int device_index) -> int
int64_t _cufft_get_plan_cache_max_size::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t device_index) {
    static auto op = create__cufft_get_plan_cache_max_size_typed_handle();
    return op.redispatch(dispatchKeySet, device_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_set_plan_cache_max_size, name, "aten::_cufft_set_plan_cache_max_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_set_plan_cache_max_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_set_plan_cache_max_size, schema_str, "_cufft_set_plan_cache_max_size(int device_index, int max_size) -> ()")

// aten::_cufft_set_plan_cache_max_size(int device_index, int max_size) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_cufft_set_plan_cache_max_size::schema> create__cufft_set_plan_cache_max_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cufft_set_plan_cache_max_size::name, _cufft_set_plan_cache_max_size::overload_name)
      .typed<_cufft_set_plan_cache_max_size::schema>();
}

// aten::_cufft_set_plan_cache_max_size(int device_index, int max_size) -> ()
void _cufft_set_plan_cache_max_size::call(int64_t device_index, int64_t max_size) {
    static auto op = create__cufft_set_plan_cache_max_size_typed_handle();
    return op.call(device_index, max_size);
}

// aten::_cufft_set_plan_cache_max_size(int device_index, int max_size) -> ()
void _cufft_set_plan_cache_max_size::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t device_index, int64_t max_size) {
    static auto op = create__cufft_set_plan_cache_max_size_typed_handle();
    return op.redispatch(dispatchKeySet, device_index, max_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_clear_plan_cache, name, "aten::_cufft_clear_plan_cache")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_clear_plan_cache, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cufft_clear_plan_cache, schema_str, "_cufft_clear_plan_cache(int device_index) -> ()")

// aten::_cufft_clear_plan_cache(int device_index) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_cufft_clear_plan_cache::schema> create__cufft_clear_plan_cache_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cufft_clear_plan_cache::name, _cufft_clear_plan_cache::overload_name)
      .typed<_cufft_clear_plan_cache::schema>();
}

// aten::_cufft_clear_plan_cache(int device_index) -> ()
void _cufft_clear_plan_cache::call(int64_t device_index) {
    static auto op = create__cufft_clear_plan_cache_typed_handle();
    return op.call(device_index);
}

// aten::_cufft_clear_plan_cache(int device_index) -> ()
void _cufft_clear_plan_cache::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t device_index) {
    static auto op = create__cufft_clear_plan_cache_typed_handle();
    return op.redispatch(dispatchKeySet, device_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor, name, "aten::index")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_Tensor, schema_str, "index.Tensor(Tensor self, Tensor?[] indices) -> Tensor")

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_Tensor::schema> create_index_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_Tensor::name, index_Tensor::overload_name)
      .typed<index_Tensor::schema>();
}

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
at::Tensor index_Tensor::call(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    static auto op = create_index_Tensor_typed_handle();
    return op.call(self, indices);
}

// aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
at::Tensor index_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices) {
    static auto op = create_index_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy_, name, "aten::index_copy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy_, schema_str, "index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)")

// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_copy_::schema> create_index_copy__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_copy_::name, index_copy_::overload_name)
      .typed<index_copy_::schema>();
}

// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & index_copy_::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy__typed_handle();
    return op.call(self, dim, index, source);
}

// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & index_copy_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy, name, "aten::index_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy, schema_str, "index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor")

// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_copy::schema> create_index_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_copy::name, index_copy::overload_name)
      .typed<index_copy::schema>();
}

// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
at::Tensor index_copy::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy_typed_handle();
    return op.call(self, dim, index, source);
}

// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
at::Tensor index_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy__dimname, name, "aten::index_copy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy__dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy__dimname, schema_str, "index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)")

// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_copy__dimname::schema> create_index_copy__dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_copy__dimname::name, index_copy__dimname::overload_name)
      .typed<index_copy__dimname::schema>();
}

// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & index_copy__dimname::call(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy__dimname_typed_handle();
    return op.call(self, dim, index, source);
}

// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & index_copy__dimname::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy__dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy_dimname, name, "aten::index_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_copy_dimname, schema_str, "index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor")

// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_copy_dimname::schema> create_index_copy_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_copy_dimname::name, index_copy_dimname::overload_name)
      .typed<index_copy_dimname::schema>();
}

// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor
at::Tensor index_copy_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy_dimname_typed_handle();
    return op.call(self, dim, index, source);
}

// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -> Tensor
at::Tensor index_copy_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_copy_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_put_, name, "aten::index_put_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_put_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_put_, schema_str, "index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)")

// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_put_::schema> create_index_put__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_put_::name, index_put_::overload_name)
      .typed<index_put_::schema>();
}

// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)
at::Tensor & index_put_::call(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    static auto op = create_index_put__typed_handle();
    return op.call(self, indices, values, accumulate);
}

// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)
at::Tensor & index_put_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    static auto op = create_index_put__typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, values, accumulate);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_put, name, "aten::index_put")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_put, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_put, schema_str, "index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor")

// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_put::schema> create_index_put_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_put::name, index_put::overload_name)
      .typed<index_put::schema>();
}

// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor
at::Tensor index_put::call(const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    static auto op = create_index_put_typed_handle();
    return op.call(self, indices, values, accumulate);
}

// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor
at::Tensor index_put::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    static auto op = create_index_put_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, values, accumulate);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_index_put_impl_, name, "aten::_index_put_impl_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_index_put_impl_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_index_put_impl_, schema_str, "_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)")

// aten::_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_index_put_impl_::schema> create__index_put_impl__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_index_put_impl_::name, _index_put_impl_::overload_name)
      .typed<_index_put_impl_::schema>();
}

// aten::_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)
at::Tensor & _index_put_impl_::call(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
    static auto op = create__index_put_impl__typed_handle();
    return op.call(self, indices, values, accumulate, unsafe);
}

// aten::_index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)
at::Tensor & _index_put_impl_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate, bool unsafe) {
    static auto op = create__index_put_impl__typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, values, accumulate, unsafe);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(instance_norm, name, "aten::instance_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(instance_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(instance_norm, schema_str, "instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor")

// aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<instance_norm::schema> create_instance_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(instance_norm::name, instance_norm::overload_name)
      .typed<instance_norm::schema>();
}

// aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor
at::Tensor instance_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
    static auto op = create_instance_norm_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}

// aten::instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -> Tensor
at::Tensor instance_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) {
    static auto op = create_instance_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse, name, "aten::inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse, schema_str, "inverse(Tensor self) -> Tensor")

// aten::inverse(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<inverse::schema> create_inverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inverse::name, inverse::overload_name)
      .typed<inverse::schema>();
}

// aten::inverse(Tensor self) -> Tensor
at::Tensor inverse::call(const at::Tensor & self) {
    static auto op = create_inverse_typed_handle();
    return op.call(self);
}

// aten::inverse(Tensor self) -> Tensor
at::Tensor inverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_inverse_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse_out, name, "aten::inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inverse_out, schema_str, "inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<inverse_out::schema> create_inverse_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inverse_out::name, inverse_out::overload_name)
      .typed<inverse_out::schema>();
}

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inverse_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_inverse_out_typed_handle();
    return op.call(self, out);
}

// aten::inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inverse_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_inverse_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_inverse_helper, name, "aten::_inverse_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_inverse_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_inverse_helper, schema_str, "_inverse_helper(Tensor self) -> Tensor")

// aten::_inverse_helper(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_inverse_helper::schema> create__inverse_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_inverse_helper::name, _inverse_helper::overload_name)
      .typed<_inverse_helper::schema>();
}

// aten::_inverse_helper(Tensor self) -> Tensor
at::Tensor _inverse_helper::call(const at::Tensor & self) {
    static auto op = create__inverse_helper_typed_handle();
    return op.call(self);
}

// aten::_inverse_helper(Tensor self) -> Tensor
at::Tensor _inverse_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__inverse_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isclose, name, "aten::isclose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isclose, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isclose, schema_str, "isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor")

// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isclose::schema> create_isclose_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isclose::name, isclose::overload_name)
      .typed<isclose::schema>();
}

// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
at::Tensor isclose::call(const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    static auto op = create_isclose_typed_handle();
    return op.call(self, other, rtol, atol, equal_nan);
}

// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
at::Tensor isclose::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, double rtol, double atol, bool equal_nan) {
    static auto op = create_isclose_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, rtol, atol, equal_nan);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Tensor_out, name, "aten::isin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Tensor_out, overload_name, "Tensor_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Tensor_out, schema_str, "isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)")

// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isin_Tensor_Tensor_out::schema> create_isin_Tensor_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isin_Tensor_Tensor_out::name, isin_Tensor_Tensor_out::overload_name)
      .typed<isin_Tensor_Tensor_out::schema>();
}

// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isin_Tensor_Tensor_out::call(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
    static auto op = create_isin_Tensor_Tensor_out_typed_handle();
    return op.call(elements, test_elements, assume_unique, invert, out);
}

// aten::isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isin_Tensor_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
    static auto op = create_isin_Tensor_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, elements, test_elements, assume_unique, invert, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Tensor, name, "aten::isin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Tensor, overload_name, "Tensor_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Tensor, schema_str, "isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor")

// aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isin_Tensor_Tensor::schema> create_isin_Tensor_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isin_Tensor_Tensor::name, isin_Tensor_Tensor::overload_name)
      .typed<isin_Tensor_Tensor::schema>();
}

// aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
at::Tensor isin_Tensor_Tensor::call(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
    static auto op = create_isin_Tensor_Tensor_typed_handle();
    return op.call(elements, test_elements, assume_unique, invert);
}

// aten::isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
at::Tensor isin_Tensor_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
    static auto op = create_isin_Tensor_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, elements, test_elements, assume_unique, invert);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Scalar_out, name, "aten::isin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Scalar_out, overload_name, "Tensor_Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Scalar_out, schema_str, "isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)")

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isin_Tensor_Scalar_out::schema> create_isin_Tensor_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isin_Tensor_Scalar_out::name, isin_Tensor_Scalar_out::overload_name)
      .typed<isin_Tensor_Scalar_out::schema>();
}

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isin_Tensor_Scalar_out::call(const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert, at::Tensor & out) {
    static auto op = create_isin_Tensor_Scalar_out_typed_handle();
    return op.call(elements, test_element, assume_unique, invert, out);
}

// aten::isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isin_Tensor_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert, at::Tensor & out) {
    static auto op = create_isin_Tensor_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, elements, test_element, assume_unique, invert, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Scalar, name, "aten::isin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Tensor_Scalar, schema_str, "isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor")

// aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isin_Tensor_Scalar::schema> create_isin_Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isin_Tensor_Scalar::name, isin_Tensor_Scalar::overload_name)
      .typed<isin_Tensor_Scalar::schema>();
}

// aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor
at::Tensor isin_Tensor_Scalar::call(const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert) {
    static auto op = create_isin_Tensor_Scalar_typed_handle();
    return op.call(elements, test_element, assume_unique, invert);
}

// aten::isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -> Tensor
at::Tensor isin_Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert) {
    static auto op = create_isin_Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, elements, test_element, assume_unique, invert);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Scalar_Tensor_out, name, "aten::isin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Scalar_Tensor_out, overload_name, "Scalar_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Scalar_Tensor_out, schema_str, "isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)")

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isin_Scalar_Tensor_out::schema> create_isin_Scalar_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isin_Scalar_Tensor_out::name, isin_Scalar_Tensor_out::overload_name)
      .typed<isin_Scalar_Tensor_out::schema>();
}

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isin_Scalar_Tensor_out::call(const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
    static auto op = create_isin_Scalar_Tensor_out_typed_handle();
    return op.call(element, test_elements, assume_unique, invert, out);
}

// aten::isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isin_Scalar_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert, at::Tensor & out) {
    static auto op = create_isin_Scalar_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, element, test_elements, assume_unique, invert, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Scalar_Tensor, name, "aten::isin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Scalar_Tensor, overload_name, "Scalar_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isin_Scalar_Tensor, schema_str, "isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor")

// aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isin_Scalar_Tensor::schema> create_isin_Scalar_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isin_Scalar_Tensor::name, isin_Scalar_Tensor::overload_name)
      .typed<isin_Scalar_Tensor::schema>();
}

// aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
at::Tensor isin_Scalar_Tensor::call(const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert) {
    static auto op = create_isin_Scalar_Tensor_typed_handle();
    return op.call(element, test_elements, assume_unique, invert);
}

// aten::isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
at::Tensor isin_Scalar_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert) {
    static auto op = create_isin_Scalar_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, element, test_elements, assume_unique, invert);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan, name, "aten::isnan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isnan, schema_str, "isnan(Tensor self) -> Tensor")

// aten::isnan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isnan::schema> create_isnan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isnan::name, isnan::overload_name)
      .typed<isnan::schema>();
}

// aten::isnan(Tensor self) -> Tensor
at::Tensor isnan::call(const at::Tensor & self) {
    static auto op = create_isnan_typed_handle();
    return op.call(self);
}

// aten::isnan(Tensor self) -> Tensor
at::Tensor isnan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_isnan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_distributed, name, "aten::is_distributed")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_distributed, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_distributed, schema_str, "is_distributed(Tensor self) -> bool")

// aten::is_distributed(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_distributed::schema> create_is_distributed_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_distributed::name, is_distributed::overload_name)
      .typed<is_distributed::schema>();
}

// aten::is_distributed(Tensor self) -> bool
bool is_distributed::call(const at::Tensor & self) {
    static auto op = create_is_distributed_typed_handle();
    return op.call(self);
}

// aten::is_distributed(Tensor self) -> bool
bool is_distributed::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_distributed_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_floating_point, name, "aten::is_floating_point")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_floating_point, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_floating_point, schema_str, "is_floating_point(Tensor self) -> bool")

// aten::is_floating_point(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_floating_point::schema> create_is_floating_point_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_floating_point::name, is_floating_point::overload_name)
      .typed<is_floating_point::schema>();
}

// aten::is_floating_point(Tensor self) -> bool
bool is_floating_point::call(const at::Tensor & self) {
    static auto op = create_is_floating_point_typed_handle();
    return op.call(self);
}

// aten::is_floating_point(Tensor self) -> bool
bool is_floating_point::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_floating_point_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_complex, name, "aten::is_complex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_complex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_complex, schema_str, "is_complex(Tensor self) -> bool")

// aten::is_complex(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_complex::schema> create_is_complex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_complex::name, is_complex::overload_name)
      .typed<is_complex::schema>();
}

// aten::is_complex(Tensor self) -> bool
bool is_complex::call(const at::Tensor & self) {
    static auto op = create_is_complex_typed_handle();
    return op.call(self);
}

// aten::is_complex(Tensor self) -> bool
bool is_complex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_complex_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_conj, name, "aten::is_conj")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_conj, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_conj, schema_str, "is_conj(Tensor self) -> bool")

// aten::is_conj(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_conj::schema> create_is_conj_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_conj::name, is_conj::overload_name)
      .typed<is_conj::schema>();
}

// aten::is_conj(Tensor self) -> bool
bool is_conj::call(const at::Tensor & self) {
    static auto op = create_is_conj_typed_handle();
    return op.call(self);
}

// aten::is_conj(Tensor self) -> bool
bool is_conj::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_conj_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_neg, name, "aten::is_neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_neg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_neg, schema_str, "is_neg(Tensor self) -> bool")

// aten::is_neg(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_neg::schema> create_is_neg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_neg::name, is_neg::overload_name)
      .typed<is_neg::schema>();
}

// aten::is_neg(Tensor self) -> bool
bool is_neg::call(const at::Tensor & self) {
    static auto op = create_is_neg_typed_handle();
    return op.call(self);
}

// aten::is_neg(Tensor self) -> bool
bool is_neg::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_neg_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isreal, name, "aten::isreal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isreal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isreal, schema_str, "isreal(Tensor self) -> Tensor")

// aten::isreal(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isreal::schema> create_isreal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isreal::name, isreal::overload_name)
      .typed<isreal::schema>();
}

// aten::isreal(Tensor self) -> Tensor
at::Tensor isreal::call(const at::Tensor & self) {
    static auto op = create_isreal_typed_handle();
    return op.call(self);
}

// aten::isreal(Tensor self) -> Tensor
at::Tensor isreal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_isreal_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_nonzero, name, "aten::is_nonzero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_nonzero, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_nonzero, schema_str, "is_nonzero(Tensor self) -> bool")

// aten::is_nonzero(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_nonzero::schema> create_is_nonzero_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_nonzero::name, is_nonzero::overload_name)
      .typed<is_nonzero::schema>();
}

// aten::is_nonzero(Tensor self) -> bool
bool is_nonzero::call(const at::Tensor & self) {
    static auto op = create_is_nonzero_typed_handle();
    return op.call(self);
}

// aten::is_nonzero(Tensor self) -> bool
bool is_nonzero::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_nonzero_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_same_size, name, "aten::is_same_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_same_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_same_size, schema_str, "is_same_size(Tensor self, Tensor other) -> bool")

// aten::is_same_size(Tensor self, Tensor other) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_same_size::schema> create_is_same_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_same_size::name, is_same_size::overload_name)
      .typed<is_same_size::schema>();
}

// aten::is_same_size(Tensor self, Tensor other) -> bool
bool is_same_size::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_is_same_size_typed_handle();
    return op.call(self, other);
}

// aten::is_same_size(Tensor self, Tensor other) -> bool
bool is_same_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_is_same_size_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_signed, name, "aten::is_signed")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_signed, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_signed, schema_str, "is_signed(Tensor self) -> bool")

// aten::is_signed(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_signed::schema> create_is_signed_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_signed::name, is_signed::overload_name)
      .typed<is_signed::schema>();
}

// aten::is_signed(Tensor self) -> bool
bool is_signed::call(const at::Tensor & self) {
    static auto op = create_is_signed_typed_handle();
    return op.call(self);
}

// aten::is_signed(Tensor self) -> bool
bool is_signed::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_signed_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_inference, name, "aten::is_inference")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_inference, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_inference, schema_str, "is_inference(Tensor self) -> bool")

// aten::is_inference(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_inference::schema> create_is_inference_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_inference::name, is_inference::overload_name)
      .typed<is_inference::schema>();
}

// aten::is_inference(Tensor self) -> bool
bool is_inference::call(const at::Tensor & self) {
    static auto op = create_is_inference_typed_handle();
    return op.call(self);
}

// aten::is_inference(Tensor self) -> bool
bool is_inference::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_inference_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kl_div, name, "aten::kl_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kl_div, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kl_div, schema_str, "kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor")

// aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<kl_div::schema> create_kl_div_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kl_div::name, kl_div::overload_name)
      .typed<kl_div::schema>();
}

// aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
at::Tensor kl_div::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    static auto op = create_kl_div_typed_handle();
    return op.call(self, target, reduction, log_target);
}

// aten::kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
at::Tensor kl_div::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    static auto op = create_kl_div_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, log_target);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kl_div_backward, name, "aten::kl_div_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kl_div_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kl_div_backward, schema_str, "kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor")

// aten::kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<kl_div_backward::schema> create_kl_div_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kl_div_backward::name, kl_div_backward::overload_name)
      .typed<kl_div_backward::schema>();
}

// aten::kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
at::Tensor kl_div_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    static auto op = create_kl_div_backward_typed_handle();
    return op.call(grad_output, self, target, reduction, log_target);
}

// aten::kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
at::Tensor kl_div_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    static auto op = create_kl_div_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, log_target);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kron, name, "aten::kron")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kron, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kron, schema_str, "kron(Tensor self, Tensor other) -> Tensor")

// aten::kron(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<kron::schema> create_kron_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kron::name, kron::overload_name)
      .typed<kron::schema>();
}

// aten::kron(Tensor self, Tensor other) -> Tensor
at::Tensor kron::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_kron_typed_handle();
    return op.call(self, other);
}

// aten::kron(Tensor self, Tensor other) -> Tensor
at::Tensor kron::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_kron_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kron_out, name, "aten::kron")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kron_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kron_out, schema_str, "kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<kron_out::schema> create_kron_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kron_out::name, kron_out::overload_name)
      .typed<kron_out::schema>();
}

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & kron_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_kron_out_typed_handle();
    return op.call(self, other, out);
}

// aten::kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & kron_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_kron_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue, schema_str, "kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue::schema> create_kthvalue_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue::name, kthvalue::overload_name)
      .typed<kthvalue::schema>();
}

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue::call(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
    static auto op = create_kthvalue_typed_handle();
    return op.call(self, k, dim, keepdim);
}

// aten::kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
    static auto op = create_kthvalue_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_values, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_values, overload_name, "values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_values, schema_str, "kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue_values::schema> create_kthvalue_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue_values::name, kthvalue_values::overload_name)
      .typed<kthvalue_values::schema>();
}

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_values::call(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_kthvalue_values_typed_handle();
    return op.call(self, k, dim, keepdim, values, indices);
}

// aten::kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_kthvalue_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname, schema_str, "kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue_dimname::schema> create_kthvalue_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue_dimname::name, kthvalue_dimname::overload_name)
      .typed<kthvalue_dimname::schema>();
}

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue_dimname::call(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
    static auto op = create_kthvalue_dimname_typed_handle();
    return op.call(self, k, dim, keepdim);
}

// aten::kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> kthvalue_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim) {
    static auto op = create_kthvalue_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname_out, name, "aten::kthvalue")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(kthvalue_dimname_out, schema_str, "kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<kthvalue_dimname_out::schema> create_kthvalue_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(kthvalue_dimname_out::name, kthvalue_dimname_out::overload_name)
      .typed<kthvalue_dimname_out::schema>();
}

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_dimname_out::call(const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_kthvalue_dimname_out_typed_handle();
    return op.call(self, k, dim, keepdim, values, indices);
}

// aten::kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> kthvalue_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_kthvalue_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(layer_norm, name, "aten::layer_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(layer_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(layer_norm, schema_str, "layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor")

// aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<layer_norm::schema> create_layer_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(layer_norm::name, layer_norm::overload_name)
      .typed<layer_norm::schema>();
}

// aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
at::Tensor layer_norm::call(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
    static auto op = create_layer_norm_typed_handle();
    return op.call(input, normalized_shape, weight, bias, eps, cudnn_enable);
}

// aten::layer_norm(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -> Tensor
at::Tensor layer_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps, bool cudnn_enable) {
    static auto op = create_layer_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, normalized_shape, weight, bias, eps, cudnn_enable);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm, name, "aten::native_layer_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm, schema_str, "native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)")

// aten::native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_layer_norm::schema> create_native_layer_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_layer_norm::name, native_layer_norm::overload_name)
      .typed<native_layer_norm::schema>();
}

// aten::native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm::call(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    static auto op = create_native_layer_norm_typed_handle();
    return op.call(input, normalized_shape, weight, bias, eps);
}

// aten::native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
    static auto op = create_native_layer_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, normalized_shape, weight, bias, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm_backward, name, "aten::native_layer_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_layer_norm_backward, schema_str, "native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_layer_norm_backward::schema> create_native_layer_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_layer_norm_backward::name, native_layer_norm_backward::overload_name)
      .typed<native_layer_norm_backward::schema>();
}

// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm_backward::call(const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
    static auto op = create_native_layer_norm_backward_typed_handle();
    return op.call(grad_out, input, normalized_shape, mean, rstd, weight, bias, output_mask);
}

// aten::native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_layer_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
    static auto op = create_native_layer_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, normalized_shape, mean, rstd, weight, bias, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num, name, "aten::nan_to_num")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num, schema_str, "nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor")

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nan_to_num::schema> create_nan_to_num_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nan_to_num::name, nan_to_num::overload_name)
      .typed<nan_to_num::schema>();
}

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
at::Tensor nan_to_num::call(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    static auto op = create_nan_to_num_typed_handle();
    return op.call(self, nan, posinf, neginf);
}

// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
at::Tensor nan_to_num::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    static auto op = create_nan_to_num_typed_handle();
    return op.redispatch(dispatchKeySet, self, nan, posinf, neginf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_, name, "aten::nan_to_num_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_, schema_str, "nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)")

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nan_to_num_::schema> create_nan_to_num__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nan_to_num_::name, nan_to_num_::overload_name)
      .typed<nan_to_num_::schema>();
}

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
at::Tensor & nan_to_num_::call(at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    static auto op = create_nan_to_num__typed_handle();
    return op.call(self, nan, posinf, neginf);
}

// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
at::Tensor & nan_to_num_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    static auto op = create_nan_to_num__typed_handle();
    return op.redispatch(dispatchKeySet, self, nan, posinf, neginf);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_out, name, "aten::nan_to_num")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nan_to_num_out, schema_str, "nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nan_to_num_out::schema> create_nan_to_num_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nan_to_num_out::name, nan_to_num_out::overload_name)
      .typed<nan_to_num_out::schema>();
}

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nan_to_num_out::call(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
    static auto op = create_nan_to_num_out_typed_handle();
    return op.call(self, nan, posinf, neginf, out);
}

// aten::nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nan_to_num_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf, at::Tensor & out) {
    static auto op = create_nan_to_num_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, nan, posinf, neginf, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear, name, "aten::linear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear, schema_str, "linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor")

// aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linear::schema> create_linear_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linear::name, linear::overload_name)
      .typed<linear::schema>();
}

// aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
at::Tensor linear::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    static auto op = create_linear_typed_handle();
    return op.call(input, weight, bias);
}

// aten::linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
at::Tensor linear::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    static auto op = create_linear_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_out, name, "aten::linear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linear_out, schema_str, "linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linear_out::schema> create_linear_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linear_out::name, linear_out::overload_name)
      .typed<linear_out::schema>();
}

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linear_out::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
    static auto op = create_linear_out_typed_handle();
    return op.call(input, weight, bias, out);
}

// aten::linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linear_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::Tensor & out) {
    static auto op = create_linear_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear, name, "aten::mkldnn_linear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear, schema_str, "mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor")

// aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear::schema> create_mkldnn_linear_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear::name, mkldnn_linear::overload_name)
      .typed<mkldnn_linear::schema>();
}

// aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor
at::Tensor mkldnn_linear::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    static auto op = create_mkldnn_linear_typed_handle();
    return op.call(self, weight, bias);
}

// aten::mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -> Tensor
at::Tensor mkldnn_linear::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias) {
    static auto op = create_mkldnn_linear_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input, name, "aten::mkldnn_linear_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_input, schema_str, "mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor")

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward_input::schema> create_mkldnn_linear_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward_input::name, mkldnn_linear_backward_input::overload_name)
      .typed<mkldnn_linear_backward_input::schema>();
}

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
at::Tensor mkldnn_linear_backward_input::call(at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
    static auto op = create_mkldnn_linear_backward_input_typed_handle();
    return op.call(input_size, grad_output, weight);
}

// aten::mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -> Tensor
at::Tensor mkldnn_linear_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef input_size, const at::Tensor & grad_output, const at::Tensor & weight) {
    static auto op = create_mkldnn_linear_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, input_size, grad_output, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_weights, name, "aten::mkldnn_linear_backward_weights")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_weights, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward_weights, schema_str, "mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)")

// aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward_weights::schema> create_mkldnn_linear_backward_weights_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward_weights::name, mkldnn_linear_backward_weights::overload_name)
      .typed<mkldnn_linear_backward_weights::schema>();
}

// aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> mkldnn_linear_backward_weights::call(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined) {
    static auto op = create_mkldnn_linear_backward_weights_typed_handle();
    return op.call(grad_output, input, weight, bias_defined);
}

// aten::mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> mkldnn_linear_backward_weights::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, bool bias_defined) {
    static auto op = create_mkldnn_linear_backward_weights_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input, weight, bias_defined);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward, name, "aten::mkldnn_linear_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_linear_backward, schema_str, "mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_linear_backward::schema> create_mkldnn_linear_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_linear_backward::name, mkldnn_linear_backward::overload_name)
      .typed<mkldnn_linear_backward::schema>();
}

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_linear_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    static auto op = create_mkldnn_linear_backward_typed_handle();
    return op.call(self, grad_output, weight, output_mask);
}

// aten::mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_linear_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, ::std::array<bool,3> output_mask) {
    static auto op = create_mkldnn_linear_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight_fp32_activation, name, "aten::fbgemm_linear_int8_weight_fp32_activation")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight_fp32_activation, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight_fp32_activation, schema_str, "fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor")

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_int8_weight_fp32_activation::schema> create_fbgemm_linear_int8_weight_fp32_activation_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_int8_weight_fp32_activation::name, fbgemm_linear_int8_weight_fp32_activation::overload_name)
      .typed<fbgemm_linear_int8_weight_fp32_activation::schema>();
}

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight_fp32_activation::call(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_int8_weight_fp32_activation_typed_handle();
    return op.call(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

// aten::fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight_fp32_activation::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_int8_weight_fp32_activation_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight, name, "aten::fbgemm_linear_int8_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_int8_weight, schema_str, "fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor")

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_int8_weight::schema> create_fbgemm_linear_int8_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_int8_weight::name, fbgemm_linear_int8_weight::overload_name)
      .typed<fbgemm_linear_int8_weight::schema>();
}

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight::call(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_int8_weight_typed_handle();
    return op.call(input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

// aten::fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_int8_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const at::Tensor & packed, const at::Tensor & col_offsets, const at::Scalar & weight_scale, const at::Scalar & weight_zero_point, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_int8_weight_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_quantize_weight, name, "aten::fbgemm_linear_quantize_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_quantize_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_quantize_weight, schema_str, "fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)")

// aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_quantize_weight::schema> create_fbgemm_linear_quantize_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_quantize_weight::name, fbgemm_linear_quantize_weight::overload_name)
      .typed<fbgemm_linear_quantize_weight::schema>();
}

// aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)
::std::tuple<at::Tensor,at::Tensor,double,int64_t> fbgemm_linear_quantize_weight::call(const at::Tensor & input) {
    static auto op = create_fbgemm_linear_quantize_weight_typed_handle();
    return op.call(input);
}

// aten::fbgemm_linear_quantize_weight(Tensor input) -> (Tensor, Tensor, float, int)
::std::tuple<at::Tensor,at::Tensor,double,int64_t> fbgemm_linear_quantize_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
    static auto op = create_fbgemm_linear_quantize_weight_typed_handle();
    return op.redispatch(dispatchKeySet, input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_gemm_matrix_fp16, name, "aten::fbgemm_pack_gemm_matrix_fp16")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_gemm_matrix_fp16, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_gemm_matrix_fp16, schema_str, "fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor")

// aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_pack_gemm_matrix_fp16::schema> create_fbgemm_pack_gemm_matrix_fp16_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_pack_gemm_matrix_fp16::name, fbgemm_pack_gemm_matrix_fp16::overload_name)
      .typed<fbgemm_pack_gemm_matrix_fp16::schema>();
}

// aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor
at::Tensor fbgemm_pack_gemm_matrix_fp16::call(const at::Tensor & input) {
    static auto op = create_fbgemm_pack_gemm_matrix_fp16_typed_handle();
    return op.call(input);
}

// aten::fbgemm_pack_gemm_matrix_fp16(Tensor input) -> Tensor
at::Tensor fbgemm_pack_gemm_matrix_fp16::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
    static auto op = create_fbgemm_pack_gemm_matrix_fp16_typed_handle();
    return op.redispatch(dispatchKeySet, input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight_fp32_activation, name, "aten::fbgemm_linear_fp16_weight_fp32_activation")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight_fp32_activation, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight_fp32_activation, schema_str, "fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor")

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_fp16_weight_fp32_activation::schema> create_fbgemm_linear_fp16_weight_fp32_activation_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_fp16_weight_fp32_activation::name, fbgemm_linear_fp16_weight_fp32_activation::overload_name)
      .typed<fbgemm_linear_fp16_weight_fp32_activation::schema>();
}

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_fp16_weight_fp32_activation::call(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_fp16_weight_fp32_activation_typed_handle();
    return op.call(input, packed_weight, bias);
}

// aten::fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_fp16_weight_fp32_activation::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_fp16_weight_fp32_activation_typed_handle();
    return op.redispatch(dispatchKeySet, input, packed_weight, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight, name, "aten::fbgemm_linear_fp16_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_linear_fp16_weight, schema_str, "fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor")

// aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_linear_fp16_weight::schema> create_fbgemm_linear_fp16_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_linear_fp16_weight::name, fbgemm_linear_fp16_weight::overload_name)
      .typed<fbgemm_linear_fp16_weight::schema>();
}

// aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_fp16_weight::call(const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_fp16_weight_typed_handle();
    return op.call(input, packed_weight, bias);
}

// aten::fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -> Tensor
at::Tensor fbgemm_linear_fp16_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & packed_weight, const at::Tensor & bias) {
    static auto op = create_fbgemm_linear_fp16_weight_typed_handle();
    return op.redispatch(dispatchKeySet, input, packed_weight, bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_quantized_matrix, name, "aten::fbgemm_pack_quantized_matrix")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_quantized_matrix, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_quantized_matrix, schema_str, "fbgemm_pack_quantized_matrix(Tensor input) -> Tensor")

// aten::fbgemm_pack_quantized_matrix(Tensor input) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_pack_quantized_matrix::schema> create_fbgemm_pack_quantized_matrix_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_pack_quantized_matrix::name, fbgemm_pack_quantized_matrix::overload_name)
      .typed<fbgemm_pack_quantized_matrix::schema>();
}

// aten::fbgemm_pack_quantized_matrix(Tensor input) -> Tensor
at::Tensor fbgemm_pack_quantized_matrix::call(const at::Tensor & input) {
    static auto op = create_fbgemm_pack_quantized_matrix_typed_handle();
    return op.call(input);
}

// aten::fbgemm_pack_quantized_matrix(Tensor input) -> Tensor
at::Tensor fbgemm_pack_quantized_matrix::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
    static auto op = create_fbgemm_pack_quantized_matrix_typed_handle();
    return op.redispatch(dispatchKeySet, input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_quantized_matrix_KN, name, "aten::fbgemm_pack_quantized_matrix")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_quantized_matrix_KN, overload_name, "KN")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fbgemm_pack_quantized_matrix_KN, schema_str, "fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor")

// aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fbgemm_pack_quantized_matrix_KN::schema> create_fbgemm_pack_quantized_matrix_KN_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fbgemm_pack_quantized_matrix_KN::name, fbgemm_pack_quantized_matrix_KN::overload_name)
      .typed<fbgemm_pack_quantized_matrix_KN::schema>();
}

// aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor
at::Tensor fbgemm_pack_quantized_matrix_KN::call(const at::Tensor & input, int64_t K, int64_t N) {
    static auto op = create_fbgemm_pack_quantized_matrix_KN_typed_handle();
    return op.call(input, K, N);
}

// aten::fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -> Tensor
at::Tensor fbgemm_pack_quantized_matrix_KN::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, int64_t K, int64_t N) {
    static auto op = create_fbgemm_pack_quantized_matrix_KN_typed_handle();
    return op.redispatch(dispatchKeySet, input, K, N);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_Tensor, name, "aten::ldexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_Tensor, schema_str, "ldexp.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ldexp_Tensor::schema> create_ldexp_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ldexp_Tensor::name, ldexp_Tensor::overload_name)
      .typed<ldexp_Tensor::schema>();
}

// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ldexp_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ldexp_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ldexp.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ldexp_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ldexp_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_, name, "aten::ldexp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_, schema_str, "ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ldexp_::schema> create_ldexp__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ldexp_::name, ldexp_::overload_name)
      .typed<ldexp_::schema>();
}

// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ldexp_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ldexp__typed_handle();
    return op.call(self, other);
}

// aten::ldexp_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ldexp_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ldexp__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_out, name, "aten::ldexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ldexp_out, schema_str, "ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ldexp_out::schema> create_ldexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ldexp_out::name, ldexp_out::overload_name)
      .typed<ldexp_out::schema>();
}

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ldexp_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_ldexp_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ldexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_ldexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linspace, name, "aten::linspace")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linspace, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linspace, schema_str, "linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linspace::schema> create_linspace_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linspace::name, linspace::overload_name)
      .typed<linspace::schema>();
}

// aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor linspace::call(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_linspace_typed_handle();
    return op.call(start, end, steps, dtype, layout, device, pin_memory);
}

// aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor linspace::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_linspace_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, steps, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linspace_out, name, "aten::linspace")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linspace_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linspace_out, schema_str, "linspace.out(Scalar start, Scalar end, int? steps=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linspace.out(Scalar start, Scalar end, int? steps=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linspace_out::schema> create_linspace_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linspace_out::name, linspace_out::overload_name)
      .typed<linspace_out::schema>();
}

// aten::linspace.out(Scalar start, Scalar end, int? steps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linspace_out::call(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, at::Tensor & out) {
    static auto op = create_linspace_out_typed_handle();
    return op.call(start, end, steps, out);
}

// aten::linspace.out(Scalar start, Scalar end, int? steps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linspace_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, at::Tensor & out) {
    static auto op = create_linspace_out_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, steps, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log, name, "aten::log")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log, schema_str, "log(Tensor self) -> Tensor")

// aten::log(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log::schema> create_log_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log::name, log::overload_name)
      .typed<log::schema>();
}

// aten::log(Tensor self) -> Tensor
at::Tensor log::call(const at::Tensor & self) {
    static auto op = create_log_typed_handle();
    return op.call(self);
}

// aten::log(Tensor self) -> Tensor
at::Tensor log::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_log_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_, name, "aten::log_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_, schema_str, "log_(Tensor(a!) self) -> Tensor(a!)")

// aten::log_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_::schema> create_log__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_::name, log_::overload_name)
      .typed<log_::schema>();
}

// aten::log_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log_::call(at::Tensor & self) {
    static auto op = create_log__typed_handle();
    return op.call(self);
}

// aten::log_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_log__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_out, name, "aten::log")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_out, schema_str, "log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_out::schema> create_log_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_out::name, log_out::overload_name)
      .typed<log_out::schema>();
}

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log_out_typed_handle();
    return op.call(self, out);
}

// aten::log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10, name, "aten::log10")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10, schema_str, "log10(Tensor self) -> Tensor")

// aten::log10(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log10::schema> create_log10_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log10::name, log10::overload_name)
      .typed<log10::schema>();
}

// aten::log10(Tensor self) -> Tensor
at::Tensor log10::call(const at::Tensor & self) {
    static auto op = create_log10_typed_handle();
    return op.call(self);
}

// aten::log10(Tensor self) -> Tensor
at::Tensor log10::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_log10_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10_, name, "aten::log10_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10_, schema_str, "log10_(Tensor(a!) self) -> Tensor(a!)")

// aten::log10_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log10_::schema> create_log10__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log10_::name, log10_::overload_name)
      .typed<log10_::schema>();
}

// aten::log10_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log10_::call(at::Tensor & self) {
    static auto op = create_log10__typed_handle();
    return op.call(self);
}

// aten::log10_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log10_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_log10__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10_out, name, "aten::log10")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log10_out, schema_str, "log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log10_out::schema> create_log10_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log10_out::name, log10_out::overload_name)
      .typed<log10_out::schema>();
}

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log10_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log10_out_typed_handle();
    return op.call(self, out);
}

// aten::log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log10_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log10_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p, name, "aten::log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p, schema_str, "log1p(Tensor self) -> Tensor")

// aten::log1p(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log1p::schema> create_log1p_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log1p::name, log1p::overload_name)
      .typed<log1p::schema>();
}

// aten::log1p(Tensor self) -> Tensor
at::Tensor log1p::call(const at::Tensor & self) {
    static auto op = create_log1p_typed_handle();
    return op.call(self);
}

// aten::log1p(Tensor self) -> Tensor
at::Tensor log1p::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_log1p_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p_, name, "aten::log1p_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p_, schema_str, "log1p_(Tensor(a!) self) -> Tensor(a!)")

// aten::log1p_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log1p_::schema> create_log1p__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log1p_::name, log1p_::overload_name)
      .typed<log1p_::schema>();
}

// aten::log1p_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log1p_::call(at::Tensor & self) {
    static auto op = create_log1p__typed_handle();
    return op.call(self);
}

// aten::log1p_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log1p_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_log1p__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p_out, name, "aten::log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log1p_out, schema_str, "log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log1p_out::schema> create_log1p_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log1p_out::name, log1p_out::overload_name)
      .typed<log1p_out::schema>();
}

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log1p_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log1p_out_typed_handle();
    return op.call(self, out);
}

// aten::log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log1p_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log1p_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2, name, "aten::log2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2, schema_str, "log2(Tensor self) -> Tensor")

// aten::log2(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log2::schema> create_log2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log2::name, log2::overload_name)
      .typed<log2::schema>();
}

// aten::log2(Tensor self) -> Tensor
at::Tensor log2::call(const at::Tensor & self) {
    static auto op = create_log2_typed_handle();
    return op.call(self);
}

// aten::log2(Tensor self) -> Tensor
at::Tensor log2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_log2_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2_, name, "aten::log2_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2_, schema_str, "log2_(Tensor(a!) self) -> Tensor(a!)")

// aten::log2_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log2_::schema> create_log2__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log2_::name, log2_::overload_name)
      .typed<log2_::schema>();
}

// aten::log2_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log2_::call(at::Tensor & self) {
    static auto op = create_log2__typed_handle();
    return op.call(self);
}

// aten::log2_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & log2_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_log2__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2_out, name, "aten::log2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log2_out, schema_str, "log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log2_out::schema> create_log2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log2_out::name, log2_out::overload_name)
      .typed<log2_out::schema>();
}

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log2_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log2_out_typed_handle();
    return op.call(self, out);
}

// aten::log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp_out, name, "aten::logaddexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp_out, schema_str, "logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logaddexp_out::schema> create_logaddexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logaddexp_out::name, logaddexp_out::overload_name)
      .typed<logaddexp_out::schema>();
}

// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logaddexp_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logaddexp_out_typed_handle();
    return op.call(self, other, out);
}

// aten::logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logaddexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logaddexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp, name, "aten::logaddexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp, schema_str, "logaddexp(Tensor self, Tensor other) -> Tensor")

// aten::logaddexp(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logaddexp::schema> create_logaddexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logaddexp::name, logaddexp::overload_name)
      .typed<logaddexp::schema>();
}

// aten::logaddexp(Tensor self, Tensor other) -> Tensor
at::Tensor logaddexp::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logaddexp_typed_handle();
    return op.call(self, other);
}

// aten::logaddexp(Tensor self, Tensor other) -> Tensor
at::Tensor logaddexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logaddexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp2_out, name, "aten::logaddexp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp2_out, schema_str, "logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logaddexp2_out::schema> create_logaddexp2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logaddexp2_out::name, logaddexp2_out::overload_name)
      .typed<logaddexp2_out::schema>();
}

// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logaddexp2_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logaddexp2_out_typed_handle();
    return op.call(self, other, out);
}

// aten::logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logaddexp2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_logaddexp2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp2, name, "aten::logaddexp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logaddexp2, schema_str, "logaddexp2(Tensor self, Tensor other) -> Tensor")

// aten::logaddexp2(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logaddexp2::schema> create_logaddexp2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logaddexp2::name, logaddexp2::overload_name)
      .typed<logaddexp2::schema>();
}

// aten::logaddexp2(Tensor self, Tensor other) -> Tensor
at::Tensor logaddexp2::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logaddexp2_typed_handle();
    return op.call(self, other);
}

// aten::logaddexp2(Tensor self, Tensor other) -> Tensor
at::Tensor logaddexp2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_logaddexp2_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Tensor, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Tensor, schema_str, "xlogy.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_Tensor::schema> create_xlogy_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_Tensor::name, xlogy_Tensor::overload_name)
      .typed<xlogy_Tensor::schema>();
}

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor xlogy_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_xlogy_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::xlogy.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor xlogy_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_xlogy_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Self, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Self, overload_name, "Scalar_Self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Self, schema_str, "xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor")

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_Scalar_Self::schema> create_xlogy_Scalar_Self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_Scalar_Self::name, xlogy_Scalar_Self::overload_name)
      .typed<xlogy_Scalar_Self::schema>();
}

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
at::Tensor xlogy_Scalar_Self::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_xlogy_Scalar_Self_typed_handle();
    return op.call(self, other);
}

// aten::xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
at::Tensor xlogy_Scalar_Self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_xlogy_Scalar_Self_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Other, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Other, overload_name, "Scalar_Other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_Scalar_Other, schema_str, "xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor")

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_Scalar_Other::schema> create_xlogy_Scalar_Other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_Scalar_Other::name, xlogy_Scalar_Other::overload_name)
      .typed<xlogy_Scalar_Other::schema>();
}

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
at::Tensor xlogy_Scalar_Other::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_xlogy_Scalar_Other_typed_handle();
    return op.call(self, other);
}

// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
at::Tensor xlogy_Scalar_Other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_xlogy_Scalar_Other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Tensor, name, "aten::xlogy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Tensor, schema_str, "xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy__Tensor::schema> create_xlogy__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy__Tensor::name, xlogy__Tensor::overload_name)
      .typed<xlogy__Tensor::schema>();
}

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & xlogy__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_xlogy__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & xlogy__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_xlogy__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Scalar_Other, name, "aten::xlogy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Scalar_Other, overload_name, "Scalar_Other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy__Scalar_Other, schema_str, "xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy__Scalar_Other::schema> create_xlogy__Scalar_Other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy__Scalar_Other::name, xlogy__Scalar_Other::overload_name)
      .typed<xlogy__Scalar_Other::schema>();
}

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & xlogy__Scalar_Other::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_xlogy__Scalar_Other_typed_handle();
    return op.call(self, other);
}

// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & xlogy__Scalar_Other::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_xlogy__Scalar_Other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutTensor, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutTensor, overload_name, "OutTensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutTensor, schema_str, "xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_OutTensor::schema> create_xlogy_OutTensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_OutTensor::name, xlogy_OutTensor::overload_name)
      .typed<xlogy_OutTensor::schema>();
}

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutTensor::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_xlogy_OutTensor_typed_handle();
    return op.call(self, other, out);
}

// aten::xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutTensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_xlogy_OutTensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Self, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Self, overload_name, "OutScalar_Self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Self, schema_str, "xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_OutScalar_Self::schema> create_xlogy_OutScalar_Self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_OutScalar_Self::name, xlogy_OutScalar_Self::overload_name)
      .typed<xlogy_OutScalar_Self::schema>();
}

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Self::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_xlogy_OutScalar_Self_typed_handle();
    return op.call(self, other, out);
}

// aten::xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_xlogy_OutScalar_Self_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Other, name, "aten::xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Other, overload_name, "OutScalar_Other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(xlogy_OutScalar_Other, schema_str, "xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<xlogy_OutScalar_Other::schema> create_xlogy_OutScalar_Other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(xlogy_OutScalar_Other::name, xlogy_OutScalar_Other::overload_name)
      .typed<xlogy_OutScalar_Other::schema>();
}

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Other::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_xlogy_OutScalar_Other_typed_handle();
    return op.call(self, other, out);
}

// aten::xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & xlogy_OutScalar_Other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_xlogy_OutScalar_Other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logdet, name, "aten::logdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logdet, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logdet, schema_str, "logdet(Tensor self) -> Tensor")

// aten::logdet(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logdet::schema> create_logdet_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logdet::name, logdet::overload_name)
      .typed<logdet::schema>();
}

// aten::logdet(Tensor self) -> Tensor
at::Tensor logdet::call(const at::Tensor & self) {
    static auto op = create_logdet_typed_handle();
    return op.call(self);
}

// aten::logdet(Tensor self) -> Tensor
at::Tensor logdet::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_logdet_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logspace, name, "aten::logspace")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logspace, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logspace, schema_str, "logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logspace::schema> create_logspace_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logspace::name, logspace::overload_name)
      .typed<logspace::schema>();
}

// aten::logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor logspace::call(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_logspace_typed_handle();
    return op.call(start, end, steps, base, dtype, layout, device, pin_memory);
}

// aten::logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor logspace::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_logspace_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, steps, base, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logspace_out, name, "aten::logspace")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logspace_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logspace_out, schema_str, "logspace.out(Scalar start, Scalar end, int? steps=None, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logspace.out(Scalar start, Scalar end, int? steps=None, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logspace_out::schema> create_logspace_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logspace_out::name, logspace_out::overload_name)
      .typed<logspace_out::schema>();
}

// aten::logspace.out(Scalar start, Scalar end, int? steps=None, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logspace_out::call(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, at::Tensor & out) {
    static auto op = create_logspace_out_typed_handle();
    return op.call(start, end, steps, base, out);
}

// aten::logspace.out(Scalar start, Scalar end, int? steps=None, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logspace_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, at::Tensor & out) {
    static auto op = create_logspace_out_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, steps, base, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_softmax_int, name, "aten::log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_softmax_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_softmax_int, schema_str, "log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_softmax_int::schema> create_log_softmax_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_softmax_int::name, log_softmax_int::overload_name)
      .typed<log_softmax_int::schema>();
}

// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor log_softmax_int::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_log_softmax_int_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor log_softmax_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_log_softmax_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_softmax_Dimname, name, "aten::log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_softmax_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_softmax_Dimname, schema_str, "log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_softmax_Dimname::schema> create_log_softmax_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_softmax_Dimname::name, log_softmax_Dimname::overload_name)
      .typed<log_softmax_Dimname::schema>();
}

// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor log_softmax_Dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_log_softmax_Dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor log_softmax_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_log_softmax_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax, name, "aten::_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax, schema_str, "_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")

// aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_log_softmax::schema> create__log_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_log_softmax::name, _log_softmax::overload_name)
      .typed<_log_softmax::schema>();
}

// aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _log_softmax::call(const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__log_softmax_typed_handle();
    return op.call(self, dim, half_to_float);
}

// aten::_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _log_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__log_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_out, name, "aten::_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_out, schema_str, "_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_log_softmax_out::schema> create__log_softmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_log_softmax_out::name, _log_softmax_out::overload_name)
      .typed<_log_softmax_out::schema>();
}

// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _log_softmax_out::call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    static auto op = create__log_softmax_out_typed_handle();
    return op.call(self, dim, half_to_float, out);
}

// aten::_log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _log_softmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    static auto op = create__log_softmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data, name, "aten::_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data, schema_str, "_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor")

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_log_softmax_backward_data::schema> create__log_softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_log_softmax_backward_data::name, _log_softmax_backward_data::overload_name)
      .typed<_log_softmax_backward_data::schema>();
}

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _log_softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__log_softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, self);
}

// aten::_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _log_softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__log_softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data_out, name, "aten::_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_log_softmax_backward_data_out, schema_str, "_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_log_softmax_backward_data_out::schema> create__log_softmax_backward_data_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_log_softmax_backward_data_out::name, _log_softmax_backward_data_out::overload_name)
      .typed<_log_softmax_backward_data_out::schema>();
}

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _log_softmax_backward_data_out::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
    static auto op = create__log_softmax_backward_data_out_typed_handle();
    return op.call(grad_output, output, dim, self, out);
}

// aten::_log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _log_softmax_backward_data_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & out) {
    static auto op = create__log_softmax_backward_data_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp, name, "aten::_logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp, schema_str, "_logcumsumexp(Tensor self, int dim) -> Tensor")

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_logcumsumexp::schema> create__logcumsumexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_logcumsumexp::name, _logcumsumexp::overload_name)
      .typed<_logcumsumexp::schema>();
}

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor _logcumsumexp::call(const at::Tensor & self, int64_t dim) {
    static auto op = create__logcumsumexp_typed_handle();
    return op.call(self, dim);
}

// aten::_logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor _logcumsumexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create__logcumsumexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp_out, name, "aten::_logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_logcumsumexp_out, schema_str, "_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_logcumsumexp_out::schema> create__logcumsumexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_logcumsumexp_out::name, _logcumsumexp_out::overload_name)
      .typed<_logcumsumexp_out::schema>();
}

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _logcumsumexp_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    static auto op = create__logcumsumexp_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::_logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _logcumsumexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    static auto op = create__logcumsumexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp, schema_str, "logcumsumexp(Tensor self, int dim) -> Tensor")

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp::schema> create_logcumsumexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp::name, logcumsumexp::overload_name)
      .typed<logcumsumexp::schema>();
}

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor logcumsumexp::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_logcumsumexp_typed_handle();
    return op.call(self, dim);
}

// aten::logcumsumexp(Tensor self, int dim) -> Tensor
at::Tensor logcumsumexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_logcumsumexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_out, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_out, schema_str, "logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp_out::schema> create_logcumsumexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp_out::name, logcumsumexp_out::overload_name)
      .typed<logcumsumexp_out::schema>();
}

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    static auto op = create_logcumsumexp_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    static auto op = create_logcumsumexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname, schema_str, "logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor")

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp_dimname::schema> create_logcumsumexp_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp_dimname::name, logcumsumexp_dimname::overload_name)
      .typed<logcumsumexp_dimname::schema>();
}

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
at::Tensor logcumsumexp_dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_logcumsumexp_dimname_typed_handle();
    return op.call(self, dim);
}

// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -> Tensor
at::Tensor logcumsumexp_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_logcumsumexp_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname_out, name, "aten::logcumsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logcumsumexp_dimname_out, schema_str, "logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logcumsumexp_dimname_out::schema> create_logcumsumexp_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logcumsumexp_dimname_out::name, logcumsumexp_dimname_out::overload_name)
      .typed<logcumsumexp_dimname_out::schema>();
}

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_dimname_out::call(const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
    static auto op = create_logcumsumexp_dimname_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logcumsumexp_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, at::Tensor & out) {
    static auto op = create_logcumsumexp_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp, name, "aten::logsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp, schema_str, "logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor")

// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logsumexp::schema> create_logsumexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logsumexp::name, logsumexp::overload_name)
      .typed<logsumexp::schema>();
}

// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor logsumexp::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_logsumexp_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor logsumexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_logsumexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_out, name, "aten::logsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_out, schema_str, "logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logsumexp_out::schema> create_logsumexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logsumexp_out::name, logsumexp_out::overload_name)
      .typed<logsumexp_out::schema>();
}

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logsumexp_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_logsumexp_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logsumexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_logsumexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_names, name, "aten::logsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_names, schema_str, "logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor")

// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logsumexp_names::schema> create_logsumexp_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logsumexp_names::name, logsumexp_names::overload_name)
      .typed<logsumexp_names::schema>();
}

// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
at::Tensor logsumexp_names::call(const at::Tensor & self, at::DimnameList dim, bool keepdim) {
    static auto op = create_logsumexp_names_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
at::Tensor logsumexp_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim) {
    static auto op = create_logsumexp_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_names_out, name, "aten::logsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logsumexp_names_out, schema_str, "logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logsumexp_names_out::schema> create_logsumexp_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logsumexp_names_out::name, logsumexp_names_out::overload_name)
      .typed<logsumexp_names_out::schema>();
}

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logsumexp_names_out::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    static auto op = create_logsumexp_names_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logsumexp_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    static auto op = create_logsumexp_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(margin_ranking_loss, name, "aten::margin_ranking_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(margin_ranking_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(margin_ranking_loss, schema_str, "margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor")

// aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<margin_ranking_loss::schema> create_margin_ranking_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(margin_ranking_loss::name, margin_ranking_loss::overload_name)
      .typed<margin_ranking_loss::schema>();
}

// aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
at::Tensor margin_ranking_loss::call(const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    static auto op = create_margin_ranking_loss_typed_handle();
    return op.call(input1, input2, target, margin, reduction);
}

// aten::margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -> Tensor
at::Tensor margin_ranking_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input1, const at::Tensor & input2, const at::Tensor & target, double margin, int64_t reduction) {
    static auto op = create_margin_ranking_loss_typed_handle();
    return op.redispatch(dispatchKeySet, input1, input2, target, margin, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matmul, name, "aten::matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matmul, schema_str, "matmul(Tensor self, Tensor other) -> Tensor")

// aten::matmul(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matmul::schema> create_matmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matmul::name, matmul::overload_name)
      .typed<matmul::schema>();
}

// aten::matmul(Tensor self, Tensor other) -> Tensor
at::Tensor matmul::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_matmul_typed_handle();
    return op.call(self, other);
}

// aten::matmul(Tensor self, Tensor other) -> Tensor
at::Tensor matmul::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_matmul_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matmul_out, name, "aten::matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matmul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matmul_out, schema_str, "matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<matmul_out::schema> create_matmul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matmul_out::name, matmul_out::overload_name)
      .typed<matmul_out::schema>();
}

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & matmul_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_matmul_out_typed_handle();
    return op.call(self, other, out);
}

// aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & matmul_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_matmul_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_rank_tol, name, "aten::matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_rank_tol, overload_name, "tol")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_rank_tol, schema_str, "matrix_rank.tol(Tensor self, float tol, bool symmetric=False) -> Tensor")

// aten::matrix_rank.tol(Tensor self, float tol, bool symmetric=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matrix_rank_tol::schema> create_matrix_rank_tol_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_rank_tol::name, matrix_rank_tol::overload_name)
      .typed<matrix_rank_tol::schema>();
}

// aten::matrix_rank.tol(Tensor self, float tol, bool symmetric=False) -> Tensor
at::Tensor matrix_rank_tol::call(const at::Tensor & self, double tol, bool symmetric) {
    static auto op = create_matrix_rank_tol_typed_handle();
    return op.call(self, tol, symmetric);
}

// aten::matrix_rank.tol(Tensor self, float tol, bool symmetric=False) -> Tensor
at::Tensor matrix_rank_tol::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double tol, bool symmetric) {
    static auto op = create_matrix_rank_tol_typed_handle();
    return op.redispatch(dispatchKeySet, self, tol, symmetric);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_rank, name, "aten::matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_rank, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_rank, schema_str, "matrix_rank(Tensor self, bool symmetric=False) -> Tensor")

// aten::matrix_rank(Tensor self, bool symmetric=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matrix_rank::schema> create_matrix_rank_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_rank::name, matrix_rank::overload_name)
      .typed<matrix_rank::schema>();
}

// aten::matrix_rank(Tensor self, bool symmetric=False) -> Tensor
at::Tensor matrix_rank::call(const at::Tensor & self, bool symmetric) {
    static auto op = create_matrix_rank_typed_handle();
    return op.call(self, symmetric);
}

// aten::matrix_rank(Tensor self, bool symmetric=False) -> Tensor
at::Tensor matrix_rank::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool symmetric) {
    static auto op = create_matrix_rank_typed_handle();
    return op.redispatch(dispatchKeySet, self, symmetric);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_power, name, "aten::matrix_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_power, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_power, schema_str, "matrix_power(Tensor self, int n) -> Tensor")

// aten::matrix_power(Tensor self, int n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matrix_power::schema> create_matrix_power_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_power::name, matrix_power::overload_name)
      .typed<matrix_power::schema>();
}

// aten::matrix_power(Tensor self, int n) -> Tensor
at::Tensor matrix_power::call(const at::Tensor & self, int64_t n) {
    static auto op = create_matrix_power_typed_handle();
    return op.call(self, n);
}

// aten::matrix_power(Tensor self, int n) -> Tensor
at::Tensor matrix_power::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n) {
    static auto op = create_matrix_power_typed_handle();
    return op.redispatch(dispatchKeySet, self, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_power_out, name, "aten::matrix_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_power_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_power_out, schema_str, "matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<matrix_power_out::schema> create_matrix_power_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_power_out::name, matrix_power_out::overload_name)
      .typed<matrix_power_out::schema>();
}

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & matrix_power_out::call(const at::Tensor & self, int64_t n, at::Tensor & out) {
    static auto op = create_matrix_power_out_typed_handle();
    return op.call(self, n, out);
}

// aten::matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & matrix_power_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, at::Tensor & out) {
    static auto op = create_matrix_power_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp, name, "aten::matrix_exp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp, schema_str, "matrix_exp(Tensor self) -> Tensor")

// aten::matrix_exp(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matrix_exp::schema> create_matrix_exp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_exp::name, matrix_exp::overload_name)
      .typed<matrix_exp::schema>();
}

// aten::matrix_exp(Tensor self) -> Tensor
at::Tensor matrix_exp::call(const at::Tensor & self) {
    static auto op = create_matrix_exp_typed_handle();
    return op.call(self);
}

// aten::matrix_exp(Tensor self) -> Tensor
at::Tensor matrix_exp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_matrix_exp_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp_backward, name, "aten::matrix_exp_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(matrix_exp_backward, schema_str, "matrix_exp_backward(Tensor self, Tensor grad) -> Tensor")

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<matrix_exp_backward::schema> create_matrix_exp_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(matrix_exp_backward::name, matrix_exp_backward::overload_name)
      .typed<matrix_exp_backward::schema>();
}

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
at::Tensor matrix_exp_backward::call(const at::Tensor & self, const at::Tensor & grad) {
    static auto op = create_matrix_exp_backward_typed_handle();
    return op.call(self, grad);
}

// aten::matrix_exp_backward(Tensor self, Tensor grad) -> Tensor
at::Tensor matrix_exp_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad) {
    static auto op = create_matrix_exp_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_aminmax, name, "aten::_aminmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_aminmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_aminmax, schema_str, "_aminmax(Tensor self) -> (Tensor, Tensor)")

// aten::_aminmax(Tensor self) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_aminmax::schema> create__aminmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_aminmax::name, _aminmax::overload_name)
      .typed<_aminmax::schema>();
}

// aten::_aminmax(Tensor self) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _aminmax::call(const at::Tensor & self) {
    static auto op = create__aminmax_typed_handle();
    return op.call(self);
}

// aten::_aminmax(Tensor self) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _aminmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__aminmax_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_aminmax_dim, name, "aten::_aminmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_aminmax_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_aminmax_dim, schema_str, "_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)")

// aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_aminmax_dim::schema> create__aminmax_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_aminmax_dim::name, _aminmax_dim::overload_name)
      .typed<_aminmax_dim::schema>();
}

// aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _aminmax_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create__aminmax_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::_aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _aminmax_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create__aminmax_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(aminmax, name, "aten::aminmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(aminmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(aminmax, schema_str, "aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)")

// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
static C10_NOINLINE c10::TypedOperatorHandle<aminmax::schema> create_aminmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(aminmax::name, aminmax::overload_name)
      .typed<aminmax::schema>();
}

// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
::std::tuple<at::Tensor,at::Tensor> aminmax::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_aminmax_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
::std::tuple<at::Tensor,at::Tensor> aminmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_aminmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(aminmax_out, name, "aten::aminmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(aminmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(aminmax_out, schema_str, "aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)")

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
static C10_NOINLINE c10::TypedOperatorHandle<aminmax_out::schema> create_aminmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(aminmax_out::name, aminmax_out::overload_name)
      .typed<aminmax_out::schema>();
}

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
::std::tuple<at::Tensor &,at::Tensor &> aminmax_out::call(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & min, at::Tensor & max) {
    static auto op = create_aminmax_out_typed_handle();
    return op.call(self, dim, keepdim, min, max);
}

// aten::aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
::std::tuple<at::Tensor &,at::Tensor &> aminmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim, at::Tensor & min, at::Tensor & max) {
    static auto op = create_aminmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_compute_linear_combination, name, "aten::_compute_linear_combination")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_compute_linear_combination, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_compute_linear_combination, schema_str, "_compute_linear_combination(Tensor input, Tensor coefficients) -> Tensor")

// aten::_compute_linear_combination(Tensor input, Tensor coefficients) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_compute_linear_combination::schema> create__compute_linear_combination_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_compute_linear_combination::name, _compute_linear_combination::overload_name)
      .typed<_compute_linear_combination::schema>();
}

// aten::_compute_linear_combination(Tensor input, Tensor coefficients) -> Tensor
at::Tensor _compute_linear_combination::call(const at::Tensor & input, const at::Tensor & coefficients) {
    static auto op = create__compute_linear_combination_typed_handle();
    return op.call(input, coefficients);
}

// aten::_compute_linear_combination(Tensor input, Tensor coefficients) -> Tensor
at::Tensor _compute_linear_combination::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & coefficients) {
    static auto op = create__compute_linear_combination_typed_handle();
    return op.redispatch(dispatchKeySet, input, coefficients);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_compute_linear_combination_out, name, "aten::_compute_linear_combination")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_compute_linear_combination_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_compute_linear_combination_out, schema_str, "_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_compute_linear_combination_out::schema> create__compute_linear_combination_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_compute_linear_combination_out::name, _compute_linear_combination_out::overload_name)
      .typed<_compute_linear_combination_out::schema>();
}

// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _compute_linear_combination_out::call(const at::Tensor & input, const at::Tensor & coefficients, at::Tensor & out) {
    static auto op = create__compute_linear_combination_out_typed_handle();
    return op.call(input, coefficients, out);
}

// aten::_compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _compute_linear_combination_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & coefficients, at::Tensor & out) {
    static auto op = create__compute_linear_combination_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, coefficients, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_dim, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_dim, schema_str, "max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<max_dim::schema> create_max_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_dim::name, max_dim::overload_name)
      .typed<max_dim::schema>();
}

// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> max_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_max_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> max_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_max_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_dim_max, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_dim_max, overload_name, "dim_max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_dim_max, schema_str, "max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<max_dim_max::schema> create_max_dim_max_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_dim_max::name, max_dim_max::overload_name)
      .typed<max_dim_max::schema>();
}

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> max_dim_max::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    static auto op = create_max_dim_max_typed_handle();
    return op.call(self, dim, keepdim, max, max_values);
}

// aten::max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> max_dim_max::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    static auto op = create_max_dim_max_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, max, max_values);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_names_dim, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_names_dim, schema_str, "max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<max_names_dim::schema> create_max_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_names_dim::name, max_names_dim::overload_name)
      .typed<max_names_dim::schema>();
}

// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> max_names_dim::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_max_names_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> max_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_max_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_names_dim_max, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_names_dim_max, overload_name, "names_dim_max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_names_dim_max, schema_str, "max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<max_names_dim_max::schema> create_max_names_dim_max_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_names_dim_max::name, max_names_dim_max::overload_name)
      .typed<max_names_dim_max::schema>();
}

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> max_names_dim_max::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    static auto op = create_max_names_dim_max_typed_handle();
    return op.call(self, dim, keepdim, max, max_values);
}

// aten::max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> max_names_dim_max::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & max, at::Tensor & max_values) {
    static auto op = create_max_names_dim_max_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, max, max_values);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(value_selecting_reduction_backward, name, "aten::value_selecting_reduction_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(value_selecting_reduction_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(value_selecting_reduction_backward, schema_str, "value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor")

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<value_selecting_reduction_backward::schema> create_value_selecting_reduction_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(value_selecting_reduction_backward::name, value_selecting_reduction_backward::overload_name)
      .typed<value_selecting_reduction_backward::schema>();
}

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor
at::Tensor value_selecting_reduction_backward::call(const at::Tensor & grad, int64_t dim, const at::Tensor & indices, at::IntArrayRef sizes, bool keepdim) {
    static auto op = create_value_selecting_reduction_backward_typed_handle();
    return op.call(grad, dim, indices, sizes, keepdim);
}

// aten::value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor
at::Tensor value_selecting_reduction_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, int64_t dim, const at::Tensor & indices, at::IntArrayRef sizes, bool keepdim) {
    static auto op = create_value_selecting_reduction_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, dim, indices, sizes, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax, name, "aten::amax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax, schema_str, "amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor")

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<amax::schema> create_amax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amax::name, amax::overload_name)
      .typed<amax::schema>();
}

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amax::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_amax_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_amax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax_out, name, "aten::amax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amax_out, schema_str, "amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<amax_out::schema> create_amax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amax_out::name, amax_out::overload_name)
      .typed<amax_out::schema>();
}

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amax_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_amax_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_amax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d_with_indices, name, "aten::max_pool1d_with_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d_with_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d_with_indices, schema_str, "max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)")

// aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<max_pool1d_with_indices::schema> create_max_pool1d_with_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool1d_with_indices::name, max_pool1d_with_indices::overload_name)
      .typed<max_pool1d_with_indices::schema>();
}

// aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> max_pool1d_with_indices::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool1d_with_indices_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> max_pool1d_with_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool1d_with_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d, name, "aten::max_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool1d, schema_str, "max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool1d::schema> create_max_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool1d::name, max_pool1d::overload_name)
      .typed<max_pool1d::schema>();
}

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool1d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool1d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d, name, "aten::max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d, schema_str, "max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool2d::schema> create_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool2d::name, max_pool2d::overload_name)
      .typed<max_pool2d::schema>();
}

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d, name, "aten::mkldnn_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d, schema_str, "mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_max_pool2d::schema> create_mkldnn_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_max_pool2d::name, mkldnn_max_pool2d::overload_name)
      .typed<mkldnn_max_pool2d::schema>();
}

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d_backward, name, "aten::mkldnn_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool2d_backward, schema_str, "mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_max_pool2d_backward::schema> create_mkldnn_max_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_max_pool2d_backward::name, mkldnn_max_pool2d_backward::overload_name)
      .typed<mkldnn_max_pool2d_backward::schema>();
}

// aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool2d_backward_typed_handle();
    return op.call(grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool3d, name, "aten::mkldnn_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool3d, schema_str, "mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_max_pool3d::schema> create_mkldnn_max_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_max_pool3d::name, mkldnn_max_pool3d::overload_name)
      .typed<mkldnn_max_pool3d::schema>();
}

// aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool3d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool3d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool3d_backward, name, "aten::mkldnn_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_max_pool3d_backward, schema_str, "mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_max_pool3d_backward::schema> create_mkldnn_max_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_max_pool3d_backward::name, mkldnn_max_pool3d_backward::overload_name)
      .typed<mkldnn_max_pool3d_backward::schema>();
}

// aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool3d_backward_typed_handle();
    return op.call(grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor mkldnn_max_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, const at::Tensor & input, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_mkldnn_max_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, input, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool1d, name, "aten::quantized_max_pool1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool1d, schema_str, "quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_max_pool1d::schema> create_quantized_max_pool1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_max_pool1d::name, quantized_max_pool1d::overload_name)
      .typed<quantized_max_pool1d::schema>();
}

// aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor quantized_max_pool1d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_quantized_max_pool1d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor quantized_max_pool1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_quantized_max_pool1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d, name, "aten::quantized_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_max_pool2d, schema_str, "quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_max_pool2d::schema> create_quantized_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_max_pool2d::name, quantized_max_pool2d::overload_name)
      .typed<quantized_max_pool2d::schema>();
}

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor quantized_max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_quantized_max_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor quantized_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_quantized_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d, name, "aten::max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d, schema_str, "max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor")

// aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool3d::schema> create_max_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool3d::name, max_pool3d::overload_name)
      .typed<max_pool3d::schema>();
}

// aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool3d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool3d_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> Tensor
at::Tensor max_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean, schema_str, "mean(Tensor self, *, ScalarType? dtype=None) -> Tensor")

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mean::schema> create_mean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean::name, mean::overload_name)
      .typed<mean::schema>();
}

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_mean_typed_handle();
    return op.call(self, dtype);
}

// aten::mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_mean_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_dim, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_dim, schema_str, "mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mean_dim::schema> create_mean_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_dim::name, mean_dim::overload_name)
      .typed<mean_dim::schema>();
}

// aten::mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_mean_dim_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_mean_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_out, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_out, schema_str, "mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mean_out::schema> create_mean_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_out::name, mean_out::overload_name)
      .typed<mean_out::schema>();
}

// aten::mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_mean_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_mean_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_dim, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_dim, schema_str, "mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mean_names_dim::schema> create_mean_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_names_dim::name, mean_names_dim::overload_name)
      .typed<mean_names_dim::schema>();
}

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_mean_names_dim_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor mean_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_mean_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_out, name, "aten::mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mean_names_out, schema_str, "mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mean_names_out::schema> create_mean_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mean_names_out::name, mean_names_out::overload_name)
      .typed<mean_names_out::schema>();
}

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_names_out::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_mean_names_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mean_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_mean_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean, name, "aten::nanmean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean, schema_str, "nanmean(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::nanmean(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanmean::schema> create_nanmean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmean::name, nanmean::overload_name)
      .typed<nanmean::schema>();
}

// aten::nanmean(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor nanmean::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_nanmean_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::nanmean(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor nanmean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_nanmean_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean_out, name, "aten::nanmean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmean_out, schema_str, "nanmean.out(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::nanmean.out(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nanmean_out::schema> create_nanmean_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmean_out::name, nanmean_out::overload_name)
      .typed<nanmean_out::schema>();
}

// aten::nanmean.out(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanmean_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_nanmean_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::nanmean.out(Tensor self, int[1] dim=[], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanmean_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_nanmean_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median, name, "aten::median")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median, schema_str, "median(Tensor self) -> Tensor")

// aten::median(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<median::schema> create_median_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(median::name, median::overload_name)
      .typed<median::schema>();
}

// aten::median(Tensor self) -> Tensor
at::Tensor median::call(const at::Tensor & self) {
    static auto op = create_median_typed_handle();
    return op.call(self);
}

// aten::median(Tensor self) -> Tensor
at::Tensor median::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_median_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_dim, name, "aten::median")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_dim, schema_str, "median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<median_dim::schema> create_median_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(median_dim::name, median_dim::overload_name)
      .typed<median_dim::schema>();
}

// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> median_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_median_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> median_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_median_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_dim_values, name, "aten::median")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_dim_values, overload_name, "dim_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_dim_values, schema_str, "median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<median_dim_values::schema> create_median_dim_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(median_dim_values::name, median_dim_values::overload_name)
      .typed<median_dim_values::schema>();
}

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> median_dim_values::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_median_dim_values_typed_handle();
    return op.call(self, dim, keepdim, values, indices);
}

// aten::median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> median_dim_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_median_dim_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_names_dim, name, "aten::median")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_names_dim, schema_str, "median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<median_names_dim::schema> create_median_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(median_names_dim::name, median_names_dim::overload_name)
      .typed<median_names_dim::schema>();
}

// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> median_names_dim::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_median_names_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> median_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_median_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_names_dim_values, name, "aten::median")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_names_dim_values, overload_name, "names_dim_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(median_names_dim_values, schema_str, "median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<median_names_dim_values::schema> create_median_names_dim_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(median_names_dim_values::name, median_names_dim_values::overload_name)
      .typed<median_names_dim_values::schema>();
}

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> median_names_dim_values::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_median_names_dim_values_typed_handle();
    return op.call(self, dim, keepdim, values, indices);
}

// aten::median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> median_names_dim_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_median_names_dim_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian, name, "aten::nanmedian")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian, schema_str, "nanmedian(Tensor self) -> Tensor")

// aten::nanmedian(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanmedian::schema> create_nanmedian_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmedian::name, nanmedian::overload_name)
      .typed<nanmedian::schema>();
}

// aten::nanmedian(Tensor self) -> Tensor
at::Tensor nanmedian::call(const at::Tensor & self) {
    static auto op = create_nanmedian_typed_handle();
    return op.call(self);
}

// aten::nanmedian(Tensor self) -> Tensor
at::Tensor nanmedian::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_nanmedian_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_dim, name, "aten::nanmedian")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_dim, schema_str, "nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<nanmedian_dim::schema> create_nanmedian_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmedian_dim::name, nanmedian_dim::overload_name)
      .typed<nanmedian_dim::schema>();
}

// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> nanmedian_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_nanmedian_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> nanmedian_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_nanmedian_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_dim_values, name, "aten::nanmedian")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_dim_values, overload_name, "dim_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_dim_values, schema_str, "nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<nanmedian_dim_values::schema> create_nanmedian_dim_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmedian_dim_values::name, nanmedian_dim_values::overload_name)
      .typed<nanmedian_dim_values::schema>();
}

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> nanmedian_dim_values::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_nanmedian_dim_values_typed_handle();
    return op.call(self, dim, keepdim, values, indices);
}

// aten::nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> nanmedian_dim_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_nanmedian_dim_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_names_dim, name, "aten::nanmedian")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_names_dim, schema_str, "nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<nanmedian_names_dim::schema> create_nanmedian_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmedian_names_dim::name, nanmedian_names_dim::overload_name)
      .typed<nanmedian_names_dim::schema>();
}

// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> nanmedian_names_dim::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_nanmedian_names_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> nanmedian_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_nanmedian_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_names_dim_values, name, "aten::nanmedian")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_names_dim_values, overload_name, "names_dim_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanmedian_names_dim_values, schema_str, "nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<nanmedian_names_dim_values::schema> create_nanmedian_names_dim_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanmedian_names_dim_values::name, nanmedian_names_dim_values::overload_name)
      .typed<nanmedian_names_dim_values::schema>();
}

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> nanmedian_names_dim_values::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_nanmedian_names_dim_values_typed_handle();
    return op.call(self, dim, keepdim, values, indices);
}

// aten::nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> nanmedian_names_dim_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_nanmedian_names_dim_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim, schema_str, "min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_dim::schema> create_min_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_dim::name, min_dim::overload_name)
      .typed<min_dim::schema>();
}

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_dim::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_min_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_min_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim_min, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim_min, overload_name, "dim_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_dim_min, schema_str, "min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_dim_min::schema> create_min_dim_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_dim_min::name, min_dim_min::overload_name)
      .typed<min_dim_min::schema>();
}

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_dim_min::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    static auto op = create_min_dim_min_typed_handle();
    return op.call(self, dim, keepdim, min, min_indices);
}

// aten::min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_dim_min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    static auto op = create_min_dim_min_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, min, min_indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim, schema_str, "min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_names_dim::schema> create_min_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_names_dim::name, min_names_dim::overload_name)
      .typed<min_names_dim::schema>();
}

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_names_dim::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_min_names_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> min_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_min_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim_min, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim_min, overload_name, "names_dim_min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_names_dim_min, schema_str, "min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<min_names_dim_min::schema> create_min_names_dim_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_names_dim_min::name, min_names_dim_min::overload_name)
      .typed<min_names_dim_min::schema>();
}

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_names_dim_min::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    static auto op = create_min_names_dim_min_typed_handle();
    return op.call(self, dim, keepdim, min, min_indices);
}

// aten::min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> min_names_dim_min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & min, at::Tensor & min_indices) {
    static auto op = create_min_names_dim_min_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, min, min_indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin, name, "aten::amin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin, schema_str, "amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor")

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<amin::schema> create_amin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amin::name, amin::overload_name)
      .typed<amin::schema>();
}

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amin::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_amin_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
at::Tensor amin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_amin_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin_out, name, "aten::amin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(amin_out, schema_str, "amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<amin_out::schema> create_amin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(amin_out::name, amin_out::overload_name)
      .typed<amin_out::schema>();
}

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amin_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_amin_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & amin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_amin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution, name, "aten::mkldnn_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution, schema_str, "mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor")

// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_convolution::schema> create_mkldnn_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_convolution::name, mkldnn_convolution::overload_name)
      .typed<mkldnn_convolution::schema>();
}

// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor
at::Tensor mkldnn_convolution::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_mkldnn_convolution_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups);
}

// aten::mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups) -> Tensor
at::Tensor mkldnn_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_mkldnn_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward_input, name, "aten::mkldnn_convolution_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward_input, schema_str, "mkldnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> Tensor")

// aten::mkldnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_convolution_backward_input::schema> create_mkldnn_convolution_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_convolution_backward_input::name, mkldnn_convolution_backward_input::overload_name)
      .typed<mkldnn_convolution_backward_input::schema>();
}

// aten::mkldnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> Tensor
at::Tensor mkldnn_convolution_backward_input::call(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
    static auto op = create_mkldnn_convolution_backward_input_typed_handle();
    return op.call(self_size, grad_output, weight, padding, stride, dilation, groups, bias_defined);
}

// aten::mkldnn_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> Tensor
at::Tensor mkldnn_convolution_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
    static auto op = create_mkldnn_convolution_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, self_size, grad_output, weight, padding, stride, dilation, groups, bias_defined);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward_weights, name, "aten::mkldnn_convolution_backward_weights")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward_weights, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward_weights, schema_str, "mkldnn_convolution_backward_weights(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> (Tensor, Tensor)")

// aten::mkldnn_convolution_backward_weights(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_convolution_backward_weights::schema> create_mkldnn_convolution_backward_weights_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_convolution_backward_weights::name, mkldnn_convolution_backward_weights::overload_name)
      .typed<mkldnn_convolution_backward_weights::schema>();
}

// aten::mkldnn_convolution_backward_weights(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> mkldnn_convolution_backward_weights::call(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
    static auto op = create_mkldnn_convolution_backward_weights_typed_handle();
    return op.call(weight_size, grad_output, self, padding, stride, dilation, groups, bias_defined);
}

// aten::mkldnn_convolution_backward_weights(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool bias_defined) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> mkldnn_convolution_backward_weights::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool bias_defined) {
    static auto op = create_mkldnn_convolution_backward_weights_typed_handle();
    return op.redispatch(dispatchKeySet, weight_size, grad_output, self, padding, stride, dilation, groups, bias_defined);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward, name, "aten::mkldnn_convolution_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_convolution_backward, schema_str, "mkldnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::mkldnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_convolution_backward::schema> create_mkldnn_convolution_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_convolution_backward::name, mkldnn_convolution_backward::overload_name)
      .typed<mkldnn_convolution_backward::schema>();
}

// aten::mkldnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_convolution_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, ::std::array<bool,3> output_mask) {
    static auto op = create_mkldnn_convolution_backward_typed_handle();
    return op.call(self, grad_output, weight, padding, stride, dilation, groups, output_mask);
}

// aten::mkldnn_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_convolution_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, ::std::array<bool,3> output_mask) {
    static auto op = create_mkldnn_convolution_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, padding, stride, dilation, groups, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_batch_norm, name, "aten::miopen_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_batch_norm, schema_str, "miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)")

// aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_batch_norm::schema> create_miopen_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_batch_norm::name, miopen_batch_norm::overload_name)
      .typed<miopen_batch_norm::schema>();
}

// aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_batch_norm::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
    static auto op = create_miopen_batch_norm_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}

// aten::miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double exponential_average_factor, double epsilon) {
    static auto op = create_miopen_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_batch_norm_backward, name, "aten::miopen_batch_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_batch_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_batch_norm_backward, schema_str, "miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)")

// aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_batch_norm_backward::schema> create_miopen_batch_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_batch_norm_backward::name, miopen_batch_norm_backward::overload_name)
      .typed<miopen_batch_norm_backward::schema>();
}

// aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_batch_norm_backward::call(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon) {
    static auto op = create_miopen_batch_norm_backward_typed_handle();
    return op.call(input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
}

// aten::miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_batch_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_var, double epsilon) {
    static auto op = create_miopen_batch_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution, name, "aten::miopen_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution, schema_str, "miopen_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution::schema> create_miopen_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution::name, miopen_convolution::overload_name)
      .typed<miopen_convolution::schema>();
}

// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_input, name, "aten::miopen_convolution_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_input, schema_str, "miopen_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_backward_input::schema> create_miopen_convolution_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_backward_input::name, miopen_convolution_backward_input::overload_name)
      .typed<miopen_convolution_backward_input::schema>();
}

// aten::miopen_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_backward_input::call(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_backward_input_typed_handle();
    return op.call(self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward, name, "aten::miopen_convolution_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward, schema_str, "miopen_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::miopen_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_backward::schema> create_miopen_convolution_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_backward::name, miopen_convolution_backward::overload_name)
      .typed<miopen_convolution_backward::schema>();
}

// aten::miopen_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask) {
    static auto op = create_miopen_convolution_backward_typed_handle();
    return op.call(self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}

// aten::miopen_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask) {
    static auto op = create_miopen_convolution_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_bias, name, "aten::miopen_convolution_backward_bias")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_bias, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_bias, schema_str, "miopen_convolution_backward_bias(Tensor grad_output) -> Tensor")

// aten::miopen_convolution_backward_bias(Tensor grad_output) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_backward_bias::schema> create_miopen_convolution_backward_bias_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_backward_bias::name, miopen_convolution_backward_bias::overload_name)
      .typed<miopen_convolution_backward_bias::schema>();
}

// aten::miopen_convolution_backward_bias(Tensor grad_output) -> Tensor
at::Tensor miopen_convolution_backward_bias::call(const at::Tensor & grad_output) {
    static auto op = create_miopen_convolution_backward_bias_typed_handle();
    return op.call(grad_output);
}

// aten::miopen_convolution_backward_bias(Tensor grad_output) -> Tensor
at::Tensor miopen_convolution_backward_bias::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output) {
    static auto op = create_miopen_convolution_backward_bias_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_weight, name, "aten::miopen_convolution_backward_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_backward_weight, schema_str, "miopen_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_backward_weight::schema> create_miopen_convolution_backward_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_backward_weight::name, miopen_convolution_backward_weight::overload_name)
      .typed<miopen_convolution_backward_weight::schema>();
}

// aten::miopen_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_backward_weight::call(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_backward_weight_typed_handle();
    return op.call(weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_backward_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_backward_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose, name, "aten::miopen_convolution_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose, schema_str, "miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_transpose::schema> create_miopen_convolution_transpose_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_transpose::name, miopen_convolution_transpose::overload_name)
      .typed<miopen_convolution_transpose::schema>();
}

// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_transpose::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_transpose_typed_handle();
    return op.call(self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_transpose::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_transpose_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward, name, "aten::miopen_convolution_transpose_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward, schema_str, "miopen_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::miopen_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_transpose_backward::schema> create_miopen_convolution_transpose_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_transpose_backward::name, miopen_convolution_transpose_backward::overload_name)
      .typed<miopen_convolution_transpose_backward::schema>();
}

// aten::miopen_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_transpose_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask) {
    static auto op = create_miopen_convolution_transpose_backward_typed_handle();
    return op.call(self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}

// aten::miopen_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_convolution_transpose_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask) {
    static auto op = create_miopen_convolution_transpose_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward_input, name, "aten::miopen_convolution_transpose_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward_input, schema_str, "miopen_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_transpose_backward_input::schema> create_miopen_convolution_transpose_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_transpose_backward_input::name, miopen_convolution_transpose_backward_input::overload_name)
      .typed<miopen_convolution_transpose_backward_input::schema>();
}

// aten::miopen_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_transpose_backward_input::call(const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_transpose_backward_input_typed_handle();
    return op.call(grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_convolution_transpose_backward_input(Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_transpose_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_transpose_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward_weight, name, "aten::miopen_convolution_transpose_backward_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_convolution_transpose_backward_weight, schema_str, "miopen_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_convolution_transpose_backward_weight::schema> create_miopen_convolution_transpose_backward_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_convolution_transpose_backward_weight::name, miopen_convolution_transpose_backward_weight::overload_name)
      .typed<miopen_convolution_transpose_backward_weight::schema>();
}

// aten::miopen_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_transpose_backward_weight::call(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_transpose_backward_weight_typed_handle();
    return op.call(weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_convolution_transpose_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_convolution_transpose_backward_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_convolution_transpose_backward_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution, name, "aten::miopen_depthwise_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution, schema_str, "miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_depthwise_convolution::schema> create_miopen_depthwise_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_depthwise_convolution::name, miopen_depthwise_convolution::overload_name)
      .typed<miopen_depthwise_convolution::schema>();
}

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution::call(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_depthwise_convolution_typed_handle();
    return op.call(self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_depthwise_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward_input, name, "aten::miopen_depthwise_convolution_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward_input, schema_str, "miopen_depthwise_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_depthwise_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_depthwise_convolution_backward_input::schema> create_miopen_depthwise_convolution_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_depthwise_convolution_backward_input::name, miopen_depthwise_convolution_backward_input::overload_name)
      .typed<miopen_depthwise_convolution_backward_input::schema>();
}

// aten::miopen_depthwise_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution_backward_input::call(at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_depthwise_convolution_backward_input_typed_handle();
    return op.call(self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_depthwise_convolution_backward_input(int[] self_size, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef self_size, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_depthwise_convolution_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward, name, "aten::miopen_depthwise_convolution_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward, schema_str, "miopen_depthwise_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::miopen_depthwise_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_depthwise_convolution_backward::schema> create_miopen_depthwise_convolution_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_depthwise_convolution_backward::name, miopen_depthwise_convolution_backward::overload_name)
      .typed<miopen_depthwise_convolution_backward::schema>();
}

// aten::miopen_depthwise_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_depthwise_convolution_backward::call(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask) {
    static auto op = create_miopen_depthwise_convolution_backward_typed_handle();
    return op.call(self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}

// aten::miopen_depthwise_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> miopen_depthwise_convolution_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, ::std::array<bool,3> output_mask) {
    static auto op = create_miopen_depthwise_convolution_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward_weight, name, "aten::miopen_depthwise_convolution_backward_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_depthwise_convolution_backward_weight, schema_str, "miopen_depthwise_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor")

// aten::miopen_depthwise_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<miopen_depthwise_convolution_backward_weight::schema> create_miopen_depthwise_convolution_backward_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_depthwise_convolution_backward_weight::name, miopen_depthwise_convolution_backward_weight::overload_name)
      .typed<miopen_depthwise_convolution_backward_weight::schema>();
}

// aten::miopen_depthwise_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution_backward_weight::call(at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_depthwise_convolution_backward_weight_typed_handle();
    return op.call(weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}

// aten::miopen_depthwise_convolution_backward_weight(int[] weight_size, Tensor grad_output, Tensor self, int[] padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic) -> Tensor
at::Tensor miopen_depthwise_convolution_backward_weight::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef weight_size, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) {
    static auto op = create_miopen_depthwise_convolution_backward_weight_typed_handle();
    return op.redispatch(dispatchKeySet, weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_rnn, name, "aten::miopen_rnn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_rnn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_rnn, schema_str, "miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<miopen_rnn::schema> create_miopen_rnn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_rnn::name, miopen_rnn::overload_name)
      .typed<miopen_rnn::schema>();
}

// aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> miopen_rnn::call(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
    static auto op = create_miopen_rnn_typed_handle();
    return op.call(input, weight, weight_stride0, hx, cx, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
}

// aten::miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> miopen_rnn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state) {
    static auto op = create_miopen_rnn_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, weight_stride0, hx, cx, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_rnn_backward, name, "aten::miopen_rnn_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_rnn_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(miopen_rnn_backward, schema_str, "miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])")

// aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
static C10_NOINLINE c10::TypedOperatorHandle<miopen_rnn_backward::schema> create_miopen_rnn_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(miopen_rnn_backward::name, miopen_rnn_backward::overload_name)
      .typed<miopen_rnn_backward::schema>();
}

// aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> miopen_rnn_backward::call(const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
    static auto op = create_miopen_rnn_backward_typed_handle();
    return op.call(input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}

// aten::miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -> (Tensor, Tensor, Tensor, Tensor[])
::std::tuple<at::Tensor,at::Tensor,at::Tensor,::std::vector<at::Tensor>> miopen_rnn_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList weight, int64_t weight_stride0, const at::Tensor & weight_buf, const at::Tensor & hx, const c10::optional<at::Tensor> & cx, const at::Tensor & output, const c10::optional<at::Tensor> & grad_output, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, at::IntArrayRef batch_sizes, const c10::optional<at::Tensor> & dropout_state, const at::Tensor & reserve, ::std::array<bool,4> output_mask) {
    static auto op = create_miopen_rnn_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm, name, "aten::mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm, schema_str, "mm(Tensor self, Tensor mat2) -> Tensor")

// aten::mm(Tensor self, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mm::schema> create_mm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mm::name, mm::overload_name)
      .typed<mm::schema>();
}

// aten::mm(Tensor self, Tensor mat2) -> Tensor
at::Tensor mm::call(const at::Tensor & self, const at::Tensor & mat2) {
    static auto op = create_mm_typed_handle();
    return op.call(self, mat2);
}

// aten::mm(Tensor self, Tensor mat2) -> Tensor
at::Tensor mm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
    static auto op = create_mm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm_out, name, "aten::mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mm_out, schema_str, "mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mm_out::schema> create_mm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mm_out::name, mm_out::overload_name)
      .typed<mm_out::schema>();
}

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mm_out::call(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    static auto op = create_mm_out_typed_handle();
    return op.call(self, mat2, out);
}

// aten::mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    static auto op = create_mm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mm, name, "aten::_sparse_mm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mm, schema_str, "_sparse_mm(Tensor sparse, Tensor dense) -> Tensor")

// aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_mm::schema> create__sparse_mm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_mm::name, _sparse_mm::overload_name)
      .typed<_sparse_mm::schema>();
}

// aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor
at::Tensor _sparse_mm::call(const at::Tensor & sparse, const at::Tensor & dense) {
    static auto op = create__sparse_mm_typed_handle();
    return op.call(sparse, dense);
}

// aten::_sparse_mm(Tensor sparse, Tensor dense) -> Tensor
at::Tensor _sparse_mm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sparse, const at::Tensor & dense) {
    static auto op = create__sparse_mm_typed_handle();
    return op.redispatch(dispatchKeySet, sparse, dense);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sparse_matmul, name, "aten::_sparse_sparse_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sparse_matmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sparse_matmul, schema_str, "_sparse_sparse_matmul(Tensor self, Tensor other) -> Tensor")

// aten::_sparse_sparse_matmul(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_sparse_matmul::schema> create__sparse_sparse_matmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_sparse_matmul::name, _sparse_sparse_matmul::overload_name)
      .typed<_sparse_sparse_matmul::schema>();
}

// aten::_sparse_sparse_matmul(Tensor self, Tensor other) -> Tensor
at::Tensor _sparse_sparse_matmul::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create__sparse_sparse_matmul_typed_handle();
    return op.call(self, other);
}

// aten::_sparse_sparse_matmul(Tensor self, Tensor other) -> Tensor
at::Tensor _sparse_sparse_matmul::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create__sparse_sparse_matmul_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mask_helper, name, "aten::_sparse_mask_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mask_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_mask_helper, schema_str, "_sparse_mask_helper(Tensor t, Tensor mask_indices) -> Tensor")

// aten::_sparse_mask_helper(Tensor t, Tensor mask_indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_mask_helper::schema> create__sparse_mask_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_mask_helper::name, _sparse_mask_helper::overload_name)
      .typed<_sparse_mask_helper::schema>();
}

// aten::_sparse_mask_helper(Tensor t, Tensor mask_indices) -> Tensor
at::Tensor _sparse_mask_helper::call(const at::Tensor & t, const at::Tensor & mask_indices) {
    static auto op = create__sparse_mask_helper_typed_handle();
    return op.call(t, mask_indices);
}

// aten::_sparse_mask_helper(Tensor t, Tensor mask_indices) -> Tensor
at::Tensor _sparse_mask_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & t, const at::Tensor & mask_indices) {
    static auto op = create__sparse_mask_helper_typed_handle();
    return op.redispatch(dispatchKeySet, t, mask_indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode, name, "aten::mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode, schema_str, "mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<mode::schema> create_mode_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mode::name, mode::overload_name)
      .typed<mode::schema>();
}

// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> mode::call(const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_mode_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> mode::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim) {
    static auto op = create_mode_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_values, name, "aten::mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_values, overload_name, "values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_values, schema_str, "mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<mode_values::schema> create_mode_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mode_values::name, mode_values::overload_name)
      .typed<mode_values::schema>();
}

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> mode_values::call(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_mode_values_typed_handle();
    return op.call(self, dim, keepdim, values, indices);
}

// aten::mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> mode_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_mode_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_dimname, name, "aten::mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_dimname, schema_str, "mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)")

// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<mode_dimname::schema> create_mode_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mode_dimname::name, mode_dimname::overload_name)
      .typed<mode_dimname::schema>();
}

// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> mode_dimname::call(const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_mode_dimname_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> mode_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim) {
    static auto op = create_mode_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_dimname_out, name, "aten::mode")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mode_dimname_out, schema_str, "mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<mode_dimname_out::schema> create_mode_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mode_dimname_out::name, mode_dimname_out::overload_name)
      .typed<mode_dimname_out::schema>();
}

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> mode_dimname_out::call(const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_mode_dimname_out_typed_handle();
    return op.call(self, dim, keepdim, values, indices);
}

// aten::mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> mode_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_mode_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_Tensor, name, "aten::mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_Tensor, schema_str, "mul.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mul_Tensor::schema> create_mul_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mul_Tensor::name, mul_Tensor::overload_name)
      .typed<mul_Tensor::schema>();
}

// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor mul_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_mul_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::mul.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor mul_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_mul_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul__Tensor, name, "aten::mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul__Tensor, schema_str, "mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mul__Tensor::schema> create_mul__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mul__Tensor::name, mul__Tensor::overload_name)
      .typed<mul__Tensor::schema>();
}

// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & mul__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_mul__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & mul__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_mul__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_out, name, "aten::mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_out, schema_str, "mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mul_out::schema> create_mul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mul_out::name, mul_out::overload_name)
      .typed<mul_out::schema>();
}

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mul_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_mul_out_typed_handle();
    return op.call(self, other, out);
}

// aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mul_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_mul_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_Scalar, name, "aten::mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul_Scalar, schema_str, "mul.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mul_Scalar::schema> create_mul_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mul_Scalar::name, mul_Scalar::overload_name)
      .typed<mul_Scalar::schema>();
}

// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor mul_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_mul_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::mul.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor mul_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_mul_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul__Scalar, name, "aten::mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mul__Scalar, schema_str, "mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mul__Scalar::schema> create_mul__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mul__Scalar::name, mul__Scalar::overload_name)
      .typed<mul__Scalar::schema>();
}

// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & mul__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_mul__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & mul__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_mul__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_Tensor, name, "aten::multiply")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_Tensor, schema_str, "multiply.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multiply_Tensor::schema> create_multiply_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multiply_Tensor::name, multiply_Tensor::overload_name)
      .typed<multiply_Tensor::schema>();
}

// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor multiply_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_multiply_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::multiply.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor multiply_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_multiply_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply__Tensor, name, "aten::multiply_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply__Tensor, schema_str, "multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multiply__Tensor::schema> create_multiply__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multiply__Tensor::name, multiply__Tensor::overload_name)
      .typed<multiply__Tensor::schema>();
}

// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & multiply__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_multiply__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & multiply__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_multiply__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_out, name, "aten::multiply")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_out, schema_str, "multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multiply_out::schema> create_multiply_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multiply_out::name, multiply_out::overload_name)
      .typed<multiply_out::schema>();
}

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multiply_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_multiply_out_typed_handle();
    return op.call(self, other, out);
}

// aten::multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multiply_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_multiply_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_Scalar, name, "aten::multiply")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply_Scalar, schema_str, "multiply.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multiply_Scalar::schema> create_multiply_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multiply_Scalar::name, multiply_Scalar::overload_name)
      .typed<multiply_Scalar::schema>();
}

// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor multiply_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_multiply_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::multiply.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor multiply_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_multiply_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply__Scalar, name, "aten::multiply_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multiply__Scalar, schema_str, "multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multiply__Scalar::schema> create_multiply__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multiply__Scalar::name, multiply__Scalar::overload_name)
      .typed<multiply__Scalar::schema>();
}

// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & multiply__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_multiply__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & multiply__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_multiply__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv, name, "aten::mv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv, schema_str, "mv(Tensor self, Tensor vec) -> Tensor")

// aten::mv(Tensor self, Tensor vec) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mv::schema> create_mv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mv::name, mv::overload_name)
      .typed<mv::schema>();
}

// aten::mv(Tensor self, Tensor vec) -> Tensor
at::Tensor mv::call(const at::Tensor & self, const at::Tensor & vec) {
    static auto op = create_mv_typed_handle();
    return op.call(self, vec);
}

// aten::mv(Tensor self, Tensor vec) -> Tensor
at::Tensor mv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec) {
    static auto op = create_mv_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv_out, name, "aten::mv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mv_out, schema_str, "mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mv_out::schema> create_mv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mv_out::name, mv_out::overload_name)
      .typed<mv_out::schema>();
}

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mv_out::call(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
    static auto op = create_mv_out_typed_handle();
    return op.call(self, vec, out);
}

// aten::mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
    static auto op = create_mv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma_out, name, "aten::mvlgamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma_out, schema_str, "mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mvlgamma_out::schema> create_mvlgamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mvlgamma_out::name, mvlgamma_out::overload_name)
      .typed<mvlgamma_out::schema>();
}

// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mvlgamma_out::call(const at::Tensor & self, int64_t p, at::Tensor & out) {
    static auto op = create_mvlgamma_out_typed_handle();
    return op.call(self, p, out);
}

// aten::mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mvlgamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p, at::Tensor & out) {
    static auto op = create_mvlgamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma, name, "aten::mvlgamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma, schema_str, "mvlgamma(Tensor self, int p) -> Tensor")

// aten::mvlgamma(Tensor self, int p) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mvlgamma::schema> create_mvlgamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mvlgamma::name, mvlgamma::overload_name)
      .typed<mvlgamma::schema>();
}

// aten::mvlgamma(Tensor self, int p) -> Tensor
at::Tensor mvlgamma::call(const at::Tensor & self, int64_t p) {
    static auto op = create_mvlgamma_typed_handle();
    return op.call(self, p);
}

// aten::mvlgamma(Tensor self, int p) -> Tensor
at::Tensor mvlgamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p) {
    static auto op = create_mvlgamma_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma_, name, "aten::mvlgamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mvlgamma_, schema_str, "mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)")

// aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mvlgamma_::schema> create_mvlgamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mvlgamma_::name, mvlgamma_::overload_name)
      .typed<mvlgamma_::schema>();
}

// aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)
at::Tensor & mvlgamma_::call(at::Tensor & self, int64_t p) {
    static auto op = create_mvlgamma__typed_handle();
    return op.call(self, p);
}

// aten::mvlgamma_(Tensor(a!) self, int p) -> Tensor(a!)
at::Tensor & mvlgamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t p) {
    static auto op = create_mvlgamma__typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy, name, "aten::narrow_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy, schema_str, "narrow_copy(Tensor self, int dim, int start, int length) -> Tensor")

// aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<narrow_copy::schema> create_narrow_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(narrow_copy::name, narrow_copy::overload_name)
      .typed<narrow_copy::schema>();
}

// aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor
at::Tensor narrow_copy::call(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    static auto op = create_narrow_copy_typed_handle();
    return op.call(self, dim, start, length);
}

// aten::narrow_copy(Tensor self, int dim, int start, int length) -> Tensor
at::Tensor narrow_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    static auto op = create_narrow_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, length);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy_out, name, "aten::narrow_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_copy_out, schema_str, "narrow_copy.out(Tensor self, int dim, int start, int length, *, Tensor(a!) out) -> Tensor(a!)")

// aten::narrow_copy.out(Tensor self, int dim, int start, int length, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<narrow_copy_out::schema> create_narrow_copy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(narrow_copy_out::name, narrow_copy_out::overload_name)
      .typed<narrow_copy_out::schema>();
}

// aten::narrow_copy.out(Tensor self, int dim, int start, int length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & narrow_copy_out::call(const at::Tensor & self, int64_t dim, int64_t start, int64_t length, at::Tensor & out) {
    static auto op = create_narrow_copy_out_typed_handle();
    return op.call(self, dim, start, length, out);
}

// aten::narrow_copy.out(Tensor self, int dim, int start, int length, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & narrow_copy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t start, int64_t length, at::Tensor & out) {
    static auto op = create_narrow_copy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, length, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow, name, "aten::narrow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow, schema_str, "narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)")

// aten::narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<narrow::schema> create_narrow_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(narrow::name, narrow::overload_name)
      .typed<narrow::schema>();
}

// aten::narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)
at::Tensor narrow::call(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    static auto op = create_narrow_typed_handle();
    return op.call(self, dim, start, length);
}

// aten::narrow(Tensor(a) self, int dim, int start, int length) -> Tensor(a)
at::Tensor narrow::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    static auto op = create_narrow_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, length);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_Tensor, name, "aten::narrow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(narrow_Tensor, schema_str, "narrow.Tensor(Tensor(a) self, int dim, Tensor start, int length) -> Tensor(a)")

// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, int length) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<narrow_Tensor::schema> create_narrow_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(narrow_Tensor::name, narrow_Tensor::overload_name)
      .typed<narrow_Tensor::schema>();
}

// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, int length) -> Tensor(a)
at::Tensor narrow_Tensor::call(const at::Tensor & self, int64_t dim, const at::Tensor & start, int64_t length) {
    static auto op = create_narrow_Tensor_typed_handle();
    return op.call(self, dim, start, length);
}

// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, int length) -> Tensor(a)
at::Tensor narrow_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & start, int64_t length) {
    static auto op = create_narrow_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, length);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm, name, "aten::native_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm, schema_str, "native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)")

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm::schema> create_native_batch_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm::name, native_batch_norm::overload_name)
      .typed<native_batch_norm::schema>();
}

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    static auto op = create_native_batch_norm_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, momentum, eps);
}

// aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps) {
    static auto op = create_native_batch_norm_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_out, name, "aten::native_batch_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_out, schema_str, "native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm_out::schema> create_native_batch_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm_out::name, native_batch_norm_out::overload_name)
      .typed<native_batch_norm_out::schema>();
}

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
    static auto op = create_native_batch_norm_out_typed_handle();
    return op.call(input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}

// aten::native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> native_batch_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, bool training, double momentum, double eps, at::Tensor & out, at::Tensor & save_mean, at::Tensor & save_invstd) {
    static auto op = create_native_batch_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, running_mean, running_var, training, momentum, eps, out, save_mean, save_invstd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats, name, "aten::batch_norm_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_stats, schema_str, "batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)")

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_stats::schema> create_batch_norm_stats_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_stats::name, batch_norm_stats::overload_name)
      .typed<batch_norm_stats::schema>();
}

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_stats::call(const at::Tensor & input, double eps) {
    static auto op = create_batch_norm_stats_typed_handle();
    return op.call(input, eps);
}

// aten::batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_stats::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, double eps) {
    static auto op = create_batch_norm_stats_typed_handle();
    return op.redispatch(dispatchKeySet, input, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_elemt, name, "aten::batch_norm_elemt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_elemt, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_elemt, schema_str, "batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor")

// aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_elemt::schema> create_batch_norm_elemt_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_elemt::name, batch_norm_elemt::overload_name)
      .typed<batch_norm_elemt::schema>();
}

// aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
at::Tensor batch_norm_elemt::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps) {
    static auto op = create_batch_norm_elemt_typed_handle();
    return op.call(input, weight, bias, mean, invstd, eps);
}

// aten::batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
at::Tensor batch_norm_elemt::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps) {
    static auto op = create_batch_norm_elemt_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, mean, invstd, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_elemt_out, name, "aten::batch_norm_elemt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_elemt_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_elemt_out, schema_str, "batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)")

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_elemt_out::schema> create_batch_norm_elemt_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_elemt_out::name, batch_norm_elemt_out::overload_name)
      .typed<batch_norm_elemt_out::schema>();
}

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & batch_norm_elemt_out::call(const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps, at::Tensor & out) {
    static auto op = create_batch_norm_elemt_out_typed_handle();
    return op.call(input, weight, bias, mean, invstd, eps, out);
}

// aten::batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & batch_norm_elemt_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, const at::Tensor & mean, const at::Tensor & invstd, double eps, at::Tensor & out) {
    static auto op = create_batch_norm_elemt_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, mean, invstd, eps, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats, name, "aten::batch_norm_gather_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats, schema_str, "batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)")

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_gather_stats::schema> create_batch_norm_gather_stats_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_gather_stats::name, batch_norm_gather_stats::overload_name)
      .typed<batch_norm_gather_stats::schema>();
}

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats::call(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count) {
    static auto op = create_batch_norm_gather_stats_typed_handle();
    return op.call(input, mean, invstd, running_mean, running_var, momentum, eps, count);
}

// aten::batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, int64_t count) {
    static auto op = create_batch_norm_gather_stats_typed_handle();
    return op.redispatch(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, count);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts, name, "aten::batch_norm_gather_stats_with_counts")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_gather_stats_with_counts, schema_str, "batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)")

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_gather_stats_with_counts::schema> create_batch_norm_gather_stats_with_counts_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_gather_stats_with_counts::name, batch_norm_gather_stats_with_counts::overload_name)
      .typed<batch_norm_gather_stats_with_counts::schema>();
}

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats_with_counts::call(const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts) {
    static auto op = create_batch_norm_gather_stats_with_counts_typed_handle();
    return op.call(input, mean, invstd, running_mean, running_var, momentum, eps, counts);
}

// aten::batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_gather_stats_with_counts::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum, double eps, const at::Tensor & counts) {
    static auto op = create_batch_norm_gather_stats_with_counts_typed_handle();
    return op.redispatch(dispatchKeySet, input, mean, invstd, running_mean, running_var, momentum, eps, counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward, name, "aten::native_batch_norm_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_batch_norm_backward, schema_str, "native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<native_batch_norm_backward::schema> create_native_batch_norm_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_batch_norm_backward::name, native_batch_norm_backward::overload_name)
      .typed<native_batch_norm_backward::schema>();
}

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm_backward::call(const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    static auto op = create_native_batch_norm_backward_typed_handle();
    return op.call(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}

// aten::native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> native_batch_norm_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, const c10::optional<at::Tensor> & save_mean, const c10::optional<at::Tensor> & save_invstd, bool train, double eps, ::std::array<bool,3> output_mask) {
    static auto op = create_native_batch_norm_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce, name, "aten::batch_norm_backward_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_reduce, schema_str, "batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)")

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_backward_reduce::schema> create_batch_norm_backward_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_backward_reduce::name, batch_norm_backward_reduce::overload_name)
      .typed<batch_norm_backward_reduce::schema>();
}

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> batch_norm_backward_reduce::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g) {
    static auto op = create_batch_norm_backward_reduce_typed_handle();
    return op.call(grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g);
}

// aten::batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> batch_norm_backward_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, bool input_g, bool weight_g, bool bias_g) {
    static auto op = create_batch_norm_backward_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, invstd, weight, input_g, weight_g, bias_g);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_elemt, name, "aten::batch_norm_backward_elemt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_elemt, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_backward_elemt, schema_str, "batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor")

// aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_backward_elemt::schema> create_batch_norm_backward_elemt_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_backward_elemt::name, batch_norm_backward_elemt::overload_name)
      .typed<batch_norm_backward_elemt::schema>();
}

// aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor
at::Tensor batch_norm_backward_elemt::call(const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, const at::Tensor & mean_dy, const at::Tensor & mean_dy_xmu, const at::Tensor & count) {
    static auto op = create_batch_norm_backward_elemt_typed_handle();
    return op.call(grad_out, input, mean, invstd, weight, mean_dy, mean_dy_xmu, count);
}

// aten::batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor
at::Tensor batch_norm_backward_elemt::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & input, const at::Tensor & mean, const at::Tensor & invstd, const c10::optional<at::Tensor> & weight, const at::Tensor & mean_dy, const at::Tensor & mean_dy_xmu, const at::Tensor & count) {
    static auto op = create_batch_norm_backward_elemt_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, input, mean, invstd, weight, mean_dy, mean_dy_xmu, count);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_update_stats, name, "aten::batch_norm_update_stats")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_update_stats, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(batch_norm_update_stats, schema_str, "batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)")

// aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<batch_norm_update_stats::schema> create_batch_norm_update_stats_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(batch_norm_update_stats::name, batch_norm_update_stats::overload_name)
      .typed<batch_norm_update_stats::schema>();
}

// aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_update_stats::call(const at::Tensor & input, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum) {
    static auto op = create_batch_norm_update_stats_typed_handle();
    return op.call(input, running_mean, running_var, momentum);
}

// aten::batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> batch_norm_update_stats::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const c10::optional<at::Tensor> & running_mean, const c10::optional<at::Tensor> & running_var, double momentum) {
    static auto op = create_batch_norm_update_stats_typed_handle();
    return op.redispatch(dispatchKeySet, input, running_mean, running_var, momentum);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_vulkan_available, name, "aten::is_vulkan_available")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_vulkan_available, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_vulkan_available, schema_str, "is_vulkan_available() -> bool")

// aten::is_vulkan_available() -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_vulkan_available::schema> create_is_vulkan_available_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_vulkan_available::name, is_vulkan_available::overload_name)
      .typed<is_vulkan_available::schema>();
}

// aten::is_vulkan_available() -> bool
bool is_vulkan_available::call() {
    static auto op = create_is_vulkan_available_typed_handle();
    return op.call();
}

// aten::is_vulkan_available() -> bool
bool is_vulkan_available::redispatch(c10::DispatchKeySet dispatchKeySet) {
    static auto op = create_is_vulkan_available_typed_handle();
    return op.redispatch(dispatchKeySet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_available, name, "aten::_nnpack_available")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_available, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_available, schema_str, "_nnpack_available() -> bool")

// aten::_nnpack_available() -> bool
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_available::schema> create__nnpack_available_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_available::name, _nnpack_available::overload_name)
      .typed<_nnpack_available::schema>();
}

// aten::_nnpack_available() -> bool
bool _nnpack_available::call() {
    static auto op = create__nnpack_available_typed_handle();
    return op.call();
}

// aten::_nnpack_available() -> bool
bool _nnpack_available::redispatch(c10::DispatchKeySet dispatchKeySet) {
    static auto op = create__nnpack_available_typed_handle();
    return op.redispatch(dispatchKeySet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution, name, "aten::_nnpack_spatial_convolution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution, schema_str, "_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding, int[2] stride=1) -> Tensor")

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding, int[2] stride=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_spatial_convolution::schema> create__nnpack_spatial_convolution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_spatial_convolution::name, _nnpack_spatial_convolution::overload_name)
      .typed<_nnpack_spatial_convolution::schema>();
}

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding, int[2] stride=1) -> Tensor
at::Tensor _nnpack_spatial_convolution::call(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create__nnpack_spatial_convolution_typed_handle();
    return op.call(input, weight, bias, padding, stride);
}

// aten::_nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding, int[2] stride=1) -> Tensor
at::Tensor _nnpack_spatial_convolution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create__nnpack_spatial_convolution_typed_handle();
    return op.redispatch(dispatchKeySet, input, weight, bias, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward, name, "aten::_nnpack_spatial_convolution_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward, schema_str, "_nnpack_spatial_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[2] padding, bool[3] output_mask) -> (Tensor, Tensor, Tensor)")

// aten::_nnpack_spatial_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[2] padding, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_spatial_convolution_backward::schema> create__nnpack_spatial_convolution_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_spatial_convolution_backward::name, _nnpack_spatial_convolution_backward::overload_name)
      .typed<_nnpack_spatial_convolution_backward::schema>();
}

// aten::_nnpack_spatial_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[2] padding, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _nnpack_spatial_convolution_backward::call(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, ::std::array<bool,3> output_mask) {
    static auto op = create__nnpack_spatial_convolution_backward_typed_handle();
    return op.call(input, grad_output, weight, padding, output_mask);
}

// aten::_nnpack_spatial_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[2] padding, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _nnpack_spatial_convolution_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, ::std::array<bool,3> output_mask) {
    static auto op = create__nnpack_spatial_convolution_backward_typed_handle();
    return op.redispatch(dispatchKeySet, input, grad_output, weight, padding, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward_input, name, "aten::_nnpack_spatial_convolution_backward_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward_input, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward_input, schema_str, "_nnpack_spatial_convolution_backward_input(Tensor input, Tensor grad_output, Tensor weight, int[2] padding) -> Tensor")

// aten::_nnpack_spatial_convolution_backward_input(Tensor input, Tensor grad_output, Tensor weight, int[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_spatial_convolution_backward_input::schema> create__nnpack_spatial_convolution_backward_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_spatial_convolution_backward_input::name, _nnpack_spatial_convolution_backward_input::overload_name)
      .typed<_nnpack_spatial_convolution_backward_input::schema>();
}

// aten::_nnpack_spatial_convolution_backward_input(Tensor input, Tensor grad_output, Tensor weight, int[2] padding) -> Tensor
at::Tensor _nnpack_spatial_convolution_backward_input::call(const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding) {
    static auto op = create__nnpack_spatial_convolution_backward_input_typed_handle();
    return op.call(input, grad_output, weight, padding);
}

// aten::_nnpack_spatial_convolution_backward_input(Tensor input, Tensor grad_output, Tensor weight, int[2] padding) -> Tensor
at::Tensor _nnpack_spatial_convolution_backward_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding) {
    static auto op = create__nnpack_spatial_convolution_backward_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, grad_output, weight, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward_weight, name, "aten::_nnpack_spatial_convolution_backward_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnpack_spatial_convolution_backward_weight, schema_str, "_nnpack_spatial_convolution_backward_weight(Tensor input, int[] weightsize, Tensor grad_output, int[2] padding) -> Tensor")

// aten::_nnpack_spatial_convolution_backward_weight(Tensor input, int[] weightsize, Tensor grad_output, int[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_nnpack_spatial_convolution_backward_weight::schema> create__nnpack_spatial_convolution_backward_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnpack_spatial_convolution_backward_weight::name, _nnpack_spatial_convolution_backward_weight::overload_name)
      .typed<_nnpack_spatial_convolution_backward_weight::schema>();
}

// aten::_nnpack_spatial_convolution_backward_weight(Tensor input, int[] weightsize, Tensor grad_output, int[2] padding) -> Tensor
at::Tensor _nnpack_spatial_convolution_backward_weight::call(const at::Tensor & input, at::IntArrayRef weightsize, const at::Tensor & grad_output, at::IntArrayRef padding) {
    static auto op = create__nnpack_spatial_convolution_backward_weight_typed_handle();
    return op.call(input, weightsize, grad_output, padding);
}

// aten::_nnpack_spatial_convolution_backward_weight(Tensor input, int[] weightsize, Tensor grad_output, int[2] padding) -> Tensor
at::Tensor _nnpack_spatial_convolution_backward_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::IntArrayRef weightsize, const at::Tensor & grad_output, at::IntArrayRef padding) {
    static auto op = create__nnpack_spatial_convolution_backward_weight_typed_handle();
    return op.redispatch(dispatchKeySet, input, weightsize, grad_output, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_names, schema_str, "ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ones_names::schema> create_ones_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones_names::name, ones_names::overload_name)
      .typed<ones_names::schema>();
}

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones_names::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_ones_names_typed_handle();
    return op.call(size, names, dtype, layout, device, pin_memory);
}

// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_ones_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones, schema_str, "ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ones::schema> create_ones_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones::name, ones::overload_name)
      .typed<ones::schema>();
}

// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_ones_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory);
}

// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor ones::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_ones_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_out, name, "aten::ones")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_out, schema_str, "ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ones_out::schema> create_ones_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones_out::name, ones_out::overload_name)
      .typed<ones_out::schema>();
}

// aten::ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ones_out::call(at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_ones_out_typed_handle();
    return op.call(size, out);
}

// aten::ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ones_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_ones_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_like, name, "aten::ones_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ones_like, schema_str, "ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ones_like::schema> create_ones_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ones_like::name, ones_like::overload_name)
      .typed<ones_like::schema>();
}

// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor ones_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_ones_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor ones_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_ones_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pairwise_distance, name, "aten::pairwise_distance")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pairwise_distance, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pairwise_distance, schema_str, "pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor")

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pairwise_distance::schema> create_pairwise_distance_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pairwise_distance::name, pairwise_distance::overload_name)
      .typed<pairwise_distance::schema>();
}

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
at::Tensor pairwise_distance::call(const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
    static auto op = create_pairwise_distance_typed_handle();
    return op.call(x1, x2, p, eps, keepdim);
}

// aten::pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -> Tensor
at::Tensor pairwise_distance::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, double eps, bool keepdim) {
    static auto op = create_pairwise_distance_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, p, eps, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cdist, name, "aten::cdist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cdist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cdist, schema_str, "cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor")

// aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cdist::schema> create_cdist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cdist::name, cdist::overload_name)
      .typed<cdist::schema>();
}

// aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
at::Tensor cdist::call(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    static auto op = create_cdist_typed_handle();
    return op.call(x1, x2, p, compute_mode);
}

// aten::cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
at::Tensor cdist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    static auto op = create_cdist_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, p, compute_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_euclidean_dist, name, "aten::_euclidean_dist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_euclidean_dist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_euclidean_dist, schema_str, "_euclidean_dist(Tensor x1, Tensor x2) -> Tensor")

// aten::_euclidean_dist(Tensor x1, Tensor x2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_euclidean_dist::schema> create__euclidean_dist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_euclidean_dist::name, _euclidean_dist::overload_name)
      .typed<_euclidean_dist::schema>();
}

// aten::_euclidean_dist(Tensor x1, Tensor x2) -> Tensor
at::Tensor _euclidean_dist::call(const at::Tensor & x1, const at::Tensor & x2) {
    static auto op = create__euclidean_dist_typed_handle();
    return op.call(x1, x2);
}

// aten::_euclidean_dist(Tensor x1, Tensor x2) -> Tensor
at::Tensor _euclidean_dist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2) {
    static auto op = create__euclidean_dist_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward, name, "aten::_cdist_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_forward, schema_str, "_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor")

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cdist_forward::schema> create__cdist_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cdist_forward::name, _cdist_forward::overload_name)
      .typed<_cdist_forward::schema>();
}

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
at::Tensor _cdist_forward::call(const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    static auto op = create__cdist_forward_typed_handle();
    return op.call(x1, x2, p, compute_mode);
}

// aten::_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
at::Tensor _cdist_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
    static auto op = create__cdist_forward_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, p, compute_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_backward, name, "aten::_cdist_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cdist_backward, schema_str, "_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor")

// aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cdist_backward::schema> create__cdist_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cdist_backward::name, _cdist_backward::overload_name)
      .typed<_cdist_backward::schema>();
}

// aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor
at::Tensor _cdist_backward::call(const at::Tensor & grad, const at::Tensor & x1, const at::Tensor & x2, double p, const at::Tensor & cdist) {
    static auto op = create__cdist_backward_typed_handle();
    return op.call(grad, x1, x2, p, cdist);
}

// aten::_cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor
at::Tensor _cdist_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & x1, const at::Tensor & x2, double p, const at::Tensor & cdist) {
    static auto op = create__cdist_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, x1, x2, p, cdist);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pdist, name, "aten::pdist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pdist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pdist, schema_str, "pdist(Tensor self, float p=2) -> Tensor")

// aten::pdist(Tensor self, float p=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pdist::schema> create_pdist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pdist::name, pdist::overload_name)
      .typed<pdist::schema>();
}

// aten::pdist(Tensor self, float p=2) -> Tensor
at::Tensor pdist::call(const at::Tensor & self, double p) {
    static auto op = create_pdist_typed_handle();
    return op.call(self, p);
}

// aten::pdist(Tensor self, float p=2) -> Tensor
at::Tensor pdist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p) {
    static auto op = create_pdist_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_forward, name, "aten::_pdist_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_forward, schema_str, "_pdist_forward(Tensor self, float p=2) -> Tensor")

// aten::_pdist_forward(Tensor self, float p=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_pdist_forward::schema> create__pdist_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pdist_forward::name, _pdist_forward::overload_name)
      .typed<_pdist_forward::schema>();
}

// aten::_pdist_forward(Tensor self, float p=2) -> Tensor
at::Tensor _pdist_forward::call(const at::Tensor & self, double p) {
    static auto op = create__pdist_forward_typed_handle();
    return op.call(self, p);
}

// aten::_pdist_forward(Tensor self, float p=2) -> Tensor
at::Tensor _pdist_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double p) {
    static auto op = create__pdist_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward, name, "aten::_pdist_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pdist_backward, schema_str, "_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor")

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_pdist_backward::schema> create__pdist_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pdist_backward::name, _pdist_backward::overload_name)
      .typed<_pdist_backward::schema>();
}

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
at::Tensor _pdist_backward::call(const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
    static auto op = create__pdist_backward_typed_handle();
    return op.call(grad, self, p, pdist);
}

// aten::_pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -> Tensor
at::Tensor _pdist_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, double p, const at::Tensor & pdist) {
    static auto op = create__pdist_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, p, pdist);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_similarity, name, "aten::cosine_similarity")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_similarity, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cosine_similarity, schema_str, "cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor")

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cosine_similarity::schema> create_cosine_similarity_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cosine_similarity::name, cosine_similarity::overload_name)
      .typed<cosine_similarity::schema>();
}

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
at::Tensor cosine_similarity::call(const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
    static auto op = create_cosine_similarity_typed_handle();
    return op.call(x1, x2, dim, eps);
}

// aten::cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -> Tensor
at::Tensor cosine_similarity::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x1, const at::Tensor & x2, int64_t dim, double eps) {
    static auto op = create_cosine_similarity_typed_handle();
    return op.redispatch(dispatchKeySet, x1, x2, dim, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(permute, name, "aten::permute")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(permute, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(permute, schema_str, "permute(Tensor(a) self, int[] dims) -> Tensor(a)")

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<permute::schema> create_permute_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(permute::name, permute::overload_name)
      .typed<permute::schema>();
}

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
at::Tensor permute::call(const at::Tensor & self, at::IntArrayRef dims) {
    static auto op = create_permute_typed_handle();
    return op.call(self, dims);
}

// aten::permute(Tensor(a) self, int[] dims) -> Tensor(a)
at::Tensor permute::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
    static auto op = create_permute_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_intlist, name, "aten::movedim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_intlist, overload_name, "intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_intlist, schema_str, "movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)")

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<movedim_intlist::schema> create_movedim_intlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(movedim_intlist::name, movedim_intlist::overload_name)
      .typed<movedim_intlist::schema>();
}

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
at::Tensor movedim_intlist::call(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    static auto op = create_movedim_intlist_typed_handle();
    return op.call(self, source, destination);
}

// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
at::Tensor movedim_intlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    static auto op = create_movedim_intlist_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, destination);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_int, name, "aten::movedim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(movedim_int, schema_str, "movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)")

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<movedim_int::schema> create_movedim_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(movedim_int::name, movedim_int::overload_name)
      .typed<movedim_int::schema>();
}

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
at::Tensor movedim_int::call(const at::Tensor & self, int64_t source, int64_t destination) {
    static auto op = create_movedim_int_typed_handle();
    return op.call(self, source, destination);
}

// aten::movedim.int(Tensor(a) self, int source, int destination) -> Tensor(a)
at::Tensor movedim_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t source, int64_t destination) {
    static auto op = create_movedim_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, destination);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(moveaxis_intlist, name, "aten::moveaxis")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(moveaxis_intlist, overload_name, "intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(moveaxis_intlist, schema_str, "moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)")

// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<moveaxis_intlist::schema> create_moveaxis_intlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(moveaxis_intlist::name, moveaxis_intlist::overload_name)
      .typed<moveaxis_intlist::schema>();
}

// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
at::Tensor moveaxis_intlist::call(const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    static auto op = create_moveaxis_intlist_typed_handle();
    return op.call(self, source, destination);
}

// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -> Tensor(a)
at::Tensor moveaxis_intlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef source, at::IntArrayRef destination) {
    static auto op = create_moveaxis_intlist_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, destination);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(moveaxis_int, name, "aten::moveaxis")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(moveaxis_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(moveaxis_int, schema_str, "moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)")

// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<moveaxis_int::schema> create_moveaxis_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(moveaxis_int::name, moveaxis_int::overload_name)
      .typed<moveaxis_int::schema>();
}

// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)
at::Tensor moveaxis_int::call(const at::Tensor & self, int64_t source, int64_t destination) {
    static auto op = create_moveaxis_int_typed_handle();
    return op.call(self, source, destination);
}

// aten::moveaxis.int(Tensor(a) self, int source, int destination) -> Tensor(a)
at::Tensor moveaxis_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t source, int64_t destination) {
    static auto op = create_moveaxis_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, destination);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(numpy_T, name, "aten::numpy_T")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(numpy_T, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(numpy_T, schema_str, "numpy_T(Tensor(a) self) -> Tensor(a)")

// aten::numpy_T(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<numpy_T::schema> create_numpy_T_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(numpy_T::name, numpy_T::overload_name)
      .typed<numpy_T::schema>();
}

// aten::numpy_T(Tensor(a) self) -> Tensor(a)
at::Tensor numpy_T::call(const at::Tensor & self) {
    static auto op = create_numpy_T_typed_handle();
    return op.call(self);
}

// aten::numpy_T(Tensor(a) self) -> Tensor(a)
at::Tensor numpy_T::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_numpy_T_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle, name, "aten::pixel_shuffle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_shuffle, schema_str, "pixel_shuffle(Tensor self, int upscale_factor) -> Tensor")

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pixel_shuffle::schema> create_pixel_shuffle_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pixel_shuffle::name, pixel_shuffle::overload_name)
      .typed<pixel_shuffle::schema>();
}

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
at::Tensor pixel_shuffle::call(const at::Tensor & self, int64_t upscale_factor) {
    static auto op = create_pixel_shuffle_typed_handle();
    return op.call(self, upscale_factor);
}

// aten::pixel_shuffle(Tensor self, int upscale_factor) -> Tensor
at::Tensor pixel_shuffle::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t upscale_factor) {
    static auto op = create_pixel_shuffle_typed_handle();
    return op.redispatch(dispatchKeySet, self, upscale_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_unshuffle, name, "aten::pixel_unshuffle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_unshuffle, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pixel_unshuffle, schema_str, "pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor")

// aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pixel_unshuffle::schema> create_pixel_unshuffle_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pixel_unshuffle::name, pixel_unshuffle::overload_name)
      .typed<pixel_unshuffle::schema>();
}

// aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor
at::Tensor pixel_unshuffle::call(const at::Tensor & self, int64_t downscale_factor) {
    static auto op = create_pixel_unshuffle_typed_handle();
    return op.call(self, downscale_factor);
}

// aten::pixel_unshuffle(Tensor self, int downscale_factor) -> Tensor
at::Tensor pixel_unshuffle::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t downscale_factor) {
    static auto op = create_pixel_unshuffle_typed_handle();
    return op.redispatch(dispatchKeySet, self, downscale_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(channel_shuffle, name, "aten::channel_shuffle")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(channel_shuffle, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(channel_shuffle, schema_str, "channel_shuffle(Tensor self, int groups) -> Tensor")

// aten::channel_shuffle(Tensor self, int groups) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<channel_shuffle::schema> create_channel_shuffle_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(channel_shuffle::name, channel_shuffle::overload_name)
      .typed<channel_shuffle::schema>();
}

// aten::channel_shuffle(Tensor self, int groups) -> Tensor
at::Tensor channel_shuffle::call(const at::Tensor & self, int64_t groups) {
    static auto op = create_channel_shuffle_typed_handle();
    return op.call(self, groups);
}

// aten::channel_shuffle(Tensor self, int groups) -> Tensor
at::Tensor channel_shuffle::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t groups) {
    static auto op = create_channel_shuffle_typed_handle();
    return op.redispatch(dispatchKeySet, self, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_pinned, name, "aten::is_pinned")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_pinned, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_pinned, schema_str, "is_pinned(Tensor self, Device? device=None) -> bool")

// aten::is_pinned(Tensor self, Device? device=None) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_pinned::schema> create_is_pinned_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_pinned::name, is_pinned::overload_name)
      .typed<is_pinned::schema>();
}

// aten::is_pinned(Tensor self, Device? device=None) -> bool
bool is_pinned::call(const at::Tensor & self, c10::optional<at::Device> device) {
    static auto op = create_is_pinned_typed_handle();
    return op.call(self, device);
}

// aten::is_pinned(Tensor self, Device? device=None) -> bool
bool is_pinned::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Device> device) {
    static auto op = create_is_pinned_typed_handle();
    return op.redispatch(dispatchKeySet, self, device);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pin_memory, name, "aten::pin_memory")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pin_memory, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pin_memory, schema_str, "pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)")

// aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<pin_memory::schema> create_pin_memory_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pin_memory::name, pin_memory::overload_name)
      .typed<pin_memory::schema>();
}

// aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)
at::Tensor pin_memory::call(const at::Tensor & self, c10::optional<at::Device> device) {
    static auto op = create_pin_memory_typed_handle();
    return op.call(self, device);
}

// aten::pin_memory(Tensor(a) self, Device? device=None) -> Tensor(a)
at::Tensor pin_memory::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Device> device) {
    static auto op = create_pin_memory_typed_handle();
    return op.redispatch(dispatchKeySet, self, device);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pin_memory, name, "aten::_pin_memory")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pin_memory, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pin_memory, schema_str, "_pin_memory(Tensor self, Device? device=None) -> Tensor")

// aten::_pin_memory(Tensor self, Device? device=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_pin_memory::schema> create__pin_memory_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pin_memory::name, _pin_memory::overload_name)
      .typed<_pin_memory::schema>();
}

// aten::_pin_memory(Tensor self, Device? device=None) -> Tensor
at::Tensor _pin_memory::call(const at::Tensor & self, c10::optional<at::Device> device) {
    static auto op = create__pin_memory_typed_handle();
    return op.call(self, device);
}

// aten::_pin_memory(Tensor self, Device? device=None) -> Tensor
at::Tensor _pin_memory::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Device> device) {
    static auto op = create__pin_memory_typed_handle();
    return op.redispatch(dispatchKeySet, self, device);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pinverse, name, "aten::pinverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pinverse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pinverse, schema_str, "pinverse(Tensor self, float rcond=1e-15) -> Tensor")

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pinverse::schema> create_pinverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pinverse::name, pinverse::overload_name)
      .typed<pinverse::schema>();
}

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
at::Tensor pinverse::call(const at::Tensor & self, double rcond) {
    static auto op = create_pinverse_typed_handle();
    return op.call(self, rcond);
}

// aten::pinverse(Tensor self, float rcond=1e-15) -> Tensor
at::Tensor pinverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond) {
    static auto op = create_pinverse_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(poisson_nll_loss, name, "aten::poisson_nll_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(poisson_nll_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(poisson_nll_loss, schema_str, "poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor")

// aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<poisson_nll_loss::schema> create_poisson_nll_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(poisson_nll_loss::name, poisson_nll_loss::overload_name)
      .typed<poisson_nll_loss::schema>();
}

// aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor
at::Tensor poisson_nll_loss::call(const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
    static auto op = create_poisson_nll_loss_typed_handle();
    return op.call(input, target, log_input, full, eps, reduction);
}

// aten::poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -> Tensor
at::Tensor poisson_nll_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & target, bool log_input, bool full, double eps, int64_t reduction) {
    static auto op = create_poisson_nll_loss_typed_handle();
    return op.redispatch(dispatchKeySet, input, target, log_input, full, eps, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg, name, "aten::rad2deg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg, schema_str, "rad2deg(Tensor self) -> Tensor")

// aten::rad2deg(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rad2deg::schema> create_rad2deg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rad2deg::name, rad2deg::overload_name)
      .typed<rad2deg::schema>();
}

// aten::rad2deg(Tensor self) -> Tensor
at::Tensor rad2deg::call(const at::Tensor & self) {
    static auto op = create_rad2deg_typed_handle();
    return op.call(self);
}

// aten::rad2deg(Tensor self) -> Tensor
at::Tensor rad2deg::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_rad2deg_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg_, name, "aten::rad2deg_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg_, schema_str, "rad2deg_(Tensor(a!) self) -> Tensor(a!)")

// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rad2deg_::schema> create_rad2deg__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rad2deg_::name, rad2deg_::overload_name)
      .typed<rad2deg_::schema>();
}

// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & rad2deg_::call(at::Tensor & self) {
    static auto op = create_rad2deg__typed_handle();
    return op.call(self);
}

// aten::rad2deg_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & rad2deg_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_rad2deg__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg_out, name, "aten::rad2deg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rad2deg_out, schema_str, "rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rad2deg_out::schema> create_rad2deg_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rad2deg_out::name, rad2deg_out::overload_name)
      .typed<rad2deg_out::schema>();
}

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rad2deg_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_rad2deg_out_typed_handle();
    return op.call(self, out);
}

// aten::rad2deg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rad2deg_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_rad2deg_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad, name, "aten::deg2rad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad, schema_str, "deg2rad(Tensor self) -> Tensor")

// aten::deg2rad(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<deg2rad::schema> create_deg2rad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(deg2rad::name, deg2rad::overload_name)
      .typed<deg2rad::schema>();
}

// aten::deg2rad(Tensor self) -> Tensor
at::Tensor deg2rad::call(const at::Tensor & self) {
    static auto op = create_deg2rad_typed_handle();
    return op.call(self);
}

// aten::deg2rad(Tensor self) -> Tensor
at::Tensor deg2rad::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_deg2rad_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad_, name, "aten::deg2rad_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad_, schema_str, "deg2rad_(Tensor(a!) self) -> Tensor(a!)")

// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<deg2rad_::schema> create_deg2rad__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(deg2rad_::name, deg2rad_::overload_name)
      .typed<deg2rad_::schema>();
}

// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & deg2rad_::call(at::Tensor & self) {
    static auto op = create_deg2rad__typed_handle();
    return op.call(self);
}

// aten::deg2rad_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & deg2rad_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_deg2rad__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad_out, name, "aten::deg2rad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(deg2rad_out, schema_str, "deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<deg2rad_out::schema> create_deg2rad_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(deg2rad_out::name, deg2rad_out::overload_name)
      .typed<deg2rad_out::schema>();
}

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & deg2rad_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_deg2rad_out_typed_handle();
    return op.call(self, out);
}

// aten::deg2rad.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & deg2rad_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_deg2rad_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scalar_tensor, name, "aten::scalar_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scalar_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scalar_tensor, schema_str, "scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scalar_tensor::schema> create_scalar_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scalar_tensor::name, scalar_tensor::overload_name)
      .typed<scalar_tensor::schema>();
}

// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor scalar_tensor::call(const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_scalar_tensor_typed_handle();
    return op.call(s, dtype, layout, device, pin_memory);
}

// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor scalar_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_scalar_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, s, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_names, name, "aten::rand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_names, schema_str, "rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rand_names::schema> create_rand_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_names::name, rand_names::overload_name)
      .typed<rand_names::schema>();
}

// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand_names::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_names_typed_handle();
    return op.call(size, names, dtype, layout, device, pin_memory);
}

// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator_with_names, name, "aten::rand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator_with_names, overload_name, "generator_with_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator_with_names, schema_str, "rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rand_generator_with_names::schema> create_rand_generator_with_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_generator_with_names::name, rand_generator_with_names::overload_name)
      .typed<rand_generator_with_names::schema>();
}

// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand_generator_with_names::call(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_generator_with_names_typed_handle();
    return op.call(size, generator, names, dtype, layout, device, pin_memory);
}

// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand_generator_with_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_generator_with_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, generator, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand, name, "aten::rand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand, schema_str, "rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rand::schema> create_rand_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand::name, rand::overload_name)
      .typed<rand::schema>();
}

// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory);
}

// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator, name, "aten::rand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator, overload_name, "generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator, schema_str, "rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rand_generator::schema> create_rand_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_generator::name, rand_generator::overload_name)
      .typed<rand_generator::schema>();
}

// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand_generator::call(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_generator_typed_handle();
    return op.call(size, generator, dtype, layout, device, pin_memory);
}

// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor rand_generator::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_rand_generator_typed_handle();
    return op.redispatch(dispatchKeySet, size, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_out, name, "aten::rand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_out, schema_str, "rand.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::rand.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rand_out::schema> create_rand_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_out::name, rand_out::overload_name)
      .typed<rand_out::schema>();
}

// aten::rand.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rand_out::call(at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_rand_out_typed_handle();
    return op.call(size, out);
}

// aten::rand.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rand_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_rand_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator_out, name, "aten::rand")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator_out, overload_name, "generator_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_generator_out, schema_str, "rand.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)")

// aten::rand.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rand_generator_out::schema> create_rand_generator_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_generator_out::name, rand_generator_out::overload_name)
      .typed<rand_generator_out::schema>();
}

// aten::rand.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rand_generator_out::call(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_rand_generator_out_typed_handle();
    return op.call(size, generator, out);
}

// aten::rand.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rand_generator_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_rand_generator_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like, name, "aten::rand_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rand_like, schema_str, "rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rand_like::schema> create_rand_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rand_like::name, rand_like::overload_name)
      .typed<rand_like::schema>();
}

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor rand_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_rand_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor rand_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_rand_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint, schema_str, "randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint::schema> create_randint_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint::name, randint::overload_name)
      .typed<randint::schema>();
}

// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint::call(int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_typed_handle();
    return op.call(high, size, dtype, layout, device, pin_memory);
}

// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_typed_handle();
    return op.redispatch(dispatchKeySet, high, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_generator, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_generator, overload_name, "generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_generator, schema_str, "randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_generator::schema> create_randint_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_generator::name, randint_generator::overload_name)
      .typed<randint_generator::schema>();
}

// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint_generator::call(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_generator_typed_handle();
    return op.call(high, size, generator, dtype, layout, device, pin_memory);
}

// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint_generator::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_generator_typed_handle();
    return op.redispatch(dispatchKeySet, high, size, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low, overload_name, "low")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low, schema_str, "randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_low::schema> create_randint_low_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_low::name, randint_low::overload_name)
      .typed<randint_low::schema>();
}

// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint_low::call(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_low_typed_handle();
    return op.call(low, high, size, dtype, layout, device, pin_memory);
}

// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint_low::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_low_typed_handle();
    return op.redispatch(dispatchKeySet, low, high, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_generator, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_generator, overload_name, "low_generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_generator, schema_str, "randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_low_generator::schema> create_randint_low_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_low_generator::name, randint_low_generator::overload_name)
      .typed<randint_low_generator::schema>();
}

// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint_low_generator::call(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_low_generator_typed_handle();
    return op.call(low, high, size, generator, dtype, layout, device, pin_memory);
}

// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randint_low_generator::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randint_low_generator_typed_handle();
    return op.redispatch(dispatchKeySet, low, high, size, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_out, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_out, schema_str, "randint.out(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::randint.out(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randint_out::schema> create_randint_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_out::name, randint_out::overload_name)
      .typed<randint_out::schema>();
}

// aten::randint.out(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_out::call(int64_t high, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_randint_out_typed_handle();
    return op.call(high, size, out);
}

// aten::randint.out(int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_randint_out_typed_handle();
    return op.redispatch(dispatchKeySet, high, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_generator_out, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_generator_out, overload_name, "generator_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_generator_out, schema_str, "randint.generator_out(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)")

// aten::randint.generator_out(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randint_generator_out::schema> create_randint_generator_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_generator_out::name, randint_generator_out::overload_name)
      .typed<randint_generator_out::schema>();
}

// aten::randint.generator_out(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_generator_out::call(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randint_generator_out_typed_handle();
    return op.call(high, size, generator, out);
}

// aten::randint.generator_out(int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_generator_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randint_generator_out_typed_handle();
    return op.redispatch(dispatchKeySet, high, size, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_out, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_out, overload_name, "low_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_out, schema_str, "randint.low_out(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::randint.low_out(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randint_low_out::schema> create_randint_low_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_low_out::name, randint_low_out::overload_name)
      .typed<randint_low_out::schema>();
}

// aten::randint.low_out(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_low_out::call(int64_t low, int64_t high, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_randint_low_out_typed_handle();
    return op.call(low, high, size, out);
}

// aten::randint.low_out(int low, int high, int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_low_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_randint_low_out_typed_handle();
    return op.redispatch(dispatchKeySet, low, high, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_generator_out, name, "aten::randint")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_generator_out, overload_name, "low_generator_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_low_generator_out, schema_str, "randint.low_generator_out(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)")

// aten::randint.low_generator_out(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randint_low_generator_out::schema> create_randint_low_generator_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_low_generator_out::name, randint_low_generator_out::overload_name)
      .typed<randint_low_generator_out::schema>();
}

// aten::randint.low_generator_out(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_low_generator_out::call(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randint_low_generator_out_typed_handle();
    return op.call(low, high, size, generator, out);
}

// aten::randint.low_generator_out(int low, int high, int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randint_low_generator_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randint_low_generator_out_typed_handle();
    return op.redispatch(dispatchKeySet, low, high, size, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like, name, "aten::randint_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like, schema_str, "randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_like::schema> create_randint_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_like::name, randint_like::overload_name)
      .typed<randint_like::schema>();
}

// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like::call(const at::Tensor & self, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_randint_like_typed_handle();
    return op.call(self, high, dtype, layout, device, pin_memory, memory_format);
}

// aten::randint_like(Tensor self, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_randint_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, high, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype, name, "aten::randint_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype, overload_name, "low_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randint_like_low_dtype, schema_str, "randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randint_like_low_dtype::schema> create_randint_like_low_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randint_like_low_dtype::name, randint_like_low_dtype::overload_name)
      .typed<randint_like_low_dtype::schema>();
}

// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like_low_dtype::call(const at::Tensor & self, int64_t low, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_randint_like_low_dtype_typed_handle();
    return op.call(self, low, high, dtype, layout, device, pin_memory, memory_format);
}

// aten::randint_like.low_dtype(Tensor self, int low, int high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randint_like_low_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t low, int64_t high, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_randint_like_low_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, low, high, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn, name, "aten::randn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn, schema_str, "randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randn::schema> create_randn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn::name, randn::overload_name)
      .typed<randn::schema>();
}

// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory);
}

// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator, name, "aten::randn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator, overload_name, "generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator, schema_str, "randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randn_generator::schema> create_randn_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn_generator::name, randn_generator::overload_name)
      .typed<randn_generator::schema>();
}

// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn_generator::call(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_generator_typed_handle();
    return op.call(size, generator, dtype, layout, device, pin_memory);
}

// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn_generator::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_generator_typed_handle();
    return op.redispatch(dispatchKeySet, size, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_names, name, "aten::randn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_names, schema_str, "randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randn_names::schema> create_randn_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn_names::name, randn_names::overload_name)
      .typed<randn_names::schema>();
}

// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn_names::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_names_typed_handle();
    return op.call(size, names, dtype, layout, device, pin_memory);
}

// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator_with_names, name, "aten::randn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator_with_names, overload_name, "generator_with_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator_with_names, schema_str, "randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randn_generator_with_names::schema> create_randn_generator_with_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn_generator_with_names::name, randn_generator_with_names::overload_name)
      .typed<randn_generator_with_names::schema>();
}

// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn_generator_with_names::call(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_generator_with_names_typed_handle();
    return op.call(size, generator, names, dtype, layout, device, pin_memory);
}

// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randn_generator_with_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randn_generator_with_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, generator, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_out, name, "aten::randn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_out, schema_str, "randn.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::randn.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randn_out::schema> create_randn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn_out::name, randn_out::overload_name)
      .typed<randn_out::schema>();
}

// aten::randn.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randn_out::call(at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_randn_out_typed_handle();
    return op.call(size, out);
}

// aten::randn.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randn_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_randn_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator_out, name, "aten::randn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator_out, overload_name, "generator_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_generator_out, schema_str, "randn.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)")

// aten::randn.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randn_generator_out::schema> create_randn_generator_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn_generator_out::name, randn_generator_out::overload_name)
      .typed<randn_generator_out::schema>();
}

// aten::randn.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randn_generator_out::call(at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randn_generator_out_typed_handle();
    return op.call(size, generator, out);
}

// aten::randn.generator_out(int[] size, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randn_generator_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randn_generator_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_like, name, "aten::randn_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randn_like, schema_str, "randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randn_like::schema> create_randn_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randn_like::name, randn_like::overload_name)
      .typed<randn_like::schema>();
}

// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randn_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_randn_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor randn_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_randn_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm, name, "aten::randperm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm, schema_str, "randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randperm::schema> create_randperm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randperm::name, randperm::overload_name)
      .typed<randperm::schema>();
}

// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randperm::call(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randperm_typed_handle();
    return op.call(n, dtype, layout, device, pin_memory);
}

// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randperm::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randperm_typed_handle();
    return op.redispatch(dispatchKeySet, n, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_generator, name, "aten::randperm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_generator, overload_name, "generator")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_generator, schema_str, "randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<randperm_generator::schema> create_randperm_generator_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randperm_generator::name, randperm_generator::overload_name)
      .typed<randperm_generator::schema>();
}

// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randperm_generator::call(int64_t n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randperm_generator_typed_handle();
    return op.call(n, generator, dtype, layout, device, pin_memory);
}

// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor randperm_generator::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_randperm_generator_typed_handle();
    return op.redispatch(dispatchKeySet, n, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_out, name, "aten::randperm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_out, schema_str, "randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randperm_out::schema> create_randperm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randperm_out::name, randperm_out::overload_name)
      .typed<randperm_out::schema>();
}

// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randperm_out::call(int64_t n, at::Tensor & out) {
    static auto op = create_randperm_out_typed_handle();
    return op.call(n, out);
}

// aten::randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randperm_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, at::Tensor & out) {
    static auto op = create_randperm_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_generator_out, name, "aten::randperm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_generator_out, overload_name, "generator_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(randperm_generator_out, schema_str, "randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)")

// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<randperm_generator_out::schema> create_randperm_generator_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(randperm_generator_out::name, randperm_generator_out::overload_name)
      .typed<randperm_generator_out::schema>();
}

// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randperm_generator_out::call(int64_t n, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randperm_generator_out_typed_handle();
    return op.call(n, generator, out);
}

// aten::randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
at::Tensor & randperm_generator_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_randperm_generator_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range_step, name, "aten::range")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range_step, overload_name, "step")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range_step, schema_str, "range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<range_step::schema> create_range_step_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(range_step::name, range_step::overload_name)
      .typed<range_step::schema>();
}

// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor range_step::call(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_range_step_typed_handle();
    return op.call(start, end, step, dtype, layout, device, pin_memory);
}

// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor range_step::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_range_step_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, step, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range, name, "aten::range")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range, schema_str, "range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<range::schema> create_range_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(range::name, range::overload_name)
      .typed<range::schema>();
}

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor range::call(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_range_typed_handle();
    return op.call(start, end, dtype, layout, device, pin_memory);
}

// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor range::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_range_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range_out, name, "aten::range")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(range_out, schema_str, "range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<range_out::schema> create_range_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(range_out::name, range_out::overload_name)
      .typed<range_out::schema>();
}

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & range_out::call(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    static auto op = create_range_out_typed_handle();
    return op.call(start, end, step, out);
}

// aten::range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & range_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
    static auto op = create_range_out_typed_handle();
    return op.redispatch(dispatchKeySet, start, end, step, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ravel, name, "aten::ravel")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ravel, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ravel, schema_str, "ravel(Tensor(a) self) -> Tensor(a)")

// aten::ravel(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<ravel::schema> create_ravel_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ravel::name, ravel::overload_name)
      .typed<ravel::schema>();
}

// aten::ravel(Tensor(a) self) -> Tensor(a)
at::Tensor ravel::call(const at::Tensor & self) {
    static auto op = create_ravel_typed_handle();
    return op.call(self);
}

// aten::ravel(Tensor(a) self) -> Tensor(a)
at::Tensor ravel::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_ravel_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal, name, "aten::reciprocal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal, schema_str, "reciprocal(Tensor self) -> Tensor")

// aten::reciprocal(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reciprocal::schema> create_reciprocal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reciprocal::name, reciprocal::overload_name)
      .typed<reciprocal::schema>();
}

// aten::reciprocal(Tensor self) -> Tensor
at::Tensor reciprocal::call(const at::Tensor & self) {
    static auto op = create_reciprocal_typed_handle();
    return op.call(self);
}

// aten::reciprocal(Tensor self) -> Tensor
at::Tensor reciprocal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_reciprocal_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal_, name, "aten::reciprocal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal_, schema_str, "reciprocal_(Tensor(a!) self) -> Tensor(a!)")

// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reciprocal_::schema> create_reciprocal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reciprocal_::name, reciprocal_::overload_name)
      .typed<reciprocal_::schema>();
}

// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & reciprocal_::call(at::Tensor & self) {
    static auto op = create_reciprocal__typed_handle();
    return op.call(self);
}

// aten::reciprocal_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & reciprocal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_reciprocal__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal_out, name, "aten::reciprocal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reciprocal_out, schema_str, "reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reciprocal_out::schema> create_reciprocal_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reciprocal_out::name, reciprocal_out::overload_name)
      .typed<reciprocal_out::schema>();
}

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reciprocal_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_reciprocal_out_typed_handle();
    return op.call(self, out);
}

// aten::reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reciprocal_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_reciprocal_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg, name, "aten::neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg, schema_str, "neg(Tensor self) -> Tensor")

// aten::neg(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<neg::schema> create_neg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(neg::name, neg::overload_name)
      .typed<neg::schema>();
}

// aten::neg(Tensor self) -> Tensor
at::Tensor neg::call(const at::Tensor & self) {
    static auto op = create_neg_typed_handle();
    return op.call(self);
}

// aten::neg(Tensor self) -> Tensor
at::Tensor neg::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_neg_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg_, name, "aten::neg_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg_, schema_str, "neg_(Tensor(a!) self) -> Tensor(a!)")

// aten::neg_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<neg_::schema> create_neg__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(neg_::name, neg_::overload_name)
      .typed<neg_::schema>();
}

// aten::neg_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & neg_::call(at::Tensor & self) {
    static auto op = create_neg__typed_handle();
    return op.call(self);
}

// aten::neg_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & neg_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_neg__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg_out, name, "aten::neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(neg_out, schema_str, "neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<neg_out::schema> create_neg_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(neg_out::name, neg_out::overload_name)
      .typed<neg_out::schema>();
}

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & neg_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_neg_out_typed_handle();
    return op.call(self, out);
}

// aten::neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & neg_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_neg_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative, name, "aten::negative")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative, schema_str, "negative(Tensor self) -> Tensor")

// aten::negative(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<negative::schema> create_negative_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(negative::name, negative::overload_name)
      .typed<negative::schema>();
}

// aten::negative(Tensor self) -> Tensor
at::Tensor negative::call(const at::Tensor & self) {
    static auto op = create_negative_typed_handle();
    return op.call(self);
}

// aten::negative(Tensor self) -> Tensor
at::Tensor negative::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_negative_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative_, name, "aten::negative_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative_, schema_str, "negative_(Tensor(a!) self) -> Tensor(a!)")

// aten::negative_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<negative_::schema> create_negative__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(negative_::name, negative_::overload_name)
      .typed<negative_::schema>();
}

// aten::negative_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & negative_::call(at::Tensor & self) {
    static auto op = create_negative__typed_handle();
    return op.call(self);
}

// aten::negative_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & negative_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_negative__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative_out, name, "aten::negative")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(negative_out, schema_str, "negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<negative_out::schema> create_negative_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(negative_out::name, negative_out::overload_name)
      .typed<negative_out::schema>();
}

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & negative_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_negative_out_typed_handle();
    return op.call(self, out);
}

// aten::negative.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & negative_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_negative_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat, name, "aten::repeat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat, schema_str, "repeat(Tensor self, int[] repeats) -> Tensor")

// aten::repeat(Tensor self, int[] repeats) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<repeat::schema> create_repeat_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(repeat::name, repeat::overload_name)
      .typed<repeat::schema>();
}

// aten::repeat(Tensor self, int[] repeats) -> Tensor
at::Tensor repeat::call(const at::Tensor & self, at::IntArrayRef repeats) {
    static auto op = create_repeat_typed_handle();
    return op.call(self, repeats);
}

// aten::repeat(Tensor self, int[] repeats) -> Tensor
at::Tensor repeat::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef repeats) {
    static auto op = create_repeat_typed_handle();
    return op.redispatch(dispatchKeySet, self, repeats);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_Tensor, name, "aten::repeat_interleave")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_Tensor, schema_str, "repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor")

// aten::repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<repeat_interleave_Tensor::schema> create_repeat_interleave_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(repeat_interleave_Tensor::name, repeat_interleave_Tensor::overload_name)
      .typed<repeat_interleave_Tensor::schema>();
}

// aten::repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor
at::Tensor repeat_interleave_Tensor::call(const at::Tensor & repeats, c10::optional<int64_t> output_size) {
    static auto op = create_repeat_interleave_Tensor_typed_handle();
    return op.call(repeats, output_size);
}

// aten::repeat_interleave.Tensor(Tensor repeats, *, int? output_size=None) -> Tensor
at::Tensor repeat_interleave_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & repeats, c10::optional<int64_t> output_size) {
    static auto op = create_repeat_interleave_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, repeats, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_self_Tensor, name, "aten::repeat_interleave")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_self_Tensor, overload_name, "self_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_self_Tensor, schema_str, "repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor")

// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<repeat_interleave_self_Tensor::schema> create_repeat_interleave_self_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(repeat_interleave_self_Tensor::name, repeat_interleave_self_Tensor::overload_name)
      .typed<repeat_interleave_self_Tensor::schema>();
}

// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
at::Tensor repeat_interleave_self_Tensor::call(const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
    static auto op = create_repeat_interleave_self_Tensor_typed_handle();
    return op.call(self, repeats, dim, output_size);
}

// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
at::Tensor repeat_interleave_self_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
    static auto op = create_repeat_interleave_self_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, repeats, dim, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_self_int, name, "aten::repeat_interleave")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_self_int, overload_name, "self_int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(repeat_interleave_self_int, schema_str, "repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor")

// aten::repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<repeat_interleave_self_int::schema> create_repeat_interleave_self_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(repeat_interleave_self_int::name, repeat_interleave_self_int::overload_name)
      .typed<repeat_interleave_self_int::schema>();
}

// aten::repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor
at::Tensor repeat_interleave_self_int::call(const at::Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
    static auto op = create_repeat_interleave_self_int_typed_handle();
    return op.call(self, repeats, dim, output_size);
}

// aten::repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor
at::Tensor repeat_interleave_self_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t repeats, c10::optional<int64_t> dim, c10::optional<int64_t> output_size) {
    static auto op = create_repeat_interleave_self_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, repeats, dim, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape, name, "aten::reshape")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape, schema_str, "reshape(Tensor(a) self, int[] shape) -> Tensor(a)")

// aten::reshape(Tensor(a) self, int[] shape) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<reshape::schema> create_reshape_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reshape::name, reshape::overload_name)
      .typed<reshape::schema>();
}

// aten::reshape(Tensor(a) self, int[] shape) -> Tensor(a)
at::Tensor reshape::call(const at::Tensor & self, at::IntArrayRef shape) {
    static auto op = create_reshape_typed_handle();
    return op.call(self, shape);
}

// aten::reshape(Tensor(a) self, int[] shape) -> Tensor(a)
at::Tensor reshape::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape) {
    static auto op = create_reshape_typed_handle();
    return op.redispatch(dispatchKeySet, self, shape);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias, name, "aten::_reshape_alias")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_reshape_alias, schema_str, "_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)")

// aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_reshape_alias::schema> create__reshape_alias_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_reshape_alias::name, _reshape_alias::overload_name)
      .typed<_reshape_alias::schema>();
}

// aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)
at::Tensor _reshape_alias::call(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride) {
    static auto op = create__reshape_alias_typed_handle();
    return op.call(self, size, stride);
}

// aten::_reshape_alias(Tensor(a) self, int[] size, int[] stride) -> Tensor(a)
at::Tensor _reshape_alias::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride) {
    static auto op = create__reshape_alias_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_reshape, name, "aten::_mkldnn_reshape")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_reshape, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_reshape, schema_str, "_mkldnn_reshape(Tensor self, int[] shape) -> Tensor")

// aten::_mkldnn_reshape(Tensor self, int[] shape) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_mkldnn_reshape::schema> create__mkldnn_reshape_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mkldnn_reshape::name, _mkldnn_reshape::overload_name)
      .typed<_mkldnn_reshape::schema>();
}

// aten::_mkldnn_reshape(Tensor self, int[] shape) -> Tensor
at::Tensor _mkldnn_reshape::call(const at::Tensor & self, at::IntArrayRef shape) {
    static auto op = create__mkldnn_reshape_typed_handle();
    return op.call(self, shape);
}

// aten::_mkldnn_reshape(Tensor self, int[] shape) -> Tensor
at::Tensor _mkldnn_reshape::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape) {
    static auto op = create__mkldnn_reshape_typed_handle();
    return op.redispatch(dispatchKeySet, self, shape);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape_as, name, "aten::reshape_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reshape_as, schema_str, "reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)")

// aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<reshape_as::schema> create_reshape_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reshape_as::name, reshape_as::overload_name)
      .typed<reshape_as::schema>();
}

// aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor reshape_as::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_reshape_as_typed_handle();
    return op.call(self, other);
}

// aten::reshape_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor reshape_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_reshape_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round, name, "aten::round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round, schema_str, "round(Tensor self) -> Tensor")

// aten::round(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<round::schema> create_round_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round::name, round::overload_name)
      .typed<round::schema>();
}

// aten::round(Tensor self) -> Tensor
at::Tensor round::call(const at::Tensor & self) {
    static auto op = create_round_typed_handle();
    return op.call(self);
}

// aten::round(Tensor self) -> Tensor
at::Tensor round::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_round_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_, name, "aten::round_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_, schema_str, "round_(Tensor(a!) self) -> Tensor(a!)")

// aten::round_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<round_::schema> create_round__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round_::name, round_::overload_name)
      .typed<round_::schema>();
}

// aten::round_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & round_::call(at::Tensor & self) {
    static auto op = create_round__typed_handle();
    return op.call(self);
}

// aten::round_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & round_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_round__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_out, name, "aten::round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(round_out, schema_str, "round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<round_out::schema> create_round_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(round_out::name, round_out::overload_name)
      .typed<round_out::schema>();
}

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & round_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_round_out_typed_handle();
    return op.call(self, out);
}

// aten::round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & round_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_round_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu, name, "aten::rrelu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu, schema_str, "rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor")

// aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rrelu::schema> create_rrelu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rrelu::name, rrelu::overload_name)
      .typed<rrelu::schema>();
}

// aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
at::Tensor rrelu::call(const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu_typed_handle();
    return op.call(self, lower, upper, training, generator);
}

// aten::rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
at::Tensor rrelu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu_typed_handle();
    return op.redispatch(dispatchKeySet, self, lower, upper, training, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_, name, "aten::rrelu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_, schema_str, "rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)")

// aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rrelu_::schema> create_rrelu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rrelu_::name, rrelu_::overload_name)
      .typed<rrelu_::schema>();
}

// aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
at::Tensor & rrelu_::call(at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu__typed_handle();
    return op.call(self, lower, upper, training, generator);
}

// aten::rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
at::Tensor & rrelu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu__typed_handle();
    return op.redispatch(dispatchKeySet, self, lower, upper, training, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu, name, "aten::relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu, schema_str, "relu(Tensor self) -> Tensor")

// aten::relu(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<relu::schema> create_relu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(relu::name, relu::overload_name)
      .typed<relu::schema>();
}

// aten::relu(Tensor self) -> Tensor
at::Tensor relu::call(const at::Tensor & self) {
    static auto op = create_relu_typed_handle();
    return op.call(self);
}

// aten::relu(Tensor self) -> Tensor
at::Tensor relu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_relu_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu_, name, "aten::relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu_, schema_str, "relu_(Tensor(a!) self) -> Tensor(a!)")

// aten::relu_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<relu_::schema> create_relu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(relu_::name, relu_::overload_name)
      .typed<relu_::schema>();
}

// aten::relu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & relu_::call(at::Tensor & self) {
    static auto op = create_relu__typed_handle();
    return op.call(self);
}

// aten::relu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & relu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_relu__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu6, name, "aten::relu6")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu6, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu6, schema_str, "relu6(Tensor self) -> Tensor")

// aten::relu6(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<relu6::schema> create_relu6_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(relu6::name, relu6::overload_name)
      .typed<relu6::schema>();
}

// aten::relu6(Tensor self) -> Tensor
at::Tensor relu6::call(const at::Tensor & self) {
    static auto op = create_relu6_typed_handle();
    return op.call(self);
}

// aten::relu6(Tensor self) -> Tensor
at::Tensor relu6::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_relu6_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu6_, name, "aten::relu6_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu6_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(relu6_, schema_str, "relu6_(Tensor(a!) self) -> Tensor(a!)")

// aten::relu6_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<relu6_::schema> create_relu6__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(relu6_::name, relu6_::overload_name)
      .typed<relu6_::schema>();
}

// aten::relu6_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & relu6_::call(at::Tensor & self) {
    static auto op = create_relu6__typed_handle();
    return op.call(self);
}

// aten::relu6_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & relu6_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_relu6__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prelu, name, "aten::prelu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prelu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prelu, schema_str, "prelu(Tensor self, Tensor weight) -> Tensor")

// aten::prelu(Tensor self, Tensor weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<prelu::schema> create_prelu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prelu::name, prelu::overload_name)
      .typed<prelu::schema>();
}

// aten::prelu(Tensor self, Tensor weight) -> Tensor
at::Tensor prelu::call(const at::Tensor & self, const at::Tensor & weight) {
    static auto op = create_prelu_typed_handle();
    return op.call(self, weight);
}

// aten::prelu(Tensor self, Tensor weight) -> Tensor
at::Tensor prelu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight) {
    static auto op = create_prelu_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prelu_backward, name, "aten::prelu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prelu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prelu_backward, schema_str, "prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)")

// aten::prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<prelu_backward::schema> create_prelu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prelu_backward::name, prelu_backward::overload_name)
      .typed<prelu_backward::schema>();
}

// aten::prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> prelu_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
    static auto op = create_prelu_backward_typed_handle();
    return op.call(grad_output, self, weight);
}

// aten::prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> prelu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight) {
    static auto op = create_prelu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_out, name, "aten::gelu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_out, schema_str, "gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gelu_out::schema> create_gelu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu_out::name, gelu_out::overload_name)
      .typed<gelu_out::schema>();
}

// aten::gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gelu_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_gelu_out_typed_handle();
    return op.call(self, out);
}

// aten::gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gelu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_gelu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu, name, "aten::gelu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu, schema_str, "gelu(Tensor self) -> Tensor")

// aten::gelu(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gelu::schema> create_gelu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu::name, gelu::overload_name)
      .typed<gelu::schema>();
}

// aten::gelu(Tensor self) -> Tensor
at::Tensor gelu::call(const at::Tensor & self) {
    static auto op = create_gelu_typed_handle();
    return op.call(self);
}

// aten::gelu(Tensor self) -> Tensor
at::Tensor gelu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_gelu_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_backward_grad_input, name, "aten::gelu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_backward_grad_input, schema_str, "gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gelu_backward_grad_input::schema> create_gelu_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu_backward_grad_input::name, gelu_backward_grad_input::overload_name)
      .typed<gelu_backward_grad_input::schema>();
}

// aten::gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & gelu_backward_grad_input::call(const at::Tensor & grad, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_gelu_backward_grad_input_typed_handle();
    return op.call(grad, self, grad_input);
}

// aten::gelu_backward.grad_input(Tensor grad, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & gelu_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_gelu_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_backward, name, "aten::gelu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gelu_backward, schema_str, "gelu_backward(Tensor grad, Tensor self) -> Tensor")

// aten::gelu_backward(Tensor grad, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gelu_backward::schema> create_gelu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gelu_backward::name, gelu_backward::overload_name)
      .typed<gelu_backward::schema>();
}

// aten::gelu_backward(Tensor grad, Tensor self) -> Tensor
at::Tensor gelu_backward::call(const at::Tensor & grad, const at::Tensor & self) {
    static auto op = create_gelu_backward_typed_handle();
    return op.call(grad, self);
}

// aten::gelu_backward(Tensor grad, Tensor self) -> Tensor
at::Tensor gelu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self) {
    static auto op = create_gelu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(infinitely_differentiable_gelu_backward, name, "aten::infinitely_differentiable_gelu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(infinitely_differentiable_gelu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(infinitely_differentiable_gelu_backward, schema_str, "infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor")

// aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<infinitely_differentiable_gelu_backward::schema> create_infinitely_differentiable_gelu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(infinitely_differentiable_gelu_backward::name, infinitely_differentiable_gelu_backward::overload_name)
      .typed<infinitely_differentiable_gelu_backward::schema>();
}

// aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor
at::Tensor infinitely_differentiable_gelu_backward::call(const at::Tensor & grad, const at::Tensor & self) {
    static auto op = create_infinitely_differentiable_gelu_backward_typed_handle();
    return op.call(grad, self);
}

// aten::infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -> Tensor
at::Tensor infinitely_differentiable_gelu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self) {
    static auto op = create_infinitely_differentiable_gelu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_out, name, "aten::hardshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_out, schema_str, "hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardshrink_out::schema> create_hardshrink_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardshrink_out::name, hardshrink_out::overload_name)
      .typed<hardshrink_out::schema>();
}

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardshrink_out::call(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    static auto op = create_hardshrink_out_typed_handle();
    return op.call(self, lambd, out);
}

// aten::hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardshrink_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    static auto op = create_hardshrink_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink, name, "aten::hardshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink, schema_str, "hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor")

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardshrink::schema> create_hardshrink_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardshrink::name, hardshrink::overload_name)
      .typed<hardshrink::schema>();
}

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor hardshrink::call(const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_hardshrink_typed_handle();
    return op.call(self, lambd);
}

// aten::hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor hardshrink::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_hardshrink_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_backward_grad_input, name, "aten::hardshrink_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_backward_grad_input, schema_str, "hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardshrink_backward_grad_input::schema> create_hardshrink_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardshrink_backward_grad_input::name, hardshrink_backward_grad_input::overload_name)
      .typed<hardshrink_backward_grad_input::schema>();
}

// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & hardshrink_backward_grad_input::call(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
    static auto op = create_hardshrink_backward_grad_input_typed_handle();
    return op.call(grad_out, self, lambd, grad_input);
}

// aten::hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & hardshrink_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
    static auto op = create_hardshrink_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, self, lambd, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_backward, name, "aten::hardshrink_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardshrink_backward, schema_str, "hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor")

// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardshrink_backward::schema> create_hardshrink_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardshrink_backward::name, hardshrink_backward::overload_name)
      .typed<hardshrink_backward::schema>();
}

// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
at::Tensor hardshrink_backward::call(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_hardshrink_backward_typed_handle();
    return op.call(grad_out, self, lambd);
}

// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
at::Tensor hardshrink_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_hardshrink_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_out, self, lambd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt, name, "aten::rsqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt, schema_str, "rsqrt(Tensor self) -> Tensor")

// aten::rsqrt(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rsqrt::schema> create_rsqrt_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsqrt::name, rsqrt::overload_name)
      .typed<rsqrt::schema>();
}

// aten::rsqrt(Tensor self) -> Tensor
at::Tensor rsqrt::call(const at::Tensor & self) {
    static auto op = create_rsqrt_typed_handle();
    return op.call(self);
}

// aten::rsqrt(Tensor self) -> Tensor
at::Tensor rsqrt::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_rsqrt_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt_, name, "aten::rsqrt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt_, schema_str, "rsqrt_(Tensor(a!) self) -> Tensor(a!)")

// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rsqrt_::schema> create_rsqrt__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsqrt_::name, rsqrt_::overload_name)
      .typed<rsqrt_::schema>();
}

// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & rsqrt_::call(at::Tensor & self) {
    static auto op = create_rsqrt__typed_handle();
    return op.call(self);
}

// aten::rsqrt_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & rsqrt_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_rsqrt__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt_out, name, "aten::rsqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsqrt_out, schema_str, "rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rsqrt_out::schema> create_rsqrt_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsqrt_out::name, rsqrt_out::overload_name)
      .typed<rsqrt_out::schema>();
}

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rsqrt_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_rsqrt_out_typed_handle();
    return op.call(self, out);
}

// aten::rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rsqrt_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_rsqrt_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_Dimname, name, "aten::select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_Dimname, schema_str, "select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)")

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<select_Dimname::schema> create_select_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_Dimname::name, select_Dimname::overload_name)
      .typed<select_Dimname::schema>();
}

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
at::Tensor select_Dimname::call(const at::Tensor & self, at::Dimname dim, int64_t index) {
    static auto op = create_select_Dimname_typed_handle();
    return op.call(self, dim, index);
}

// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -> Tensor(a)
at::Tensor select_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, int64_t index) {
    static auto op = create_select_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_int, name, "aten::select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_int, schema_str, "select.int(Tensor(a) self, int dim, int index) -> Tensor(a)")

// aten::select.int(Tensor(a) self, int dim, int index) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<select_int::schema> create_select_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_int::name, select_int::overload_name)
      .typed<select_int::schema>();
}

// aten::select.int(Tensor(a) self, int dim, int index) -> Tensor(a)
at::Tensor select_int::call(const at::Tensor & self, int64_t dim, int64_t index) {
    static auto op = create_select_int_typed_handle();
    return op.call(self, dim, index);
}

// aten::select.int(Tensor(a) self, int dim, int index) -> Tensor(a)
at::Tensor select_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, int64_t index) {
    static auto op = create_select_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward, name, "aten::select_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(select_backward, schema_str, "select_backward(Tensor grad_output, int[] input_sizes, int dim, int index) -> Tensor")

// aten::select_backward(Tensor grad_output, int[] input_sizes, int dim, int index) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<select_backward::schema> create_select_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(select_backward::name, select_backward::overload_name)
      .typed<select_backward::schema>();
}

// aten::select_backward(Tensor grad_output, int[] input_sizes, int dim, int index) -> Tensor
at::Tensor select_backward::call(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t index) {
    static auto op = create_select_backward_typed_handle();
    return op.call(grad_output, input_sizes, dim, index);
}

// aten::select_backward(Tensor grad_output, int[] input_sizes, int dim, int index) -> Tensor
at::Tensor select_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t index) {
    static auto op = create_select_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(selu, name, "aten::selu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(selu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(selu, schema_str, "selu(Tensor self) -> Tensor")

// aten::selu(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<selu::schema> create_selu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(selu::name, selu::overload_name)
      .typed<selu::schema>();
}

// aten::selu(Tensor self) -> Tensor
at::Tensor selu::call(const at::Tensor & self) {
    static auto op = create_selu_typed_handle();
    return op.call(self);
}

// aten::selu(Tensor self) -> Tensor
at::Tensor selu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_selu_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(selu_, name, "aten::selu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(selu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(selu_, schema_str, "selu_(Tensor(a!) self) -> Tensor(a!)")

// aten::selu_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<selu_::schema> create_selu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(selu_::name, selu_::overload_name)
      .typed<selu_::schema>();
}

// aten::selu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & selu_::call(at::Tensor & self) {
    static auto op = create_selu__typed_handle();
    return op.call(self);
}

// aten::selu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & selu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_selu__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu, name, "aten::celu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu, schema_str, "celu(Tensor self, Scalar alpha=1.0) -> Tensor")

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<celu::schema> create_celu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(celu::name, celu::overload_name)
      .typed<celu::schema>();
}

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
at::Tensor celu::call(const at::Tensor & self, const at::Scalar & alpha) {
    static auto op = create_celu_typed_handle();
    return op.call(self, alpha);
}

// aten::celu(Tensor self, Scalar alpha=1.0) -> Tensor
at::Tensor celu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha) {
    static auto op = create_celu_typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_, name, "aten::celu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(celu_, schema_str, "celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)")

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<celu_::schema> create_celu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(celu_::name, celu_::overload_name)
      .typed<celu_::schema>();
}

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
at::Tensor & celu_::call(at::Tensor & self, const at::Scalar & alpha) {
    static auto op = create_celu__typed_handle();
    return op.call(self, alpha);
}

// aten::celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
at::Tensor & celu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha) {
    static auto op = create_celu__typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu, name, "aten::silu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu, schema_str, "silu(Tensor self) -> Tensor")

// aten::silu(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<silu::schema> create_silu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu::name, silu::overload_name)
      .typed<silu::schema>();
}

// aten::silu(Tensor self) -> Tensor
at::Tensor silu::call(const at::Tensor & self) {
    static auto op = create_silu_typed_handle();
    return op.call(self);
}

// aten::silu(Tensor self) -> Tensor
at::Tensor silu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_silu_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_, name, "aten::silu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_, schema_str, "silu_(Tensor(a!) self) -> Tensor(a!)")

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<silu_::schema> create_silu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu_::name, silu_::overload_name)
      .typed<silu_::schema>();
}

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & silu_::call(at::Tensor & self) {
    static auto op = create_silu__typed_handle();
    return op.call(self);
}

// aten::silu_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & silu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_silu__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_out, name, "aten::silu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_out, schema_str, "silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<silu_out::schema> create_silu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu_out::name, silu_out::overload_name)
      .typed<silu_out::schema>();
}

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & silu_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_silu_out_typed_handle();
    return op.call(self, out);
}

// aten::silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & silu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_silu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_backward_grad_input, name, "aten::silu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_backward_grad_input, schema_str, "silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<silu_backward_grad_input::schema> create_silu_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu_backward_grad_input::name, silu_backward_grad_input::overload_name)
      .typed<silu_backward_grad_input::schema>();
}

// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & silu_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_silu_backward_grad_input_typed_handle();
    return op.call(grad_output, self, grad_input);
}

// aten::silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & silu_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_silu_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_backward, name, "aten::silu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(silu_backward, schema_str, "silu_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<silu_backward::schema> create_silu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(silu_backward::name, silu_backward::overload_name)
      .typed<silu_backward::schema>();
}

// aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor silu_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_silu_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::silu_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor silu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_silu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish, name, "aten::mish")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish, schema_str, "mish(Tensor self) -> Tensor")

// aten::mish(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mish::schema> create_mish_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish::name, mish::overload_name)
      .typed<mish::schema>();
}

// aten::mish(Tensor self) -> Tensor
at::Tensor mish::call(const at::Tensor & self) {
    static auto op = create_mish_typed_handle();
    return op.call(self);
}

// aten::mish(Tensor self) -> Tensor
at::Tensor mish::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_mish_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_, name, "aten::mish_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_, schema_str, "mish_(Tensor(a!) self) -> Tensor(a!)")

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mish_::schema> create_mish__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish_::name, mish_::overload_name)
      .typed<mish_::schema>();
}

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & mish_::call(at::Tensor & self) {
    static auto op = create_mish__typed_handle();
    return op.call(self);
}

// aten::mish_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & mish_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_mish__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_out, name, "aten::mish")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_out, schema_str, "mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mish_out::schema> create_mish_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish_out::name, mish_out::overload_name)
      .typed<mish_out::schema>();
}

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mish_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_mish_out_typed_handle();
    return op.call(self, out);
}

// aten::mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mish_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_mish_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_backward, name, "aten::mish_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mish_backward, schema_str, "mish_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mish_backward::schema> create_mish_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mish_backward::name, mish_backward::overload_name)
      .typed<mish_backward::schema>();
}

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor mish_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_mish_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::mish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor mish_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_mish_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid, name, "aten::sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid, schema_str, "sigmoid(Tensor self) -> Tensor")

// aten::sigmoid(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid::schema> create_sigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid::name, sigmoid::overload_name)
      .typed<sigmoid::schema>();
}

// aten::sigmoid(Tensor self) -> Tensor
at::Tensor sigmoid::call(const at::Tensor & self) {
    static auto op = create_sigmoid_typed_handle();
    return op.call(self);
}

// aten::sigmoid(Tensor self) -> Tensor
at::Tensor sigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_, name, "aten::sigmoid_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_, schema_str, "sigmoid_(Tensor(a!) self) -> Tensor(a!)")

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid_::schema> create_sigmoid__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid_::name, sigmoid_::overload_name)
      .typed<sigmoid_::schema>();
}

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sigmoid_::call(at::Tensor & self) {
    static auto op = create_sigmoid__typed_handle();
    return op.call(self);
}

// aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sigmoid_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sigmoid__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_out, name, "aten::sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_out, schema_str, "sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid_out::schema> create_sigmoid_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid_out::name, sigmoid_out::overload_name)
      .typed<sigmoid_out::schema>();
}

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sigmoid_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sigmoid_out_typed_handle();
    return op.call(self, out);
}

// aten::sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sigmoid_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sigmoid_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit, name, "aten::logit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit, schema_str, "logit(Tensor self, float? eps=None) -> Tensor")

// aten::logit(Tensor self, float? eps=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logit::schema> create_logit_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit::name, logit::overload_name)
      .typed<logit::schema>();
}

// aten::logit(Tensor self, float? eps=None) -> Tensor
at::Tensor logit::call(const at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_logit_typed_handle();
    return op.call(self, eps);
}

// aten::logit(Tensor self, float? eps=None) -> Tensor
at::Tensor logit::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_logit_typed_handle();
    return op.redispatch(dispatchKeySet, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_, name, "aten::logit_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_, schema_str, "logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)")

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logit_::schema> create_logit__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_::name, logit_::overload_name)
      .typed<logit_::schema>();
}

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
at::Tensor & logit_::call(at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_logit__typed_handle();
    return op.call(self, eps);
}

// aten::logit_(Tensor(a!) self, float? eps=None) -> Tensor(a!)
at::Tensor & logit_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_logit__typed_handle();
    return op.redispatch(dispatchKeySet, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_out, name, "aten::logit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_out, schema_str, "logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logit_out::schema> create_logit_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_out::name, logit_out::overload_name)
      .typed<logit_out::schema>();
}

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logit_out::call(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    static auto op = create_logit_out_typed_handle();
    return op.call(self, eps, out);
}

// aten::logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & logit_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    static auto op = create_logit_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, eps, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin, name, "aten::sin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin, schema_str, "sin(Tensor self) -> Tensor")

// aten::sin(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sin::schema> create_sin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sin::name, sin::overload_name)
      .typed<sin::schema>();
}

// aten::sin(Tensor self) -> Tensor
at::Tensor sin::call(const at::Tensor & self) {
    static auto op = create_sin_typed_handle();
    return op.call(self);
}

// aten::sin(Tensor self) -> Tensor
at::Tensor sin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sin_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin_, name, "aten::sin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin_, schema_str, "sin_(Tensor(a!) self) -> Tensor(a!)")

// aten::sin_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sin_::schema> create_sin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sin_::name, sin_::overload_name)
      .typed<sin_::schema>();
}

// aten::sin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sin_::call(at::Tensor & self) {
    static auto op = create_sin__typed_handle();
    return op.call(self);
}

// aten::sin_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin_out, name, "aten::sin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sin_out, schema_str, "sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sin_out::schema> create_sin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sin_out::name, sin_out::overload_name)
      .typed<sin_out::schema>();
}

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sin_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sin_out_typed_handle();
    return op.call(self, out);
}

// aten::sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc, name, "aten::sinc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc, schema_str, "sinc(Tensor self) -> Tensor")

// aten::sinc(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sinc::schema> create_sinc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinc::name, sinc::overload_name)
      .typed<sinc::schema>();
}

// aten::sinc(Tensor self) -> Tensor
at::Tensor sinc::call(const at::Tensor & self) {
    static auto op = create_sinc_typed_handle();
    return op.call(self);
}

// aten::sinc(Tensor self) -> Tensor
at::Tensor sinc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sinc_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc_, name, "aten::sinc_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc_, schema_str, "sinc_(Tensor(a!) self) -> Tensor(a!)")

// aten::sinc_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sinc_::schema> create_sinc__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinc_::name, sinc_::overload_name)
      .typed<sinc_::schema>();
}

// aten::sinc_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sinc_::call(at::Tensor & self) {
    static auto op = create_sinc__typed_handle();
    return op.call(self);
}

// aten::sinc_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sinc_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sinc__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc_out, name, "aten::sinc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinc_out, schema_str, "sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sinc_out::schema> create_sinc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinc_out::name, sinc_out::overload_name)
      .typed<sinc_out::schema>();
}

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sinc_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sinc_out_typed_handle();
    return op.call(self, out);
}

// aten::sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sinc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sinc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh, name, "aten::sinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh, schema_str, "sinh(Tensor self) -> Tensor")

// aten::sinh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sinh::schema> create_sinh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinh::name, sinh::overload_name)
      .typed<sinh::schema>();
}

// aten::sinh(Tensor self) -> Tensor
at::Tensor sinh::call(const at::Tensor & self) {
    static auto op = create_sinh_typed_handle();
    return op.call(self);
}

// aten::sinh(Tensor self) -> Tensor
at::Tensor sinh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sinh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_, name, "aten::sinh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_, schema_str, "sinh_(Tensor(a!) self) -> Tensor(a!)")

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sinh_::schema> create_sinh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinh_::name, sinh_::overload_name)
      .typed<sinh_::schema>();
}

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sinh_::call(at::Tensor & self) {
    static auto op = create_sinh__typed_handle();
    return op.call(self);
}

// aten::sinh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sinh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sinh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_out, name, "aten::sinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sinh_out, schema_str, "sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sinh_out::schema> create_sinh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sinh_out::name, sinh_out::overload_name)
      .typed<sinh_out::schema>();
}

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sinh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sinh_out_typed_handle();
    return op.call(self, out);
}

// aten::sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sinh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sinh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach, name, "aten::detach")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach, schema_str, "detach(Tensor(a) self) -> Tensor(a)")

// aten::detach(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<detach::schema> create_detach_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(detach::name, detach::overload_name)
      .typed<detach::schema>();
}

// aten::detach(Tensor(a) self) -> Tensor(a)
at::Tensor detach::call(const at::Tensor & self) {
    static auto op = create_detach_typed_handle();
    return op.call(self);
}

// aten::detach(Tensor(a) self) -> Tensor(a)
at::Tensor detach::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_detach_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_, name, "aten::detach_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(detach_, schema_str, "detach_(Tensor(a!) self) -> Tensor(a!)")

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<detach_::schema> create_detach__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(detach_::name, detach_::overload_name)
      .typed<detach_::schema>();
}

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & detach_::call(at::Tensor & self) {
    static auto op = create_detach__typed_handle();
    return op.call(self);
}

// aten::detach_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & detach_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_detach__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_int, name, "aten::size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_int, schema_str, "size.int(Tensor self, int dim) -> int")

// aten::size.int(Tensor self, int dim) -> int
static C10_NOINLINE c10::TypedOperatorHandle<size_int::schema> create_size_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(size_int::name, size_int::overload_name)
      .typed<size_int::schema>();
}

// aten::size.int(Tensor self, int dim) -> int
int64_t size_int::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_size_int_typed_handle();
    return op.call(self, dim);
}

// aten::size.int(Tensor self, int dim) -> int
int64_t size_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_size_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_Dimname, name, "aten::size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(size_Dimname, schema_str, "size.Dimname(Tensor self, Dimname dim) -> int")

// aten::size.Dimname(Tensor self, Dimname dim) -> int
static C10_NOINLINE c10::TypedOperatorHandle<size_Dimname::schema> create_size_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(size_Dimname::name, size_Dimname::overload_name)
      .typed<size_Dimname::schema>();
}

// aten::size.Dimname(Tensor self, Dimname dim) -> int
int64_t size_Dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_size_Dimname_typed_handle();
    return op.call(self, dim);
}

// aten::size.Dimname(Tensor self, Dimname dim) -> int
int64_t size_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_size_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_Tensor, name, "aten::slice")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_Tensor, schema_str, "slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)")

// aten::slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<slice_Tensor::schema> create_slice_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slice_Tensor::name, slice_Tensor::overload_name)
      .typed<slice_Tensor::schema>();
}

// aten::slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)
at::Tensor slice_Tensor::call(const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
    static auto op = create_slice_Tensor_typed_handle();
    return op.call(self, dim, start, end, step);
}

// aten::slice.Tensor(Tensor(a) self, int dim=0, int? start=None, int? end=None, int step=1) -> Tensor(a)
at::Tensor slice_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
    static auto op = create_slice_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, start, end, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward, name, "aten::slice_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slice_backward, schema_str, "slice_backward(Tensor grad_output, int[] input_sizes, int dim, int start, int end, int step) -> Tensor")

// aten::slice_backward(Tensor grad_output, int[] input_sizes, int dim, int start, int end, int step) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slice_backward::schema> create_slice_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slice_backward::name, slice_backward::overload_name)
      .typed<slice_backward::schema>();
}

// aten::slice_backward(Tensor grad_output, int[] input_sizes, int dim, int start, int end, int step) -> Tensor
at::Tensor slice_backward::call(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
    static auto op = create_slice_backward_typed_handle();
    return op.call(grad_output, input_sizes, dim, start, end, step);
}

// aten::slice_backward(Tensor grad_output, int[] input_sizes, int dim, int start, int end, int step) -> Tensor
at::Tensor slice_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
    static auto op = create_slice_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_sizes, dim, start, end, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slogdet, name, "aten::slogdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slogdet, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slogdet, schema_str, "slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)")

// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
static C10_NOINLINE c10::TypedOperatorHandle<slogdet::schema> create_slogdet_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slogdet::name, slogdet::overload_name)
      .typed<slogdet::schema>();
}

// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
::std::tuple<at::Tensor,at::Tensor> slogdet::call(const at::Tensor & self) {
    static auto op = create_slogdet_typed_handle();
    return op.call(self);
}

// aten::slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
::std::tuple<at::Tensor,at::Tensor> slogdet::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_slogdet_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smm, name, "aten::smm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smm, schema_str, "smm(Tensor self, Tensor mat2) -> Tensor")

// aten::smm(Tensor self, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<smm::schema> create_smm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smm::name, smm::overload_name)
      .typed<smm::schema>();
}

// aten::smm(Tensor self, Tensor mat2) -> Tensor
at::Tensor smm::call(const at::Tensor & self, const at::Tensor & mat2) {
    static auto op = create_smm_typed_handle();
    return op.call(self, mat2);
}

// aten::smm(Tensor self, Tensor mat2) -> Tensor
at::Tensor smm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat2) {
    static auto op = create_smm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int, name, "aten::softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_int, schema_str, "softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softmax_int::schema> create_softmax_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softmax_int::name, softmax_int::overload_name)
      .typed<softmax_int::schema>();
}

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_int::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_softmax_int_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_softmax_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_Dimname, name, "aten::softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softmax_Dimname, schema_str, "softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softmax_Dimname::schema> create_softmax_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softmax_Dimname::name, softmax_Dimname::overload_name)
      .typed<softmax_Dimname::schema>();
}

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_Dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_softmax_Dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor softmax_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_softmax_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax, name, "aten::_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax, schema_str, "_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_softmax::schema> create__softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax::name, _softmax::overload_name)
      .typed<_softmax::schema>();
}

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _softmax::call(const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__softmax_typed_handle();
    return op.call(self, dim, half_to_float);
}

// aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_out, name, "aten::_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_out, schema_str, "_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_softmax_out::schema> create__softmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax_out::name, _softmax_out::overload_name)
      .typed<_softmax_out::schema>();
}

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _softmax_out::call(const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    static auto op = create__softmax_out_typed_handle();
    return op.call(self, dim, half_to_float, out);
}

// aten::_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _softmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float, at::Tensor & out) {
    static auto op = create__softmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data, name, "aten::_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data, schema_str, "_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor")

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_softmax_backward_data::schema> create__softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax_backward_data::name, _softmax_backward_data::overload_name)
      .typed<_softmax_backward_data::schema>();
}

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, self);
}

// aten::_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data_out, name, "aten::_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_softmax_backward_data_out, schema_str, "_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_softmax_backward_data_out::schema> create__softmax_backward_data_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_softmax_backward_data_out::name, _softmax_backward_data_out::overload_name)
      .typed<_softmax_backward_data_out::schema>();
}

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & _softmax_backward_data_out::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create__softmax_backward_data_out_typed_handle();
    return op.call(grad_output, output, dim, self, grad_input);
}

// aten::_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & _softmax_backward_data_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create__softmax_backward_data_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor, name, "aten::unsafe_split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_Tensor, schema_str, "unsafe_split.Tensor(Tensor self, int split_size, int dim=0) -> Tensor[]")

// aten::unsafe_split.Tensor(Tensor self, int split_size, int dim=0) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<unsafe_split_Tensor::schema> create_unsafe_split_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsafe_split_Tensor::name, unsafe_split_Tensor::overload_name)
      .typed<unsafe_split_Tensor::schema>();
}

// aten::unsafe_split.Tensor(Tensor self, int split_size, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_split_Tensor::call(const at::Tensor & self, int64_t split_size, int64_t dim) {
    static auto op = create_unsafe_split_Tensor_typed_handle();
    return op.call(self, split_size, dim);
}

// aten::unsafe_split.Tensor(Tensor self, int split_size, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_split_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t split_size, int64_t dim) {
    static auto op = create_unsafe_split_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_size, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_Tensor, name, "aten::split")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_Tensor, schema_str, "split.Tensor(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]")

// aten::split.Tensor(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<split_Tensor::schema> create_split_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(split_Tensor::name, split_Tensor::overload_name)
      .typed<split_Tensor::schema>();
}

// aten::split.Tensor(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> split_Tensor::call(const at::Tensor & self, int64_t split_size, int64_t dim) {
    static auto op = create_split_Tensor_typed_handle();
    return op.call(self, split_size, dim);
}

// aten::split.Tensor(Tensor(a) self, int split_size, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> split_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t split_size, int64_t dim) {
    static auto op = create_split_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_size, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_with_sizes, name, "aten::unsafe_split_with_sizes")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_with_sizes, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsafe_split_with_sizes, schema_str, "unsafe_split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]")

// aten::unsafe_split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<unsafe_split_with_sizes::schema> create_unsafe_split_with_sizes_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsafe_split_with_sizes::name, unsafe_split_with_sizes::overload_name)
      .typed<unsafe_split_with_sizes::schema>();
}

// aten::unsafe_split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_split_with_sizes::call(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
    static auto op = create_unsafe_split_with_sizes_typed_handle();
    return op.call(self, split_sizes, dim);
}

// aten::unsafe_split_with_sizes(Tensor self, int[] split_sizes, int dim=0) -> Tensor[]
::std::vector<at::Tensor> unsafe_split_with_sizes::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
    static auto op = create_unsafe_split_with_sizes_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_sizes, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_with_sizes, name, "aten::split_with_sizes")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_with_sizes, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(split_with_sizes, schema_str, "split_with_sizes(Tensor(a) self, int[] split_sizes, int dim=0) -> Tensor(a)[]")

// aten::split_with_sizes(Tensor(a) self, int[] split_sizes, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<split_with_sizes::schema> create_split_with_sizes_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(split_with_sizes::name, split_with_sizes::overload_name)
      .typed<split_with_sizes::schema>();
}

// aten::split_with_sizes(Tensor(a) self, int[] split_sizes, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> split_with_sizes::call(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
    static auto op = create_split_with_sizes_typed_handle();
    return op.call(self, split_sizes, dim);
}

// aten::split_with_sizes(Tensor(a) self, int[] split_sizes, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> split_with_sizes::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
    static auto op = create_split_with_sizes_typed_handle();
    return op.redispatch(dispatchKeySet, self, split_sizes, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_int, name, "aten::hsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_int, schema_str, "hsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]")

// aten::hsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<hsplit_int::schema> create_hsplit_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hsplit_int::name, hsplit_int::overload_name)
      .typed<hsplit_int::schema>();
}

// aten::hsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_int::call(const at::Tensor & self, int64_t sections) {
    static auto op = create_hsplit_int_typed_handle();
    return op.call(self, sections);
}

// aten::hsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
    static auto op = create_hsplit_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, sections);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_array, name, "aten::hsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_array, overload_name, "array")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hsplit_array, schema_str, "hsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]")

// aten::hsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<hsplit_array::schema> create_hsplit_array_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hsplit_array::name, hsplit_array::overload_name)
      .typed<hsplit_array::schema>();
}

// aten::hsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_array::call(const at::Tensor & self, at::IntArrayRef indices) {
    static auto op = create_hsplit_array_typed_handle();
    return op.call(self, indices);
}

// aten::hsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> hsplit_array::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
    static auto op = create_hsplit_array_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vsplit_int, name, "aten::vsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vsplit_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vsplit_int, schema_str, "vsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]")

// aten::vsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<vsplit_int::schema> create_vsplit_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vsplit_int::name, vsplit_int::overload_name)
      .typed<vsplit_int::schema>();
}

// aten::vsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> vsplit_int::call(const at::Tensor & self, int64_t sections) {
    static auto op = create_vsplit_int_typed_handle();
    return op.call(self, sections);
}

// aten::vsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> vsplit_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
    static auto op = create_vsplit_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, sections);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vsplit_array, name, "aten::vsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vsplit_array, overload_name, "array")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vsplit_array, schema_str, "vsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]")

// aten::vsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<vsplit_array::schema> create_vsplit_array_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vsplit_array::name, vsplit_array::overload_name)
      .typed<vsplit_array::schema>();
}

// aten::vsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> vsplit_array::call(const at::Tensor & self, at::IntArrayRef indices) {
    static auto op = create_vsplit_array_typed_handle();
    return op.call(self, indices);
}

// aten::vsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> vsplit_array::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
    static auto op = create_vsplit_array_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_int, name, "aten::dsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_int, schema_str, "dsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]")

// aten::dsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<dsplit_int::schema> create_dsplit_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dsplit_int::name, dsplit_int::overload_name)
      .typed<dsplit_int::schema>();
}

// aten::dsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_int::call(const at::Tensor & self, int64_t sections) {
    static auto op = create_dsplit_int_typed_handle();
    return op.call(self, sections);
}

// aten::dsplit.int(Tensor(a) self, int sections) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sections) {
    static auto op = create_dsplit_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, sections);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_array, name, "aten::dsplit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_array, overload_name, "array")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dsplit_array, schema_str, "dsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]")

// aten::dsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<dsplit_array::schema> create_dsplit_array_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dsplit_array::name, dsplit_array::overload_name)
      .typed<dsplit_array::schema>();
}

// aten::dsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_array::call(const at::Tensor & self, at::IntArrayRef indices) {
    static auto op = create_dsplit_array_typed_handle();
    return op.call(self, indices);
}

// aten::dsplit.array(Tensor(a) self, int[] indices) -> Tensor(a)[]
::std::vector<at::Tensor> dsplit_array::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef indices) {
    static auto op = create_dsplit_array_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze, name, "aten::squeeze")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze, schema_str, "squeeze(Tensor(a) self) -> Tensor(a)")

// aten::squeeze(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze::schema> create_squeeze_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze::name, squeeze::overload_name)
      .typed<squeeze::schema>();
}

// aten::squeeze(Tensor(a) self) -> Tensor(a)
at::Tensor squeeze::call(const at::Tensor & self) {
    static auto op = create_squeeze_typed_handle();
    return op.call(self);
}

// aten::squeeze(Tensor(a) self) -> Tensor(a)
at::Tensor squeeze::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_squeeze_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_dim, name, "aten::squeeze")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_dim, schema_str, "squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)")

// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_dim::schema> create_squeeze_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_dim::name, squeeze_dim::overload_name)
      .typed<squeeze_dim::schema>();
}

// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
at::Tensor squeeze_dim::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_squeeze_dim_typed_handle();
    return op.call(self, dim);
}

// aten::squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)
at::Tensor squeeze_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_squeeze_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_dimname, name, "aten::squeeze")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_dimname, schema_str, "squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)")

// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_dimname::schema> create_squeeze_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_dimname::name, squeeze_dimname::overload_name)
      .typed<squeeze_dimname::schema>();
}

// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)
at::Tensor squeeze_dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_squeeze_dimname_typed_handle();
    return op.call(self, dim);
}

// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -> Tensor(a)
at::Tensor squeeze_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_squeeze_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_, name, "aten::squeeze_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze_, schema_str, "squeeze_(Tensor(a!) self) -> Tensor(a!)")

// aten::squeeze_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze_::schema> create_squeeze__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze_::name, squeeze_::overload_name)
      .typed<squeeze_::schema>();
}

// aten::squeeze_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & squeeze_::call(at::Tensor & self) {
    static auto op = create_squeeze__typed_handle();
    return op.call(self);
}

// aten::squeeze_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & squeeze_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_squeeze__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze__dim, name, "aten::squeeze_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze__dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze__dim, schema_str, "squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)")

// aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze__dim::schema> create_squeeze__dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze__dim::name, squeeze__dim::overload_name)
      .typed<squeeze__dim::schema>();
}

// aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)
at::Tensor & squeeze__dim::call(at::Tensor & self, int64_t dim) {
    static auto op = create_squeeze__dim_typed_handle();
    return op.call(self, dim);
}

// aten::squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)
at::Tensor & squeeze__dim::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim) {
    static auto op = create_squeeze__dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze__dimname, name, "aten::squeeze_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze__dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(squeeze__dimname, schema_str, "squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)")

// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<squeeze__dimname::schema> create_squeeze__dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(squeeze__dimname::name, squeeze__dimname::overload_name)
      .typed<squeeze__dimname::schema>();
}

// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)
at::Tensor & squeeze__dimname::call(at::Tensor & self, at::Dimname dim) {
    static auto op = create_squeeze__dimname_typed_handle();
    return op.call(self, dim);
}

// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -> Tensor(a!)
at::Tensor & squeeze__dimname::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim) {
    static auto op = create_squeeze__dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sspaddmm, name, "aten::sspaddmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sspaddmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sspaddmm, schema_str, "sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sspaddmm::schema> create_sspaddmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sspaddmm::name, sspaddmm::overload_name)
      .typed<sspaddmm::schema>();
}

// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor sspaddmm::call(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_sspaddmm_typed_handle();
    return op.call(self, mat1, mat2, beta, alpha);
}

// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor sspaddmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_sspaddmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sspaddmm_out, name, "aten::sspaddmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sspaddmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sspaddmm_out, schema_str, "sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sspaddmm_out::schema> create_sspaddmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sspaddmm_out::name, sspaddmm_out::overload_name)
      .typed<sspaddmm_out::schema>();
}

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sspaddmm_out::call(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_sspaddmm_out_typed_handle();
    return op.call(self, mat1, mat2, beta, alpha, out);
}

// aten::sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sspaddmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_sspaddmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack, name, "aten::stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack, schema_str, "stack(Tensor[] tensors, int dim=0) -> Tensor")

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<stack::schema> create_stack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stack::name, stack::overload_name)
      .typed<stack::schema>();
}

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor stack::call(at::TensorList tensors, int64_t dim) {
    static auto op = create_stack_typed_handle();
    return op.call(tensors, dim);
}

// aten::stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor stack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    static auto op = create_stack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack_out, name, "aten::stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stack_out, schema_str, "stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<stack_out::schema> create_stack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stack_out::name, stack_out::overload_name)
      .typed<stack_out::schema>();
}

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & stack_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create_stack_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & stack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create_stack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack, name, "aten::_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack, schema_str, "_stack(Tensor[] tensors, int dim=0) -> Tensor")

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_stack::schema> create__stack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_stack::name, _stack::overload_name)
      .typed<_stack::schema>();
}

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor _stack::call(at::TensorList tensors, int64_t dim) {
    static auto op = create__stack_typed_handle();
    return op.call(tensors, dim);
}

// aten::_stack(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor _stack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    static auto op = create__stack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack_out, name, "aten::_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_stack_out, schema_str, "_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_stack_out::schema> create__stack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_stack_out::name, _stack_out::overload_name)
      .typed<_stack_out::schema>();
}

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _stack_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create__stack_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::_stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _stack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create__stack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hstack, name, "aten::hstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hstack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hstack, schema_str, "hstack(Tensor[] tensors) -> Tensor")

// aten::hstack(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hstack::schema> create_hstack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hstack::name, hstack::overload_name)
      .typed<hstack::schema>();
}

// aten::hstack(Tensor[] tensors) -> Tensor
at::Tensor hstack::call(at::TensorList tensors) {
    static auto op = create_hstack_typed_handle();
    return op.call(tensors);
}

// aten::hstack(Tensor[] tensors) -> Tensor
at::Tensor hstack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_hstack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hstack_out, name, "aten::hstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hstack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hstack_out, schema_str, "hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hstack_out::schema> create_hstack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hstack_out::name, hstack_out::overload_name)
      .typed<hstack_out::schema>();
}

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hstack_out::call(at::TensorList tensors, at::Tensor & out) {
    static auto op = create_hstack_out_typed_handle();
    return op.call(tensors, out);
}

// aten::hstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hstack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    static auto op = create_hstack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack, name, "aten::vstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack, schema_str, "vstack(Tensor[] tensors) -> Tensor")

// aten::vstack(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<vstack::schema> create_vstack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vstack::name, vstack::overload_name)
      .typed<vstack::schema>();
}

// aten::vstack(Tensor[] tensors) -> Tensor
at::Tensor vstack::call(at::TensorList tensors) {
    static auto op = create_vstack_typed_handle();
    return op.call(tensors);
}

// aten::vstack(Tensor[] tensors) -> Tensor
at::Tensor vstack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_vstack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack_out, name, "aten::vstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vstack_out, schema_str, "vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<vstack_out::schema> create_vstack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vstack_out::name, vstack_out::overload_name)
      .typed<vstack_out::schema>();
}

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & vstack_out::call(at::TensorList tensors, at::Tensor & out) {
    static auto op = create_vstack_out_typed_handle();
    return op.call(tensors, out);
}

// aten::vstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & vstack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    static auto op = create_vstack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dstack, name, "aten::dstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dstack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dstack, schema_str, "dstack(Tensor[] tensors) -> Tensor")

// aten::dstack(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<dstack::schema> create_dstack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dstack::name, dstack::overload_name)
      .typed<dstack::schema>();
}

// aten::dstack(Tensor[] tensors) -> Tensor
at::Tensor dstack::call(at::TensorList tensors) {
    static auto op = create_dstack_typed_handle();
    return op.call(tensors);
}

// aten::dstack(Tensor[] tensors) -> Tensor
at::Tensor dstack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_dstack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dstack_out, name, "aten::dstack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dstack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dstack_out, schema_str, "dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<dstack_out::schema> create_dstack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dstack_out::name, dstack_out::overload_name)
      .typed<dstack_out::schema>();
}

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & dstack_out::call(at::TensorList tensors, at::Tensor & out) {
    static auto op = create_dstack_out_typed_handle();
    return op.call(tensors, out);
}

// aten::dstack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & dstack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    static auto op = create_dstack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft, name, "aten::stft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stft, schema_str, "stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor")

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<stft::schema> create_stft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stft::name, stft::overload_name)
      .typed<stft::schema>();
}

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
at::Tensor stft::call(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    static auto op = create_stft_typed_handle();
    return op.call(self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}

// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
at::Tensor stft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool normalized, c10::optional<bool> onesided, c10::optional<bool> return_complex) {
    static auto op = create_stft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n_fft, hop_length, win_length, window, normalized, onesided, return_complex);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(istft, name, "aten::istft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(istft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(istft, schema_str, "istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor")

// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<istft::schema> create_istft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(istft::name, istft::overload_name)
      .typed<istft::schema>();
}

// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor
at::Tensor istft::call(const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
    static auto op = create_istft_typed_handle();
    return op.call(self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
}

// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -> Tensor
at::Tensor istft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const c10::optional<at::Tensor> & window, bool center, bool normalized, c10::optional<bool> onesided, c10::optional<int64_t> length, bool return_complex) {
    static auto op = create_istft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stride_int, name, "aten::stride")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stride_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stride_int, schema_str, "stride.int(Tensor self, int dim) -> int")

// aten::stride.int(Tensor self, int dim) -> int
static C10_NOINLINE c10::TypedOperatorHandle<stride_int::schema> create_stride_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stride_int::name, stride_int::overload_name)
      .typed<stride_int::schema>();
}

// aten::stride.int(Tensor self, int dim) -> int
int64_t stride_int::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_stride_int_typed_handle();
    return op.call(self, dim);
}

// aten::stride.int(Tensor self, int dim) -> int
int64_t stride_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_stride_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stride_Dimname, name, "aten::stride")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stride_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(stride_Dimname, schema_str, "stride.Dimname(Tensor self, Dimname dim) -> int")

// aten::stride.Dimname(Tensor self, Dimname dim) -> int
static C10_NOINLINE c10::TypedOperatorHandle<stride_Dimname::schema> create_stride_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(stride_Dimname::name, stride_Dimname::overload_name)
      .typed<stride_Dimname::schema>();
}

// aten::stride.Dimname(Tensor self, Dimname dim) -> int
int64_t stride_Dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_stride_Dimname_typed_handle();
    return op.call(self, dim);
}

// aten::stride.Dimname(Tensor self, Dimname dim) -> int
int64_t stride_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_stride_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum, name, "aten::sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum, schema_str, "sum(Tensor self, *, ScalarType? dtype=None) -> Tensor")

// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sum::schema> create_sum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum::name, sum::overload_name)
      .typed<sum::schema>();
}

// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor sum::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_sum_typed_handle();
    return op.call(self, dtype);
}

// aten::sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor sum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_sum_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_dim_IntList, name, "aten::sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_dim_IntList, overload_name, "dim_IntList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_dim_IntList, schema_str, "sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sum_dim_IntList::schema> create_sum_dim_IntList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum_dim_IntList::name, sum_dim_IntList::overload_name)
      .typed<sum_dim_IntList::schema>();
}

// aten::sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor sum_dim_IntList::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_sum_dim_IntList_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor sum_dim_IntList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_sum_dim_IntList_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_dim_DimnameList, name, "aten::sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_dim_DimnameList, overload_name, "dim_DimnameList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_dim_DimnameList, schema_str, "sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sum_dim_DimnameList::schema> create_sum_dim_DimnameList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum_dim_DimnameList::name, sum_dim_DimnameList::overload_name)
      .typed<sum_dim_DimnameList::schema>();
}

// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor sum_dim_DimnameList::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_sum_dim_DimnameList_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor sum_dim_DimnameList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_sum_dim_DimnameList_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_IntList_out, name, "aten::sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_IntList_out, overload_name, "IntList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_IntList_out, schema_str, "sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sum_IntList_out::schema> create_sum_IntList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum_IntList_out::name, sum_IntList_out::overload_name)
      .typed<sum_IntList_out::schema>();
}

// aten::sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sum_IntList_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_sum_IntList_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sum_IntList_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_sum_IntList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_DimnameList_out, name, "aten::sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_DimnameList_out, overload_name, "DimnameList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_DimnameList_out, schema_str, "sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sum_DimnameList_out::schema> create_sum_DimnameList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum_DimnameList_out::name, sum_DimnameList_out::overload_name)
      .typed<sum_DimnameList_out::schema>();
}

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sum_DimnameList_out::call(const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_sum_DimnameList_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sum_DimnameList_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_sum_DimnameList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum, name, "aten::nansum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum, schema_str, "nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor")

// aten::nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nansum::schema> create_nansum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nansum::name, nansum::overload_name)
      .typed<nansum::schema>();
}

// aten::nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor nansum::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_nansum_typed_handle();
    return op.call(self, dtype);
}

// aten::nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor nansum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_nansum_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum_dim_IntList, name, "aten::nansum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum_dim_IntList, overload_name, "dim_IntList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum_dim_IntList, schema_str, "nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nansum_dim_IntList::schema> create_nansum_dim_IntList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nansum_dim_IntList::name, nansum_dim_IntList::overload_name)
      .typed<nansum_dim_IntList::schema>();
}

// aten::nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor nansum_dim_IntList::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_nansum_dim_IntList_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor nansum_dim_IntList::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_nansum_dim_IntList_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum_IntList_out, name, "aten::nansum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum_IntList_out, overload_name, "IntList_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nansum_IntList_out, schema_str, "nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nansum_IntList_out::schema> create_nansum_IntList_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nansum_IntList_out::name, nansum_IntList_out::overload_name)
      .typed<nansum_IntList_out::schema>();
}

// aten::nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nansum_IntList_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_nansum_IntList_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nansum_IntList_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_nansum_IntList_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_to_size, name, "aten::sum_to_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_to_size, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sum_to_size, schema_str, "sum_to_size(Tensor self, int[] size) -> Tensor")

// aten::sum_to_size(Tensor self, int[] size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sum_to_size::schema> create_sum_to_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sum_to_size::name, sum_to_size::overload_name)
      .typed<sum_to_size::schema>();
}

// aten::sum_to_size(Tensor self, int[] size) -> Tensor
at::Tensor sum_to_size::call(const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create_sum_to_size_typed_handle();
    return op.call(self, size);
}

// aten::sum_to_size(Tensor self, int[] size) -> Tensor
at::Tensor sum_to_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create_sum_to_size_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt, name, "aten::sqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt, schema_str, "sqrt(Tensor self) -> Tensor")

// aten::sqrt(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sqrt::schema> create_sqrt_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sqrt::name, sqrt::overload_name)
      .typed<sqrt::schema>();
}

// aten::sqrt(Tensor self) -> Tensor
at::Tensor sqrt::call(const at::Tensor & self) {
    static auto op = create_sqrt_typed_handle();
    return op.call(self);
}

// aten::sqrt(Tensor self) -> Tensor
at::Tensor sqrt::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sqrt_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_, name, "aten::sqrt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_, schema_str, "sqrt_(Tensor(a!) self) -> Tensor(a!)")

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sqrt_::schema> create_sqrt__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sqrt_::name, sqrt_::overload_name)
      .typed<sqrt_::schema>();
}

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sqrt_::call(at::Tensor & self) {
    static auto op = create_sqrt__typed_handle();
    return op.call(self);
}

// aten::sqrt_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sqrt_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sqrt__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_out, name, "aten::sqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sqrt_out, schema_str, "sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sqrt_out::schema> create_sqrt_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sqrt_out::name, sqrt_out::overload_name)
      .typed<sqrt_out::schema>();
}

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sqrt_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sqrt_out_typed_handle();
    return op.call(self, out);
}

// aten::sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sqrt_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sqrt_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square, name, "aten::square")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square, schema_str, "square(Tensor self) -> Tensor")

// aten::square(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<square::schema> create_square_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(square::name, square::overload_name)
      .typed<square::schema>();
}

// aten::square(Tensor self) -> Tensor
at::Tensor square::call(const at::Tensor & self) {
    static auto op = create_square_typed_handle();
    return op.call(self);
}

// aten::square(Tensor self) -> Tensor
at::Tensor square::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_square_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_, name, "aten::square_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_, schema_str, "square_(Tensor(a!) self) -> Tensor(a!)")

// aten::square_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<square_::schema> create_square__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(square_::name, square_::overload_name)
      .typed<square_::schema>();
}

// aten::square_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & square_::call(at::Tensor & self) {
    static auto op = create_square__typed_handle();
    return op.call(self);
}

// aten::square_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & square_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_square__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_out, name, "aten::square")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(square_out, schema_str, "square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<square_out::schema> create_square_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(square_out::name, square_out::overload_name)
      .typed<square_out::schema>();
}

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & square_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_square_out_typed_handle();
    return op.call(self, out);
}

// aten::square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & square_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_square_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std, schema_str, "std(Tensor self, bool unbiased=True) -> Tensor")

// aten::std(Tensor self, bool unbiased=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std::schema> create_std_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std::name, std::overload_name)
      .typed<std::schema>();
}

// aten::std(Tensor self, bool unbiased=True) -> Tensor
at::Tensor std::call(const at::Tensor & self, bool unbiased) {
    static auto op = create_std_typed_handle();
    return op.call(self, unbiased);
}

// aten::std(Tensor self, bool unbiased=True) -> Tensor
at::Tensor std::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
    static auto op = create_std_typed_handle();
    return op.redispatch(dispatchKeySet, self, unbiased);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_dim, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_dim, schema_str, "std.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor")

// aten::std.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_dim::schema> create_std_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_dim::name, std_dim::overload_name)
      .typed<std_dim::schema>();
}

// aten::std.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_std_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_std_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction, overload_name, "correction")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction, schema_str, "std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor")

// aten::std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_correction::schema> create_std_correction_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction::name, std_correction::overload_name)
      .typed<std_correction::schema>();
}

// aten::std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor std_correction::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_correction_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor std_correction::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_correction_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean, schema_str, "std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)")

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean::schema> create_std_mean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean::name, std_mean::overload_name)
      .typed<std_mean::schema>();
}

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean::call(const at::Tensor & self, bool unbiased) {
    static auto op = create_std_mean_typed_handle();
    return op.call(self, unbiased);
}

// aten::std_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
    static auto op = create_std_mean_typed_handle();
    return op.redispatch(dispatchKeySet, self, unbiased);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_dim, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_dim, schema_str, "std_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_dim::schema> create_std_mean_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_dim::name, std_mean_dim::overload_name)
      .typed<std_mean_dim::schema>();
}

// aten::std_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_std_mean_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_std_mean_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction, overload_name, "correction")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction, schema_str, "std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_correction::schema> create_std_mean_correction_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_correction::name, std_mean_correction::overload_name)
      .typed<std_mean_correction::schema>();
}

// aten::std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_mean_correction_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_mean_correction_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_names_dim, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_names_dim, schema_str, "std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_names_dim::schema> create_std_mean_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_names_dim::name, std_mean_names_dim::overload_name)
      .typed<std_mean_names_dim::schema>();
}

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_std_mean_names_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_std_mean_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_names, name, "aten::std_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_names, overload_name, "correction_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_mean_correction_names, schema_str, "std_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)")

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<std_mean_correction_names::schema> create_std_mean_correction_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_mean_correction_names::name, std_mean_correction_names::overload_name)
      .typed<std_mean_correction_names::schema>();
}

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction_names::call(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_mean_correction_names_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> std_mean_correction_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_mean_correction_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_out, schema_str, "std.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::std.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_out::schema> create_std_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_out::name, std_out::overload_name)
      .typed<std_out::schema>();
}

// aten::std.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_out::call(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_std_out_typed_handle();
    return op.call(self, dim, unbiased, keepdim, out);
}

// aten::std.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_std_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_out, overload_name, "correction_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_out, schema_str, "std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)")

// aten::std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_correction_out::schema> create_std_correction_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction_out::name, std_correction_out::overload_name)
      .typed<std_correction_out::schema>();
}

// aten::std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_std_correction_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out);
}

// aten::std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_std_correction_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_dim, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_dim, schema_str, "std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor")

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_names_dim::schema> create_std_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_names_dim::name, std_names_dim::overload_name)
      .typed<std_names_dim::schema>();
}

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_std_names_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor std_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_std_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_names_out, schema_str, "std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_names_out::schema> create_std_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_names_out::name, std_names_out::overload_name)
      .typed<std_names_out::schema>();
}

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_names_out::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_std_names_out_typed_handle();
    return op.call(self, dim, unbiased, keepdim, out);
}

// aten::std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_std_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names, overload_name, "correction_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names, schema_str, "std.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor")

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<std_correction_names::schema> create_std_correction_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction_names::name, std_correction_names::overload_name)
      .typed<std_correction_names::schema>();
}

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor std_correction_names::call(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_correction_names_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::std.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor std_correction_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_std_correction_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names_out, name, "aten::std")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names_out, overload_name, "correction_names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(std_correction_names_out, schema_str, "std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)")

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<std_correction_names_out::schema> create_std_correction_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(std_correction_names_out::name, std_correction_names_out::overload_name)
      .typed<std_correction_names_out::schema>();
}

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_names_out::call(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_std_correction_names_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out);
}

// aten::std.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & std_correction_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_std_correction_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod, name, "aten::prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod, schema_str, "prod(Tensor self, *, ScalarType? dtype=None) -> Tensor")

// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<prod::schema> create_prod_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prod::name, prod::overload_name)
      .typed<prod::schema>();
}

// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor prod::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_prod_typed_handle();
    return op.call(self, dtype);
}

// aten::prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
at::Tensor prod::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_prod_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_dim_int, name, "aten::prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_dim_int, overload_name, "dim_int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_dim_int, schema_str, "prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<prod_dim_int::schema> create_prod_dim_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prod_dim_int::name, prod_dim_int::overload_name)
      .typed<prod_dim_int::schema>();
}

// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor prod_dim_int::call(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_prod_dim_int_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor prod_dim_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_prod_dim_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_int_out, name, "aten::prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_int_out, overload_name, "int_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_int_out, schema_str, "prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<prod_int_out::schema> create_prod_int_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prod_int_out::name, prod_int_out::overload_name)
      .typed<prod_int_out::schema>();
}

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & prod_int_out::call(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_prod_int_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & prod_int_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_prod_int_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_dim_Dimname, name, "aten::prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_dim_Dimname, overload_name, "dim_Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_dim_Dimname, schema_str, "prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<prod_dim_Dimname::schema> create_prod_dim_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prod_dim_Dimname::name, prod_dim_Dimname::overload_name)
      .typed<prod_dim_Dimname::schema>();
}

// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor prod_dim_Dimname::call(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_prod_dim_Dimname_typed_handle();
    return op.call(self, dim, keepdim, dtype);
}

// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor prod_dim_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_prod_dim_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_Dimname_out, name, "aten::prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_Dimname_out, overload_name, "Dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(prod_Dimname_out, schema_str, "prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<prod_Dimname_out::schema> create_prod_Dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(prod_Dimname_out::name, prod_Dimname_out::overload_name)
      .typed<prod_Dimname_out::schema>();
}

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & prod_Dimname_out::call(const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_prod_Dimname_out_typed_handle();
    return op.call(self, dim, keepdim, dtype, out);
}

// aten::prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & prod_Dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_prod_Dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t, name, "aten::t")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t, schema_str, "t(Tensor(a) self) -> Tensor(a)")

// aten::t(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<t::schema> create_t_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(t::name, t::overload_name)
      .typed<t::schema>();
}

// aten::t(Tensor(a) self) -> Tensor(a)
at::Tensor t::call(const at::Tensor & self) {
    static auto op = create_t_typed_handle();
    return op.call(self);
}

// aten::t(Tensor(a) self) -> Tensor(a)
at::Tensor t::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_t_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t_, name, "aten::t_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(t_, schema_str, "t_(Tensor(a!) self) -> Tensor(a!)")

// aten::t_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<t_::schema> create_t__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(t_::name, t_::overload_name)
      .typed<t_::schema>();
}

// aten::t_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & t_::call(at::Tensor & self) {
    static auto op = create_t__typed_handle();
    return op.call(self);
}

// aten::t_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & t_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_t__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan, name, "aten::tan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan, schema_str, "tan(Tensor self) -> Tensor")

// aten::tan(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tan::schema> create_tan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tan::name, tan::overload_name)
      .typed<tan::schema>();
}

// aten::tan(Tensor self) -> Tensor
at::Tensor tan::call(const at::Tensor & self) {
    static auto op = create_tan_typed_handle();
    return op.call(self);
}

// aten::tan(Tensor self) -> Tensor
at::Tensor tan::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_tan_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan_, name, "aten::tan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan_, schema_str, "tan_(Tensor(a!) self) -> Tensor(a!)")

// aten::tan_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tan_::schema> create_tan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tan_::name, tan_::overload_name)
      .typed<tan_::schema>();
}

// aten::tan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & tan_::call(at::Tensor & self) {
    static auto op = create_tan__typed_handle();
    return op.call(self);
}

// aten::tan_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & tan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_tan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan_out, name, "aten::tan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tan_out, schema_str, "tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tan_out::schema> create_tan_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tan_out::name, tan_out::overload_name)
      .typed<tan_out::schema>();
}

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tan_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_tan_out_typed_handle();
    return op.call(self, out);
}

// aten::tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tan_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_tan_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh, name, "aten::tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh, schema_str, "tanh(Tensor self) -> Tensor")

// aten::tanh(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tanh::schema> create_tanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh::name, tanh::overload_name)
      .typed<tanh::schema>();
}

// aten::tanh(Tensor self) -> Tensor
at::Tensor tanh::call(const at::Tensor & self) {
    static auto op = create_tanh_typed_handle();
    return op.call(self);
}

// aten::tanh(Tensor self) -> Tensor
at::Tensor tanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_tanh_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_, name, "aten::tanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_, schema_str, "tanh_(Tensor(a!) self) -> Tensor(a!)")

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tanh_::schema> create_tanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh_::name, tanh_::overload_name)
      .typed<tanh_::schema>();
}

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & tanh_::call(at::Tensor & self) {
    static auto op = create_tanh__typed_handle();
    return op.call(self);
}

// aten::tanh_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & tanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_tanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_out, name, "aten::tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_out, schema_str, "tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tanh_out::schema> create_tanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh_out::name, tanh_out::overload_name)
      .typed<tanh_out::schema>();
}

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tanh_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_tanh_out_typed_handle();
    return op.call(self, out);
}

// aten::tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_tanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot, name, "aten::tensordot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot, schema_str, "tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor")

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tensordot::schema> create_tensordot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensordot::name, tensordot::overload_name)
      .typed<tensordot::schema>();
}

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
at::Tensor tensordot::call(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
    static auto op = create_tensordot_typed_handle();
    return op.call(self, other, dims_self, dims_other);
}

// aten::tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -> Tensor
at::Tensor tensordot::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other) {
    static auto op = create_tensordot_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims_self, dims_other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot_out, name, "aten::tensordot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tensordot_out, schema_str, "tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tensordot_out::schema> create_tensordot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tensordot_out::name, tensordot_out::overload_name)
      .typed<tensordot_out::schema>();
}

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tensordot_out::call(const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
    static auto op = create_tensordot_out_typed_handle();
    return op.call(self, other, dims_self, dims_other, out);
}

// aten::tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tensordot_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::IntArrayRef dims_self, at::IntArrayRef dims_other, at::Tensor & out) {
    static auto op = create_tensordot_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims_self, dims_other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold, name, "aten::threshold")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold, schema_str, "threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor")

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<threshold::schema> create_threshold_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold::name, threshold::overload_name)
      .typed<threshold::schema>();
}

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
at::Tensor threshold::call(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    static auto op = create_threshold_typed_handle();
    return op.call(self, threshold, value);
}

// aten::threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
at::Tensor threshold::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    static auto op = create_threshold_typed_handle();
    return op.redispatch(dispatchKeySet, self, threshold, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_, name, "aten::threshold_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_, schema_str, "threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)")

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<threshold_::schema> create_threshold__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold_::name, threshold_::overload_name)
      .typed<threshold_::schema>();
}

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
at::Tensor & threshold_::call(at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    static auto op = create_threshold__typed_handle();
    return op.call(self, threshold, value);
}

// aten::threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
at::Tensor & threshold_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
    static auto op = create_threshold__typed_handle();
    return op.redispatch(dispatchKeySet, self, threshold, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_out, name, "aten::threshold")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_out, schema_str, "threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)")

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<threshold_out::schema> create_threshold_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold_out::name, threshold_out::overload_name)
      .typed<threshold_out::schema>();
}

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & threshold_out::call(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_threshold_out_typed_handle();
    return op.call(self, threshold, value, out);
}

// aten::threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & threshold_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_threshold_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, threshold, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_backward_grad_input, name, "aten::threshold_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_backward_grad_input, schema_str, "threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<threshold_backward_grad_input::schema> create_threshold_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold_backward_grad_input::name, threshold_backward_grad_input::overload_name)
      .typed<threshold_backward_grad_input::schema>();
}

// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & threshold_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
    static auto op = create_threshold_backward_grad_input_typed_handle();
    return op.call(grad_output, self, threshold, grad_input);
}

// aten::threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & threshold_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
    static auto op = create_threshold_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, threshold, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_backward, name, "aten::threshold_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(threshold_backward, schema_str, "threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor")

// aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<threshold_backward::schema> create_threshold_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(threshold_backward::name, threshold_backward::overload_name)
      .typed<threshold_backward::schema>();
}

// aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
at::Tensor threshold_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    static auto op = create_threshold_backward_typed_handle();
    return op.call(grad_output, self, threshold);
}

// aten::threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
at::Tensor threshold_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    static auto op = create_threshold_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, threshold);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tile, name, "aten::tile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tile, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tile, schema_str, "tile(Tensor self, int[] dims) -> Tensor")

// aten::tile(Tensor self, int[] dims) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tile::schema> create_tile_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tile::name, tile::overload_name)
      .typed<tile::schema>();
}

// aten::tile(Tensor self, int[] dims) -> Tensor
at::Tensor tile::call(const at::Tensor & self, at::IntArrayRef dims) {
    static auto op = create_tile_typed_handle();
    return op.call(self, dims);
}

// aten::tile(Tensor self, int[] dims) -> Tensor
at::Tensor tile::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
    static auto op = create_tile_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_int, name, "aten::transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_int, schema_str, "transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)")

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<transpose_int::schema> create_transpose_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(transpose_int::name, transpose_int::overload_name)
      .typed<transpose_int::schema>();
}

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
at::Tensor transpose_int::call(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_transpose_int_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
at::Tensor transpose_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_transpose_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_Dimname, name, "aten::transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_Dimname, schema_str, "transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)")

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<transpose_Dimname::schema> create_transpose_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(transpose_Dimname::name, transpose_Dimname::overload_name)
      .typed<transpose_Dimname::schema>();
}

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
at::Tensor transpose_Dimname::call(const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
    static auto op = create_transpose_Dimname_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -> Tensor(a)
at::Tensor transpose_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim0, at::Dimname dim1) {
    static auto op = create_transpose_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose, name, "aten::_mkldnn_transpose")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose, schema_str, "_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor")

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_mkldnn_transpose::schema> create__mkldnn_transpose_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mkldnn_transpose::name, _mkldnn_transpose::overload_name)
      .typed<_mkldnn_transpose::schema>();
}

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
at::Tensor _mkldnn_transpose::call(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create__mkldnn_transpose_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor
at::Tensor _mkldnn_transpose::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create__mkldnn_transpose_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_, name, "aten::transpose_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(transpose_, schema_str, "transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)")

// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<transpose_::schema> create_transpose__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(transpose_::name, transpose_::overload_name)
      .typed<transpose_::schema>();
}

// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & transpose_::call(at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_transpose__typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & transpose_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_transpose__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_, name, "aten::_mkldnn_transpose_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_mkldnn_transpose_, schema_str, "_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)")

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_mkldnn_transpose_::schema> create__mkldnn_transpose__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_mkldnn_transpose_::name, _mkldnn_transpose_::overload_name)
      .typed<_mkldnn_transpose_::schema>();
}

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & _mkldnn_transpose_::call(at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create__mkldnn_transpose__typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & _mkldnn_transpose_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create__mkldnn_transpose__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(one_hot, name, "aten::one_hot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(one_hot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(one_hot, schema_str, "one_hot(Tensor self, int num_classes=-1) -> Tensor")

// aten::one_hot(Tensor self, int num_classes=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<one_hot::schema> create_one_hot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(one_hot::name, one_hot::overload_name)
      .typed<one_hot::schema>();
}

// aten::one_hot(Tensor self, int num_classes=-1) -> Tensor
at::Tensor one_hot::call(const at::Tensor & self, int64_t num_classes) {
    static auto op = create_one_hot_typed_handle();
    return op.call(self, num_classes);
}

// aten::one_hot(Tensor self, int num_classes=-1) -> Tensor
at::Tensor one_hot::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_classes) {
    static auto op = create_one_hot_typed_handle();
    return op.redispatch(dispatchKeySet, self, num_classes);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip, name, "aten::flip")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flip, schema_str, "flip(Tensor self, int[] dims) -> Tensor")

// aten::flip(Tensor self, int[] dims) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<flip::schema> create_flip_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flip::name, flip::overload_name)
      .typed<flip::schema>();
}

// aten::flip(Tensor self, int[] dims) -> Tensor
at::Tensor flip::call(const at::Tensor & self, at::IntArrayRef dims) {
    static auto op = create_flip_typed_handle();
    return op.call(self, dims);
}

// aten::flip(Tensor self, int[] dims) -> Tensor
at::Tensor flip::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dims) {
    static auto op = create_flip_typed_handle();
    return op.redispatch(dispatchKeySet, self, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fliplr, name, "aten::fliplr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fliplr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fliplr, schema_str, "fliplr(Tensor self) -> Tensor")

// aten::fliplr(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fliplr::schema> create_fliplr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fliplr::name, fliplr::overload_name)
      .typed<fliplr::schema>();
}

// aten::fliplr(Tensor self) -> Tensor
at::Tensor fliplr::call(const at::Tensor & self) {
    static auto op = create_fliplr_typed_handle();
    return op.call(self);
}

// aten::fliplr(Tensor self) -> Tensor
at::Tensor fliplr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_fliplr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flipud, name, "aten::flipud")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flipud, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flipud, schema_str, "flipud(Tensor self) -> Tensor")

// aten::flipud(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<flipud::schema> create_flipud_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flipud::name, flipud::overload_name)
      .typed<flipud::schema>();
}

// aten::flipud(Tensor self) -> Tensor
at::Tensor flipud::call(const at::Tensor & self) {
    static auto op = create_flipud_typed_handle();
    return op.call(self);
}

// aten::flipud(Tensor self) -> Tensor
at::Tensor flipud::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_flipud_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll, name, "aten::roll")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(roll, schema_str, "roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor")

// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<roll::schema> create_roll_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(roll::name, roll::overload_name)
      .typed<roll::schema>();
}

// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor
at::Tensor roll::call(const at::Tensor & self, at::IntArrayRef shifts, at::IntArrayRef dims) {
    static auto op = create_roll_typed_handle();
    return op.call(self, shifts, dims);
}

// aten::roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor
at::Tensor roll::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shifts, at::IntArrayRef dims) {
    static auto op = create_roll_typed_handle();
    return op.redispatch(dispatchKeySet, self, shifts, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rot90, name, "aten::rot90")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rot90, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rot90, schema_str, "rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor")

// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rot90::schema> create_rot90_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rot90::name, rot90::overload_name)
      .typed<rot90::schema>();
}

// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor
at::Tensor rot90::call(const at::Tensor & self, int64_t k, at::IntArrayRef dims) {
    static auto op = create_rot90_typed_handle();
    return op.call(self, k, dims);
}

// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -> Tensor
at::Tensor rot90::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, at::IntArrayRef dims) {
    static auto op = create_rot90_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapezoid_x, name, "aten::trapezoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapezoid_x, overload_name, "x")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapezoid_x, schema_str, "trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor")

// aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trapezoid_x::schema> create_trapezoid_x_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trapezoid_x::name, trapezoid_x::overload_name)
      .typed<trapezoid_x::schema>();
}

// aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor trapezoid_x::call(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    static auto op = create_trapezoid_x_typed_handle();
    return op.call(y, x, dim);
}

// aten::trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor trapezoid_x::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    static auto op = create_trapezoid_x_typed_handle();
    return op.redispatch(dispatchKeySet, y, x, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapezoid_dx, name, "aten::trapezoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapezoid_dx, overload_name, "dx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapezoid_dx, schema_str, "trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor")

// aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trapezoid_dx::schema> create_trapezoid_dx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trapezoid_dx::name, trapezoid_dx::overload_name)
      .typed<trapezoid_dx::schema>();
}

// aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
at::Tensor trapezoid_dx::call(const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    static auto op = create_trapezoid_dx_typed_handle();
    return op.call(y, dx, dim);
}

// aten::trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -> Tensor
at::Tensor trapezoid_dx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Scalar & dx, int64_t dim) {
    static auto op = create_trapezoid_dx_typed_handle();
    return op.redispatch(dispatchKeySet, y, dx, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapz_x, name, "aten::trapz")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapz_x, overload_name, "x")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapz_x, schema_str, "trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor")

// aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trapz_x::schema> create_trapz_x_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trapz_x::name, trapz_x::overload_name)
      .typed<trapz_x::schema>();
}

// aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor trapz_x::call(const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    static auto op = create_trapz_x_typed_handle();
    return op.call(y, x, dim);
}

// aten::trapz.x(Tensor y, Tensor x, *, int dim=-1) -> Tensor
at::Tensor trapz_x::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, const at::Tensor & x, int64_t dim) {
    static auto op = create_trapz_x_typed_handle();
    return op.redispatch(dispatchKeySet, y, x, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapz_dx, name, "aten::trapz")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapz_dx, overload_name, "dx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trapz_dx, schema_str, "trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor")

// aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trapz_dx::schema> create_trapz_dx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trapz_dx::name, trapz_dx::overload_name)
      .typed<trapz_dx::schema>();
}

// aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor
at::Tensor trapz_dx::call(const at::Tensor & y, double dx, int64_t dim) {
    static auto op = create_trapz_dx_typed_handle();
    return op.call(y, dx, dim);
}

// aten::trapz.dx(Tensor y, *, float dx=1, int dim=-1) -> Tensor
at::Tensor trapz_dx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & y, double dx, int64_t dim) {
    static auto op = create_trapz_dx_typed_handle();
    return op.redispatch(dispatchKeySet, y, dx, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear, name, "aten::_trilinear")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_trilinear, schema_str, "_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor")

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_trilinear::schema> create__trilinear_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_trilinear::name, _trilinear::overload_name)
      .typed<_trilinear::schema>();
}

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
at::Tensor _trilinear::call(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
    static auto op = create__trilinear_typed_handle();
    return op.call(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}

// aten::_trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -> Tensor
at::Tensor _trilinear::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
    static auto op = create__trilinear_typed_handle();
    return op.redispatch(dispatchKeySet, i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triplet_margin_loss, name, "aten::triplet_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triplet_margin_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triplet_margin_loss, schema_str, "triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor")

// aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<triplet_margin_loss::schema> create_triplet_margin_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triplet_margin_loss::name, triplet_margin_loss::overload_name)
      .typed<triplet_margin_loss::schema>();
}

// aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor
at::Tensor triplet_margin_loss::call(const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
    static auto op = create_triplet_margin_loss_typed_handle();
    return op.call(anchor, positive, negative, margin, p, eps, swap, reduction);
}

// aten::triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -> Tensor
at::Tensor triplet_margin_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & anchor, const at::Tensor & positive, const at::Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) {
    static auto op = create_triplet_margin_loss_typed_handle();
    return op.redispatch(dispatchKeySet, anchor, positive, negative, margin, p, eps, swap, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc, name, "aten::trunc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc, schema_str, "trunc(Tensor self) -> Tensor")

// aten::trunc(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trunc::schema> create_trunc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trunc::name, trunc::overload_name)
      .typed<trunc::schema>();
}

// aten::trunc(Tensor self) -> Tensor
at::Tensor trunc::call(const at::Tensor & self) {
    static auto op = create_trunc_typed_handle();
    return op.call(self);
}

// aten::trunc(Tensor self) -> Tensor
at::Tensor trunc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_trunc_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc_, name, "aten::trunc_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc_, schema_str, "trunc_(Tensor(a!) self) -> Tensor(a!)")

// aten::trunc_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<trunc_::schema> create_trunc__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trunc_::name, trunc_::overload_name)
      .typed<trunc_::schema>();
}

// aten::trunc_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & trunc_::call(at::Tensor & self) {
    static auto op = create_trunc__typed_handle();
    return op.call(self);
}

// aten::trunc_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & trunc_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_trunc__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc_out, name, "aten::trunc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trunc_out, schema_str, "trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<trunc_out::schema> create_trunc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trunc_out::name, trunc_out::overload_name)
      .typed<trunc_out::schema>();
}

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & trunc_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_trunc_out_typed_handle();
    return op.call(self, out);
}

// aten::trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & trunc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_trunc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix, name, "aten::fix")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix, schema_str, "fix(Tensor self) -> Tensor")

// aten::fix(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fix::schema> create_fix_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fix::name, fix::overload_name)
      .typed<fix::schema>();
}

// aten::fix(Tensor self) -> Tensor
at::Tensor fix::call(const at::Tensor & self) {
    static auto op = create_fix_typed_handle();
    return op.call(self);
}

// aten::fix(Tensor self) -> Tensor
at::Tensor fix::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_fix_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_, name, "aten::fix_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_, schema_str, "fix_(Tensor(a!) self) -> Tensor(a!)")

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fix_::schema> create_fix__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fix_::name, fix_::overload_name)
      .typed<fix_::schema>();
}

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & fix_::call(at::Tensor & self) {
    static auto op = create_fix__typed_handle();
    return op.call(self);
}

// aten::fix_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & fix_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_fix__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_out, name, "aten::fix")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fix_out, schema_str, "fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fix_out::schema> create_fix_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fix_out::name, fix_out::overload_name)
      .typed<fix_out::schema>();
}

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fix_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_fix_out_typed_handle();
    return op.call(self, out);
}

// aten::fix.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fix_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_fix_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(type_as, name, "aten::type_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(type_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(type_as, schema_str, "type_as(Tensor self, Tensor other) -> Tensor")

// aten::type_as(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<type_as::schema> create_type_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(type_as::name, type_as::overload_name)
      .typed<type_as::schema>();
}

// aten::type_as(Tensor self, Tensor other) -> Tensor
at::Tensor type_as::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_type_as_typed_handle();
    return op.call(self, other);
}

// aten::type_as(Tensor self, Tensor other) -> Tensor
at::Tensor type_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_type_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_has_compatible_shallow_copy_type, name, "aten::_has_compatible_shallow_copy_type")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_has_compatible_shallow_copy_type, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_has_compatible_shallow_copy_type, schema_str, "_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool")

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<_has_compatible_shallow_copy_type::schema> create__has_compatible_shallow_copy_type_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_has_compatible_shallow_copy_type::name, _has_compatible_shallow_copy_type::overload_name)
      .typed<_has_compatible_shallow_copy_type::schema>();
}

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
bool _has_compatible_shallow_copy_type::call(const at::Tensor & self, const at::Tensor & from) {
    static auto op = create__has_compatible_shallow_copy_type_typed_handle();
    return op.call(self, from);
}

// aten::_has_compatible_shallow_copy_type(Tensor self, Tensor from) -> bool
bool _has_compatible_shallow_copy_type::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & from) {
    static auto op = create__has_compatible_shallow_copy_type_typed_handle();
    return op.redispatch(dispatchKeySet, self, from);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique, name, "aten::_unique")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique, schema_str, "_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)")

// aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_unique::schema> create__unique_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unique::name, _unique::overload_name)
      .typed<_unique::schema>();
}

// aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _unique::call(const at::Tensor & self, bool sorted, bool return_inverse) {
    static auto op = create__unique_typed_handle();
    return op.call(self, sorted, return_inverse);
}

// aten::_unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _unique::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse) {
    static auto op = create__unique_typed_handle();
    return op.redispatch(dispatchKeySet, self, sorted, return_inverse);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim, name, "aten::unique_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim, schema_str, "unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)")

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<unique_dim::schema> create_unique_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_dim::name, unique_dim::overload_name)
      .typed<unique_dim::schema>();
}

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim::call(const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
    static auto op = create_unique_dim_typed_handle();
    return op.call(self, dim, sorted, return_inverse, return_counts);
}

// aten::unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
    static auto op = create_unique_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, sorted, return_inverse, return_counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive, name, "aten::unique_consecutive")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_consecutive, schema_str, "unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)")

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<unique_consecutive::schema> create_unique_consecutive_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_consecutive::name, unique_consecutive::overload_name)
      .typed<unique_consecutive::schema>();
}

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_consecutive::call(const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
    static auto op = create_unique_consecutive_typed_handle();
    return op.call(self, return_inverse, return_counts, dim);
}

// aten::unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_consecutive::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
    static auto op = create_unique_consecutive_typed_handle();
    return op.redispatch(dispatchKeySet, self, return_inverse, return_counts, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim_consecutive, name, "aten::unique_dim_consecutive")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim_consecutive, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unique_dim_consecutive, schema_str, "unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)")

// aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<unique_dim_consecutive::schema> create_unique_dim_consecutive_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unique_dim_consecutive::name, unique_dim_consecutive::overload_name)
      .typed<unique_dim_consecutive::schema>();
}

// aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim_consecutive::call(const at::Tensor & self, int64_t dim, bool return_inverse, bool return_counts) {
    static auto op = create_unique_dim_consecutive_typed_handle();
    return op.call(self, dim, return_inverse, return_counts);
}

// aten::unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> unique_dim_consecutive::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool return_inverse, bool return_counts) {
    static auto op = create_unique_dim_consecutive_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, return_inverse, return_counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2, name, "aten::_unique2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unique2, schema_str, "_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)")

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_unique2::schema> create__unique2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unique2::name, _unique2::overload_name)
      .typed<_unique2::schema>();
}

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _unique2::call(const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
    static auto op = create__unique2_typed_handle();
    return op.call(self, sorted, return_inverse, return_counts);
}

// aten::_unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _unique2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
    static auto op = create__unique2_typed_handle();
    return op.redispatch(dispatchKeySet, self, sorted, return_inverse, return_counts);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unsafe_view, name, "aten::_unsafe_view")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unsafe_view, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_unsafe_view, schema_str, "_unsafe_view(Tensor self, int[] size) -> Tensor")

// aten::_unsafe_view(Tensor self, int[] size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_unsafe_view::schema> create__unsafe_view_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_unsafe_view::name, _unsafe_view::overload_name)
      .typed<_unsafe_view::schema>();
}

// aten::_unsafe_view(Tensor self, int[] size) -> Tensor
at::Tensor _unsafe_view::call(const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create__unsafe_view_typed_handle();
    return op.call(self, size);
}

// aten::_unsafe_view(Tensor self, int[] size) -> Tensor
at::Tensor _unsafe_view::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create__unsafe_view_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsqueeze, name, "aten::unsqueeze")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsqueeze, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsqueeze, schema_str, "unsqueeze(Tensor(a) self, int dim) -> Tensor(a)")

// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<unsqueeze::schema> create_unsqueeze_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsqueeze::name, unsqueeze::overload_name)
      .typed<unsqueeze::schema>();
}

// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
at::Tensor unsqueeze::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_unsqueeze_typed_handle();
    return op.call(self, dim);
}

// aten::unsqueeze(Tensor(a) self, int dim) -> Tensor(a)
at::Tensor unsqueeze::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_unsqueeze_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsqueeze_, name, "aten::unsqueeze_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsqueeze_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unsqueeze_, schema_str, "unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)")

// aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<unsqueeze_::schema> create_unsqueeze__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unsqueeze_::name, unsqueeze_::overload_name)
      .typed<unsqueeze_::schema>();
}

// aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)
at::Tensor & unsqueeze_::call(at::Tensor & self, int64_t dim) {
    static auto op = create_unsqueeze__typed_handle();
    return op.call(self, dim);
}

// aten::unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)
at::Tensor & unsqueeze_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim) {
    static auto op = create_unsqueeze__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vander, name, "aten::vander")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vander, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(vander, schema_str, "vander(Tensor x, int? N=None, bool increasing=False) -> Tensor")

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<vander::schema> create_vander_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(vander::name, vander::overload_name)
      .typed<vander::schema>();
}

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
at::Tensor vander::call(const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
    static auto op = create_vander_typed_handle();
    return op.call(x, N, increasing);
}

// aten::vander(Tensor x, int? N=None, bool increasing=False) -> Tensor
at::Tensor vander::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, c10::optional<int64_t> N, bool increasing) {
    static auto op = create_vander_typed_handle();
    return op.redispatch(dispatchKeySet, x, N, increasing);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var, schema_str, "var(Tensor self, bool unbiased=True) -> Tensor")

// aten::var(Tensor self, bool unbiased=True) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<var::schema> create_var_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var::name, var::overload_name)
      .typed<var::schema>();
}

// aten::var(Tensor self, bool unbiased=True) -> Tensor
at::Tensor var::call(const at::Tensor & self, bool unbiased) {
    static auto op = create_var_typed_handle();
    return op.call(self, unbiased);
}

// aten::var(Tensor self, bool unbiased=True) -> Tensor
at::Tensor var::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
    static auto op = create_var_typed_handle();
    return op.redispatch(dispatchKeySet, self, unbiased);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_dim, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_dim, schema_str, "var.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor")

// aten::var.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<var_dim::schema> create_var_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_dim::name, var_dim::overload_name)
      .typed<var_dim::schema>();
}

// aten::var.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor var_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_var_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::var.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor var_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_var_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction, overload_name, "correction")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction, schema_str, "var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor")

// aten::var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<var_correction::schema> create_var_correction_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_correction::name, var_correction::overload_name)
      .typed<var_correction::schema>();
}

// aten::var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor var_correction::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_correction_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor var_correction::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_correction_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_out, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_out, schema_str, "var.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::var.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<var_out::schema> create_var_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_out::name, var_out::overload_name)
      .typed<var_out::schema>();
}

// aten::var.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_out::call(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_var_out_typed_handle();
    return op.call(self, dim, unbiased, keepdim, out);
}

// aten::var.out(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_var_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_out, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_out, overload_name, "correction_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_out, schema_str, "var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)")

// aten::var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<var_correction_out::schema> create_var_correction_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_correction_out::name, var_correction_out::overload_name)
      .typed<var_correction_out::schema>();
}

// aten::var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_correction_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_var_correction_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out);
}

// aten::var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_correction_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_var_correction_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_names_dim, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_names_dim, schema_str, "var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor")

// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<var_names_dim::schema> create_var_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_names_dim::name, var_names_dim::overload_name)
      .typed<var_names_dim::schema>();
}

// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor var_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_var_names_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> Tensor
at::Tensor var_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_var_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_names_out, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_names_out, schema_str, "var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<var_names_out::schema> create_var_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_names_out::name, var_names_out::overload_name)
      .typed<var_names_out::schema>();
}

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_names_out::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_var_names_out_typed_handle();
    return op.call(self, dim, unbiased, keepdim, out);
}

// aten::var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim, at::Tensor & out) {
    static auto op = create_var_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_names, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_names, overload_name, "correction_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_names, schema_str, "var.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor")

// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<var_correction_names::schema> create_var_correction_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_correction_names::name, var_correction_names::overload_name)
      .typed<var_correction_names::schema>();
}

// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor var_correction_names::call(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_correction_names_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::var.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> Tensor
at::Tensor var_correction_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_correction_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_names_out, name, "aten::var")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_names_out, overload_name, "correction_names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_correction_names_out, schema_str, "var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)")

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<var_correction_names_out::schema> create_var_correction_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_correction_names_out::name, var_correction_names_out::overload_name)
      .typed<var_correction_names_out::schema>();
}

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_correction_names_out::call(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_var_correction_names_out_typed_handle();
    return op.call(self, dim, correction, keepdim, out);
}

// aten::var.correction_names_out(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & var_correction_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim, at::Tensor & out) {
    static auto op = create_var_correction_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean, name, "aten::var_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean, schema_str, "var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)")

// aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<var_mean::schema> create_var_mean_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_mean::name, var_mean::overload_name)
      .typed<var_mean::schema>();
}

// aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean::call(const at::Tensor & self, bool unbiased) {
    static auto op = create_var_mean_typed_handle();
    return op.call(self, unbiased);
}

// aten::var_mean(Tensor self, bool unbiased=True) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool unbiased) {
    static auto op = create_var_mean_typed_handle();
    return op.redispatch(dispatchKeySet, self, unbiased);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_dim, name, "aten::var_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_dim, schema_str, "var_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")

// aten::var_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<var_mean_dim::schema> create_var_mean_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_mean_dim::name, var_mean_dim::overload_name)
      .typed<var_mean_dim::schema>();
}

// aten::var_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_var_mean_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::var_mean.dim(Tensor self, int[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
    static auto op = create_var_mean_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_correction, name, "aten::var_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_correction, overload_name, "correction")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_correction, schema_str, "var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)")

// aten::var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<var_mean_correction::schema> create_var_mean_correction_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_mean_correction::name, var_mean_correction::overload_name)
      .typed<var_mean_correction::schema>();
}

// aten::var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_correction::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_mean_correction_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_correction::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_mean_correction_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_names_dim, name, "aten::var_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_names_dim, overload_name, "names_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_names_dim, schema_str, "var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)")

// aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<var_mean_names_dim::schema> create_var_mean_names_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_mean_names_dim::name, var_mean_names_dim::overload_name)
      .typed<var_mean_names_dim::schema>();
}

// aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_names_dim::call(const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_var_mean_names_dim_typed_handle();
    return op.call(self, dim, unbiased, keepdim);
}

// aten::var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_names_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, bool unbiased, bool keepdim) {
    static auto op = create_var_mean_names_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, unbiased, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_correction_names, name, "aten::var_mean")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_correction_names, overload_name, "correction_names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(var_mean_correction_names, schema_str, "var_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)")

// aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<var_mean_correction_names::schema> create_var_mean_correction_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(var_mean_correction_names::name, var_mean_correction_names::overload_name)
      .typed<var_mean_correction_names::schema>();
}

// aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_correction_names::call(const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_mean_correction_names_typed_handle();
    return op.call(self, dim, correction, keepdim);
}

// aten::var_mean.correction_names(Tensor self, Dimname[1] dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> var_mean_correction_names::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::DimnameList dim, c10::optional<int64_t> correction, bool keepdim) {
    static auto op = create_var_mean_correction_names_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, correction, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as, name, "aten::view_as")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_as, schema_str, "view_as(Tensor(a) self, Tensor other) -> Tensor(a)")

// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view_as::schema> create_view_as_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_as::name, view_as::overload_name)
      .typed<view_as::schema>();
}

// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor view_as::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_view_as_typed_handle();
    return op.call(self, other);
}

// aten::view_as(Tensor(a) self, Tensor other) -> Tensor(a)
at::Tensor view_as::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_view_as_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_self, name, "aten::where")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_self, overload_name, "self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_self, schema_str, "where.self(Tensor condition, Tensor self, Tensor other) -> Tensor")

// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<where_self::schema> create_where_self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(where_self::name, where_self::overload_name)
      .typed<where_self::schema>();
}

// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
at::Tensor where_self::call(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_where_self_typed_handle();
    return op.call(condition, self, other);
}

// aten::where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
at::Tensor where_self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_where_self_typed_handle();
    return op.redispatch(dispatchKeySet, condition, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_ScalarSelf, name, "aten::where")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_ScalarSelf, overload_name, "ScalarSelf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_ScalarSelf, schema_str, "where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor")

// aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<where_ScalarSelf::schema> create_where_ScalarSelf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(where_ScalarSelf::name, where_ScalarSelf::overload_name)
      .typed<where_ScalarSelf::schema>();
}

// aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor
at::Tensor where_ScalarSelf::call(const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_where_ScalarSelf_typed_handle();
    return op.call(condition, self, other);
}

// aten::where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -> Tensor
at::Tensor where_ScalarSelf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_where_ScalarSelf_typed_handle();
    return op.redispatch(dispatchKeySet, condition, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_ScalarOther, name, "aten::where")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_ScalarOther, overload_name, "ScalarOther")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_ScalarOther, schema_str, "where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor")

// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<where_ScalarOther::schema> create_where_ScalarOther_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(where_ScalarOther::name, where_ScalarOther::overload_name)
      .typed<where_ScalarOther::schema>();
}

// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor
at::Tensor where_ScalarOther::call(const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_where_ScalarOther_typed_handle();
    return op.call(condition, self, other);
}

// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -> Tensor
at::Tensor where_ScalarOther::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_where_ScalarOther_typed_handle();
    return op.redispatch(dispatchKeySet, condition, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_Scalar, name, "aten::where")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where_Scalar, schema_str, "where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor")

// aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<where_Scalar::schema> create_where_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(where_Scalar::name, where_Scalar::overload_name)
      .typed<where_Scalar::schema>();
}

// aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor
at::Tensor where_Scalar::call(const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
    static auto op = create_where_Scalar_typed_handle();
    return op.call(condition, self, other);
}

// aten::where.Scalar(Tensor condition, Scalar self, Scalar other) -> Tensor
at::Tensor where_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Scalar & self, const at::Scalar & other) {
    static auto op = create_where_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, condition, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where, name, "aten::where")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(where, schema_str, "where(Tensor condition) -> Tensor[]")

// aten::where(Tensor condition) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<where::schema> create_where_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(where::name, where::overload_name)
      .typed<where::schema>();
}

// aten::where(Tensor condition) -> Tensor[]
::std::vector<at::Tensor> where::call(const at::Tensor & condition) {
    static auto op = create_where_typed_handle();
    return op.call(condition);
}

// aten::where(Tensor condition) -> Tensor[]
::std::vector<at::Tensor> where::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition) {
    static auto op = create_where_typed_handle();
    return op.redispatch(dispatchKeySet, condition);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_s_where, name, "aten::_s_where")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_s_where, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_s_where, schema_str, "_s_where(Tensor condition, Tensor self, Tensor other) -> Tensor")

// aten::_s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_s_where::schema> create__s_where_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_s_where::name, _s_where::overload_name)
      .typed<_s_where::schema>();
}

// aten::_s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
at::Tensor _s_where::call(const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create__s_where_typed_handle();
    return op.call(condition, self, other);
}

// aten::_s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
at::Tensor _s_where::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & condition, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create__s_where_typed_handle();
    return op.redispatch(dispatchKeySet, condition, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_except_dim, name, "aten::norm_except_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_except_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_except_dim, schema_str, "norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor")

// aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_except_dim::schema> create_norm_except_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_except_dim::name, norm_except_dim::overload_name)
      .typed<norm_except_dim::schema>();
}

// aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor
at::Tensor norm_except_dim::call(const at::Tensor & v, int64_t pow, int64_t dim) {
    static auto op = create_norm_except_dim_typed_handle();
    return op.call(v, pow, dim);
}

// aten::norm_except_dim(Tensor v, int pow=2, int dim=0) -> Tensor
at::Tensor norm_except_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, int64_t pow, int64_t dim) {
    static auto op = create_norm_except_dim_typed_handle();
    return op.redispatch(dispatchKeySet, v, pow, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm, name, "aten::_weight_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm, schema_str, "_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor")

// aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_weight_norm::schema> create__weight_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_norm::name, _weight_norm::overload_name)
      .typed<_weight_norm::schema>();
}

// aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor
at::Tensor _weight_norm::call(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
    static auto op = create__weight_norm_typed_handle();
    return op.call(v, g, dim);
}

// aten::_weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor
at::Tensor _weight_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, const at::Tensor & g, int64_t dim) {
    static auto op = create__weight_norm_typed_handle();
    return op.redispatch(dispatchKeySet, v, g, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_cuda_interface, name, "aten::_weight_norm_cuda_interface")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_cuda_interface, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_cuda_interface, schema_str, "_weight_norm_cuda_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)")

// aten::_weight_norm_cuda_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_weight_norm_cuda_interface::schema> create__weight_norm_cuda_interface_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_norm_cuda_interface::name, _weight_norm_cuda_interface::overload_name)
      .typed<_weight_norm_cuda_interface::schema>();
}

// aten::_weight_norm_cuda_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_cuda_interface::call(const at::Tensor & v, const at::Tensor & g, int64_t dim) {
    static auto op = create__weight_norm_cuda_interface_typed_handle();
    return op.call(v, g, dim);
}

// aten::_weight_norm_cuda_interface(Tensor v, Tensor g, int dim=0) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_cuda_interface::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & v, const at::Tensor & g, int64_t dim) {
    static auto op = create__weight_norm_cuda_interface_typed_handle();
    return op.redispatch(dispatchKeySet, v, g, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_cuda_interface_backward, name, "aten::_weight_norm_cuda_interface_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_cuda_interface_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_cuda_interface_backward, schema_str, "_weight_norm_cuda_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)")

// aten::_weight_norm_cuda_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_weight_norm_cuda_interface_backward::schema> create__weight_norm_cuda_interface_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_norm_cuda_interface_backward::name, _weight_norm_cuda_interface_backward::overload_name)
      .typed<_weight_norm_cuda_interface_backward::schema>();
}

// aten::_weight_norm_cuda_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_cuda_interface_backward::call(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    static auto op = create__weight_norm_cuda_interface_backward_typed_handle();
    return op.call(grad_w, saved_v, saved_g, saved_norms, dim);
}

// aten::_weight_norm_cuda_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_cuda_interface_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    static auto op = create__weight_norm_cuda_interface_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_w, saved_v, saved_g, saved_norms, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_differentiable_backward, name, "aten::_weight_norm_differentiable_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_differentiable_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_weight_norm_differentiable_backward, schema_str, "_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)")

// aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_weight_norm_differentiable_backward::schema> create__weight_norm_differentiable_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_weight_norm_differentiable_backward::name, _weight_norm_differentiable_backward::overload_name)
      .typed<_weight_norm_differentiable_backward::schema>();
}

// aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_differentiable_backward::call(const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    static auto op = create__weight_norm_differentiable_backward_typed_handle();
    return op.call(grad_w, saved_v, saved_g, saved_norms, dim);
}

// aten::_weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _weight_norm_differentiable_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_w, const at::Tensor & saved_v, const at::Tensor & saved_g, const at::Tensor & saved_norms, int64_t dim) {
    static auto op = create__weight_norm_differentiable_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_w, saved_v, saved_g, saved_norms, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_names, name, "aten::zeros")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_names, overload_name, "names")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_names, schema_str, "zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<zeros_names::schema> create_zeros_names_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zeros_names::name, zeros_names::overload_name)
      .typed<zeros_names::schema>();
}

// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor zeros_names::call(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_zeros_names_typed_handle();
    return op.call(size, names, dtype, layout, device, pin_memory);
}

// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor zeros_names::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_zeros_names_typed_handle();
    return op.redispatch(dispatchKeySet, size, names, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros, name, "aten::zeros")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros, schema_str, "zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<zeros::schema> create_zeros_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zeros::name, zeros::overload_name)
      .typed<zeros::schema>();
}

// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor zeros::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_zeros_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory);
}

// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor zeros::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_zeros_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_out, name, "aten::zeros")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_out, schema_str, "zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<zeros_out::schema> create_zeros_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zeros_out::name, zeros_out::overload_name)
      .typed<zeros_out::schema>();
}

// aten::zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & zeros_out::call(at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_zeros_out_typed_handle();
    return op.call(size, out);
}

// aten::zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & zeros_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, at::Tensor & out) {
    static auto op = create_zeros_out_typed_handle();
    return op.redispatch(dispatchKeySet, size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like, name, "aten::zeros_like")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zeros_like, schema_str, "zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor")

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<zeros_like::schema> create_zeros_like_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zeros_like::name, zeros_like::overload_name)
      .typed<zeros_like::schema>();
}

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor zeros_like::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_zeros_like_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, memory_format);
}

// aten::zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
at::Tensor zeros_like::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_zeros_like_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_standard_gamma_grad, name, "aten::_standard_gamma_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_standard_gamma_grad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_standard_gamma_grad, schema_str, "_standard_gamma_grad(Tensor self, Tensor output) -> Tensor")

// aten::_standard_gamma_grad(Tensor self, Tensor output) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_standard_gamma_grad::schema> create__standard_gamma_grad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_standard_gamma_grad::name, _standard_gamma_grad::overload_name)
      .typed<_standard_gamma_grad::schema>();
}

// aten::_standard_gamma_grad(Tensor self, Tensor output) -> Tensor
at::Tensor _standard_gamma_grad::call(const at::Tensor & self, const at::Tensor & output) {
    static auto op = create__standard_gamma_grad_typed_handle();
    return op.call(self, output);
}

// aten::_standard_gamma_grad(Tensor self, Tensor output) -> Tensor
at::Tensor _standard_gamma_grad::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & output) {
    static auto op = create__standard_gamma_grad_typed_handle();
    return op.redispatch(dispatchKeySet, self, output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_standard_gamma, name, "aten::_standard_gamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_standard_gamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_standard_gamma, schema_str, "_standard_gamma(Tensor self, Generator? generator=None) -> Tensor")

// aten::_standard_gamma(Tensor self, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_standard_gamma::schema> create__standard_gamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_standard_gamma::name, _standard_gamma::overload_name)
      .typed<_standard_gamma::schema>();
}

// aten::_standard_gamma(Tensor self, Generator? generator=None) -> Tensor
at::Tensor _standard_gamma::call(const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create__standard_gamma_typed_handle();
    return op.call(self, generator);
}

// aten::_standard_gamma(Tensor self, Generator? generator=None) -> Tensor
at::Tensor _standard_gamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create__standard_gamma_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad, name, "aten::_dirichlet_grad")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dirichlet_grad, schema_str, "_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor")

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_dirichlet_grad::schema> create__dirichlet_grad_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dirichlet_grad::name, _dirichlet_grad::overload_name)
      .typed<_dirichlet_grad::schema>();
}

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
at::Tensor _dirichlet_grad::call(const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total) {
    static auto op = create__dirichlet_grad_typed_handle();
    return op.call(x, alpha, total);
}

// aten::_dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -> Tensor
at::Tensor _dirichlet_grad::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & x, const at::Tensor & alpha, const at::Tensor & total) {
    static auto op = create__dirichlet_grad_typed_handle();
    return op.redispatch(dispatchKeySet, x, alpha, total);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sample_dirichlet, name, "aten::_sample_dirichlet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sample_dirichlet, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sample_dirichlet, schema_str, "_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor")

// aten::_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sample_dirichlet::schema> create__sample_dirichlet_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sample_dirichlet::name, _sample_dirichlet::overload_name)
      .typed<_sample_dirichlet::schema>();
}

// aten::_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor
at::Tensor _sample_dirichlet::call(const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create__sample_dirichlet_typed_handle();
    return op.call(self, generator);
}

// aten::_sample_dirichlet(Tensor self, Generator? generator=None) -> Tensor
at::Tensor _sample_dirichlet::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create__sample_dirichlet_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(poisson, name, "aten::poisson")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(poisson, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(poisson, schema_str, "poisson(Tensor self, Generator? generator=None) -> Tensor")

// aten::poisson(Tensor self, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<poisson::schema> create_poisson_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(poisson::name, poisson::overload_name)
      .typed<poisson::schema>();
}

// aten::poisson(Tensor self, Generator? generator=None) -> Tensor
at::Tensor poisson::call(const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create_poisson_typed_handle();
    return op.call(self, generator);
}

// aten::poisson(Tensor self, Generator? generator=None) -> Tensor
at::Tensor poisson::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create_poisson_typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binomial, name, "aten::binomial")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binomial, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(binomial, schema_str, "binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor")

// aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<binomial::schema> create_binomial_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(binomial::name, binomial::overload_name)
      .typed<binomial::schema>();
}

// aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor
at::Tensor binomial::call(const at::Tensor & count, const at::Tensor & prob, c10::optional<at::Generator> generator) {
    static auto op = create_binomial_typed_handle();
    return op.call(count, prob, generator);
}

// aten::binomial(Tensor count, Tensor prob, Generator? generator=None) -> Tensor
at::Tensor binomial::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & count, const at::Tensor & prob, c10::optional<at::Generator> generator) {
    static auto op = create_binomial_typed_handle();
    return op.redispatch(dispatchKeySet, count, prob, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_norm, name, "aten::native_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_norm, schema_str, "native_norm(Tensor self, Scalar p=2) -> Tensor")

// aten::native_norm(Tensor self, Scalar p=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<native_norm::schema> create_native_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_norm::name, native_norm::overload_name)
      .typed<native_norm::schema>();
}

// aten::native_norm(Tensor self, Scalar p=2) -> Tensor
at::Tensor native_norm::call(const at::Tensor & self, const at::Scalar & p) {
    static auto op = create_native_norm_typed_handle();
    return op.call(self, p);
}

// aten::native_norm(Tensor self, Scalar p=2) -> Tensor
at::Tensor native_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p) {
    static auto op = create_native_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_norm_ScalarOpt_dim_dtype, name, "aten::native_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_norm_ScalarOpt_dim_dtype, overload_name, "ScalarOpt_dim_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(native_norm_ScalarOpt_dim_dtype, schema_str, "native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor")

// aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<native_norm_ScalarOpt_dim_dtype::schema> create_native_norm_ScalarOpt_dim_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(native_norm_ScalarOpt_dim_dtype::name, native_norm_ScalarOpt_dim_dtype::overload_name)
      .typed<native_norm_ScalarOpt_dim_dtype::schema>();
}

// aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor
at::Tensor native_norm_ScalarOpt_dim_dtype::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_native_norm_ScalarOpt_dim_dtype_typed_handle();
    return op.call(self, p, dim, keepdim, dtype);
}

// aten::native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -> Tensor
at::Tensor native_norm_ScalarOpt_dim_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_native_norm_ScalarOpt_dim_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum, name, "aten::_sparse_sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum, schema_str, "_sparse_sum(Tensor self) -> Tensor")

// aten::_sparse_sum(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_sum::schema> create__sparse_sum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_sum::name, _sparse_sum::overload_name)
      .typed<_sparse_sum::schema>();
}

// aten::_sparse_sum(Tensor self) -> Tensor
at::Tensor _sparse_sum::call(const at::Tensor & self) {
    static auto op = create__sparse_sum_typed_handle();
    return op.call(self);
}

// aten::_sparse_sum(Tensor self) -> Tensor
at::Tensor _sparse_sum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__sparse_sum_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dtype, name, "aten::_sparse_sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dtype, overload_name, "dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dtype, schema_str, "_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -> Tensor")

// aten::_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_sum_dtype::schema> create__sparse_sum_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_sum_dtype::name, _sparse_sum_dtype::overload_name)
      .typed<_sparse_sum_dtype::schema>();
}

// aten::_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -> Tensor
at::Tensor _sparse_sum_dtype::call(const at::Tensor & self, at::ScalarType dtype) {
    static auto op = create__sparse_sum_dtype_typed_handle();
    return op.call(self, dtype);
}

// aten::_sparse_sum.dtype(Tensor self, *, ScalarType dtype) -> Tensor
at::Tensor _sparse_sum_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype) {
    static auto op = create__sparse_sum_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dim, name, "aten::_sparse_sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dim, schema_str, "_sparse_sum.dim(Tensor self, int[1] dim) -> Tensor")

// aten::_sparse_sum.dim(Tensor self, int[1] dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_sum_dim::schema> create__sparse_sum_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_sum_dim::name, _sparse_sum_dim::overload_name)
      .typed<_sparse_sum_dim::schema>();
}

// aten::_sparse_sum.dim(Tensor self, int[1] dim) -> Tensor
at::Tensor _sparse_sum_dim::call(const at::Tensor & self, at::IntArrayRef dim) {
    static auto op = create__sparse_sum_dim_typed_handle();
    return op.call(self, dim);
}

// aten::_sparse_sum.dim(Tensor self, int[1] dim) -> Tensor
at::Tensor _sparse_sum_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim) {
    static auto op = create__sparse_sum_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dim_dtype, name, "aten::_sparse_sum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dim_dtype, overload_name, "dim_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_dim_dtype, schema_str, "_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor")

// aten::_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_sum_dim_dtype::schema> create__sparse_sum_dim_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_sum_dim_dtype::name, _sparse_sum_dim_dtype::overload_name)
      .typed<_sparse_sum_dim_dtype::schema>();
}

// aten::_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor
at::Tensor _sparse_sum_dim_dtype::call(const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
    static auto op = create__sparse_sum_dim_dtype_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -> Tensor
at::Tensor _sparse_sum_dim_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, at::ScalarType dtype) {
    static auto op = create__sparse_sum_dim_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_backward, name, "aten::_sparse_sum_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_sum_backward, schema_str, "_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor")

// aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_sum_backward::schema> create__sparse_sum_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_sum_backward::name, _sparse_sum_backward::overload_name)
      .typed<_sparse_sum_backward::schema>();
}

// aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor
at::Tensor _sparse_sum_backward::call(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
    static auto op = create__sparse_sum_backward_typed_handle();
    return op.call(grad, self, dim);
}

// aten::_sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -> Tensor
at::Tensor _sparse_sum_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
    static auto op = create__sparse_sum_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_int, name, "aten::_sparse_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_int, schema_str, "_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_softmax_int::schema> create__sparse_softmax_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_softmax_int::name, _sparse_softmax_int::overload_name)
      .typed<_sparse_softmax_int::schema>();
}

// aten::_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_softmax_int::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_softmax_int_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_softmax_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_softmax_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_Dimname, name, "aten::_sparse_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_Dimname, schema_str, "_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_softmax_Dimname::schema> create__sparse_softmax_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_softmax_Dimname::name, _sparse_softmax_Dimname::overload_name)
      .typed<_sparse_softmax_Dimname::schema>();
}

// aten::_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_softmax_Dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_softmax_Dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_softmax_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_softmax_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax, name, "aten::_sparse_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax, schema_str, "_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")

// aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_softmax::schema> create__sparse_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_softmax::name, _sparse_softmax::overload_name)
      .typed<_sparse_softmax::schema>();
}

// aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _sparse_softmax::call(const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__sparse_softmax_typed_handle();
    return op.call(self, dim, half_to_float);
}

// aten::_sparse_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _sparse_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__sparse_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data, name, "aten::_sparse_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_softmax_backward_data, schema_str, "_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor")

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_softmax_backward_data::schema> create__sparse_softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_softmax_backward_data::name, _sparse_softmax_backward_data::overload_name)
      .typed<_sparse_softmax_backward_data::schema>();
}

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__sparse_softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, self);
}

// aten::_sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__sparse_softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_int, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_int, schema_str, "_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_int::schema> create__sparse_log_softmax_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_int::name, _sparse_log_softmax_int::overload_name)
      .typed<_sparse_log_softmax_int::schema>();
}

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_int::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_log_softmax_int_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_log_softmax_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_Dimname, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_Dimname, schema_str, "_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor")

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_Dimname::schema> create__sparse_log_softmax_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_Dimname::name, _sparse_log_softmax_Dimname::overload_name)
      .typed<_sparse_log_softmax_Dimname::schema>();
}

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_Dimname::call(const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_log_softmax_Dimname_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::_sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor _sparse_log_softmax_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create__sparse_log_softmax_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax, name, "aten::_sparse_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax, schema_str, "_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor")

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax::schema> create__sparse_log_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax::name, _sparse_log_softmax::overload_name)
      .typed<_sparse_log_softmax::schema>();
}

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _sparse_log_softmax::call(const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__sparse_log_softmax_typed_handle();
    return op.call(self, dim, half_to_float);
}

// aten::_sparse_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
at::Tensor _sparse_log_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool half_to_float) {
    static auto op = create__sparse_log_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, half_to_float);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data, name, "aten::_sparse_log_softmax_backward_data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_log_softmax_backward_data, schema_str, "_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor")

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_log_softmax_backward_data::schema> create__sparse_log_softmax_backward_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_log_softmax_backward_data::name, _sparse_log_softmax_backward_data::overload_name)
      .typed<_sparse_log_softmax_backward_data::schema>();
}

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_log_softmax_backward_data::call(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__sparse_log_softmax_backward_data_typed_handle();
    return op.call(grad_output, output, dim, self);
}

// aten::_sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -> Tensor
at::Tensor _sparse_log_softmax_backward_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
    static auto op = create__sparse_log_softmax_backward_data_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, dim, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dtype, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dtype, overload_name, "ScalarOpt_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dtype, schema_str, "norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor")

// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_ScalarOpt_dtype::schema> create_norm_ScalarOpt_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_ScalarOpt_dtype::name, norm_ScalarOpt_dtype::overload_name)
      .typed<norm_ScalarOpt_dtype::schema>();
}

// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
at::Tensor norm_ScalarOpt_dtype::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype) {
    static auto op = create_norm_ScalarOpt_dtype_typed_handle();
    return op.call(self, p, dtype);
}

// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
at::Tensor norm_ScalarOpt_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype) {
    static auto op = create_norm_ScalarOpt_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_Scalar, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_Scalar, schema_str, "norm.Scalar(Tensor self, Scalar p=2) -> Tensor")

// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_Scalar::schema> create_norm_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_Scalar::name, norm_Scalar::overload_name)
      .typed<norm_Scalar::schema>();
}

// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor
at::Tensor norm_Scalar::call(const at::Tensor & self, const at::Scalar & p) {
    static auto op = create_norm_Scalar_typed_handle();
    return op.call(self, p);
}

// aten::norm.Scalar(Tensor self, Scalar p=2) -> Tensor
at::Tensor norm_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p) {
    static auto op = create_norm_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dim_dtype, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dim_dtype, overload_name, "ScalarOpt_dim_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dim_dtype, schema_str, "norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor")

// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_ScalarOpt_dim_dtype::schema> create_norm_ScalarOpt_dim_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_ScalarOpt_dim_dtype::name, norm_ScalarOpt_dim_dtype::overload_name)
      .typed<norm_ScalarOpt_dim_dtype::schema>();
}

// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
at::Tensor norm_ScalarOpt_dim_dtype::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
    static auto op = create_norm_ScalarOpt_dim_dtype_typed_handle();
    return op.call(self, p, dim, keepdim, dtype);
}

// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
at::Tensor norm_ScalarOpt_dim_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
    static auto op = create_norm_ScalarOpt_dim_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dim, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dim, overload_name, "ScalarOpt_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_ScalarOpt_dim, schema_str, "norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor")

// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_ScalarOpt_dim::schema> create_norm_ScalarOpt_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_ScalarOpt_dim::name, norm_ScalarOpt_dim::overload_name)
      .typed<norm_ScalarOpt_dim::schema>();
}

// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor norm_ScalarOpt_dim::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_norm_ScalarOpt_dim_typed_handle();
    return op.call(self, p, dim, keepdim);
}

// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor norm_ScalarOpt_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_norm_ScalarOpt_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_dtype_out, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_dtype_out, overload_name, "dtype_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_dtype_out, schema_str, "norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)")

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<norm_dtype_out::schema> create_norm_dtype_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_dtype_out::name, norm_dtype_out::overload_name)
      .typed<norm_dtype_out::schema>();
}

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_dtype_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
    static auto op = create_norm_dtype_out_typed_handle();
    return op.call(self, p, dim, keepdim, dtype, out);
}

// aten::norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_dtype_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
    static auto op = create_norm_dtype_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_out, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_out, schema_str, "norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<norm_out::schema> create_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_out::name, norm_out::overload_name)
      .typed<norm_out::schema>();
}

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_norm_out_typed_handle();
    return op.call(self, p, dim, keepdim, out);
}

// aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_ScalarOpt_dim_dtype, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_ScalarOpt_dim_dtype, overload_name, "names_ScalarOpt_dim_dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_ScalarOpt_dim_dtype, schema_str, "norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor")

// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_names_ScalarOpt_dim_dtype::schema> create_norm_names_ScalarOpt_dim_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_names_ScalarOpt_dim_dtype::name, norm_names_ScalarOpt_dim_dtype::overload_name)
      .typed<norm_names_ScalarOpt_dim_dtype::schema>();
}

// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
at::Tensor norm_names_ScalarOpt_dim_dtype::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
    static auto op = create_norm_names_ScalarOpt_dim_dtype_typed_handle();
    return op.call(self, p, dim, keepdim, dtype);
}

// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
at::Tensor norm_names_ScalarOpt_dim_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) {
    static auto op = create_norm_names_ScalarOpt_dim_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_ScalarOpt_dim, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_ScalarOpt_dim, overload_name, "names_ScalarOpt_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_ScalarOpt_dim, schema_str, "norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor")

// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<norm_names_ScalarOpt_dim::schema> create_norm_names_ScalarOpt_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_names_ScalarOpt_dim::name, norm_names_ScalarOpt_dim::overload_name)
      .typed<norm_names_ScalarOpt_dim::schema>();
}

// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor
at::Tensor norm_names_ScalarOpt_dim::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
    static auto op = create_norm_names_ScalarOpt_dim_typed_handle();
    return op.call(self, p, dim, keepdim);
}

// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -> Tensor
at::Tensor norm_names_ScalarOpt_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim) {
    static auto op = create_norm_names_ScalarOpt_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_dtype_out, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_dtype_out, overload_name, "names_dtype_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_dtype_out, schema_str, "norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)")

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<norm_names_dtype_out::schema> create_norm_names_dtype_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_names_dtype_out::name, norm_names_dtype_out::overload_name)
      .typed<norm_names_dtype_out::schema>();
}

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_names_dtype_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
    static auto op = create_norm_names_dtype_out_typed_handle();
    return op.call(self, p, dim, keepdim, dtype, out);
}

// aten::norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_names_dtype_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::ScalarType dtype, at::Tensor & out) {
    static auto op = create_norm_names_dtype_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_out, name, "aten::norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_out, overload_name, "names_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(norm_names_out, schema_str, "norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<norm_names_out::schema> create_norm_names_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(norm_names_out::name, norm_names_out::overload_name)
      .typed<norm_names_out::schema>();
}

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_names_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    static auto op = create_norm_names_out_typed_handle();
    return op.call(self, p, dim, keepdim, out);
}

// aten::norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & norm_names_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::DimnameList dim, bool keepdim, at::Tensor & out) {
    static auto op = create_norm_names_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor, name, "aten::frexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor, schema_str, "frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)")

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
static C10_NOINLINE c10::TypedOperatorHandle<frexp_Tensor::schema> create_frexp_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frexp_Tensor::name, frexp_Tensor::overload_name)
      .typed<frexp_Tensor::schema>();
}

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
::std::tuple<at::Tensor,at::Tensor> frexp_Tensor::call(const at::Tensor & self) {
    static auto op = create_frexp_Tensor_typed_handle();
    return op.call(self);
}

// aten::frexp.Tensor(Tensor self) -> (Tensor mantissa, Tensor exponent)
::std::tuple<at::Tensor,at::Tensor> frexp_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_frexp_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor_out, name, "aten::frexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frexp_Tensor_out, schema_str, "frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)")

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
static C10_NOINLINE c10::TypedOperatorHandle<frexp_Tensor_out::schema> create_frexp_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frexp_Tensor_out::name, frexp_Tensor_out::overload_name)
      .typed<frexp_Tensor_out::schema>();
}

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
::std::tuple<at::Tensor &,at::Tensor &> frexp_Tensor_out::call(const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
    static auto op = create_frexp_Tensor_out_typed_handle();
    return op.call(self, mantissa, exponent);
}

// aten::frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -> (Tensor(a!) mantissa, Tensor(b!) exponent)
::std::tuple<at::Tensor &,at::Tensor &> frexp_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & mantissa, at::Tensor & exponent) {
    static auto op = create_frexp_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mantissa, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm, name, "aten::frobenius_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm, schema_str, "frobenius_norm(Tensor self) -> Tensor")

// aten::frobenius_norm(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<frobenius_norm::schema> create_frobenius_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frobenius_norm::name, frobenius_norm::overload_name)
      .typed<frobenius_norm::schema>();
}

// aten::frobenius_norm(Tensor self) -> Tensor
at::Tensor frobenius_norm::call(const at::Tensor & self) {
    static auto op = create_frobenius_norm_typed_handle();
    return op.call(self);
}

// aten::frobenius_norm(Tensor self) -> Tensor
at::Tensor frobenius_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_frobenius_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_dim, name, "aten::frobenius_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_dim, schema_str, "frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor")

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<frobenius_norm_dim::schema> create_frobenius_norm_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frobenius_norm_dim::name, frobenius_norm_dim::overload_name)
      .typed<frobenius_norm_dim::schema>();
}

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor frobenius_norm_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_frobenius_norm_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor frobenius_norm_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_frobenius_norm_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_out, name, "aten::frobenius_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(frobenius_norm_out, schema_str, "frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<frobenius_norm_out::schema> create_frobenius_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(frobenius_norm_out::name, frobenius_norm_out::overload_name)
      .typed<frobenius_norm_out::schema>();
}

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & frobenius_norm_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_frobenius_norm_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & frobenius_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_frobenius_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm, name, "aten::nuclear_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm, schema_str, "nuclear_norm(Tensor self, bool keepdim=False) -> Tensor")

// aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nuclear_norm::schema> create_nuclear_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nuclear_norm::name, nuclear_norm::overload_name)
      .typed<nuclear_norm::schema>();
}

// aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor
at::Tensor nuclear_norm::call(const at::Tensor & self, bool keepdim) {
    static auto op = create_nuclear_norm_typed_handle();
    return op.call(self, keepdim);
}

// aten::nuclear_norm(Tensor self, bool keepdim=False) -> Tensor
at::Tensor nuclear_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool keepdim) {
    static auto op = create_nuclear_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_out, name, "aten::nuclear_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_out, schema_str, "nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nuclear_norm_out::schema> create_nuclear_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nuclear_norm_out::name, nuclear_norm_out::overload_name)
      .typed<nuclear_norm_out::schema>();
}

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nuclear_norm_out::call(const at::Tensor & self, bool keepdim, at::Tensor & out) {
    static auto op = create_nuclear_norm_out_typed_handle();
    return op.call(self, keepdim, out);
}

// aten::nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nuclear_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool keepdim, at::Tensor & out) {
    static auto op = create_nuclear_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_dim, name, "aten::nuclear_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_dim, overload_name, "dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_dim, schema_str, "nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor")

// aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nuclear_norm_dim::schema> create_nuclear_norm_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nuclear_norm_dim::name, nuclear_norm_dim::overload_name)
      .typed<nuclear_norm_dim::schema>();
}

// aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor
at::Tensor nuclear_norm_dim::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_nuclear_norm_dim_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -> Tensor
at::Tensor nuclear_norm_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_nuclear_norm_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_dim_out, name, "aten::nuclear_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_dim_out, overload_name, "dim_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nuclear_norm_dim_out, schema_str, "nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nuclear_norm_dim_out::schema> create_nuclear_norm_dim_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nuclear_norm_dim_out::name, nuclear_norm_dim_out::overload_name)
      .typed<nuclear_norm_dim_out::schema>();
}

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nuclear_norm_dim_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_nuclear_norm_dim_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nuclear_norm_dim_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_nuclear_norm_dim_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone, name, "aten::clone")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(clone, schema_str, "clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor")

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<clone::schema> create_clone_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(clone::name, clone::overload_name)
      .typed<clone::schema>();
}

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
at::Tensor clone::call(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_clone_typed_handle();
    return op.call(self, memory_format);
}

// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor
at::Tensor clone::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_clone_typed_handle();
    return op.redispatch(dispatchKeySet, self, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(positive, name, "aten::positive")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(positive, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(positive, schema_str, "positive(Tensor(a) self) -> Tensor(a)")

// aten::positive(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<positive::schema> create_positive_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(positive::name, positive::overload_name)
      .typed<positive::schema>();
}

// aten::positive(Tensor(a) self) -> Tensor(a)
at::Tensor positive::call(const at::Tensor & self) {
    static auto op = create_positive_typed_handle();
    return op.call(self);
}

// aten::positive(Tensor(a) self) -> Tensor(a)
at::Tensor positive::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_positive_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_, name, "aten::resize_as_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_, schema_str, "resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)")

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_::schema> create_resize_as__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_::name, resize_as_::overload_name)
      .typed<resize_as_::schema>();
}

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
const at::Tensor & resize_as_::call(const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_resize_as__typed_handle();
    return op.call(self, the_template, memory_format);
}

// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -> Tensor(a!)
const at::Tensor & resize_as_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_resize_as__typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_, name, "aten::resize_as_sparse_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(resize_as_sparse_, schema_str, "resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)")

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<resize_as_sparse_::schema> create_resize_as_sparse__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(resize_as_sparse_::name, resize_as_sparse_::overload_name)
      .typed<resize_as_sparse_::schema>();
}

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
const at::Tensor & resize_as_sparse_::call(const at::Tensor & self, const at::Tensor & the_template) {
    static auto op = create_resize_as_sparse__typed_handle();
    return op.call(self, the_template);
}

// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -> Tensor(a!)
const at::Tensor & resize_as_sparse_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & the_template) {
    static auto op = create_resize_as_sparse__typed_handle();
    return op.redispatch(dispatchKeySet, self, the_template);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_, name, "aten::zero_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(zero_, schema_str, "zero_(Tensor(a!) self) -> Tensor(a!)")

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<zero_::schema> create_zero__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(zero_::name, zero_::overload_name)
      .typed<zero_::schema>();
}

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & zero_::call(at::Tensor & self) {
    static auto op = create_zero__typed_handle();
    return op.call(self);
}

// aten::zero_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & zero_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_zero__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_out, name, "aten::sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_out, schema_str, "sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sub_out::schema> create_sub_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sub_out::name, sub_out::overload_name)
      .typed<sub_out::schema>();
}

// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sub_out::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_sub_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sub_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_sub_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_Tensor, name, "aten::sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_Tensor, schema_str, "sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sub_Tensor::schema> create_sub_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sub_Tensor::name, sub_Tensor::overload_name)
      .typed<sub_Tensor::schema>();
}

// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor sub_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_sub_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor sub_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_sub_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub__Tensor, name, "aten::sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub__Tensor, schema_str, "sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")

// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sub__Tensor::schema> create_sub__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sub__Tensor::name, sub__Tensor::overload_name)
      .typed<sub__Tensor::schema>();
}

// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & sub__Tensor::call(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_sub__Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & sub__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_sub__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_Scalar, name, "aten::sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub_Scalar, schema_str, "sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sub_Scalar::schema> create_sub_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sub_Scalar::name, sub_Scalar::overload_name)
      .typed<sub_Scalar::schema>();
}

// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor sub_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_sub_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor sub_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_sub_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub__Scalar, name, "aten::sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sub__Scalar, schema_str, "sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)")

// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sub__Scalar::schema> create_sub__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sub__Scalar::name, sub__Scalar::overload_name)
      .typed<sub__Scalar::schema>();
}

// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & sub__Scalar::call(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_sub__Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & sub__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_sub__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_out, name, "aten::subtract")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_out, schema_str, "subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<subtract_out::schema> create_subtract_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(subtract_out::name, subtract_out::overload_name)
      .typed<subtract_out::schema>();
}

// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & subtract_out::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_subtract_out_typed_handle();
    return op.call(self, other, alpha, out);
}

// aten::subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & subtract_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_subtract_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_Tensor, name, "aten::subtract")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_Tensor, schema_str, "subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<subtract_Tensor::schema> create_subtract_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(subtract_Tensor::name, subtract_Tensor::overload_name)
      .typed<subtract_Tensor::schema>();
}

// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor subtract_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_subtract_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor subtract_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_subtract_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract__Tensor, name, "aten::subtract_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract__Tensor, schema_str, "subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)")

// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<subtract__Tensor::schema> create_subtract__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(subtract__Tensor::name, subtract__Tensor::overload_name)
      .typed<subtract__Tensor::schema>();
}

// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & subtract__Tensor::call(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_subtract__Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
at::Tensor & subtract__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_subtract__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_Scalar, name, "aten::subtract")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract_Scalar, schema_str, "subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<subtract_Scalar::schema> create_subtract_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(subtract_Scalar::name, subtract_Scalar::overload_name)
      .typed<subtract_Scalar::schema>();
}

// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor subtract_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_subtract_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor subtract_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_subtract_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract__Scalar, name, "aten::subtract_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(subtract__Scalar, schema_str, "subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)")

// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<subtract__Scalar::schema> create_subtract__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(subtract__Scalar::name, subtract__Scalar::overload_name)
      .typed<subtract__Scalar::schema>();
}

// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & subtract__Scalar::call(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_subtract__Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
at::Tensor & subtract__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_subtract__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor, name, "aten::rsub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Tensor, schema_str, "rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor")

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rsub_Tensor::schema> create_rsub_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsub_Tensor::name, rsub_Tensor::overload_name)
      .typed<rsub_Tensor::schema>();
}

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor rsub_Tensor::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_rsub_Tensor_typed_handle();
    return op.call(self, other, alpha);
}

// aten::rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
at::Tensor rsub_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create_rsub_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside_out, name, "aten::heaviside")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside_out, schema_str, "heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)")

// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<heaviside_out::schema> create_heaviside_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(heaviside_out::name, heaviside_out::overload_name)
      .typed<heaviside_out::schema>();
}

// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & heaviside_out::call(const at::Tensor & self, const at::Tensor & values, at::Tensor & out) {
    static auto op = create_heaviside_out_typed_handle();
    return op.call(self, values, out);
}

// aten::heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & heaviside_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & values, at::Tensor & out) {
    static auto op = create_heaviside_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, values, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside, name, "aten::heaviside")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside, schema_str, "heaviside(Tensor self, Tensor values) -> Tensor")

// aten::heaviside(Tensor self, Tensor values) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<heaviside::schema> create_heaviside_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(heaviside::name, heaviside::overload_name)
      .typed<heaviside::schema>();
}

// aten::heaviside(Tensor self, Tensor values) -> Tensor
at::Tensor heaviside::call(const at::Tensor & self, const at::Tensor & values) {
    static auto op = create_heaviside_typed_handle();
    return op.call(self, values);
}

// aten::heaviside(Tensor self, Tensor values) -> Tensor
at::Tensor heaviside::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & values) {
    static auto op = create_heaviside_typed_handle();
    return op.redispatch(dispatchKeySet, self, values);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside_, name, "aten::heaviside_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(heaviside_, schema_str, "heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)")

// aten::heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<heaviside_::schema> create_heaviside__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(heaviside_::name, heaviside_::overload_name)
      .typed<heaviside_::schema>();
}

// aten::heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)
at::Tensor & heaviside_::call(at::Tensor & self, const at::Tensor & values) {
    static auto op = create_heaviside__typed_handle();
    return op.call(self, values);
}

// aten::heaviside_(Tensor(a!) self, Tensor values) -> Tensor(a!)
at::Tensor & heaviside_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & values) {
    static auto op = create_heaviside__typed_handle();
    return op.redispatch(dispatchKeySet, self, values);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar, name, "aten::rsub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rsub_Scalar, schema_str, "rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor")

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rsub_Scalar::schema> create_rsub_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rsub_Scalar::name, rsub_Scalar::overload_name)
      .typed<rsub_Scalar::schema>();
}

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor rsub_Scalar::call(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_rsub_Scalar_typed_handle();
    return op.call(self, other, alpha);
}

// aten::rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
at::Tensor rsub_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    static auto op = create_rsub_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_addmm, name, "aten::_sparse_addmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_addmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_addmm, schema_str, "_sparse_addmm(Tensor self, Tensor sparse, Tensor dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::_sparse_addmm(Tensor self, Tensor sparse, Tensor dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_addmm::schema> create__sparse_addmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_addmm::name, _sparse_addmm::overload_name)
      .typed<_sparse_addmm::schema>();
}

// aten::_sparse_addmm(Tensor self, Tensor sparse, Tensor dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor _sparse_addmm::call(const at::Tensor & self, const at::Tensor & sparse, const at::Tensor & dense, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create__sparse_addmm_typed_handle();
    return op.call(self, sparse, dense, beta, alpha);
}

// aten::_sparse_addmm(Tensor self, Tensor sparse, Tensor dense, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor _sparse_addmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & sparse, const at::Tensor & dense, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create__sparse_addmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, sparse, dense, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm_out, name, "aten::addmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm_out, schema_str, "addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addmm_out::schema> create_addmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmm_out::name, addmm_out::overload_name)
      .typed<addmm_out::schema>();
}

// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addmm_out::call(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addmm_out_typed_handle();
    return op.call(self, mat1, mat2, beta, alpha, out);
}

// aten::addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm, name, "aten::addmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm, schema_str, "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addmm::schema> create_addmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmm::name, addmm::overload_name)
      .typed<addmm::schema>();
}

// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addmm::call(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmm_typed_handle();
    return op.call(self, mat1, mat2, beta, alpha);
}

// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm_, name, "aten::addmm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addmm_, schema_str, "addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addmm_::schema> create_addmm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addmm_::name, addmm_::overload_name)
      .typed<addmm_::schema>();
}

// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addmm_::call(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmm__typed_handle();
    return op.call(self, mat1, mat2, beta, alpha);
}

// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addmm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addmm__typed_handle();
    return op.redispatch(dispatchKeySet, self, mat1, mat2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value_size, name, "aten::sparse_csr_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value_size, overload_name, "crow_col_value_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value_size, schema_str, "sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_csr_tensor_crow_col_value_size::schema> create_sparse_csr_tensor_crow_col_value_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_csr_tensor_crow_col_value_size::name, sparse_csr_tensor_crow_col_value_size::overload_name)
      .typed<sparse_csr_tensor_crow_col_value_size::schema>();
}

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value_size::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_csr_tensor_crow_col_value_size_typed_handle();
    return op.call(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_csr_tensor_crow_col_value_size_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value, name, "aten::sparse_csr_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value, overload_name, "crow_col_value")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_csr_tensor_crow_col_value, schema_str, "sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_csr_tensor_crow_col_value::schema> create_sparse_csr_tensor_crow_col_value_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_csr_tensor_crow_col_value::name, sparse_csr_tensor_crow_col_value::overload_name)
      .typed<sparse_csr_tensor_crow_col_value::schema>();
}

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_csr_tensor_crow_col_value_typed_handle();
    return op.call(crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}

// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_csr_tensor_crow_col_value::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_csr_tensor_crow_col_value_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_tensor_unsafe, name, "aten::_sparse_csr_tensor_unsafe")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_tensor_unsafe, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_csr_tensor_unsafe, schema_str, "_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_csr_tensor_unsafe::schema> create__sparse_csr_tensor_unsafe_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_csr_tensor_unsafe::name, _sparse_csr_tensor_unsafe::overload_name)
      .typed<_sparse_csr_tensor_unsafe::schema>();
}

// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_csr_tensor_unsafe::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_csr_tensor_unsafe_typed_handle();
    return op.call(crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_csr_tensor_unsafe::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_csr_tensor_unsafe_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_size, name, "aten::sparse_coo_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_size, overload_name, "size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_size, schema_str, "sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_coo_tensor_size::schema> create_sparse_coo_tensor_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_coo_tensor_size::name, sparse_coo_tensor_size::overload_name)
      .typed<sparse_coo_tensor_size::schema>();
}

// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_coo_tensor_size::call(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_coo_tensor_size_typed_handle();
    return op.call(size, dtype, layout, device, pin_memory);
}

// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor sparse_coo_tensor_size::redispatch(c10::DispatchKeySet dispatchKeySet, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_coo_tensor_size_typed_handle();
    return op.redispatch(dispatchKeySet, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_indices, name, "aten::sparse_coo_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_indices, overload_name, "indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_indices, schema_str, "sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_coo_tensor_indices::schema> create_sparse_coo_tensor_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_coo_tensor_indices::name, sparse_coo_tensor_indices::overload_name)
      .typed<sparse_coo_tensor_indices::schema>();
}

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor sparse_coo_tensor_indices::call(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_coo_tensor_indices_typed_handle();
    return op.call(indices, values, dtype, layout, device, pin_memory);
}

// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor sparse_coo_tensor_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_coo_tensor_indices_typed_handle();
    return op.redispatch(dispatchKeySet, indices, values, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_indices_size, name, "aten::sparse_coo_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_indices_size, overload_name, "indices_size")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_coo_tensor_indices_size, schema_str, "sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_coo_tensor_indices_size::schema> create_sparse_coo_tensor_indices_size_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_coo_tensor_indices_size::name, sparse_coo_tensor_indices_size::overload_name)
      .typed<sparse_coo_tensor_indices_size::schema>();
}

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor sparse_coo_tensor_indices_size::call(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_coo_tensor_indices_size_typed_handle();
    return op.call(indices, values, size, dtype, layout, device, pin_memory);
}

// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor sparse_coo_tensor_indices_size::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_sparse_coo_tensor_indices_size_typed_handle();
    return op.redispatch(dispatchKeySet, indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_unsafe, name, "aten::_sparse_coo_tensor_unsafe")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_unsafe, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_unsafe, schema_str, "_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_coo_tensor_unsafe::schema> create__sparse_coo_tensor_unsafe_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_coo_tensor_unsafe::name, _sparse_coo_tensor_unsafe::overload_name)
      .typed<_sparse_coo_tensor_unsafe::schema>();
}

// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_coo_tensor_unsafe::call(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_coo_tensor_unsafe_typed_handle();
    return op.call(indices, values, size, dtype, layout, device, pin_memory);
}

// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor _sparse_coo_tensor_unsafe::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_coo_tensor_unsafe_typed_handle();
    return op.redispatch(dispatchKeySet, indices, values, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_coo_tensor_args, name, "aten::_validate_sparse_coo_tensor_args")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_coo_tensor_args, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_coo_tensor_args, schema_str, "_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size) -> ()")

// aten::_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_validate_sparse_coo_tensor_args::schema> create__validate_sparse_coo_tensor_args_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_validate_sparse_coo_tensor_args::name, _validate_sparse_coo_tensor_args::overload_name)
      .typed<_validate_sparse_coo_tensor_args::schema>();
}

// aten::_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size) -> ()
void _validate_sparse_coo_tensor_args::call(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size) {
    static auto op = create__validate_sparse_coo_tensor_args_typed_handle();
    return op.call(indices, values, size);
}

// aten::_validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size) -> ()
void _validate_sparse_coo_tensor_args::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size) {
    static auto op = create__validate_sparse_coo_tensor_args_typed_handle();
    return op.redispatch(dispatchKeySet, indices, values, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_csr_tensor_args, name, "aten::_validate_sparse_csr_tensor_args")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_csr_tensor_args, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_validate_sparse_csr_tensor_args, schema_str, "_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()")

// aten::_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_validate_sparse_csr_tensor_args::schema> create__validate_sparse_csr_tensor_args_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_validate_sparse_csr_tensor_args::name, _validate_sparse_csr_tensor_args::overload_name)
      .typed<_validate_sparse_csr_tensor_args::schema>();
}

// aten::_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()
void _validate_sparse_csr_tensor_args::call(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
    static auto op = create__validate_sparse_csr_tensor_args_typed_handle();
    return op.call(crow_indices, col_indices, values, size);
}

// aten::_validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size) -> ()
void _validate_sparse_csr_tensor_args::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size) {
    static auto op = create__validate_sparse_csr_tensor_args_typed_handle();
    return op.redispatch(dispatchKeySet, crow_indices, col_indices, values, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims, name, "aten::_sparse_coo_tensor_with_dims")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims, schema_str, "_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_coo_tensor_with_dims::schema> create__sparse_coo_tensor_with_dims_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_coo_tensor_with_dims::name, _sparse_coo_tensor_with_dims::overload_name)
      .typed<_sparse_coo_tensor_with_dims::schema>();
}

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _sparse_coo_tensor_with_dims::call(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_coo_tensor_with_dims_typed_handle();
    return op.call(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}

// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _sparse_coo_tensor_with_dims::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_coo_tensor_with_dims_typed_handle();
    return op.redispatch(dispatchKeySet, sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims_and_tensors, name, "aten::_sparse_coo_tensor_with_dims_and_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims_and_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_sparse_coo_tensor_with_dims_and_tensors, schema_str, "_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor")

// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_sparse_coo_tensor_with_dims_and_tensors::schema> create__sparse_coo_tensor_with_dims_and_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_sparse_coo_tensor_with_dims_and_tensors::name, _sparse_coo_tensor_with_dims_and_tensors::overload_name)
      .typed<_sparse_coo_tensor_with_dims_and_tensors::schema>();
}

// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _sparse_coo_tensor_with_dims_and_tensors::call(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_coo_tensor_with_dims_and_tensors_typed_handle();
    return op.call(sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory);
}

// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
at::Tensor _sparse_coo_tensor_with_dims_and_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create__sparse_coo_tensor_with_dims_and_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_resize_, name, "aten::sparse_resize_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_resize_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_resize_, schema_str, "sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)")

// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sparse_resize_::schema> create_sparse_resize__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_resize_::name, sparse_resize_::overload_name)
      .typed<sparse_resize_::schema>();
}

// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)
const at::Tensor & sparse_resize_::call(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
    static auto op = create_sparse_resize__typed_handle();
    return op.call(self, size, sparse_dim, dense_dim);
}

// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)
const at::Tensor & sparse_resize_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
    static auto op = create_sparse_resize__typed_handle();
    return op.redispatch(dispatchKeySet, self, size, sparse_dim, dense_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_resize_and_clear_, name, "aten::sparse_resize_and_clear_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_resize_and_clear_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_resize_and_clear_, schema_str, "sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)")

// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sparse_resize_and_clear_::schema> create_sparse_resize_and_clear__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_resize_and_clear_::name, sparse_resize_and_clear_::overload_name)
      .typed<sparse_resize_and_clear_::schema>();
}

// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)
const at::Tensor & sparse_resize_and_clear_::call(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
    static auto op = create_sparse_resize_and_clear__typed_handle();
    return op.call(self, size, sparse_dim, dense_dim);
}

// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -> Tensor(a!)
const at::Tensor & sparse_resize_and_clear_::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
    static auto op = create_sparse_resize_and_clear__typed_handle();
    return op.redispatch(dispatchKeySet, self, size, sparse_dim, dense_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_mask, name, "aten::sparse_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_mask, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_mask, schema_str, "sparse_mask(Tensor self, Tensor mask) -> Tensor")

// aten::sparse_mask(Tensor self, Tensor mask) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sparse_mask::schema> create_sparse_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_mask::name, sparse_mask::overload_name)
      .typed<sparse_mask::schema>();
}

// aten::sparse_mask(Tensor self, Tensor mask) -> Tensor
at::Tensor sparse_mask::call(const at::Tensor & self, const at::Tensor & mask) {
    static auto op = create_sparse_mask_typed_handle();
    return op.call(self, mask);
}

// aten::sparse_mask(Tensor self, Tensor mask) -> Tensor
at::Tensor sparse_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask) {
    static auto op = create_sparse_mask_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_cpu, name, "aten::_to_cpu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_cpu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_cpu, schema_str, "_to_cpu(Tensor[] tensors) -> Tensor[]")

// aten::_to_cpu(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_to_cpu::schema> create__to_cpu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_cpu::name, _to_cpu::overload_name)
      .typed<_to_cpu::schema>();
}

// aten::_to_cpu(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _to_cpu::call(at::TensorList tensors) {
    static auto op = create__to_cpu_typed_handle();
    return op.call(tensors);
}

// aten::_to_cpu(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _to_cpu::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__to_cpu_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense, name, "aten::to_dense")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense, schema_str, "to_dense(Tensor self, ScalarType? dtype=None) -> Tensor")

// aten::to_dense(Tensor self, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_dense::schema> create_to_dense_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dense::name, to_dense::overload_name)
      .typed<to_dense::schema>();
}

// aten::to_dense(Tensor self, ScalarType? dtype=None) -> Tensor
at::Tensor to_dense::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_to_dense_typed_handle();
    return op.call(self, dtype);
}

// aten::to_dense(Tensor self, ScalarType? dtype=None) -> Tensor
at::Tensor to_dense::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_to_dense_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense_backward, name, "aten::to_dense_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dense_backward, schema_str, "to_dense_backward(Tensor grad, Tensor input) -> Tensor")

// aten::to_dense_backward(Tensor grad, Tensor input) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_dense_backward::schema> create_to_dense_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dense_backward::name, to_dense_backward::overload_name)
      .typed<to_dense_backward::schema>();
}

// aten::to_dense_backward(Tensor grad, Tensor input) -> Tensor
at::Tensor to_dense_backward::call(const at::Tensor & grad, const at::Tensor & input) {
    static auto op = create_to_dense_backward_typed_handle();
    return op.call(grad, input);
}

// aten::to_dense_backward(Tensor grad, Tensor input) -> Tensor
at::Tensor to_dense_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input) {
    static auto op = create_to_dense_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_dim, name, "aten::sparse_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sparse_dim, schema_str, "sparse_dim(Tensor self) -> int")

// aten::sparse_dim(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<sparse_dim::schema> create_sparse_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sparse_dim::name, sparse_dim::overload_name)
      .typed<sparse_dim::schema>();
}

// aten::sparse_dim(Tensor self) -> int
int64_t sparse_dim::call(const at::Tensor & self) {
    static auto op = create_sparse_dim_typed_handle();
    return op.call(self);
}

// aten::sparse_dim(Tensor self) -> int
int64_t sparse_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sparse_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimI, name, "aten::_dimI")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimI, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimI, schema_str, "_dimI(Tensor self) -> int")

// aten::_dimI(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_dimI::schema> create__dimI_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dimI::name, _dimI::overload_name)
      .typed<_dimI::schema>();
}

// aten::_dimI(Tensor self) -> int
int64_t _dimI::call(const at::Tensor & self) {
    static auto op = create__dimI_typed_handle();
    return op.call(self);
}

// aten::_dimI(Tensor self) -> int
int64_t _dimI::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__dimI_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dense_dim, name, "aten::dense_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dense_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dense_dim, schema_str, "dense_dim(Tensor self) -> int")

// aten::dense_dim(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<dense_dim::schema> create_dense_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dense_dim::name, dense_dim::overload_name)
      .typed<dense_dim::schema>();
}

// aten::dense_dim(Tensor self) -> int
int64_t dense_dim::call(const at::Tensor & self) {
    static auto op = create_dense_dim_typed_handle();
    return op.call(self);
}

// aten::dense_dim(Tensor self) -> int
int64_t dense_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_dense_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimV, name, "aten::_dimV")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimV, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_dimV, schema_str, "_dimV(Tensor self) -> int")

// aten::_dimV(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_dimV::schema> create__dimV_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_dimV::name, _dimV::overload_name)
      .typed<_dimV::schema>();
}

// aten::_dimV(Tensor self) -> int
int64_t _dimV::call(const at::Tensor & self) {
    static auto op = create__dimV_typed_handle();
    return op.call(self);
}

// aten::_dimV(Tensor self) -> int
int64_t _dimV::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__dimV_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnz, name, "aten::_nnz")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnz, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_nnz, schema_str, "_nnz(Tensor self) -> int")

// aten::_nnz(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<_nnz::schema> create__nnz_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_nnz::name, _nnz::overload_name)
      .typed<_nnz::schema>();
}

// aten::_nnz(Tensor self) -> int
int64_t _nnz::call(const at::Tensor & self) {
    static auto op = create__nnz_typed_handle();
    return op.call(self);
}

// aten::_nnz(Tensor self) -> int
int64_t _nnz::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__nnz_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(coalesce, name, "aten::coalesce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(coalesce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(coalesce, schema_str, "coalesce(Tensor(a) self) -> Tensor(a)")

// aten::coalesce(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<coalesce::schema> create_coalesce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(coalesce::name, coalesce::overload_name)
      .typed<coalesce::schema>();
}

// aten::coalesce(Tensor(a) self) -> Tensor(a)
at::Tensor coalesce::call(const at::Tensor & self) {
    static auto op = create_coalesce_typed_handle();
    return op.call(self);
}

// aten::coalesce(Tensor(a) self) -> Tensor(a)
at::Tensor coalesce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_coalesce_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce, name, "aten::_coalesce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesce, schema_str, "_coalesce(Tensor self) -> Tensor")

// aten::_coalesce(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_coalesce::schema> create__coalesce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_coalesce::name, _coalesce::overload_name)
      .typed<_coalesce::schema>();
}

// aten::_coalesce(Tensor self) -> Tensor
at::Tensor _coalesce::call(const at::Tensor & self) {
    static auto op = create__coalesce_typed_handle();
    return op.call(self);
}

// aten::_coalesce(Tensor self) -> Tensor
at::Tensor _coalesce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__coalesce_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_coalesced, name, "aten::is_coalesced")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_coalesced, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_coalesced, schema_str, "is_coalesced(Tensor self) -> bool")

// aten::is_coalesced(Tensor self) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_coalesced::schema> create_is_coalesced_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_coalesced::name, is_coalesced::overload_name)
      .typed<is_coalesced::schema>();
}

// aten::is_coalesced(Tensor self) -> bool
bool is_coalesced::call(const at::Tensor & self) {
    static auto op = create_is_coalesced_typed_handle();
    return op.call(self);
}

// aten::is_coalesced(Tensor self) -> bool
bool is_coalesced::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_is_coalesced_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_indices, name, "aten::_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_indices, schema_str, "_indices(Tensor(a) self) -> Tensor(a)")

// aten::_indices(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_indices::schema> create__indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_indices::name, _indices::overload_name)
      .typed<_indices::schema>();
}

// aten::_indices(Tensor(a) self) -> Tensor(a)
at::Tensor _indices::call(const at::Tensor & self) {
    static auto op = create__indices_typed_handle();
    return op.call(self);
}

// aten::_indices(Tensor(a) self) -> Tensor(a)
at::Tensor _indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__indices_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_values, name, "aten::_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_values, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_values, schema_str, "_values(Tensor(a) self) -> Tensor(a)")

// aten::_values(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<_values::schema> create__values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_values::name, _values::overload_name)
      .typed<_values::schema>();
}

// aten::_values(Tensor(a) self) -> Tensor(a)
at::Tensor _values::call(const at::Tensor & self) {
    static auto op = create__values_typed_handle();
    return op.call(self);
}

// aten::_values(Tensor(a) self) -> Tensor(a)
at::Tensor _values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__values_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesced_, name, "aten::_coalesced_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesced_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_coalesced_, schema_str, "_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)")

// aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_coalesced_::schema> create__coalesced__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_coalesced_::name, _coalesced_::overload_name)
      .typed<_coalesced_::schema>();
}

// aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)
at::Tensor & _coalesced_::call(at::Tensor & self, bool coalesced) {
    static auto op = create__coalesced__typed_handle();
    return op.call(self, coalesced);
}

// aten::_coalesced_(Tensor(a!) self, bool coalesced) -> Tensor(a!)
at::Tensor & _coalesced_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, bool coalesced) {
    static auto op = create__coalesced__typed_handle();
    return op.redispatch(dispatchKeySet, self, coalesced);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices, name, "aten::indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(indices, schema_str, "indices(Tensor(a) self) -> Tensor(a)")

// aten::indices(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<indices::schema> create_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(indices::name, indices::overload_name)
      .typed<indices::schema>();
}

// aten::indices(Tensor(a) self) -> Tensor(a)
at::Tensor indices::call(const at::Tensor & self) {
    static auto op = create_indices_typed_handle();
    return op.call(self);
}

// aten::indices(Tensor(a) self) -> Tensor(a)
at::Tensor indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(values, name, "aten::values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(values, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(values, schema_str, "values(Tensor(a) self) -> Tensor(a)")

// aten::values(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<values::schema> create_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(values::name, values::overload_name)
      .typed<values::schema>();
}

// aten::values(Tensor(a) self) -> Tensor(a)
at::Tensor values::call(const at::Tensor & self) {
    static auto op = create_values_typed_handle();
    return op.call(self);
}

// aten::values(Tensor(a) self) -> Tensor(a)
at::Tensor values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_values_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(crow_indices, name, "aten::crow_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(crow_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(crow_indices, schema_str, "crow_indices(Tensor(a) self) -> Tensor(a)")

// aten::crow_indices(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<crow_indices::schema> create_crow_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(crow_indices::name, crow_indices::overload_name)
      .typed<crow_indices::schema>();
}

// aten::crow_indices(Tensor(a) self) -> Tensor(a)
at::Tensor crow_indices::call(const at::Tensor & self) {
    static auto op = create_crow_indices_typed_handle();
    return op.call(self);
}

// aten::crow_indices(Tensor(a) self) -> Tensor(a)
at::Tensor crow_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_crow_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col_indices, name, "aten::col_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col_indices, schema_str, "col_indices(Tensor(a) self) -> Tensor(a)")

// aten::col_indices(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<col_indices::schema> create_col_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col_indices::name, col_indices::overload_name)
      .typed<col_indices::schema>();
}

// aten::col_indices(Tensor(a) self) -> Tensor(a)
at::Tensor col_indices::call(const at::Tensor & self) {
    static auto op = create_col_indices_typed_handle();
    return op.call(self);
}

// aten::col_indices(Tensor(a) self) -> Tensor(a)
at::Tensor col_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_col_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hspmm_out, name, "aten::hspmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hspmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hspmm_out, schema_str, "hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hspmm_out::schema> create_hspmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hspmm_out::name, hspmm_out::overload_name)
      .typed<hspmm_out::schema>();
}

// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hspmm_out::call(const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
    static auto op = create_hspmm_out_typed_handle();
    return op.call(mat1, mat2, out);
}

// aten::hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hspmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mat1, const at::Tensor & mat2, at::Tensor & out) {
    static auto op = create_hspmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, mat1, mat2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hspmm, name, "aten::hspmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hspmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hspmm, schema_str, "hspmm(Tensor mat1, Tensor mat2) -> Tensor")

// aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hspmm::schema> create_hspmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hspmm::name, hspmm::overload_name)
      .typed<hspmm::schema>();
}

// aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor
at::Tensor hspmm::call(const at::Tensor & mat1, const at::Tensor & mat2) {
    static auto op = create_hspmm_typed_handle();
    return op.call(mat1, mat2);
}

// aten::hspmm(Tensor mat1, Tensor mat2) -> Tensor
at::Tensor hspmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mat1, const at::Tensor & mat2) {
    static auto op = create_hspmm_typed_handle();
    return op.redispatch(dispatchKeySet, mat1, mat2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_sparse_to_sparse_, name, "aten::copy_sparse_to_sparse_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_sparse_to_sparse_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(copy_sparse_to_sparse_, schema_str, "copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)")

// aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<copy_sparse_to_sparse_::schema> create_copy_sparse_to_sparse__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(copy_sparse_to_sparse_::name, copy_sparse_to_sparse_::overload_name)
      .typed<copy_sparse_to_sparse_::schema>();
}

// aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
at::Tensor & copy_sparse_to_sparse_::call(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    static auto op = create_copy_sparse_to_sparse__typed_handle();
    return op.call(self, src, non_blocking);
}

// aten::copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
at::Tensor & copy_sparse_to_sparse_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    static auto op = create_copy_sparse_to_sparse__typed_handle();
    return op.redispatch(dispatchKeySet, self, src, non_blocking);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unbind_int, name, "aten::unbind")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unbind_int, overload_name, "int")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unbind_int, schema_str, "unbind.int(Tensor(a) self, int dim=0) -> Tensor(a)[]")

// aten::unbind.int(Tensor(a) self, int dim=0) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<unbind_int::schema> create_unbind_int_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unbind_int::name, unbind_int::overload_name)
      .typed<unbind_int::schema>();
}

// aten::unbind.int(Tensor(a) self, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> unbind_int::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_unbind_int_typed_handle();
    return op.call(self, dim);
}

// aten::unbind.int(Tensor(a) self, int dim=0) -> Tensor(a)[]
::std::vector<at::Tensor> unbind_int::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_unbind_int_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unbind_Dimname, name, "aten::unbind")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unbind_Dimname, overload_name, "Dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unbind_Dimname, schema_str, "unbind.Dimname(Tensor(a) self, Dimname dim) -> Tensor(a)[]")

// aten::unbind.Dimname(Tensor(a) self, Dimname dim) -> Tensor(a)[]
static C10_NOINLINE c10::TypedOperatorHandle<unbind_Dimname::schema> create_unbind_Dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unbind_Dimname::name, unbind_Dimname::overload_name)
      .typed<unbind_Dimname::schema>();
}

// aten::unbind.Dimname(Tensor(a) self, Dimname dim) -> Tensor(a)[]
::std::vector<at::Tensor> unbind_Dimname::call(const at::Tensor & self, at::Dimname dim) {
    static auto op = create_unbind_Dimname_typed_handle();
    return op.call(self, dim);
}

// aten::unbind.Dimname(Tensor(a) self, Dimname dim) -> Tensor(a)[]
::std::vector<at::Tensor> unbind_Dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim) {
    static auto op = create_unbind_Dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse_sparse_dim, name, "aten::to_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse_sparse_dim, overload_name, "sparse_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse_sparse_dim, schema_str, "to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor")

// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_sparse_sparse_dim::schema> create_to_sparse_sparse_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_sparse_sparse_dim::name, to_sparse_sparse_dim::overload_name)
      .typed<to_sparse_sparse_dim::schema>();
}

// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor
at::Tensor to_sparse_sparse_dim::call(const at::Tensor & self, int64_t sparse_dim) {
    static auto op = create_to_sparse_sparse_dim_typed_handle();
    return op.call(self, sparse_dim);
}

// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -> Tensor
at::Tensor to_sparse_sparse_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t sparse_dim) {
    static auto op = create_to_sparse_sparse_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, sparse_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse, name, "aten::to_sparse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_sparse, schema_str, "to_sparse(Tensor self) -> Tensor")

// aten::to_sparse(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_sparse::schema> create_to_sparse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_sparse::name, to_sparse::overload_name)
      .typed<to_sparse::schema>();
}

// aten::to_sparse(Tensor self) -> Tensor
at::Tensor to_sparse::call(const at::Tensor & self) {
    static auto op = create_to_sparse_typed_handle();
    return op.call(self);
}

// aten::to_sparse(Tensor self) -> Tensor
at::Tensor to_sparse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_to_sparse_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_mkldnn, name, "aten::to_mkldnn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_mkldnn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_mkldnn, schema_str, "to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor")

// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_mkldnn::schema> create_to_mkldnn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_mkldnn::name, to_mkldnn::overload_name)
      .typed<to_mkldnn::schema>();
}

// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor
at::Tensor to_mkldnn::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_to_mkldnn_typed_handle();
    return op.call(self, dtype);
}

// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor
at::Tensor to_mkldnn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    static auto op = create_to_mkldnn_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight, name, "aten::mkldnn_reorder_conv2d_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv2d_weight, schema_str, "mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor")

// aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_reorder_conv2d_weight::schema> create_mkldnn_reorder_conv2d_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_reorder_conv2d_weight::name, mkldnn_reorder_conv2d_weight::overload_name)
      .typed<mkldnn_reorder_conv2d_weight::schema>();
}

// aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor
at::Tensor mkldnn_reorder_conv2d_weight::call(const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_mkldnn_reorder_conv2d_weight_typed_handle();
    return op.call(self, padding, stride, dilation, groups);
}

// aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor
at::Tensor mkldnn_reorder_conv2d_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_mkldnn_reorder_conv2d_weight_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, stride, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv3d_weight, name, "aten::mkldnn_reorder_conv3d_weight")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv3d_weight, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_reorder_conv3d_weight, schema_str, "mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor")

// aten::mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_reorder_conv3d_weight::schema> create_mkldnn_reorder_conv3d_weight_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_reorder_conv3d_weight::name, mkldnn_reorder_conv3d_weight::overload_name)
      .typed<mkldnn_reorder_conv3d_weight::schema>();
}

// aten::mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor
at::Tensor mkldnn_reorder_conv3d_weight::call(const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_mkldnn_reorder_conv3d_weight_typed_handle();
    return op.call(self, padding, stride, dilation, groups);
}

// aten::mkldnn_reorder_conv3d_weight(Tensor self, int[3] padding=0, int[3] stride=1, int[3] dilation=1, int groups=1) -> Tensor
at::Tensor mkldnn_reorder_conv3d_weight::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    static auto op = create_mkldnn_reorder_conv3d_weight_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, stride, dilation, groups);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_mkldnn_backward, name, "aten::to_mkldnn_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_mkldnn_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_mkldnn_backward, schema_str, "to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor")

// aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<to_mkldnn_backward::schema> create_to_mkldnn_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_mkldnn_backward::name, to_mkldnn_backward::overload_name)
      .typed<to_mkldnn_backward::schema>();
}

// aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor
at::Tensor to_mkldnn_backward::call(const at::Tensor & grad, const at::Tensor & input) {
    static auto op = create_to_mkldnn_backward_typed_handle();
    return op.call(grad, input);
}

// aten::to_mkldnn_backward(Tensor grad, Tensor input) -> Tensor
at::Tensor to_mkldnn_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input) {
    static auto op = create_to_mkldnn_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor, name, "aten::quantize_per_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor, schema_str, "quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor")

// aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantize_per_tensor::schema> create_quantize_per_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantize_per_tensor::name, quantize_per_tensor::overload_name)
      .typed<quantize_per_tensor::schema>();
}

// aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
at::Tensor quantize_per_tensor::call(const at::Tensor & self, double scale, int64_t zero_point, at::ScalarType dtype) {
    static auto op = create_quantize_per_tensor_typed_handle();
    return op.call(self, scale, zero_point, dtype);
}

// aten::quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
at::Tensor quantize_per_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, at::ScalarType dtype) {
    static auto op = create_quantize_per_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor_tensor_qparams, name, "aten::quantize_per_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor_tensor_qparams, overload_name, "tensor_qparams")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor_tensor_qparams, schema_str, "quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor")

// aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantize_per_tensor_tensor_qparams::schema> create_quantize_per_tensor_tensor_qparams_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantize_per_tensor_tensor_qparams::name, quantize_per_tensor_tensor_qparams::overload_name)
      .typed<quantize_per_tensor_tensor_qparams::schema>();
}

// aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor
at::Tensor quantize_per_tensor_tensor_qparams::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, at::ScalarType dtype) {
    static auto op = create_quantize_per_tensor_tensor_qparams_typed_handle();
    return op.call(self, scale, zero_point, dtype);
}

// aten::quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -> Tensor
at::Tensor quantize_per_tensor_tensor_qparams::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, at::ScalarType dtype) {
    static auto op = create_quantize_per_tensor_tensor_qparams_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor_tensors, name, "aten::quantize_per_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor_tensors, overload_name, "tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_tensor_tensors, schema_str, "quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]")

// aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<quantize_per_tensor_tensors::schema> create_quantize_per_tensor_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantize_per_tensor_tensors::name, quantize_per_tensor_tensors::overload_name)
      .typed<quantize_per_tensor_tensors::schema>();
}

// aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]
::std::vector<at::Tensor> quantize_per_tensor_tensors::call(at::TensorList tensors, const at::Tensor & scales, const at::Tensor & zero_points, at::ScalarType dtype) {
    static auto op = create_quantize_per_tensor_tensors_typed_handle();
    return op.call(tensors, scales, zero_points, dtype);
}

// aten::quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -> Tensor[]
::std::vector<at::Tensor> quantize_per_tensor_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Tensor & scales, const at::Tensor & zero_points, at::ScalarType dtype) {
    static auto op = create_quantize_per_tensor_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scales, zero_points, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel, name, "aten::quantize_per_channel")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantize_per_channel, schema_str, "quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor")

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantize_per_channel::schema> create_quantize_per_channel_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantize_per_channel::name, quantize_per_channel::overload_name)
      .typed<quantize_per_channel::schema>();
}

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
at::Tensor quantize_per_channel::call(const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype) {
    static auto op = create_quantize_per_channel_typed_handle();
    return op.call(self, scales, zero_points, axis, dtype);
}

// aten::quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
at::Tensor quantize_per_channel::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, at::ScalarType dtype) {
    static auto op = create_quantize_per_channel_typed_handle();
    return op.redispatch(dispatchKeySet, self, scales, zero_points, axis, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self, name, "aten::dequantize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self, overload_name, "self")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_self, schema_str, "dequantize.self(Tensor self) -> Tensor")

// aten::dequantize.self(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<dequantize_self::schema> create_dequantize_self_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dequantize_self::name, dequantize_self::overload_name)
      .typed<dequantize_self::schema>();
}

// aten::dequantize.self(Tensor self) -> Tensor
at::Tensor dequantize_self::call(const at::Tensor & self) {
    static auto op = create_dequantize_self_typed_handle();
    return op.call(self);
}

// aten::dequantize.self(Tensor self) -> Tensor
at::Tensor dequantize_self::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_dequantize_self_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors, name, "aten::dequantize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors, overload_name, "tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dequantize_tensors, schema_str, "dequantize.tensors(Tensor[] tensors) -> Tensor[]")

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<dequantize_tensors::schema> create_dequantize_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dequantize_tensors::name, dequantize_tensors::overload_name)
      .typed<dequantize_tensors::schema>();
}

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> dequantize_tensors::call(at::TensorList tensors) {
    static auto op = create_dequantize_tensors_typed_handle();
    return op.call(tensors);
}

// aten::dequantize.tensors(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> dequantize_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_dequantize_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_scale, name, "aten::q_scale")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_scale, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_scale, schema_str, "q_scale(Tensor self) -> float")

// aten::q_scale(Tensor self) -> float
static C10_NOINLINE c10::TypedOperatorHandle<q_scale::schema> create_q_scale_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_scale::name, q_scale::overload_name)
      .typed<q_scale::schema>();
}

// aten::q_scale(Tensor self) -> float
double q_scale::call(const at::Tensor & self) {
    static auto op = create_q_scale_typed_handle();
    return op.call(self);
}

// aten::q_scale(Tensor self) -> float
double q_scale::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_q_scale_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_zero_point, name, "aten::q_zero_point")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_zero_point, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_zero_point, schema_str, "q_zero_point(Tensor self) -> int")

// aten::q_zero_point(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<q_zero_point::schema> create_q_zero_point_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_zero_point::name, q_zero_point::overload_name)
      .typed<q_zero_point::schema>();
}

// aten::q_zero_point(Tensor self) -> int
int64_t q_zero_point::call(const at::Tensor & self) {
    static auto op = create_q_zero_point_typed_handle();
    return op.call(self);
}

// aten::q_zero_point(Tensor self) -> int
int64_t q_zero_point::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_q_zero_point_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales, name, "aten::q_per_channel_scales")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_scales, schema_str, "q_per_channel_scales(Tensor self) -> Tensor")

// aten::q_per_channel_scales(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_scales::schema> create_q_per_channel_scales_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_scales::name, q_per_channel_scales::overload_name)
      .typed<q_per_channel_scales::schema>();
}

// aten::q_per_channel_scales(Tensor self) -> Tensor
at::Tensor q_per_channel_scales::call(const at::Tensor & self) {
    static auto op = create_q_per_channel_scales_typed_handle();
    return op.call(self);
}

// aten::q_per_channel_scales(Tensor self) -> Tensor
at::Tensor q_per_channel_scales::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_q_per_channel_scales_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points, name, "aten::q_per_channel_zero_points")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_zero_points, schema_str, "q_per_channel_zero_points(Tensor self) -> Tensor")

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_zero_points::schema> create_q_per_channel_zero_points_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_zero_points::name, q_per_channel_zero_points::overload_name)
      .typed<q_per_channel_zero_points::schema>();
}

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
at::Tensor q_per_channel_zero_points::call(const at::Tensor & self) {
    static auto op = create_q_per_channel_zero_points_typed_handle();
    return op.call(self);
}

// aten::q_per_channel_zero_points(Tensor self) -> Tensor
at::Tensor q_per_channel_zero_points::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_q_per_channel_zero_points_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_axis, name, "aten::q_per_channel_axis")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_axis, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(q_per_channel_axis, schema_str, "q_per_channel_axis(Tensor self) -> int")

// aten::q_per_channel_axis(Tensor self) -> int
static C10_NOINLINE c10::TypedOperatorHandle<q_per_channel_axis::schema> create_q_per_channel_axis_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(q_per_channel_axis::name, q_per_channel_axis::overload_name)
      .typed<q_per_channel_axis::schema>();
}

// aten::q_per_channel_axis(Tensor self) -> int
int64_t q_per_channel_axis::call(const at::Tensor & self) {
    static auto op = create_q_per_channel_axis_typed_handle();
    return op.call(self);
}

// aten::q_per_channel_axis(Tensor self) -> int
int64_t q_per_channel_axis::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_q_per_channel_axis_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(int_repr, name, "aten::int_repr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(int_repr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(int_repr, schema_str, "int_repr(Tensor self) -> Tensor")

// aten::int_repr(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<int_repr::schema> create_int_repr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(int_repr::name, int_repr::overload_name)
      .typed<int_repr::schema>();
}

// aten::int_repr(Tensor self) -> Tensor
at::Tensor int_repr::call(const at::Tensor & self) {
    static auto op = create_int_repr_typed_handle();
    return op.call(self);
}

// aten::int_repr(Tensor self) -> Tensor
at::Tensor int_repr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_int_repr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_per_tensor_quantized_tensor, name, "aten::_make_per_tensor_quantized_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_per_tensor_quantized_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_per_tensor_quantized_tensor, schema_str, "_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor")

// aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_make_per_tensor_quantized_tensor::schema> create__make_per_tensor_quantized_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_make_per_tensor_quantized_tensor::name, _make_per_tensor_quantized_tensor::overload_name)
      .typed<_make_per_tensor_quantized_tensor::schema>();
}

// aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor
at::Tensor _make_per_tensor_quantized_tensor::call(const at::Tensor & self, double scale, int64_t zero_point) {
    static auto op = create__make_per_tensor_quantized_tensor_typed_handle();
    return op.call(self, scale, zero_point);
}

// aten::_make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -> Tensor
at::Tensor _make_per_tensor_quantized_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point) {
    static auto op = create__make_per_tensor_quantized_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_per_channel_quantized_tensor, name, "aten::_make_per_channel_quantized_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_per_channel_quantized_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_make_per_channel_quantized_tensor, schema_str, "_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -> Tensor")

// aten::_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_make_per_channel_quantized_tensor::schema> create__make_per_channel_quantized_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_make_per_channel_quantized_tensor::name, _make_per_channel_quantized_tensor::overload_name)
      .typed<_make_per_channel_quantized_tensor::schema>();
}

// aten::_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -> Tensor
at::Tensor _make_per_channel_quantized_tensor::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis) {
    static auto op = create__make_per_channel_quantized_tensor_typed_handle();
    return op.call(self, scale, zero_point, axis);
}

// aten::_make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -> Tensor
at::Tensor _make_per_channel_quantized_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis) {
    static auto op = create__make_per_channel_quantized_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, axis);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qscheme, name, "aten::qscheme")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qscheme, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qscheme, schema_str, "qscheme(Tensor self) -> QScheme")

// aten::qscheme(Tensor self) -> QScheme
static C10_NOINLINE c10::TypedOperatorHandle<qscheme::schema> create_qscheme_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(qscheme::name, qscheme::overload_name)
      .typed<qscheme::schema>();
}

// aten::qscheme(Tensor self) -> QScheme
at::QScheme qscheme::call(const at::Tensor & self) {
    static auto op = create_qscheme_typed_handle();
    return op.call(self);
}

// aten::qscheme(Tensor self) -> QScheme
at::QScheme qscheme::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_qscheme_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine, name, "aten::fake_quantize_per_tensor_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine, schema_str, "fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor")

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_tensor_affine::schema> create_fake_quantize_per_tensor_affine_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_tensor_affine::name, fake_quantize_per_tensor_affine::overload_name)
      .typed<fake_quantize_per_tensor_affine::schema>();
}

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine::call(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_tensor_affine_typed_handle();
    return op.call(self, scale, zero_point, quant_min, quant_max);
}

// aten::fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_tensor_affine_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_tensor_qparams, name, "aten::fake_quantize_per_tensor_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_tensor_qparams, overload_name, "tensor_qparams")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_tensor_qparams, schema_str, "fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor")

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_tensor_affine_tensor_qparams::schema> create_fake_quantize_per_tensor_affine_tensor_qparams_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_tensor_affine_tensor_qparams::name, fake_quantize_per_tensor_affine_tensor_qparams::overload_name)
      .typed<fake_quantize_per_tensor_affine_tensor_qparams::schema>();
}

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine_tensor_qparams::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_tensor_affine_tensor_qparams_typed_handle();
    return op.call(self, scale, zero_point, quant_min, quant_max);
}

// aten::fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_tensor_affine_tensor_qparams::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_tensor_affine_tensor_qparams_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_cachemask, name, "aten::fake_quantize_per_tensor_affine_cachemask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_cachemask, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_cachemask, schema_str, "fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)")

// aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_tensor_affine_cachemask::schema> create_fake_quantize_per_tensor_affine_cachemask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_tensor_affine_cachemask::name, fake_quantize_per_tensor_affine_cachemask::overload_name)
      .typed<fake_quantize_per_tensor_affine_cachemask::schema>();
}

// aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> fake_quantize_per_tensor_affine_cachemask::call(const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_tensor_affine_cachemask_typed_handle();
    return op.call(self, scale, zero_point, quant_min, quant_max);
}

// aten::fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> fake_quantize_per_tensor_affine_cachemask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_tensor_affine_cachemask_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_per_tensor_affine_cachemask_tensor_qparams, name, "aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_per_tensor_affine_cachemask_tensor_qparams, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_per_tensor_affine_cachemask_tensor_qparams, schema_str, "_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)")

// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_per_tensor_affine_cachemask_tensor_qparams::schema> create__fake_quantize_per_tensor_affine_cachemask_tensor_qparams_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_per_tensor_affine_cachemask_tensor_qparams::name, _fake_quantize_per_tensor_affine_cachemask_tensor_qparams::overload_name)
      .typed<_fake_quantize_per_tensor_affine_cachemask_tensor_qparams::schema>();
}

// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> _fake_quantize_per_tensor_affine_cachemask_tensor_qparams::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, const at::Tensor & fake_quant_enabled, int64_t quant_min, int64_t quant_max) {
    static auto op = create__fake_quantize_per_tensor_affine_cachemask_tensor_qparams_typed_handle();
    return op.call(self, scale, zero_point, fake_quant_enabled, quant_min, quant_max);
}

// aten::_fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> _fake_quantize_per_tensor_affine_cachemask_tensor_qparams::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, const at::Tensor & fake_quant_enabled, int64_t quant_min, int64_t quant_max) {
    static auto op = create__fake_quantize_per_tensor_affine_cachemask_tensor_qparams_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, fake_quant_enabled, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_cachemask_backward, name, "aten::fake_quantize_per_tensor_affine_cachemask_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_cachemask_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_tensor_affine_cachemask_backward, schema_str, "fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor")

// aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_tensor_affine_cachemask_backward::schema> create_fake_quantize_per_tensor_affine_cachemask_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_tensor_affine_cachemask_backward::name, fake_quantize_per_tensor_affine_cachemask_backward::overload_name)
      .typed<fake_quantize_per_tensor_affine_cachemask_backward::schema>();
}

// aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
at::Tensor fake_quantize_per_tensor_affine_cachemask_backward::call(const at::Tensor & grad, const at::Tensor & mask) {
    static auto op = create_fake_quantize_per_tensor_affine_cachemask_backward_typed_handle();
    return op.call(grad, mask);
}

// aten::fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
at::Tensor fake_quantize_per_tensor_affine_cachemask_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & mask) {
    static auto op = create_fake_quantize_per_tensor_affine_cachemask_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine, name, "aten::_fake_quantize_learnable_per_tensor_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine, schema_str, "_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor")

// aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_tensor_affine::schema> create__fake_quantize_learnable_per_tensor_affine_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_tensor_affine::name, _fake_quantize_learnable_per_tensor_affine::overload_name)
      .typed<_fake_quantize_learnable_per_tensor_affine::schema>();
}

// aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
at::Tensor _fake_quantize_learnable_per_tensor_affine::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_tensor_affine_typed_handle();
    return op.call(self, scale, zero_point, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
at::Tensor _fake_quantize_learnable_per_tensor_affine::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_tensor_affine_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine_backward, name, "aten::_fake_quantize_learnable_per_tensor_affine_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_tensor_affine_backward, schema_str, "_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)")

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_tensor_affine_backward::schema> create__fake_quantize_learnable_per_tensor_affine_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_tensor_affine_backward::name, _fake_quantize_learnable_per_tensor_affine_backward::overload_name)
      .typed<_fake_quantize_learnable_per_tensor_affine_backward::schema>();
}

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_tensor_affine_backward::call(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_tensor_affine_backward_typed_handle();
    return op.call(grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_tensor_affine_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_tensor_affine_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, scale, zero_point, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine, name, "aten::fake_quantize_per_channel_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine, schema_str, "fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor")

// aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_channel_affine::schema> create_fake_quantize_per_channel_affine_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_channel_affine::name, fake_quantize_per_channel_affine::overload_name)
      .typed<fake_quantize_per_channel_affine::schema>();
}

// aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_channel_affine::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_channel_affine_typed_handle();
    return op.call(self, scale, zero_point, axis, quant_min, quant_max);
}

// aten::fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> Tensor
at::Tensor fake_quantize_per_channel_affine::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_channel_affine_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine_cachemask, name, "aten::fake_quantize_per_channel_affine_cachemask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine_cachemask, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine_cachemask, schema_str, "fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)")

// aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_channel_affine_cachemask::schema> create_fake_quantize_per_channel_affine_cachemask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_channel_affine_cachemask::name, fake_quantize_per_channel_affine_cachemask::overload_name)
      .typed<fake_quantize_per_channel_affine_cachemask::schema>();
}

// aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> fake_quantize_per_channel_affine_cachemask::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_channel_affine_cachemask_typed_handle();
    return op.call(self, scale, zero_point, axis, quant_min, quant_max);
}

// aten::fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> fake_quantize_per_channel_affine_cachemask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max) {
    static auto op = create_fake_quantize_per_channel_affine_cachemask_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine_cachemask_backward, name, "aten::fake_quantize_per_channel_affine_cachemask_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine_cachemask_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fake_quantize_per_channel_affine_cachemask_backward, schema_str, "fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor")

// aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fake_quantize_per_channel_affine_cachemask_backward::schema> create_fake_quantize_per_channel_affine_cachemask_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fake_quantize_per_channel_affine_cachemask_backward::name, fake_quantize_per_channel_affine_cachemask_backward::overload_name)
      .typed<fake_quantize_per_channel_affine_cachemask_backward::schema>();
}

// aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
at::Tensor fake_quantize_per_channel_affine_cachemask_backward::call(const at::Tensor & grad, const at::Tensor & mask) {
    static auto op = create_fake_quantize_per_channel_affine_cachemask_backward_typed_handle();
    return op.call(grad, mask);
}

// aten::fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -> Tensor
at::Tensor fake_quantize_per_channel_affine_cachemask_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & mask) {
    static auto op = create_fake_quantize_per_channel_affine_cachemask_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine, name, "aten::_fake_quantize_learnable_per_channel_affine")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine, schema_str, "_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor")

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_channel_affine::schema> create__fake_quantize_learnable_per_channel_affine_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_channel_affine::name, _fake_quantize_learnable_per_channel_affine::overload_name)
      .typed<_fake_quantize_learnable_per_channel_affine::schema>();
}

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
at::Tensor _fake_quantize_learnable_per_channel_affine::call(const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_channel_affine_typed_handle();
    return op.call(self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> Tensor
at::Tensor _fake_quantize_learnable_per_channel_affine::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_channel_affine_typed_handle();
    return op.redispatch(dispatchKeySet, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_backward, name, "aten::_fake_quantize_learnable_per_channel_affine_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fake_quantize_learnable_per_channel_affine_backward, schema_str, "_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)")

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_fake_quantize_learnable_per_channel_affine_backward::schema> create__fake_quantize_learnable_per_channel_affine_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fake_quantize_learnable_per_channel_affine_backward::name, _fake_quantize_learnable_per_channel_affine_backward::overload_name)
      .typed<_fake_quantize_learnable_per_channel_affine_backward::schema>();
}

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_channel_affine_backward::call(const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_channel_affine_backward_typed_handle();
    return op.call(grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

// aten::_fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _fake_quantize_learnable_per_channel_affine_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, const at::Tensor & scale, const at::Tensor & zero_point, int64_t axis, int64_t quant_min, int64_t quant_max, double grad_factor) {
    static auto op = create__fake_quantize_learnable_per_channel_affine_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, scale, zero_point, axis, quant_min, quant_max, grad_factor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fused_moving_avg_obs_fake_quant, name, "aten::fused_moving_avg_obs_fake_quant")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fused_moving_avg_obs_fake_quant, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fused_moving_avg_obs_fake_quant, schema_str, "fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor")

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fused_moving_avg_obs_fake_quant::schema> create_fused_moving_avg_obs_fake_quant_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fused_moving_avg_obs_fake_quant::name, fused_moving_avg_obs_fake_quant::overload_name)
      .typed<fused_moving_avg_obs_fake_quant::schema>();
}

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
at::Tensor fused_moving_avg_obs_fake_quant::call(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    static auto op = create_fused_moving_avg_obs_fake_quant_typed_handle();
    return op.call(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

// aten::fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> Tensor
at::Tensor fused_moving_avg_obs_fake_quant::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    static auto op = create_fused_moving_avg_obs_fake_quant_typed_handle();
    return op.redispatch(dispatchKeySet, self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_moving_avg_obs_fq_helper, name, "aten::_fused_moving_avg_obs_fq_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_moving_avg_obs_fq_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_fused_moving_avg_obs_fq_helper, schema_str, "_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)")

// aten::_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)
static C10_NOINLINE c10::TypedOperatorHandle<_fused_moving_avg_obs_fq_helper::schema> create__fused_moving_avg_obs_fq_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_fused_moving_avg_obs_fq_helper::name, _fused_moving_avg_obs_fq_helper::overload_name)
      .typed<_fused_moving_avg_obs_fq_helper::schema>();
}

// aten::_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> _fused_moving_avg_obs_fq_helper::call(const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    static auto op = create__fused_moving_avg_obs_fq_helper_typed_handle();
    return op.call(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

// aten::_fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)
::std::tuple<at::Tensor,at::Tensor> _fused_moving_avg_obs_fq_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & observer_on, const at::Tensor & fake_quant_on, at::Tensor & running_min, at::Tensor & running_max, at::Tensor & scale, at::Tensor & zero_point, double averaging_const, int64_t quant_min, int64_t quant_max, int64_t ch_axis, bool per_row_fake_quant, bool symmetric_quant) {
    static auto op = create__fused_moving_avg_obs_fq_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant, symmetric_quant);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_choose_qparams_per_tensor, name, "aten::_choose_qparams_per_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_choose_qparams_per_tensor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_choose_qparams_per_tensor, schema_str, "_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)")

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
static C10_NOINLINE c10::TypedOperatorHandle<_choose_qparams_per_tensor::schema> create__choose_qparams_per_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_choose_qparams_per_tensor::name, _choose_qparams_per_tensor::overload_name)
      .typed<_choose_qparams_per_tensor::schema>();
}

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
::std::tuple<double,int64_t> _choose_qparams_per_tensor::call(const at::Tensor & self, bool reduce_range) {
    static auto op = create__choose_qparams_per_tensor_typed_handle();
    return op.call(self, reduce_range);
}

// aten::_choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -> (float, int)
::std::tuple<double,int64_t> _choose_qparams_per_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool reduce_range) {
    static auto op = create__choose_qparams_per_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, reduce_range);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_saturate_weight_to_fp16, name, "aten::_saturate_weight_to_fp16")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_saturate_weight_to_fp16, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_saturate_weight_to_fp16, schema_str, "_saturate_weight_to_fp16(Tensor weight) -> Tensor")

// aten::_saturate_weight_to_fp16(Tensor weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_saturate_weight_to_fp16::schema> create__saturate_weight_to_fp16_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_saturate_weight_to_fp16::name, _saturate_weight_to_fp16::overload_name)
      .typed<_saturate_weight_to_fp16::schema>();
}

// aten::_saturate_weight_to_fp16(Tensor weight) -> Tensor
at::Tensor _saturate_weight_to_fp16::call(const at::Tensor & weight) {
    static auto op = create__saturate_weight_to_fp16_typed_handle();
    return op.call(weight);
}

// aten::_saturate_weight_to_fp16(Tensor weight) -> Tensor
at::Tensor _saturate_weight_to_fp16::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & weight) {
    static auto op = create__saturate_weight_to_fp16_typed_handle();
    return op.redispatch(dispatchKeySet, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(choose_qparams_optimized, name, "aten::choose_qparams_optimized")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(choose_qparams_optimized, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(choose_qparams_optimized, schema_str, "choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)")

// aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<choose_qparams_optimized::schema> create_choose_qparams_optimized_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(choose_qparams_optimized::name, choose_qparams_optimized::overload_name)
      .typed<choose_qparams_optimized::schema>();
}

// aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> choose_qparams_optimized::call(const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
    static auto op = create_choose_qparams_optimized_typed_handle();
    return op.call(input, numel, n_bins, ratio, bit_width);
}

// aten::choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> choose_qparams_optimized::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, int64_t numel, int64_t n_bins, double ratio, int64_t bit_width) {
    static auto op = create_choose_qparams_optimized_typed_handle();
    return op.redispatch(dispatchKeySet, input, numel, n_bins, ratio, bit_width);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_copy, name, "aten::_to_copy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_copy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_to_copy, schema_str, "_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor")

// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_to_copy::schema> create__to_copy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_to_copy::name, _to_copy::overload_name)
      .typed<_to_copy::schema>();
}

// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
at::Tensor _to_copy::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create__to_copy_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}

// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
at::Tensor _to_copy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create__to_copy_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype_layout, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype_layout, overload_name, "dtype_layout")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype_layout, schema_str, "to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_dtype_layout::schema> create_to_dtype_layout_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dtype_layout::name, to_dtype_layout::overload_name)
      .typed<to_dtype_layout::schema>();
}

// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype_layout::call(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_dtype_layout_typed_handle();
    return op.call(self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}

// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype_layout::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_dtype_layout_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_device, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_device, overload_name, "device")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_device, schema_str, "to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_device::schema> create_to_device_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_device::name, to_device::overload_name)
      .typed<to_device::schema>();
}

// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_device::call(const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_device_typed_handle();
    return op.call(self, device, dtype, non_blocking, copy, memory_format);
}

// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_device::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_device_typed_handle();
    return op.redispatch(dispatchKeySet, self, device, dtype, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype, overload_name, "dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_dtype, schema_str, "to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_dtype::schema> create_to_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_dtype::name, to_dtype::overload_name)
      .typed<to_dtype::schema>();
}

// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype::call(const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_dtype_typed_handle();
    return op.call(self, dtype, non_blocking, copy, memory_format);
}

// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_other, name, "aten::to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_other, overload_name, "other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(to_other, schema_str, "to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)")

// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<to_other::schema> create_to_other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(to_other::name, to_other::overload_name)
      .typed<to_other::schema>();
}

// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_other::call(const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_other_typed_handle();
    return op.call(self, other, non_blocking, copy, memory_format);
}

// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
at::Tensor to_other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
    static auto op = create_to_other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, non_blocking, copy, memory_format);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid, name, "aten::meshgrid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid, schema_str, "meshgrid(Tensor[] tensors) -> Tensor[]")

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<meshgrid::schema> create_meshgrid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(meshgrid::name, meshgrid::overload_name)
      .typed<meshgrid::schema>();
}

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> meshgrid::call(at::TensorList tensors) {
    static auto op = create_meshgrid_typed_handle();
    return op.call(tensors);
}

// aten::meshgrid(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> meshgrid::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_meshgrid_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid_indexing, name, "aten::meshgrid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid_indexing, overload_name, "indexing")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(meshgrid_indexing, schema_str, "meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]")

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<meshgrid_indexing::schema> create_meshgrid_indexing_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(meshgrid_indexing::name, meshgrid_indexing::overload_name)
      .typed<meshgrid_indexing::schema>();
}

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
::std::vector<at::Tensor> meshgrid_indexing::call(at::TensorList tensors, c10::string_view indexing) {
    static auto op = create_meshgrid_indexing_typed_handle();
    return op.call(tensors, indexing);
}

// aten::meshgrid.indexing(Tensor[] tensors, *, str indexing) -> Tensor[]
::std::vector<at::Tensor> meshgrid_indexing::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, c10::string_view indexing) {
    static auto op = create_meshgrid_indexing_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, indexing);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cartesian_prod, name, "aten::cartesian_prod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cartesian_prod, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cartesian_prod, schema_str, "cartesian_prod(Tensor[] tensors) -> Tensor")

// aten::cartesian_prod(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cartesian_prod::schema> create_cartesian_prod_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cartesian_prod::name, cartesian_prod::overload_name)
      .typed<cartesian_prod::schema>();
}

// aten::cartesian_prod(Tensor[] tensors) -> Tensor
at::Tensor cartesian_prod::call(at::TensorList tensors) {
    static auto op = create_cartesian_prod_typed_handle();
    return op.call(tensors);
}

// aten::cartesian_prod(Tensor[] tensors) -> Tensor
at::Tensor cartesian_prod::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_cartesian_prod_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(combinations, name, "aten::combinations")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(combinations, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(combinations, schema_str, "combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor")

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<combinations::schema> create_combinations_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(combinations::name, combinations::overload_name)
      .typed<combinations::schema>();
}

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
at::Tensor combinations::call(const at::Tensor & self, int64_t r, bool with_replacement) {
    static auto op = create_combinations_typed_handle();
    return op.call(self, r, with_replacement);
}

// aten::combinations(Tensor self, int r=2, bool with_replacement=False) -> Tensor
at::Tensor combinations::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t r, bool with_replacement) {
    static auto op = create_combinations_typed_handle();
    return op.redispatch(dispatchKeySet, self, r, with_replacement);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(item, name, "aten::item")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(item, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(item, schema_str, "item(Tensor self) -> Scalar")

// aten::item(Tensor self) -> Scalar
static C10_NOINLINE c10::TypedOperatorHandle<item::schema> create_item_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(item::name, item::overload_name)
      .typed<item::schema>();
}

// aten::item(Tensor self) -> Scalar
at::Scalar item::call(const at::Tensor & self) {
    static auto op = create_item_typed_handle();
    return op.call(self);
}

// aten::item(Tensor self) -> Scalar
at::Scalar item::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_item_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Tensor, name, "aten::result_type")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Tensor, schema_str, "result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType")

// aten::result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType
static C10_NOINLINE c10::TypedOperatorHandle<result_type_Tensor::schema> create_result_type_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(result_type_Tensor::name, result_type_Tensor::overload_name)
      .typed<result_type_Tensor::schema>();
}

// aten::result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType
at::ScalarType result_type_Tensor::call(const at::Tensor & tensor, const at::Tensor & other) {
    static auto op = create_result_type_Tensor_typed_handle();
    return op.call(tensor, other);
}

// aten::result_type.Tensor(Tensor tensor, Tensor other) -> ScalarType
at::ScalarType result_type_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & tensor, const at::Tensor & other) {
    static auto op = create_result_type_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, tensor, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar, name, "aten::result_type")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar, schema_str, "result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType")

// aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType
static C10_NOINLINE c10::TypedOperatorHandle<result_type_Scalar::schema> create_result_type_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(result_type_Scalar::name, result_type_Scalar::overload_name)
      .typed<result_type_Scalar::schema>();
}

// aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType
at::ScalarType result_type_Scalar::call(const at::Tensor & tensor, const at::Scalar & other) {
    static auto op = create_result_type_Scalar_typed_handle();
    return op.call(tensor, other);
}

// aten::result_type.Scalar(Tensor tensor, Scalar other) -> ScalarType
at::ScalarType result_type_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & tensor, const at::Scalar & other) {
    static auto op = create_result_type_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, tensor, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar_Tensor, name, "aten::result_type")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar_Tensor, overload_name, "Scalar_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar_Tensor, schema_str, "result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType")

// aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType
static C10_NOINLINE c10::TypedOperatorHandle<result_type_Scalar_Tensor::schema> create_result_type_Scalar_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(result_type_Scalar_Tensor::name, result_type_Scalar_Tensor::overload_name)
      .typed<result_type_Scalar_Tensor::schema>();
}

// aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType
at::ScalarType result_type_Scalar_Tensor::call(const at::Scalar & scalar, const at::Tensor & tensor) {
    static auto op = create_result_type_Scalar_Tensor_typed_handle();
    return op.call(scalar, tensor);
}

// aten::result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -> ScalarType
at::ScalarType result_type_Scalar_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & scalar, const at::Tensor & tensor) {
    static auto op = create_result_type_Scalar_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, scalar, tensor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar_Scalar, name, "aten::result_type")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar_Scalar, overload_name, "Scalar_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(result_type_Scalar_Scalar, schema_str, "result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType")

// aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType
static C10_NOINLINE c10::TypedOperatorHandle<result_type_Scalar_Scalar::schema> create_result_type_Scalar_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(result_type_Scalar_Scalar::name, result_type_Scalar_Scalar::overload_name)
      .typed<result_type_Scalar_Scalar::schema>();
}

// aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType
at::ScalarType result_type_Scalar_Scalar::call(const at::Scalar & scalar1, const at::Scalar & scalar2) {
    static auto op = create_result_type_Scalar_Scalar_typed_handle();
    return op.call(scalar1, scalar2);
}

// aten::result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -> ScalarType
at::ScalarType result_type_Scalar_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & scalar1, const at::Scalar & scalar2) {
    static auto op = create_result_type_Scalar_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, scalar1, scalar2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(can_cast, name, "aten::can_cast")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(can_cast, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(can_cast, schema_str, "can_cast(ScalarType from, ScalarType to) -> bool")

// aten::can_cast(ScalarType from, ScalarType to) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<can_cast::schema> create_can_cast_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(can_cast::name, can_cast::overload_name)
      .typed<can_cast::schema>();
}

// aten::can_cast(ScalarType from, ScalarType to) -> bool
bool can_cast::call(at::ScalarType from, at::ScalarType to) {
    static auto op = create_can_cast_typed_handle();
    return op.call(from, to);
}

// aten::can_cast(ScalarType from, ScalarType to) -> bool
bool can_cast::redispatch(c10::DispatchKeySet dispatchKeySet, at::ScalarType from, at::ScalarType to) {
    static auto op = create_can_cast_typed_handle();
    return op.redispatch(dispatchKeySet, from, to);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(promote_types, name, "aten::promote_types")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(promote_types, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(promote_types, schema_str, "promote_types(ScalarType type1, ScalarType type2) -> ScalarType")

// aten::promote_types(ScalarType type1, ScalarType type2) -> ScalarType
static C10_NOINLINE c10::TypedOperatorHandle<promote_types::schema> create_promote_types_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(promote_types::name, promote_types::overload_name)
      .typed<promote_types::schema>();
}

// aten::promote_types(ScalarType type1, ScalarType type2) -> ScalarType
at::ScalarType promote_types::call(at::ScalarType type1, at::ScalarType type2) {
    static auto op = create_promote_types_typed_handle();
    return op.call(type1, type2);
}

// aten::promote_types(ScalarType type1, ScalarType type2) -> ScalarType
at::ScalarType promote_types::redispatch(c10::DispatchKeySet dispatchKeySet, at::ScalarType type1, at::ScalarType type2) {
    static auto op = create_promote_types_typed_handle();
    return op.redispatch(dispatchKeySet, type1, type2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_local_scalar_dense, name, "aten::_local_scalar_dense")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_local_scalar_dense, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_local_scalar_dense, schema_str, "_local_scalar_dense(Tensor self) -> Scalar")

// aten::_local_scalar_dense(Tensor self) -> Scalar
static C10_NOINLINE c10::TypedOperatorHandle<_local_scalar_dense::schema> create__local_scalar_dense_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_local_scalar_dense::name, _local_scalar_dense::overload_name)
      .typed<_local_scalar_dense::schema>();
}

// aten::_local_scalar_dense(Tensor self) -> Scalar
at::Scalar _local_scalar_dense::call(const at::Tensor & self) {
    static auto op = create__local_scalar_dense_typed_handle();
    return op.call(self);
}

// aten::_local_scalar_dense(Tensor self) -> Scalar
at::Scalar _local_scalar_dense::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__local_scalar_dense_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell, name, "aten::_thnn_fused_lstm_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell, schema_str, "_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)")

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_lstm_cell::schema> create__thnn_fused_lstm_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_lstm_cell::name, _thnn_fused_lstm_cell::overload_name)
      .typed<_thnn_fused_lstm_cell::schema>();
}

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell::call(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    static auto op = create__thnn_fused_lstm_cell_typed_handle();
    return op.call(input_gates, hidden_gates, cx, input_bias, hidden_bias);
}

// aten::_thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & cx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    static auto op = create__thnn_fused_lstm_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input_gates, hidden_gates, cx, input_bias, hidden_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward, name, "aten::_thnn_fused_lstm_cell_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_lstm_cell_backward, schema_str, "_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_lstm_cell_backward::schema> create__thnn_fused_lstm_cell_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_lstm_cell_backward::name, _thnn_fused_lstm_cell_backward::overload_name)
      .typed<_thnn_fused_lstm_cell_backward::schema>();
}

// aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell_backward::call(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
    static auto op = create__thnn_fused_lstm_cell_backward_typed_handle();
    return op.call(grad_hy, grad_cy, cx, cy, workspace, has_bias);
}

// aten::_thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_fused_lstm_cell_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & cx, const at::Tensor & cy, const at::Tensor & workspace, bool has_bias) {
    static auto op = create__thnn_fused_lstm_cell_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_hy, grad_cy, cx, cy, workspace, has_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_differentiable_lstm_cell_backward, name, "aten::_thnn_differentiable_lstm_cell_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_differentiable_lstm_cell_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_differentiable_lstm_cell_backward, schema_str, "_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -> (Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_differentiable_lstm_cell_backward::schema> create__thnn_differentiable_lstm_cell_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_differentiable_lstm_cell_backward::name, _thnn_differentiable_lstm_cell_backward::overload_name)
      .typed<_thnn_differentiable_lstm_cell_backward::schema>();
}

// aten::_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_lstm_cell_backward::call(const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
    static auto op = create__thnn_differentiable_lstm_cell_backward_typed_handle();
    return op.call(grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}

// aten::_thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_lstm_cell_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const c10::optional<at::Tensor> & grad_hy, const c10::optional<at::Tensor> & grad_cy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias, const at::Tensor & cx, const at::Tensor & cy) {
    static auto op = create__thnn_differentiable_lstm_cell_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_hy, grad_cy, input_gates, hidden_gates, input_bias, hidden_bias, cx, cy);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell, name, "aten::_thnn_fused_gru_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell, schema_str, "_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)")

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_gru_cell::schema> create__thnn_fused_gru_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_gru_cell::name, _thnn_fused_gru_cell::overload_name)
      .typed<_thnn_fused_gru_cell::schema>();
}

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _thnn_fused_gru_cell::call(const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    static auto op = create__thnn_fused_gru_cell_typed_handle();
    return op.call(input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

// aten::_thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _thnn_fused_gru_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    static auto op = create__thnn_fused_gru_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell_backward, name, "aten::_thnn_fused_gru_cell_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_fused_gru_cell_backward, schema_str, "_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_fused_gru_cell_backward::schema> create__thnn_fused_gru_cell_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_fused_gru_cell_backward::name, _thnn_fused_gru_cell_backward::overload_name)
      .typed<_thnn_fused_gru_cell_backward::schema>();
}

// aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_fused_gru_cell_backward::call(const at::Tensor & grad_hy, const at::Tensor & workspace, bool has_bias) {
    static auto op = create__thnn_fused_gru_cell_backward_typed_handle();
    return op.call(grad_hy, workspace, has_bias);
}

// aten::_thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_fused_gru_cell_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_hy, const at::Tensor & workspace, bool has_bias) {
    static auto op = create__thnn_fused_gru_cell_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_hy, workspace, has_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_differentiable_gru_cell_backward, name, "aten::_thnn_differentiable_gru_cell_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_differentiable_gru_cell_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_thnn_differentiable_gru_cell_backward, schema_str, "_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)")

// aten::_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_thnn_differentiable_gru_cell_backward::schema> create__thnn_differentiable_gru_cell_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_thnn_differentiable_gru_cell_backward::name, _thnn_differentiable_gru_cell_backward::overload_name)
      .typed<_thnn_differentiable_gru_cell_backward::schema>();
}

// aten::_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_gru_cell_backward::call(const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    static auto op = create__thnn_differentiable_gru_cell_backward_typed_handle();
    return op.call(grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

// aten::_thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor,at::Tensor> _thnn_differentiable_gru_cell_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_hy, const at::Tensor & input_gates, const at::Tensor & hidden_gates, const at::Tensor & hx, const c10::optional<at::Tensor> & input_bias, const c10::optional<at::Tensor> & hidden_bias) {
    static auto op = create__thnn_differentiable_gru_cell_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_hy, input_gates, hidden_gates, hx, input_bias, hidden_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_input, name, "aten::lstm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_input, schema_str, "lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)")

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<lstm_input::schema> create_lstm_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_input::name, lstm_input::overload_name)
      .typed<lstm_input::schema>();
}

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_input::call(const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_lstm_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_lstm_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_data, name, "aten::lstm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_data, schema_str, "lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)")

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<lstm_data::schema> create_lstm_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_data::name, lstm_data::overload_name)
      .typed<lstm_data::schema>();
}

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_lstm_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lstm_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, at::TensorList hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_lstm_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_input, name, "aten::gru")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_input, schema_str, "gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)")

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<gru_input::schema> create_gru_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gru_input::name, gru_input::overload_name)
      .typed<gru_input::schema>();
}

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_input::call(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_gru_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_gru_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_data, name, "aten::gru")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_data, schema_str, "gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)")

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<gru_data::schema> create_gru_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gru_data::name, gru_data::overload_name)
      .typed<gru_data::schema>();
}

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_gru_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> gru_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_gru_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_input, name, "aten::rnn_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_input, schema_str, "rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)")

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<rnn_tanh_input::schema> create_rnn_tanh_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_tanh_input::name, rnn_tanh_input::overload_name)
      .typed<rnn_tanh_input::schema>();
}

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_input::call(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_rnn_tanh_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_rnn_tanh_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_data, name, "aten::rnn_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_data, schema_str, "rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)")

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<rnn_tanh_data::schema> create_rnn_tanh_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_tanh_data::name, rnn_tanh_data::overload_name)
      .typed<rnn_tanh_data::schema>();
}

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_rnn_tanh_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_tanh_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_rnn_tanh_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_input, name, "aten::rnn_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_input, overload_name, "input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_input, schema_str, "rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)")

// aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<rnn_relu_input::schema> create_rnn_relu_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_relu_input::name, rnn_relu_input::overload_name)
      .typed<rnn_relu_input::schema>();
}

// aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_relu_input::call(const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_rnn_relu_input_typed_handle();
    return op.call(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

// aten::rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_relu_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) {
    static auto op = create_rnn_relu_input_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_data, name, "aten::rnn_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_data, overload_name, "data")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_data, schema_str, "rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)")

// aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<rnn_relu_data::schema> create_rnn_relu_data_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_relu_data::name, rnn_relu_data::overload_name)
      .typed<rnn_relu_data::schema>();
}

// aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_relu_data::call(const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_rnn_relu_data_typed_handle();
    return op.call(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

// aten::rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> rnn_relu_data::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, const at::Tensor & hx, at::TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) {
    static auto op = create_rnn_relu_data_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_cell, name, "aten::lstm_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstm_cell, schema_str, "lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)")

// aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<lstm_cell::schema> create_lstm_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstm_cell::name, lstm_cell::overload_name)
      .typed<lstm_cell::schema>();
}

// aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> lstm_cell::call(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_lstm_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh);
}

// aten::lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> lstm_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_lstm_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_cell, name, "aten::gru_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gru_cell, schema_str, "gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor")

// aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gru_cell::schema> create_gru_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gru_cell::name, gru_cell::overload_name)
      .typed<gru_cell::schema>();
}

// aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor gru_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_gru_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh);
}

// aten::gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor gru_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_gru_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_cell, name, "aten::rnn_tanh_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_tanh_cell, schema_str, "rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor")

// aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rnn_tanh_cell::schema> create_rnn_tanh_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_tanh_cell::name, rnn_tanh_cell::overload_name)
      .typed<rnn_tanh_cell::schema>();
}

// aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor rnn_tanh_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_rnn_tanh_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh);
}

// aten::rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor rnn_tanh_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_rnn_tanh_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_cell, name, "aten::rnn_relu_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rnn_relu_cell, schema_str, "rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor")

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rnn_relu_cell::schema> create_rnn_relu_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rnn_relu_cell::name, rnn_relu_cell::overload_name)
      .typed<rnn_relu_cell::schema>();
}

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor rnn_relu_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_rnn_relu_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh);
}

// aten::rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> Tensor
at::Tensor rnn_relu_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const c10::optional<at::Tensor> & b_ih, const c10::optional<at::Tensor> & b_hh) {
    static auto op = create_rnn_relu_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_lstm_cell, name, "aten::quantized_lstm_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_lstm_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_lstm_cell, schema_str, "quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)")

// aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<quantized_lstm_cell::schema> create_quantized_lstm_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_lstm_cell::name, quantized_lstm_cell::overload_name)
      .typed<quantized_lstm_cell::schema>();
}

// aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> quantized_lstm_cell::call(const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_lstm_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

// aten::quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> quantized_lstm_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::TensorList hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_lstm_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_gru_cell, name, "aten::quantized_gru_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_gru_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_gru_cell, schema_str, "quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor")

// aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_gru_cell::schema> create_quantized_gru_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_gru_cell::name, quantized_gru_cell::overload_name)
      .typed<quantized_gru_cell::schema>();
}

// aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_gru_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_gru_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

// aten::quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_gru_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_gru_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_relu_cell, name, "aten::quantized_rnn_relu_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_relu_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_relu_cell, schema_str, "quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor")

// aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_rnn_relu_cell::schema> create_quantized_rnn_relu_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_rnn_relu_cell::name, quantized_rnn_relu_cell::overload_name)
      .typed<quantized_rnn_relu_cell::schema>();
}

// aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_rnn_relu_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_rnn_relu_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

// aten::quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_rnn_relu_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_rnn_relu_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_tanh_cell, name, "aten::quantized_rnn_tanh_cell")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_tanh_cell, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantized_rnn_tanh_cell, schema_str, "quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor")

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantized_rnn_tanh_cell::schema> create_quantized_rnn_tanh_cell_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantized_rnn_tanh_cell::name, quantized_rnn_tanh_cell::overload_name)
      .typed<quantized_rnn_tanh_cell::schema>();
}

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_rnn_tanh_cell::call(const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_rnn_tanh_cell_typed_handle();
    return op.call(input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

// aten::quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -> Tensor
at::Tensor quantized_rnn_tanh_cell::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & hx, const at::Tensor & w_ih, const at::Tensor & w_hh, const at::Tensor & b_ih, const at::Tensor & b_hh, const at::Tensor & packed_ih, const at::Tensor & packed_hh, const at::Tensor & col_offsets_ih, const at::Tensor & col_offsets_hh, const at::Scalar & scale_ih, const at::Scalar & scale_hh, const at::Scalar & zero_point_ih, const at::Scalar & zero_point_hh) {
    static auto op = create_quantized_rnn_tanh_cell_typed_handle();
    return op.redispatch(dispatchKeySet, input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence, name, "aten::_pack_padded_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence, schema_str, "_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)")

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_pack_padded_sequence::schema> create__pack_padded_sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pack_padded_sequence::name, _pack_padded_sequence::overload_name)
      .typed<_pack_padded_sequence::schema>();
}

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pack_padded_sequence::call(const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
    static auto op = create__pack_padded_sequence_typed_handle();
    return op.call(input, lengths, batch_first);
}

// aten::_pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pack_padded_sequence::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
    static auto op = create__pack_padded_sequence_typed_handle();
    return op.redispatch(dispatchKeySet, input, lengths, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence_backward, name, "aten::_pack_padded_sequence_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pack_padded_sequence_backward, schema_str, "_pack_padded_sequence_backward(Tensor grad, int[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor")

// aten::_pack_padded_sequence_backward(Tensor grad, int[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_pack_padded_sequence_backward::schema> create__pack_padded_sequence_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pack_padded_sequence_backward::name, _pack_padded_sequence_backward::overload_name)
      .typed<_pack_padded_sequence_backward::schema>();
}

// aten::_pack_padded_sequence_backward(Tensor grad, int[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor
at::Tensor _pack_padded_sequence_backward::call(const at::Tensor & grad, at::IntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
    static auto op = create__pack_padded_sequence_backward_typed_handle();
    return op.call(grad, input_size, batch_sizes, batch_first);
}

// aten::_pack_padded_sequence_backward(Tensor grad, int[] input_size, Tensor batch_sizes, bool batch_first) -> Tensor
at::Tensor _pack_padded_sequence_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_size, const at::Tensor & batch_sizes, bool batch_first) {
    static auto op = create__pack_padded_sequence_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input_size, batch_sizes, batch_first);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_packed_sequence, name, "aten::_pad_packed_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_packed_sequence, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_pad_packed_sequence, schema_str, "_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)")

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_pad_packed_sequence::schema> create__pad_packed_sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_pad_packed_sequence::name, _pad_packed_sequence::overload_name)
      .typed<_pad_packed_sequence::schema>();
}

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pad_packed_sequence::call(const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
    static auto op = create__pad_packed_sequence_typed_handle();
    return op.call(data, batch_sizes, batch_first, padding_value, total_length);
}

// aten::_pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _pad_packed_sequence::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, const at::Tensor & batch_sizes, bool batch_first, const at::Scalar & padding_value, int64_t total_length) {
    static auto op = create__pad_packed_sequence_typed_handle();
    return op.redispatch(dispatchKeySet, data, batch_sizes, batch_first, padding_value, total_length);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Storage, name, "aten::set_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Storage, overload_name, "source_Storage")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Storage, schema_str, "set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)")

// aten::set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<set__source_Storage::schema> create_set__source_Storage_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(set__source_Storage::name, set__source_Storage::overload_name)
      .typed<set__source_Storage::schema>();
}

// aten::set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)
at::Tensor & set__source_Storage::call(at::Tensor & self, at::Storage source) {
    static auto op = create_set__source_Storage_typed_handle();
    return op.call(self, source);
}

// aten::set_.source_Storage(Tensor(a!) self, Storage source) -> Tensor(a!)
at::Tensor & set__source_Storage::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Storage source) {
    static auto op = create_set__source_Storage_typed_handle();
    return op.redispatch(dispatchKeySet, self, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Storage_storage_offset, name, "aten::set_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Storage_storage_offset, overload_name, "source_Storage_storage_offset")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Storage_storage_offset, schema_str, "set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)")

// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<set__source_Storage_storage_offset::schema> create_set__source_Storage_storage_offset_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(set__source_Storage_storage_offset::name, set__source_Storage_storage_offset::overload_name)
      .typed<set__source_Storage_storage_offset::schema>();
}

// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)
at::Tensor & set__source_Storage_storage_offset::call(at::Tensor & self, at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) {
    static auto op = create_set__source_Storage_storage_offset_typed_handle();
    return op.call(self, source, storage_offset, size, stride);
}

// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, int storage_offset, int[] size, int[] stride=[]) -> Tensor(a!)
at::Tensor & set__source_Storage_storage_offset::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) {
    static auto op = create_set__source_Storage_storage_offset_typed_handle();
    return op.redispatch(dispatchKeySet, self, source, storage_offset, size, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Tensor, name, "aten::set_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Tensor, overload_name, "source_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set__source_Tensor, schema_str, "set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)")

// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<set__source_Tensor::schema> create_set__source_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(set__source_Tensor::name, set__source_Tensor::overload_name)
      .typed<set__source_Tensor::schema>();
}

// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)
at::Tensor & set__source_Tensor::call(at::Tensor & self, const at::Tensor & source) {
    static auto op = create_set__source_Tensor_typed_handle();
    return op.call(self, source);
}

// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -> Tensor(a!)
at::Tensor & set__source_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & source) {
    static auto op = create_set__source_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set_, name, "aten::set_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(set_, schema_str, "set_(Tensor(a!) self) -> Tensor(a!)")

// aten::set_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<set_::schema> create_set__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(set_::name, set_::overload_name)
      .typed<set_::schema>();
}

// aten::set_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & set_::call(at::Tensor & self) {
    static auto op = create_set__typed_handle();
    return op.call(self);
}

// aten::set_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & set_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_set__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_set_to, name, "aten::is_set_to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_set_to, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(is_set_to, schema_str, "is_set_to(Tensor self, Tensor tensor) -> bool")

// aten::is_set_to(Tensor self, Tensor tensor) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<is_set_to::schema> create_is_set_to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(is_set_to::name, is_set_to::overload_name)
      .typed<is_set_to::schema>();
}

// aten::is_set_to(Tensor self, Tensor tensor) -> bool
bool is_set_to::call(const at::Tensor & self, const at::Tensor & tensor) {
    static auto op = create_is_set_to_typed_handle();
    return op.call(self, tensor);
}

// aten::is_set_to(Tensor self, Tensor tensor) -> bool
bool is_set_to::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor) {
    static auto op = create_is_set_to_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill__Scalar, name, "aten::masked_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill__Scalar, schema_str, "masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)")

// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<masked_fill__Scalar::schema> create_masked_fill__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_fill__Scalar::name, masked_fill__Scalar::overload_name)
      .typed<masked_fill__Scalar::schema>();
}

// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)
at::Tensor & masked_fill__Scalar::call(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    static auto op = create_masked_fill__Scalar_typed_handle();
    return op.call(self, mask, value);
}

// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)
at::Tensor & masked_fill__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    static auto op = create_masked_fill__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill_Scalar, name, "aten::masked_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill_Scalar, schema_str, "masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor")

// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<masked_fill_Scalar::schema> create_masked_fill_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_fill_Scalar::name, masked_fill_Scalar::overload_name)
      .typed<masked_fill_Scalar::schema>();
}

// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
at::Tensor masked_fill_Scalar::call(const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    static auto op = create_masked_fill_Scalar_typed_handle();
    return op.call(self, mask, value);
}

// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -> Tensor
at::Tensor masked_fill_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
    static auto op = create_masked_fill_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill__Tensor, name, "aten::masked_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill__Tensor, schema_str, "masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)")

// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<masked_fill__Tensor::schema> create_masked_fill__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_fill__Tensor::name, masked_fill__Tensor::overload_name)
      .typed<masked_fill__Tensor::schema>();
}

// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)
at::Tensor & masked_fill__Tensor::call(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    static auto op = create_masked_fill__Tensor_typed_handle();
    return op.call(self, mask, value);
}

// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)
at::Tensor & masked_fill__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    static auto op = create_masked_fill__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill_Tensor, name, "aten::masked_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_fill_Tensor, schema_str, "masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor")

// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<masked_fill_Tensor::schema> create_masked_fill_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_fill_Tensor::name, masked_fill_Tensor::overload_name)
      .typed<masked_fill_Tensor::schema>();
}

// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
at::Tensor masked_fill_Tensor::call(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    static auto op = create_masked_fill_Tensor_typed_handle();
    return op.call(self, mask, value);
}

// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -> Tensor
at::Tensor masked_fill_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
    static auto op = create_masked_fill_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_scatter_, name, "aten::masked_scatter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_scatter_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_scatter_, schema_str, "masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)")

// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<masked_scatter_::schema> create_masked_scatter__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_scatter_::name, masked_scatter_::overload_name)
      .typed<masked_scatter_::schema>();
}

// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)
at::Tensor & masked_scatter_::call(at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
    static auto op = create_masked_scatter__typed_handle();
    return op.call(self, mask, source);
}

// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)
at::Tensor & masked_scatter_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
    static auto op = create_masked_scatter__typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_scatter, name, "aten::masked_scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_scatter, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_scatter, schema_str, "masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor")

// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<masked_scatter::schema> create_masked_scatter_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_scatter::name, masked_scatter::overload_name)
      .typed<masked_scatter::schema>();
}

// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor
at::Tensor masked_scatter::call(const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
    static auto op = create_masked_scatter_typed_handle();
    return op.call(self, mask, source);
}

// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor
at::Tensor masked_scatter::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, const at::Tensor & source) {
    static auto op = create_masked_scatter_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view, name, "aten::view")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view, schema_str, "view(Tensor(a) self, int[] size) -> Tensor(a)")

// aten::view(Tensor(a) self, int[] size) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view::schema> create_view_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view::name, view::overload_name)
      .typed<view::schema>();
}

// aten::view(Tensor(a) self, int[] size) -> Tensor(a)
at::Tensor view::call(const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create_view_typed_handle();
    return op.call(self, size);
}

// aten::view(Tensor(a) self, int[] size) -> Tensor(a)
at::Tensor view::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size) {
    static auto op = create_view_typed_handle();
    return op.redispatch(dispatchKeySet, self, size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_dtype, name, "aten::view")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_dtype, overload_name, "dtype")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(view_dtype, schema_str, "view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)")

// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<view_dtype::schema> create_view_dtype_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(view_dtype::name, view_dtype::overload_name)
      .typed<view_dtype::schema>();
}

// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
at::Tensor view_dtype::call(const at::Tensor & self, at::ScalarType dtype) {
    static auto op = create_view_dtype_typed_handle();
    return op.call(self, dtype);
}

// aten::view.dtype(Tensor(a) self, ScalarType dtype) -> Tensor(a)
at::Tensor view_dtype::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::ScalarType dtype) {
    static auto op = create_view_dtype_typed_handle();
    return op.redispatch(dispatchKeySet, self, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(put_, name, "aten::put_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(put_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(put_, schema_str, "put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)")

// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<put_::schema> create_put__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(put_::name, put_::overload_name)
      .typed<put_::schema>();
}

// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)
at::Tensor & put_::call(at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
    static auto op = create_put__typed_handle();
    return op.call(self, index, source, accumulate);
}

// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)
at::Tensor & put_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
    static auto op = create_put__typed_handle();
    return op.redispatch(dispatchKeySet, self, index, source, accumulate);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(put, name, "aten::put")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(put, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(put, schema_str, "put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor")

// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<put::schema> create_put_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(put::name, put::overload_name)
      .typed<put::schema>();
}

// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor
at::Tensor put::call(const at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
    static auto op = create_put_typed_handle();
    return op.call(self, index, source, accumulate);
}

// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -> Tensor
at::Tensor put::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index, const at::Tensor & source, bool accumulate) {
    static auto op = create_put_typed_handle();
    return op.redispatch(dispatchKeySet, self, index, source, accumulate);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_, name, "aten::index_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_, schema_str, "index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)")

// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_add_::schema> create_index_add__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_add_::name, index_add_::overload_name)
      .typed<index_add_::schema>();
}

// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & index_add_::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_add__typed_handle();
    return op.call(self, dim, index, source);
}

// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & index_add_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_add__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add__alpha, name, "aten::index_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add__alpha, overload_name, "alpha")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add__alpha, schema_str, "index_add_.alpha(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor(a!)")

// aten::index_add_.alpha(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_add__alpha::schema> create_index_add__alpha_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_add__alpha::name, index_add__alpha::overload_name)
      .typed<index_add__alpha::schema>();
}

// aten::index_add_.alpha(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor(a!)
at::Tensor & index_add__alpha::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    static auto op = create_index_add__alpha_typed_handle();
    return op.call(self, dim, index, source, alpha);
}

// aten::index_add_.alpha(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor(a!)
at::Tensor & index_add__alpha::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    static auto op = create_index_add__alpha_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add, name, "aten::index_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add, schema_str, "index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor")

// aten::index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_add::schema> create_index_add_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_add::name, index_add::overload_name)
      .typed<index_add::schema>();
}

// aten::index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
at::Tensor index_add::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_add_typed_handle();
    return op.call(self, dim, index, source);
}

// aten::index_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
at::Tensor index_add::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create_index_add_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_alpha, name, "aten::index_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_alpha, overload_name, "alpha")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_alpha, schema_str, "index_add.alpha(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor")

// aten::index_add.alpha(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_add_alpha::schema> create_index_add_alpha_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_add_alpha::name, index_add_alpha::overload_name)
      .typed<index_add_alpha::schema>();
}

// aten::index_add.alpha(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor
at::Tensor index_add_alpha::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    static auto op = create_index_add_alpha_typed_handle();
    return op.call(self, dim, index, source, alpha);
}

// aten::index_add.alpha(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha) -> Tensor
at::Tensor index_add_alpha::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    static auto op = create_index_add_alpha_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_dimname, name, "aten::index_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_add_dimname, schema_str, "index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor")

// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_add_dimname::schema> create_index_add_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_add_dimname::name, index_add_dimname::overload_name)
      .typed<index_add_dimname::schema>();
}

// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
at::Tensor index_add_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    static auto op = create_index_add_dimname_typed_handle();
    return op.call(self, dim, index, source, alpha);
}

// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
at::Tensor index_add_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & source, const at::Scalar & alpha) {
    static auto op = create_index_add_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Scalar, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Scalar, overload_name, "int_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Scalar, schema_str, "index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)")

// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__int_Scalar::schema> create_index_fill__int_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__int_Scalar::name, index_fill__int_Scalar::overload_name)
      .typed<index_fill__int_Scalar::schema>();
}

// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__int_Scalar::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill__int_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__int_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill__int_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar, overload_name, "int_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Scalar, schema_str, "index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor")

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_int_Scalar::schema> create_index_fill_int_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_int_Scalar::name, index_fill_int_Scalar::overload_name)
      .typed<index_fill_int_Scalar::schema>();
}

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_int_Scalar::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill_int_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_int_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill_int_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Tensor, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Tensor, overload_name, "int_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__int_Tensor, schema_str, "index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)")

// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__int_Tensor::schema> create_index_fill__int_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__int_Tensor::name, index_fill__int_Tensor::overload_name)
      .typed<index_fill__int_Tensor::schema>();
}

// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__int_Tensor::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill__int_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__int_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill__int_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor, overload_name, "int_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_int_Tensor, schema_str, "index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor")

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_int_Tensor::schema> create_index_fill_int_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_int_Tensor::name, index_fill_int_Tensor::overload_name)
      .typed<index_fill_int_Tensor::schema>();
}

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_int_Tensor::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill_int_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_int_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill_int_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Scalar, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Scalar, overload_name, "Dimname_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Scalar, schema_str, "index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)")

// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__Dimname_Scalar::schema> create_index_fill__Dimname_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__Dimname_Scalar::name, index_fill__Dimname_Scalar::overload_name)
      .typed<index_fill__Dimname_Scalar::schema>();
}

// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Scalar::call(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill__Dimname_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill__Dimname_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Tensor, name, "aten::index_fill_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Tensor, overload_name, "Dimname_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill__Dimname_Tensor, schema_str, "index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)")

// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_fill__Dimname_Tensor::schema> create_index_fill__Dimname_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill__Dimname_Tensor::name, index_fill__Dimname_Tensor::overload_name)
      .typed<index_fill__Dimname_Tensor::schema>();
}

// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Tensor::call(at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill__Dimname_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -> Tensor(a!)
at::Tensor & index_fill__Dimname_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill__Dimname_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Scalar, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Scalar, overload_name, "Dimname_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Scalar, schema_str, "index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor")

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_Dimname_Scalar::schema> create_index_fill_Dimname_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_Dimname_Scalar::name, index_fill_Dimname_Scalar::overload_name)
      .typed<index_fill_Dimname_Scalar::schema>();
}

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_Dimname_Scalar::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill_Dimname_Scalar_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
at::Tensor index_fill_Dimname_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_index_fill_Dimname_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Tensor, name, "aten::index_fill")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Tensor, overload_name, "Dimname_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_fill_Dimname_Tensor, schema_str, "index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor")

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_fill_Dimname_Tensor::schema> create_index_fill_Dimname_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_fill_Dimname_Tensor::name, index_fill_Dimname_Tensor::overload_name)
      .typed<index_fill_Dimname_Tensor::schema>();
}

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_Dimname_Tensor::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill_Dimname_Tensor_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -> Tensor
at::Tensor index_fill_Dimname_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & value) {
    static auto op = create_index_fill_Dimname_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_src, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_src, overload_name, "src")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_src, schema_str, "scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor")

// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_src::schema> create_scatter_src_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_src::name, scatter_src::overload_name)
      .typed<scatter_src::schema>();
}

// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_src::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_src_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_src::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_src_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__src, name, "aten::scatter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__src, overload_name, "src")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__src, schema_str, "scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)")

// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter__src::schema> create_scatter__src_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter__src::name, scatter__src::overload_name)
      .typed<scatter__src::schema>();
}

// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
at::Tensor & scatter__src::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter__src_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
at::Tensor & scatter__src::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter__src_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_src_out, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_src_out, overload_name, "src_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_src_out, schema_str, "scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)")

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_src_out::schema> create_scatter_src_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_src_out::name, scatter_src_out::overload_name)
      .typed<scatter_src_out::schema>();
}

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_src_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
    static auto op = create_scatter_src_out_typed_handle();
    return op.call(self, dim, index, src, out);
}

// aten::scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_src_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
    static auto op = create_scatter_src_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value, overload_name, "value")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value, schema_str, "scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor")

// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_value::schema> create_scatter_value_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_value::name, scatter_value::overload_name)
      .typed<scatter_value::schema>();
}

// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
at::Tensor scatter_value::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_scatter_value_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
at::Tensor scatter_value::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_scatter_value_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__value, name, "aten::scatter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__value, overload_name, "value")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__value, schema_str, "scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)")

// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter__value::schema> create_scatter__value_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter__value::name, scatter__value::overload_name)
      .typed<scatter__value::schema>();
}

// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & scatter__value::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_scatter__value_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
at::Tensor & scatter__value::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_scatter__value_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_out, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_out, overload_name, "value_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_out, schema_str, "scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)")

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_value_out::schema> create_scatter_value_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_value_out::name, scatter_value_out::overload_name)
      .typed<scatter_value_out::schema>();
}

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_value_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_scatter_value_out_typed_handle();
    return op.call(self, dim, index, value, out);
}

// aten::scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_value_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_scatter_value_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_reduce, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_reduce, overload_name, "reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_reduce, schema_str, "scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor")

// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_reduce::schema> create_scatter_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_reduce::name, scatter_reduce::overload_name)
      .typed<scatter_reduce::schema>();
}

// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor
at::Tensor scatter_reduce::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
    static auto op = create_scatter_reduce_typed_handle();
    return op.call(self, dim, index, src, reduce);
}

// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor
at::Tensor scatter_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
    static auto op = create_scatter_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src, reduce);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__reduce, name, "aten::scatter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__reduce, overload_name, "reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__reduce, schema_str, "scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)")

// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter__reduce::schema> create_scatter__reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter__reduce::name, scatter__reduce::overload_name)
      .typed<scatter__reduce::schema>();
}

// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)
at::Tensor & scatter__reduce::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
    static auto op = create_scatter__reduce_typed_handle();
    return op.call(self, dim, index, src, reduce);
}

// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -> Tensor(a!)
at::Tensor & scatter__reduce::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
    static auto op = create_scatter__reduce_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src, reduce);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_reduce_out, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_reduce_out, overload_name, "reduce_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_reduce_out, schema_str, "scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)")

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_reduce_out::schema> create_scatter_reduce_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_reduce_out::name, scatter_reduce_out::overload_name)
      .typed<scatter_reduce_out::schema>();
}

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_reduce_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, at::Tensor & out) {
    static auto op = create_scatter_reduce_out_typed_handle();
    return op.call(self, dim, index, src, reduce, out);
}

// aten::scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_reduce_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, at::Tensor & out) {
    static auto op = create_scatter_reduce_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src, reduce, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_reduce, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_reduce, overload_name, "value_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_reduce, schema_str, "scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor")

// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_value_reduce::schema> create_scatter_value_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_value_reduce::name, scatter_value_reduce::overload_name)
      .typed<scatter_value_reduce::schema>();
}

// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor
at::Tensor scatter_value_reduce::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
    static auto op = create_scatter_value_reduce_typed_handle();
    return op.call(self, dim, index, value, reduce);
}

// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor
at::Tensor scatter_value_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
    static auto op = create_scatter_value_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value, reduce);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__value_reduce, name, "aten::scatter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__value_reduce, overload_name, "value_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter__value_reduce, schema_str, "scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)")

// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter__value_reduce::schema> create_scatter__value_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter__value_reduce::name, scatter__value_reduce::overload_name)
      .typed<scatter__value_reduce::schema>();
}

// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)
at::Tensor & scatter__value_reduce::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
    static auto op = create_scatter__value_reduce_typed_handle();
    return op.call(self, dim, index, value, reduce);
}

// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -> Tensor(a!)
at::Tensor & scatter__value_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
    static auto op = create_scatter__value_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value, reduce);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_reduce_out, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_reduce_out, overload_name, "value_reduce_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_value_reduce_out, schema_str, "scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)")

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_value_reduce_out::schema> create_scatter_value_reduce_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_value_reduce_out::name, scatter_value_reduce_out::overload_name)
      .typed<scatter_value_reduce_out::schema>();
}

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_value_reduce_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce, at::Tensor & out) {
    static auto op = create_scatter_value_reduce_out_typed_handle();
    return op.call(self, dim, index, value, reduce, out);
}

// aten::scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_value_reduce_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce, at::Tensor & out) {
    static auto op = create_scatter_value_reduce_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value, reduce, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_dimname_src, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_dimname_src, overload_name, "dimname_src")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_dimname_src, schema_str, "scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor")

// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_dimname_src::schema> create_scatter_dimname_src_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_dimname_src::name, scatter_dimname_src::overload_name)
      .typed<scatter_dimname_src::schema>();
}

// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_dimname_src::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_dimname_src_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_dimname_src::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_dimname_src_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_dimname_value, name, "aten::scatter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_dimname_value, overload_name, "dimname_value")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_dimname_value, schema_str, "scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor")

// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_dimname_value::schema> create_scatter_dimname_value_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_dimname_value::name, scatter_dimname_value::overload_name)
      .typed<scatter_dimname_value::schema>();
}

// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
at::Tensor scatter_dimname_value::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_scatter_dimname_value_typed_handle();
    return op.call(self, dim, index, value);
}

// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -> Tensor
at::Tensor scatter_dimname_value::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Scalar & value) {
    static auto op = create_scatter_dimname_value_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add, name, "aten::scatter_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add, schema_str, "scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor")

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add::schema> create_scatter_add_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add::name, scatter_add::overload_name)
      .typed<scatter_add::schema>();
}

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_add_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_add_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_, name, "aten::scatter_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_, schema_str, "scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)")

// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add_::schema> create_scatter_add__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add_::name, scatter_add_::overload_name)
      .typed<scatter_add_::schema>();
}

// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
at::Tensor & scatter_add_::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_add__typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
at::Tensor & scatter_add_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_add__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_out, name, "aten::scatter_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_out, schema_str, "scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)")

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add_out::schema> create_scatter_add_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add_out::name, scatter_add_out::overload_name)
      .typed<scatter_add_out::schema>();
}

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_add_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
    static auto op = create_scatter_add_out_typed_handle();
    return op.call(self, dim, index, src, out);
}

// aten::scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & scatter_add_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, at::Tensor & out) {
    static auto op = create_scatter_add_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_dimname, name, "aten::scatter_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(scatter_add_dimname, schema_str, "scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor")

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<scatter_add_dimname::schema> create_scatter_add_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(scatter_add_dimname::name, scatter_add_dimname::overload_name)
      .typed<scatter_add_dimname::schema>();
}

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_add_dimname_typed_handle();
    return op.call(self, dim, index, src);
}

// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
at::Tensor scatter_add_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, const at::Tensor & src) {
    static auto op = create_scatter_add_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, src);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq__Scalar, name, "aten::eq_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq__Scalar, schema_str, "eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<eq__Scalar::schema> create_eq__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eq__Scalar::name, eq__Scalar::overload_name)
      .typed<eq__Scalar::schema>();
}

// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & eq__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_eq__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & eq__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_eq__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq__Tensor, name, "aten::eq_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq__Tensor, schema_str, "eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<eq__Tensor::schema> create_eq__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eq__Tensor::name, eq__Tensor::overload_name)
      .typed<eq__Tensor::schema>();
}

// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & eq__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_eq__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & eq__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_eq__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Tensor_out, name, "aten::bitwise_and")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Tensor_out, schema_str, "bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_and_Tensor_out::schema> create_bitwise_and_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_and_Tensor_out::name, bitwise_and_Tensor_out::overload_name)
      .typed<bitwise_and_Tensor_out::schema>();
}

// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_and_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_and_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_and_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_and_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Scalar_out, name, "aten::bitwise_and")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Scalar_out, schema_str, "bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_and_Scalar_out::schema> create_bitwise_and_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_and_Scalar_out::name, bitwise_and_Scalar_out::overload_name)
      .typed<bitwise_and_Scalar_out::schema>();
}

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_and_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_and_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_and_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_and_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Scalar, name, "aten::bitwise_and")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Scalar, schema_str, "bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_and_Scalar::schema> create_bitwise_and_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_and_Scalar::name, bitwise_and_Scalar::overload_name)
      .typed<bitwise_and_Scalar::schema>();
}

// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_and_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_and_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_and_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_and_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Tensor, name, "aten::bitwise_and")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and_Tensor, schema_str, "bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_and_Tensor::schema> create_bitwise_and_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_and_Tensor::name, bitwise_and_Tensor::overload_name)
      .typed<bitwise_and_Tensor::schema>();
}

// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_and_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_and_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_and_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_and_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and__Scalar, name, "aten::bitwise_and_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and__Scalar, schema_str, "bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_and__Scalar::schema> create_bitwise_and__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_and__Scalar::name, bitwise_and__Scalar::overload_name)
      .typed<bitwise_and__Scalar::schema>();
}

// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_and__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_and__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_and__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_and__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and__Tensor, name, "aten::bitwise_and_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_and__Tensor, schema_str, "bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_and__Tensor::schema> create_bitwise_and__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_and__Tensor::name, bitwise_and__Tensor::overload_name)
      .typed<bitwise_and__Tensor::schema>();
}

// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_and__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_and__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_and__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_and__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__and___Scalar, name, "aten::__and__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__and___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__and___Scalar, schema_str, "__and__.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__and___Scalar::schema> create___and___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__and___Scalar::name, __and___Scalar::overload_name)
      .typed<__and___Scalar::schema>();
}

// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __and___Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___and___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__and__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __and___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___and___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__and___Tensor, name, "aten::__and__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__and___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__and___Tensor, schema_str, "__and__.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__and___Tensor::schema> create___and___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__and___Tensor::name, __and___Tensor::overload_name)
      .typed<__and___Tensor::schema>();
}

// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __and___Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___and___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__and__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __and___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___and___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__iand___Scalar, name, "aten::__iand__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__iand___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__iand___Scalar, schema_str, "__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__iand___Scalar::schema> create___iand___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__iand___Scalar::name, __iand___Scalar::overload_name)
      .typed<__iand___Scalar::schema>();
}

// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __iand___Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create___iand___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __iand___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create___iand___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__iand___Tensor, name, "aten::__iand__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__iand___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__iand___Tensor, schema_str, "__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__iand___Tensor::schema> create___iand___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__iand___Tensor::name, __iand___Tensor::overload_name)
      .typed<__iand___Tensor::schema>();
}

// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __iand___Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create___iand___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __iand___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create___iand___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Tensor_out, name, "aten::bitwise_or")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Tensor_out, schema_str, "bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_or_Tensor_out::schema> create_bitwise_or_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_or_Tensor_out::name, bitwise_or_Tensor_out::overload_name)
      .typed<bitwise_or_Tensor_out::schema>();
}

// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_or_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_or_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_or_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_or_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Scalar_out, name, "aten::bitwise_or")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Scalar_out, schema_str, "bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_or_Scalar_out::schema> create_bitwise_or_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_or_Scalar_out::name, bitwise_or_Scalar_out::overload_name)
      .typed<bitwise_or_Scalar_out::schema>();
}

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_or_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_or_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_or_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_or_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Scalar, name, "aten::bitwise_or")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Scalar, schema_str, "bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_or_Scalar::schema> create_bitwise_or_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_or_Scalar::name, bitwise_or_Scalar::overload_name)
      .typed<bitwise_or_Scalar::schema>();
}

// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_or_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_or_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_or_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_or_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Tensor, name, "aten::bitwise_or")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or_Tensor, schema_str, "bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_or_Tensor::schema> create_bitwise_or_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_or_Tensor::name, bitwise_or_Tensor::overload_name)
      .typed<bitwise_or_Tensor::schema>();
}

// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_or_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_or_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_or_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_or_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or__Scalar, name, "aten::bitwise_or_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or__Scalar, schema_str, "bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_or__Scalar::schema> create_bitwise_or__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_or__Scalar::name, bitwise_or__Scalar::overload_name)
      .typed<bitwise_or__Scalar::schema>();
}

// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_or__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_or__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_or__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_or__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or__Tensor, name, "aten::bitwise_or_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_or__Tensor, schema_str, "bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_or__Tensor::schema> create_bitwise_or__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_or__Tensor::name, bitwise_or__Tensor::overload_name)
      .typed<bitwise_or__Tensor::schema>();
}

// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_or__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_or__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_or__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_or__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__or___Scalar, name, "aten::__or__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__or___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__or___Scalar, schema_str, "__or__.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__or___Scalar::schema> create___or___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__or___Scalar::name, __or___Scalar::overload_name)
      .typed<__or___Scalar::schema>();
}

// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __or___Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___or___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__or__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __or___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___or___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__or___Tensor, name, "aten::__or__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__or___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__or___Tensor, schema_str, "__or__.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__or___Tensor::schema> create___or___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__or___Tensor::name, __or___Tensor::overload_name)
      .typed<__or___Tensor::schema>();
}

// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __or___Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___or___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__or__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __or___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___or___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ior___Scalar, name, "aten::__ior__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ior___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ior___Scalar, schema_str, "__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ior___Scalar::schema> create___ior___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ior___Scalar::name, __ior___Scalar::overload_name)
      .typed<__ior___Scalar::schema>();
}

// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ior___Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create___ior___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ior___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create___ior___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ior___Tensor, name, "aten::__ior__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ior___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ior___Tensor, schema_str, "__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ior___Tensor::schema> create___ior___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ior___Tensor::name, __ior___Tensor::overload_name)
      .typed<__ior___Tensor::schema>();
}

// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ior___Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create___ior___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ior___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create___ior___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Tensor_out, name, "aten::bitwise_xor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Tensor_out, schema_str, "bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_xor_Tensor_out::schema> create_bitwise_xor_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_xor_Tensor_out::name, bitwise_xor_Tensor_out::overload_name)
      .typed<bitwise_xor_Tensor_out::schema>();
}

// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_xor_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_xor_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_xor_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_xor_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Scalar_out, name, "aten::bitwise_xor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Scalar_out, schema_str, "bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_xor_Scalar_out::schema> create_bitwise_xor_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_xor_Scalar_out::name, bitwise_xor_Scalar_out::overload_name)
      .typed<bitwise_xor_Scalar_out::schema>();
}

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_xor_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_xor_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_xor_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_xor_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Scalar, name, "aten::bitwise_xor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Scalar, schema_str, "bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_xor_Scalar::schema> create_bitwise_xor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_xor_Scalar::name, bitwise_xor_Scalar::overload_name)
      .typed<bitwise_xor_Scalar::schema>();
}

// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_xor_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_xor_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_xor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_xor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Tensor, name, "aten::bitwise_xor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor_Tensor, schema_str, "bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_xor_Tensor::schema> create_bitwise_xor_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_xor_Tensor::name, bitwise_xor_Tensor::overload_name)
      .typed<bitwise_xor_Tensor::schema>();
}

// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_xor_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_xor_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_xor_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_xor_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor__Scalar, name, "aten::bitwise_xor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor__Scalar, schema_str, "bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_xor__Scalar::schema> create_bitwise_xor__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_xor__Scalar::name, bitwise_xor__Scalar::overload_name)
      .typed<bitwise_xor__Scalar::schema>();
}

// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_xor__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_xor__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_xor__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_xor__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor__Tensor, name, "aten::bitwise_xor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_xor__Tensor, schema_str, "bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_xor__Tensor::schema> create_bitwise_xor__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_xor__Tensor::name, bitwise_xor__Tensor::overload_name)
      .typed<bitwise_xor__Tensor::schema>();
}

// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_xor__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_xor__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_xor__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_xor__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Scalar, name, "aten::__xor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Scalar, schema_str, "__xor__.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__xor___Scalar::schema> create___xor___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__xor___Scalar::name, __xor___Scalar::overload_name)
      .typed<__xor___Scalar::schema>();
}

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __xor___Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___xor___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__xor__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __xor___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___xor___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Tensor, name, "aten::__xor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__xor___Tensor, schema_str, "__xor__.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__xor___Tensor::schema> create___xor___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__xor___Tensor::name, __xor___Tensor::overload_name)
      .typed<__xor___Tensor::schema>();
}

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __xor___Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___xor___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__xor__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __xor___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___xor___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Scalar, name, "aten::__ixor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Scalar, schema_str, "__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ixor___Scalar::schema> create___ixor___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ixor___Scalar::name, __ixor___Scalar::overload_name)
      .typed<__ixor___Scalar::schema>();
}

// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ixor___Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create___ixor___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ixor___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create___ixor___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Tensor, name, "aten::__ixor__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ixor___Tensor, schema_str, "__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ixor___Tensor::schema> create___ixor___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ixor___Tensor::name, __ixor___Tensor::overload_name)
      .typed<__ixor___Tensor::schema>();
}

// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ixor___Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create___ixor___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ixor___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create___ixor___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__lshift___Scalar, name, "aten::__lshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__lshift___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__lshift___Scalar, schema_str, "__lshift__.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__lshift___Scalar::schema> create___lshift___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__lshift___Scalar::name, __lshift___Scalar::overload_name)
      .typed<__lshift___Scalar::schema>();
}

// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __lshift___Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___lshift___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __lshift___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___lshift___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__lshift___Tensor, name, "aten::__lshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__lshift___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__lshift___Tensor, schema_str, "__lshift__.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__lshift___Tensor::schema> create___lshift___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__lshift___Tensor::name, __lshift___Tensor::overload_name)
      .typed<__lshift___Tensor::schema>();
}

// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __lshift___Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___lshift___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __lshift___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___lshift___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ilshift___Scalar, name, "aten::__ilshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ilshift___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ilshift___Scalar, schema_str, "__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ilshift___Scalar::schema> create___ilshift___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ilshift___Scalar::name, __ilshift___Scalar::overload_name)
      .typed<__ilshift___Scalar::schema>();
}

// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ilshift___Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create___ilshift___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __ilshift___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create___ilshift___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ilshift___Tensor, name, "aten::__ilshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ilshift___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__ilshift___Tensor, schema_str, "__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__ilshift___Tensor::schema> create___ilshift___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__ilshift___Tensor::name, __ilshift___Tensor::overload_name)
      .typed<__ilshift___Tensor::schema>();
}

// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ilshift___Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create___ilshift___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __ilshift___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create___ilshift___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor, name, "aten::bitwise_left_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor, schema_str, "bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift_Tensor::schema> create_bitwise_left_shift_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift_Tensor::name, bitwise_left_shift_Tensor::overload_name)
      .typed<bitwise_left_shift_Tensor::schema>();
}

// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_left_shift_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_left_shift_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_left_shift_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_left_shift_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift__Tensor, name, "aten::bitwise_left_shift_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift__Tensor, schema_str, "bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift__Tensor::schema> create_bitwise_left_shift__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift__Tensor::name, bitwise_left_shift__Tensor::overload_name)
      .typed<bitwise_left_shift__Tensor::schema>();
}

// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_left_shift__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_left_shift__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_left_shift__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_left_shift__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_out, name, "aten::bitwise_left_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_out, schema_str, "bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift_Tensor_out::schema> create_bitwise_left_shift_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift_Tensor_out::name, bitwise_left_shift_Tensor_out::overload_name)
      .typed<bitwise_left_shift_Tensor_out::schema>();
}

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_left_shift_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_left_shift_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_left_shift_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_left_shift_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_Scalar, name, "aten::bitwise_left_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_Scalar, schema_str, "bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor")

// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift_Tensor_Scalar::schema> create_bitwise_left_shift_Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift_Tensor_Scalar::name, bitwise_left_shift_Tensor_Scalar::overload_name)
      .typed<bitwise_left_shift_Tensor_Scalar::schema>();
}

// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_left_shift_Tensor_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_left_shift_Tensor_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_left_shift_Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_left_shift_Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift__Tensor_Scalar, name, "aten::bitwise_left_shift_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift__Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift__Tensor_Scalar, schema_str, "bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift__Tensor_Scalar::schema> create_bitwise_left_shift__Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift__Tensor_Scalar::name, bitwise_left_shift__Tensor_Scalar::overload_name)
      .typed<bitwise_left_shift__Tensor_Scalar::schema>();
}

// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_left_shift__Tensor_Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_left_shift__Tensor_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_left_shift__Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_left_shift__Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_Scalar_out, name, "aten::bitwise_left_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_Scalar_out, overload_name, "Tensor_Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Tensor_Scalar_out, schema_str, "bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift_Tensor_Scalar_out::schema> create_bitwise_left_shift_Tensor_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift_Tensor_Scalar_out::name, bitwise_left_shift_Tensor_Scalar_out::overload_name)
      .typed<bitwise_left_shift_Tensor_Scalar_out::schema>();
}

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_left_shift_Tensor_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_left_shift_Tensor_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_left_shift_Tensor_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_left_shift_Tensor_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Scalar_Tensor, name, "aten::bitwise_left_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Scalar_Tensor, overload_name, "Scalar_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_left_shift_Scalar_Tensor, schema_str, "bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor")

// aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_left_shift_Scalar_Tensor::schema> create_bitwise_left_shift_Scalar_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_left_shift_Scalar_Tensor::name, bitwise_left_shift_Scalar_Tensor::overload_name)
      .typed<bitwise_left_shift_Scalar_Tensor::schema>();
}

// aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
at::Tensor bitwise_left_shift_Scalar_Tensor::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_bitwise_left_shift_Scalar_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
at::Tensor bitwise_left_shift_Scalar_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_bitwise_left_shift_Scalar_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__rshift___Scalar, name, "aten::__rshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__rshift___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__rshift___Scalar, schema_str, "__rshift__.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__rshift___Scalar::schema> create___rshift___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__rshift___Scalar::name, __rshift___Scalar::overload_name)
      .typed<__rshift___Scalar::schema>();
}

// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __rshift___Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___rshift___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor __rshift___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create___rshift___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__rshift___Tensor, name, "aten::__rshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__rshift___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__rshift___Tensor, schema_str, "__rshift__.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<__rshift___Tensor::schema> create___rshift___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__rshift___Tensor::name, __rshift___Tensor::overload_name)
      .typed<__rshift___Tensor::schema>();
}

// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __rshift___Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___rshift___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor __rshift___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create___rshift___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__irshift___Scalar, name, "aten::__irshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__irshift___Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__irshift___Scalar, schema_str, "__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__irshift___Scalar::schema> create___irshift___Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__irshift___Scalar::name, __irshift___Scalar::overload_name)
      .typed<__irshift___Scalar::schema>();
}

// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __irshift___Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create___irshift___Scalar_typed_handle();
    return op.call(self, other);
}

// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & __irshift___Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create___irshift___Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__irshift___Tensor, name, "aten::__irshift__")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__irshift___Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(__irshift___Tensor, schema_str, "__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<__irshift___Tensor::schema> create___irshift___Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(__irshift___Tensor::name, __irshift___Tensor::overload_name)
      .typed<__irshift___Tensor::schema>();
}

// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __irshift___Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create___irshift___Tensor_typed_handle();
    return op.call(self, other);
}

// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & __irshift___Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create___irshift___Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor, name, "aten::bitwise_right_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor, schema_str, "bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift_Tensor::schema> create_bitwise_right_shift_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift_Tensor::name, bitwise_right_shift_Tensor::overload_name)
      .typed<bitwise_right_shift_Tensor::schema>();
}

// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_right_shift_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_right_shift_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor bitwise_right_shift_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_right_shift_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift__Tensor, name, "aten::bitwise_right_shift_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift__Tensor, schema_str, "bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift__Tensor::schema> create_bitwise_right_shift__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift__Tensor::name, bitwise_right_shift__Tensor::overload_name)
      .typed<bitwise_right_shift__Tensor::schema>();
}

// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_right_shift__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_right_shift__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & bitwise_right_shift__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_bitwise_right_shift__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_out, name, "aten::bitwise_right_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_out, schema_str, "bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift_Tensor_out::schema> create_bitwise_right_shift_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift_Tensor_out::name, bitwise_right_shift_Tensor_out::overload_name)
      .typed<bitwise_right_shift_Tensor_out::schema>();
}

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_right_shift_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_right_shift_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_right_shift_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_bitwise_right_shift_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_Scalar, name, "aten::bitwise_right_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_Scalar, schema_str, "bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor")

// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift_Tensor_Scalar::schema> create_bitwise_right_shift_Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift_Tensor_Scalar::name, bitwise_right_shift_Tensor_Scalar::overload_name)
      .typed<bitwise_right_shift_Tensor_Scalar::schema>();
}

// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_right_shift_Tensor_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_right_shift_Tensor_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor bitwise_right_shift_Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_right_shift_Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift__Tensor_Scalar, name, "aten::bitwise_right_shift_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift__Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift__Tensor_Scalar, schema_str, "bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift__Tensor_Scalar::schema> create_bitwise_right_shift__Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift__Tensor_Scalar::name, bitwise_right_shift__Tensor_Scalar::overload_name)
      .typed<bitwise_right_shift__Tensor_Scalar::schema>();
}

// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_right_shift__Tensor_Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_right_shift__Tensor_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & bitwise_right_shift__Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_bitwise_right_shift__Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_Scalar_out, name, "aten::bitwise_right_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_Scalar_out, overload_name, "Tensor_Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Tensor_Scalar_out, schema_str, "bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift_Tensor_Scalar_out::schema> create_bitwise_right_shift_Tensor_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift_Tensor_Scalar_out::name, bitwise_right_shift_Tensor_Scalar_out::overload_name)
      .typed<bitwise_right_shift_Tensor_Scalar_out::schema>();
}

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_right_shift_Tensor_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_right_shift_Tensor_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bitwise_right_shift_Tensor_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_bitwise_right_shift_Tensor_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Scalar_Tensor, name, "aten::bitwise_right_shift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Scalar_Tensor, overload_name, "Scalar_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bitwise_right_shift_Scalar_Tensor, schema_str, "bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor")

// aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bitwise_right_shift_Scalar_Tensor::schema> create_bitwise_right_shift_Scalar_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bitwise_right_shift_Scalar_Tensor::name, bitwise_right_shift_Scalar_Tensor::overload_name)
      .typed<bitwise_right_shift_Scalar_Tensor::schema>();
}

// aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
at::Tensor bitwise_right_shift_Scalar_Tensor::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_bitwise_right_shift_Scalar_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
at::Tensor bitwise_right_shift_Scalar_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_bitwise_right_shift_Scalar_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_, name, "aten::tril_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_, schema_str, "tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)")

// aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tril_::schema> create_tril__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tril_::name, tril_::overload_name)
      .typed<tril_::schema>();
}

// aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
at::Tensor & tril_::call(at::Tensor & self, int64_t diagonal) {
    static auto op = create_tril__typed_handle();
    return op.call(self, diagonal);
}

// aten::tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
at::Tensor & tril_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
    static auto op = create_tril__typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_, name, "aten::triu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_, schema_str, "triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)")

// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<triu_::schema> create_triu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu_::name, triu_::overload_name)
      .typed<triu_::schema>();
}

// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
at::Tensor & triu_::call(at::Tensor & self, int64_t diagonal) {
    static auto op = create_triu__typed_handle();
    return op.call(self, diagonal);
}

// aten::triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
at::Tensor & triu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t diagonal) {
    static auto op = create_triu__typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_, name, "aten::digamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_, schema_str, "digamma_(Tensor(a!) self) -> Tensor(a!)")

// aten::digamma_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<digamma_::schema> create_digamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(digamma_::name, digamma_::overload_name)
      .typed<digamma_::schema>();
}

// aten::digamma_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & digamma_::call(at::Tensor & self) {
    static auto op = create_digamma__typed_handle();
    return op.call(self);
}

// aten::digamma_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & digamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_digamma__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Scalar, name, "aten::lerp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Scalar, schema_str, "lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)")

// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp__Scalar::schema> create_lerp__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp__Scalar::name, lerp__Scalar::overload_name)
      .typed<lerp__Scalar::schema>();
}

// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
at::Tensor & lerp__Scalar::call(at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    static auto op = create_lerp__Scalar_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
at::Tensor & lerp__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    static auto op = create_lerp__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Tensor, name, "aten::lerp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp__Tensor, schema_str, "lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)")

// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp__Tensor::schema> create_lerp__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp__Tensor::name, lerp__Tensor::overload_name)
      .typed<lerp__Tensor::schema>();
}

// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
at::Tensor & lerp__Tensor::call(at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    static auto op = create_lerp__Tensor_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
at::Tensor & lerp__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    static auto op = create_lerp__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_, name, "aten::addbmm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_, schema_str, "addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)")

// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addbmm_::schema> create_addbmm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addbmm_::name, addbmm_::overload_name)
      .typed<addbmm_::schema>();
}

// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addbmm_::call(at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addbmm__typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
at::Tensor & addbmm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addbmm__typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_out, name, "aten::addbmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm_out, schema_str, "addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addbmm_out::schema> create_addbmm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addbmm_out::name, addbmm_out::overload_name)
      .typed<addbmm_out::schema>();
}

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addbmm_out::call(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addbmm_out_typed_handle();
    return op.call(self, batch1, batch2, beta, alpha, out);
}

// aten::addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addbmm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
    static auto op = create_addbmm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm, name, "aten::addbmm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addbmm, schema_str, "addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor")

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addbmm::schema> create_addbmm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addbmm::name, addbmm::overload_name)
      .typed<addbmm::schema>();
}

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addbmm::call(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addbmm_typed_handle();
    return op.call(self, batch1, batch2, beta, alpha);
}

// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
at::Tensor addbmm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
    static auto op = create_addbmm_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch1, batch2, beta, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__from, name, "aten::random_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__from, overload_name, "from")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__from, schema_str, "random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)")

// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random__from::schema> create_random__from_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random__from::name, random__from::overload_name)
      .typed<random__from::schema>();
}

// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__from::call(at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    static auto op = create_random__from_typed_handle();
    return op.call(self, from, to, generator);
}

// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__from::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
    static auto op = create_random__from_typed_handle();
    return op.redispatch(dispatchKeySet, self, from, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__to, name, "aten::random_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__to, overload_name, "to")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random__to, schema_str, "random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)")

// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random__to::schema> create_random__to_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random__to::name, random__to::overload_name)
      .typed<random__to::schema>();
}

// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__to::call(at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    static auto op = create_random__to_typed_handle();
    return op.call(self, to, generator);
}

// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random__to::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
    static auto op = create_random__to_typed_handle();
    return op.redispatch(dispatchKeySet, self, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_, name, "aten::random_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(random_, schema_str, "random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)")

// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<random_::schema> create_random__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(random_::name, random_::overload_name)
      .typed<random_::schema>();
}

// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random_::call(at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create_random__typed_handle();
    return op.call(self, generator);
}

// aten::random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & random_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, c10::optional<at::Generator> generator) {
    static auto op = create_random__typed_handle();
    return op.redispatch(dispatchKeySet, self, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(uniform_, name, "aten::uniform_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(uniform_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(uniform_, schema_str, "uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)")

// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<uniform_::schema> create_uniform__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(uniform_::name, uniform_::overload_name)
      .typed<uniform_::schema>();
}

// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & uniform_::call(at::Tensor & self, double from, double to, c10::optional<at::Generator> generator) {
    static auto op = create_uniform__typed_handle();
    return op.call(self, from, to, generator);
}

// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & uniform_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double from, double to, c10::optional<at::Generator> generator) {
    static auto op = create_uniform__typed_handle();
    return op.redispatch(dispatchKeySet, self, from, to, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_, name, "aten::cauchy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cauchy_, schema_str, "cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)")

// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cauchy_::schema> create_cauchy__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cauchy_::name, cauchy_::overload_name)
      .typed<cauchy_::schema>();
}

// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & cauchy_::call(at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
    static auto op = create_cauchy__typed_handle();
    return op.call(self, median, sigma, generator);
}

// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & cauchy_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double median, double sigma, c10::optional<at::Generator> generator) {
    static auto op = create_cauchy__typed_handle();
    return op.redispatch(dispatchKeySet, self, median, sigma, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_, name, "aten::log_normal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_normal_, schema_str, "log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)")

// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_normal_::schema> create_log_normal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_normal_::name, log_normal_::overload_name)
      .typed<log_normal_::schema>();
}

// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & log_normal_::call(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    static auto op = create_log_normal__typed_handle();
    return op.call(self, mean, std, generator);
}

// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & log_normal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    static auto op = create_log_normal__typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exponential_, name, "aten::exponential_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exponential_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(exponential_, schema_str, "exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)")

// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<exponential_::schema> create_exponential__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(exponential_::name, exponential_::overload_name)
      .typed<exponential_::schema>();
}

// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & exponential_::call(at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
    static auto op = create_exponential__typed_handle();
    return op.call(self, lambd, generator);
}

// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & exponential_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double lambd, c10::optional<at::Generator> generator) {
    static auto op = create_exponential__typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geometric_, name, "aten::geometric_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geometric_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geometric_, schema_str, "geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)")

// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<geometric_::schema> create_geometric__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(geometric_::name, geometric_::overload_name)
      .typed<geometric_::schema>();
}

// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & geometric_::call(at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create_geometric__typed_handle();
    return op.call(self, p, generator);
}

// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & geometric_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double p, c10::optional<at::Generator> generator) {
    static auto op = create_geometric__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_out, name, "aten::diag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_out, schema_str, "diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<diag_out::schema> create_diag_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diag_out::name, diag_out::overload_name)
      .typed<diag_out::schema>();
}

// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & diag_out::call(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    static auto op = create_diag_out_typed_handle();
    return op.call(self, diagonal, out);
}

// aten::diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & diag_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    static auto op = create_diag_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag, name, "aten::diag")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag, schema_str, "diag(Tensor self, int diagonal=0) -> Tensor")

// aten::diag(Tensor self, int diagonal=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diag::schema> create_diag_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diag::name, diag::overload_name)
      .typed<diag::schema>();
}

// aten::diag(Tensor self, int diagonal=0) -> Tensor
at::Tensor diag::call(const at::Tensor & self, int64_t diagonal) {
    static auto op = create_diag_typed_handle();
    return op.call(self, diagonal);
}

// aten::diag(Tensor self, int diagonal=0) -> Tensor
at::Tensor diag::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
    static auto op = create_diag_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_backward, name, "aten::diag_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(diag_backward, schema_str, "diag_backward(Tensor grad, int[] input_sizes, int diagonal) -> Tensor")

// aten::diag_backward(Tensor grad, int[] input_sizes, int diagonal) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<diag_backward::schema> create_diag_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(diag_backward::name, diag_backward::overload_name)
      .typed<diag_backward::schema>();
}

// aten::diag_backward(Tensor grad, int[] input_sizes, int diagonal) -> Tensor
at::Tensor diag_backward::call(const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t diagonal) {
    static auto op = create_diag_backward_typed_handle();
    return op.call(grad, input_sizes, diagonal);
}

// aten::diag_backward(Tensor grad, int[] input_sizes, int diagonal) -> Tensor
at::Tensor diag_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef input_sizes, int64_t diagonal) {
    static auto op = create_diag_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input_sizes, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_out, name, "aten::cross")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_out, schema_str, "cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cross_out::schema> create_cross_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cross_out::name, cross_out::overload_name)
      .typed<cross_out::schema>();
}

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cross_out::call(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
    static auto op = create_cross_out_typed_handle();
    return op.call(self, other, dim, out);
}

// aten::cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cross_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim, at::Tensor & out) {
    static auto op = create_cross_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross, name, "aten::cross")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross, schema_str, "cross(Tensor self, Tensor other, int? dim=None) -> Tensor")

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cross::schema> create_cross_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cross::name, cross::overload_name)
      .typed<cross::schema>();
}

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
at::Tensor cross::call(const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
    static auto op = create_cross_typed_handle();
    return op.call(self, other, dim);
}

// aten::cross(Tensor self, Tensor other, int? dim=None) -> Tensor
at::Tensor cross::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<int64_t> dim) {
    static auto op = create_cross_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_out, name, "aten::triu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_out, schema_str, "triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<triu_out::schema> create_triu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu_out::name, triu_out::overload_name)
      .typed<triu_out::schema>();
}

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & triu_out::call(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    static auto op = create_triu_out_typed_handle();
    return op.call(self, diagonal, out);
}

// aten::triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & triu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    static auto op = create_triu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu, name, "aten::triu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu, schema_str, "triu(Tensor self, int diagonal=0) -> Tensor")

// aten::triu(Tensor self, int diagonal=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<triu::schema> create_triu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu::name, triu::overload_name)
      .typed<triu::schema>();
}

// aten::triu(Tensor self, int diagonal=0) -> Tensor
at::Tensor triu::call(const at::Tensor & self, int64_t diagonal) {
    static auto op = create_triu_typed_handle();
    return op.call(self, diagonal);
}

// aten::triu(Tensor self, int diagonal=0) -> Tensor
at::Tensor triu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
    static auto op = create_triu_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_out, name, "aten::tril")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_out, schema_str, "tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tril_out::schema> create_tril_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tril_out::name, tril_out::overload_name)
      .typed<tril_out::schema>();
}

// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tril_out::call(const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    static auto op = create_tril_out_typed_handle();
    return op.call(self, diagonal, out);
}

// aten::tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & tril_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal, at::Tensor & out) {
    static auto op = create_tril_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril, name, "aten::tril")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril, schema_str, "tril(Tensor self, int diagonal=0) -> Tensor")

// aten::tril(Tensor self, int diagonal=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tril::schema> create_tril_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tril::name, tril::overload_name)
      .typed<tril::schema>();
}

// aten::tril(Tensor self, int diagonal=0) -> Tensor
at::Tensor tril::call(const at::Tensor & self, int64_t diagonal) {
    static auto op = create_tril_typed_handle();
    return op.call(self, diagonal);
}

// aten::tril(Tensor self, int diagonal=0) -> Tensor
at::Tensor tril::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t diagonal) {
    static auto op = create_tril_typed_handle();
    return op.redispatch(dispatchKeySet, self, diagonal);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_indices, name, "aten::tril_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tril_indices, schema_str, "tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tril_indices::schema> create_tril_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tril_indices::name, tril_indices::overload_name)
      .typed<tril_indices::schema>();
}

// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor tril_indices::call(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_tril_indices_typed_handle();
    return op.call(row, col, offset, dtype, layout, device, pin_memory);
}

// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor tril_indices::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_tril_indices_typed_handle();
    return op.redispatch(dispatchKeySet, row, col, offset, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_indices, name, "aten::triu_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triu_indices, schema_str, "triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<triu_indices::schema> create_triu_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triu_indices::name, triu_indices::overload_name)
      .typed<triu_indices::schema>();
}

// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor triu_indices::call(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_triu_indices_typed_handle();
    return op.call(row, col, offset, dtype, layout, device, pin_memory);
}

// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor triu_indices::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_triu_indices_typed_handle();
    return op.redispatch(dispatchKeySet, row, col, offset, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trace, name, "aten::trace")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trace, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trace, schema_str, "trace(Tensor self) -> Tensor")

// aten::trace(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trace::schema> create_trace_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trace::name, trace::overload_name)
      .typed<trace::schema>();
}

// aten::trace(Tensor self) -> Tensor
at::Tensor trace::call(const at::Tensor & self) {
    static auto op = create_trace_typed_handle();
    return op.call(self);
}

// aten::trace(Tensor self) -> Tensor
at::Tensor trace::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_trace_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trace_backward, name, "aten::trace_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trace_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(trace_backward, schema_str, "trace_backward(Tensor grad, int[] sizes) -> Tensor")

// aten::trace_backward(Tensor grad, int[] sizes) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<trace_backward::schema> create_trace_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(trace_backward::name, trace_backward::overload_name)
      .typed<trace_backward::schema>();
}

// aten::trace_backward(Tensor grad, int[] sizes) -> Tensor
at::Tensor trace_backward::call(const at::Tensor & grad, at::IntArrayRef sizes) {
    static auto op = create_trace_backward_typed_handle();
    return op.call(grad, sizes);
}

// aten::trace_backward(Tensor grad, int[] sizes) -> Tensor
at::Tensor trace_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef sizes) {
    static auto op = create_trace_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, sizes);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar_out, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar_out, schema_str, "ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne_Scalar_out::schema> create_ne_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Scalar_out::name, ne_Scalar_out::overload_name)
      .typed<ne_Scalar_out::schema>();
}

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_ne_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_ne_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Scalar, schema_str, "ne.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ne_Scalar::schema> create_ne_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Scalar::name, ne_Scalar::overload_name)
      .typed<ne_Scalar::schema>();
}

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ne_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ne_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ne.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ne_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ne_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor_out, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor_out, schema_str, "ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne_Tensor_out::schema> create_ne_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Tensor_out::name, ne_Tensor_out::overload_name)
      .typed<ne_Tensor_out::schema>();
}

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_ne_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ne_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_ne_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor, name, "aten::ne")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne_Tensor, schema_str, "ne.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ne_Tensor::schema> create_ne_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne_Tensor::name, ne_Tensor::overload_name)
      .typed<ne_Tensor::schema>();
}

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ne_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ne_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ne.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ne_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ne_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Scalar, name, "aten::ne_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Scalar, schema_str, "ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne__Scalar::schema> create_ne__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne__Scalar::name, ne__Scalar::overload_name)
      .typed<ne__Scalar::schema>();
}

// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ne__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ne__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ne__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ne__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Tensor, name, "aten::ne_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ne__Tensor, schema_str, "ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ne__Tensor::schema> create_ne__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ne__Tensor::name, ne__Tensor::overload_name)
      .typed<ne__Tensor::schema>();
}

// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ne__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ne__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ne__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ne__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar_out, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar_out, schema_str, "not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Scalar_out::schema> create_not_equal_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Scalar_out::name, not_equal_Scalar_out::overload_name)
      .typed<not_equal_Scalar_out::schema>();
}

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_not_equal_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_not_equal_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Scalar, schema_str, "not_equal.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Scalar::schema> create_not_equal_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Scalar::name, not_equal_Scalar::overload_name)
      .typed<not_equal_Scalar::schema>();
}

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor not_equal_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_not_equal_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::not_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor not_equal_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_not_equal_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor_out, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor_out, schema_str, "not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Tensor_out::schema> create_not_equal_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Tensor_out::name, not_equal_Tensor_out::overload_name)
      .typed<not_equal_Tensor_out::schema>();
}

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_not_equal_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & not_equal_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_not_equal_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor, name, "aten::not_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal_Tensor, schema_str, "not_equal.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<not_equal_Tensor::schema> create_not_equal_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal_Tensor::name, not_equal_Tensor::overload_name)
      .typed<not_equal_Tensor::schema>();
}

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor not_equal_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_not_equal_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::not_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor not_equal_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_not_equal_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Scalar, name, "aten::not_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Scalar, schema_str, "not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal__Scalar::schema> create_not_equal__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal__Scalar::name, not_equal__Scalar::overload_name)
      .typed<not_equal__Scalar::schema>();
}

// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & not_equal__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_not_equal__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & not_equal__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_not_equal__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Tensor, name, "aten::not_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(not_equal__Tensor, schema_str, "not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<not_equal__Tensor::schema> create_not_equal__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(not_equal__Tensor::name, not_equal__Tensor::overload_name)
      .typed<not_equal__Tensor::schema>();
}

// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & not_equal__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_not_equal__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & not_equal__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_not_equal__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Scalar_out, name, "aten::eq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Scalar_out, schema_str, "eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<eq_Scalar_out::schema> create_eq_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eq_Scalar_out::name, eq_Scalar_out::overload_name)
      .typed<eq_Scalar_out::schema>();
}

// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eq_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_eq_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eq_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_eq_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Scalar, name, "aten::eq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Scalar, schema_str, "eq.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<eq_Scalar::schema> create_eq_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eq_Scalar::name, eq_Scalar::overload_name)
      .typed<eq_Scalar::schema>();
}

// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor eq_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_eq_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::eq.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor eq_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_eq_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Tensor_out, name, "aten::eq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Tensor_out, schema_str, "eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<eq_Tensor_out::schema> create_eq_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eq_Tensor_out::name, eq_Tensor_out::overload_name)
      .typed<eq_Tensor_out::schema>();
}

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eq_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_eq_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & eq_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_eq_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Tensor, name, "aten::eq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eq_Tensor, schema_str, "eq.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<eq_Tensor::schema> create_eq_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eq_Tensor::name, eq_Tensor::overload_name)
      .typed<eq_Tensor::schema>();
}

// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor eq_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_eq_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::eq.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor eq_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_eq_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar_out, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar_out, schema_str, "ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge_Scalar_out::schema> create_ge_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Scalar_out::name, ge_Scalar_out::overload_name)
      .typed<ge_Scalar_out::schema>();
}

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_ge_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_ge_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Scalar, schema_str, "ge.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ge_Scalar::schema> create_ge_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Scalar::name, ge_Scalar::overload_name)
      .typed<ge_Scalar::schema>();
}

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ge_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ge_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ge.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor ge_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ge_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor_out, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor_out, schema_str, "ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge_Tensor_out::schema> create_ge_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Tensor_out::name, ge_Tensor_out::overload_name)
      .typed<ge_Tensor_out::schema>();
}

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_ge_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ge_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_ge_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor, name, "aten::ge")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge_Tensor, schema_str, "ge.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ge_Tensor::schema> create_ge_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge_Tensor::name, ge_Tensor::overload_name)
      .typed<ge_Tensor::schema>();
}

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ge_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ge_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ge.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor ge_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ge_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Scalar, name, "aten::ge_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Scalar, schema_str, "ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge__Scalar::schema> create_ge__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge__Scalar::name, ge__Scalar::overload_name)
      .typed<ge__Scalar::schema>();
}

// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ge__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ge__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & ge__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_ge__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Tensor, name, "aten::ge_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ge__Tensor, schema_str, "ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ge__Tensor::schema> create_ge__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ge__Tensor::name, ge__Tensor::overload_name)
      .typed<ge__Tensor::schema>();
}

// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ge__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ge__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & ge__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_ge__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Scalar_out, name, "aten::greater_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Scalar_out, schema_str, "greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_equal_Scalar_out::schema> create_greater_equal_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_equal_Scalar_out::name, greater_equal_Scalar_out::overload_name)
      .typed<greater_equal_Scalar_out::schema>();
}

// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_equal_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_greater_equal_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_equal_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_greater_equal_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Scalar, name, "aten::greater_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Scalar, schema_str, "greater_equal.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<greater_equal_Scalar::schema> create_greater_equal_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_equal_Scalar::name, greater_equal_Scalar::overload_name)
      .typed<greater_equal_Scalar::schema>();
}

// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor greater_equal_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater_equal_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::greater_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor greater_equal_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater_equal_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Tensor_out, name, "aten::greater_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Tensor_out, schema_str, "greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_equal_Tensor_out::schema> create_greater_equal_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_equal_Tensor_out::name, greater_equal_Tensor_out::overload_name)
      .typed<greater_equal_Tensor_out::schema>();
}

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_equal_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_greater_equal_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_equal_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_greater_equal_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Tensor, name, "aten::greater_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal_Tensor, schema_str, "greater_equal.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<greater_equal_Tensor::schema> create_greater_equal_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_equal_Tensor::name, greater_equal_Tensor::overload_name)
      .typed<greater_equal_Tensor::schema>();
}

// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor greater_equal_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater_equal_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::greater_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor greater_equal_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater_equal_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal__Scalar, name, "aten::greater_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal__Scalar, schema_str, "greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_equal__Scalar::schema> create_greater_equal__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_equal__Scalar::name, greater_equal__Scalar::overload_name)
      .typed<greater_equal__Scalar::schema>();
}

// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & greater_equal__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater_equal__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & greater_equal__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater_equal__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal__Tensor, name, "aten::greater_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_equal__Tensor, schema_str, "greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_equal__Tensor::schema> create_greater_equal__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_equal__Tensor::name, greater_equal__Tensor::overload_name)
      .typed<greater_equal__Tensor::schema>();
}

// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & greater_equal__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater_equal__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & greater_equal__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater_equal__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Scalar_out, name, "aten::le")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Scalar_out, schema_str, "le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<le_Scalar_out::schema> create_le_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(le_Scalar_out::name, le_Scalar_out::overload_name)
      .typed<le_Scalar_out::schema>();
}

// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & le_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_le_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & le_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_le_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Scalar, name, "aten::le")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Scalar, schema_str, "le.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::le.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<le_Scalar::schema> create_le_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(le_Scalar::name, le_Scalar::overload_name)
      .typed<le_Scalar::schema>();
}

// aten::le.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor le_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_le_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::le.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor le_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_le_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Tensor_out, name, "aten::le")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Tensor_out, schema_str, "le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<le_Tensor_out::schema> create_le_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(le_Tensor_out::name, le_Tensor_out::overload_name)
      .typed<le_Tensor_out::schema>();
}

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & le_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_le_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & le_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_le_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Tensor, name, "aten::le")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le_Tensor, schema_str, "le.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::le.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<le_Tensor::schema> create_le_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(le_Tensor::name, le_Tensor::overload_name)
      .typed<le_Tensor::schema>();
}

// aten::le.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor le_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_le_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::le.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor le_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_le_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le__Scalar, name, "aten::le_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le__Scalar, schema_str, "le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<le__Scalar::schema> create_le__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(le__Scalar::name, le__Scalar::overload_name)
      .typed<le__Scalar::schema>();
}

// aten::le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & le__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_le__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & le__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_le__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le__Tensor, name, "aten::le_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(le__Tensor, schema_str, "le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<le__Tensor::schema> create_le__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(le__Tensor::name, le__Tensor::overload_name)
      .typed<le__Tensor::schema>();
}

// aten::le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & le__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_le__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & le__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_le__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Scalar_out, name, "aten::less_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Scalar_out, schema_str, "less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less_equal_Scalar_out::schema> create_less_equal_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_equal_Scalar_out::name, less_equal_Scalar_out::overload_name)
      .typed<less_equal_Scalar_out::schema>();
}

// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_equal_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_less_equal_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_equal_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_less_equal_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Scalar, name, "aten::less_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Scalar, schema_str, "less_equal.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<less_equal_Scalar::schema> create_less_equal_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_equal_Scalar::name, less_equal_Scalar::overload_name)
      .typed<less_equal_Scalar::schema>();
}

// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor less_equal_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less_equal_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::less_equal.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor less_equal_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less_equal_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Tensor_out, name, "aten::less_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Tensor_out, schema_str, "less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less_equal_Tensor_out::schema> create_less_equal_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_equal_Tensor_out::name, less_equal_Tensor_out::overload_name)
      .typed<less_equal_Tensor_out::schema>();
}

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_equal_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_less_equal_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_equal_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_less_equal_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Tensor, name, "aten::less_equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal_Tensor, schema_str, "less_equal.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<less_equal_Tensor::schema> create_less_equal_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_equal_Tensor::name, less_equal_Tensor::overload_name)
      .typed<less_equal_Tensor::schema>();
}

// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor less_equal_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less_equal_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::less_equal.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor less_equal_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less_equal_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal__Scalar, name, "aten::less_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal__Scalar, schema_str, "less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less_equal__Scalar::schema> create_less_equal__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_equal__Scalar::name, less_equal__Scalar::overload_name)
      .typed<less_equal__Scalar::schema>();
}

// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & less_equal__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less_equal__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & less_equal__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less_equal__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal__Tensor, name, "aten::less_equal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_equal__Tensor, schema_str, "less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less_equal__Tensor::schema> create_less_equal__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_equal__Tensor::name, less_equal__Tensor::overload_name)
      .typed<less_equal__Tensor::schema>();
}

// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & less_equal__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less_equal__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & less_equal__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less_equal__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Scalar_out, name, "aten::gt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Scalar_out, schema_str, "gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gt_Scalar_out::schema> create_gt_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gt_Scalar_out::name, gt_Scalar_out::overload_name)
      .typed<gt_Scalar_out::schema>();
}

// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gt_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_gt_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gt_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_gt_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Scalar, name, "aten::gt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Scalar, schema_str, "gt.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gt_Scalar::schema> create_gt_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gt_Scalar::name, gt_Scalar::overload_name)
      .typed<gt_Scalar::schema>();
}

// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor gt_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_gt_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::gt.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor gt_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_gt_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Tensor_out, name, "aten::gt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Tensor_out, schema_str, "gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gt_Tensor_out::schema> create_gt_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gt_Tensor_out::name, gt_Tensor_out::overload_name)
      .typed<gt_Tensor_out::schema>();
}

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gt_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_gt_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gt_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_gt_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Tensor, name, "aten::gt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt_Tensor, schema_str, "gt.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gt_Tensor::schema> create_gt_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gt_Tensor::name, gt_Tensor::overload_name)
      .typed<gt_Tensor::schema>();
}

// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor gt_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gt_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::gt.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor gt_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gt_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt__Scalar, name, "aten::gt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt__Scalar, schema_str, "gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gt__Scalar::schema> create_gt__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gt__Scalar::name, gt__Scalar::overload_name)
      .typed<gt__Scalar::schema>();
}

// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & gt__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_gt__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & gt__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_gt__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt__Tensor, name, "aten::gt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gt__Tensor, schema_str, "gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gt__Tensor::schema> create_gt__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gt__Tensor::name, gt__Tensor::overload_name)
      .typed<gt__Tensor::schema>();
}

// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & gt__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gt__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & gt__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_gt__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar_out, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar_out, schema_str, "greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_Scalar_out::schema> create_greater_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Scalar_out::name, greater_Scalar_out::overload_name)
      .typed<greater_Scalar_out::schema>();
}

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_greater_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_greater_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Scalar, schema_str, "greater.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<greater_Scalar::schema> create_greater_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Scalar::name, greater_Scalar::overload_name)
      .typed<greater_Scalar::schema>();
}

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor greater_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::greater.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor greater_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor_out, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor_out, schema_str, "greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater_Tensor_out::schema> create_greater_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Tensor_out::name, greater_Tensor_out::overload_name)
      .typed<greater_Tensor_out::schema>();
}

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_greater_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & greater_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_greater_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor, name, "aten::greater")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater_Tensor, schema_str, "greater.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<greater_Tensor::schema> create_greater_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater_Tensor::name, greater_Tensor::overload_name)
      .typed<greater_Tensor::schema>();
}

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor greater_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::greater.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor greater_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Scalar, name, "aten::greater_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Scalar, schema_str, "greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater__Scalar::schema> create_greater__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater__Scalar::name, greater__Scalar::overload_name)
      .typed<greater__Scalar::schema>();
}

// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & greater__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & greater__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_greater__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Tensor, name, "aten::greater_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(greater__Tensor, schema_str, "greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<greater__Tensor::schema> create_greater__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(greater__Tensor::name, greater__Tensor::overload_name)
      .typed<greater__Tensor::schema>();
}

// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & greater__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & greater__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_greater__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Scalar_out, name, "aten::lt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Scalar_out, schema_str, "lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lt_Scalar_out::schema> create_lt_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lt_Scalar_out::name, lt_Scalar_out::overload_name)
      .typed<lt_Scalar_out::schema>();
}

// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lt_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_lt_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lt_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_lt_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Scalar, name, "aten::lt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Scalar, schema_str, "lt.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lt_Scalar::schema> create_lt_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lt_Scalar::name, lt_Scalar::overload_name)
      .typed<lt_Scalar::schema>();
}

// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor lt_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_lt_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::lt.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor lt_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_lt_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Tensor_out, name, "aten::lt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Tensor_out, schema_str, "lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lt_Tensor_out::schema> create_lt_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lt_Tensor_out::name, lt_Tensor_out::overload_name)
      .typed<lt_Tensor_out::schema>();
}

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lt_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_lt_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lt_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_lt_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Tensor, name, "aten::lt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt_Tensor, schema_str, "lt.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lt_Tensor::schema> create_lt_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lt_Tensor::name, lt_Tensor::overload_name)
      .typed<lt_Tensor::schema>();
}

// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor lt_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lt_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::lt.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor lt_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lt_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt__Scalar, name, "aten::lt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt__Scalar, schema_str, "lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lt__Scalar::schema> create_lt__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lt__Scalar::name, lt__Scalar::overload_name)
      .typed<lt__Scalar::schema>();
}

// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & lt__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_lt__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & lt__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_lt__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt__Tensor, name, "aten::lt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lt__Tensor, schema_str, "lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lt__Tensor::schema> create_lt__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lt__Tensor::name, lt__Tensor::overload_name)
      .typed<lt__Tensor::schema>();
}

// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & lt__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lt__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & lt__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_lt__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Scalar_out, name, "aten::less")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Scalar_out, schema_str, "less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less_Scalar_out::schema> create_less_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_Scalar_out::name, less_Scalar_out::overload_name)
      .typed<less_Scalar_out::schema>();
}

// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_less_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_less_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Scalar, name, "aten::less")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Scalar, schema_str, "less.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::less.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<less_Scalar::schema> create_less_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_Scalar::name, less_Scalar::overload_name)
      .typed<less_Scalar::schema>();
}

// aten::less.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor less_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::less.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor less_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Tensor_out, name, "aten::less")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Tensor_out, schema_str, "less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less_Tensor_out::schema> create_less_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_Tensor_out::name, less_Tensor_out::overload_name)
      .typed<less_Tensor_out::schema>();
}

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_less_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & less_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_less_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Tensor, name, "aten::less")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less_Tensor, schema_str, "less.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::less.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<less_Tensor::schema> create_less_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less_Tensor::name, less_Tensor::overload_name)
      .typed<less_Tensor::schema>();
}

// aten::less.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor less_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::less.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor less_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less__Scalar, name, "aten::less_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less__Scalar, schema_str, "less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less__Scalar::schema> create_less__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less__Scalar::name, less__Scalar::overload_name)
      .typed<less__Scalar::schema>();
}

// aten::less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & less__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::less_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & less__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_less__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less__Tensor, name, "aten::less_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(less__Tensor, schema_str, "less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<less__Tensor::schema> create_less__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(less__Tensor::name, less__Tensor::overload_name)
      .typed<less__Tensor::schema>();
}

// aten::less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & less__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::less_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & less__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_less__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_out, name, "aten::take")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_out, schema_str, "take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)")

// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<take_out::schema> create_take_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(take_out::name, take_out::overload_name)
      .typed<take_out::schema>();
}

// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & take_out::call(const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
    static auto op = create_take_out_typed_handle();
    return op.call(self, index, out);
}

// aten::take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & take_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index, at::Tensor & out) {
    static auto op = create_take_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take, name, "aten::take")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take, schema_str, "take(Tensor self, Tensor index) -> Tensor")

// aten::take(Tensor self, Tensor index) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<take::schema> create_take_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(take::name, take::overload_name)
      .typed<take::schema>();
}

// aten::take(Tensor self, Tensor index) -> Tensor
at::Tensor take::call(const at::Tensor & self, const at::Tensor & index) {
    static auto op = create_take_typed_handle();
    return op.call(self, index);
}

// aten::take(Tensor self, Tensor index) -> Tensor
at::Tensor take::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & index) {
    static auto op = create_take_typed_handle();
    return op.redispatch(dispatchKeySet, self, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_along_dim_out, name, "aten::take_along_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_along_dim_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_along_dim_out, schema_str, "take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<take_along_dim_out::schema> create_take_along_dim_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(take_along_dim_out::name, take_along_dim_out::overload_name)
      .typed<take_along_dim_out::schema>();
}

// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & take_along_dim_out::call(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
    static auto op = create_take_along_dim_out_typed_handle();
    return op.call(self, indices, dim, out);
}

// aten::take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & take_along_dim_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim, at::Tensor & out) {
    static auto op = create_take_along_dim_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_along_dim, name, "aten::take_along_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_along_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(take_along_dim, schema_str, "take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor")

// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<take_along_dim::schema> create_take_along_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(take_along_dim::name, take_along_dim::overload_name)
      .typed<take_along_dim::schema>();
}

// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor
at::Tensor take_along_dim::call(const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
    static auto op = create_take_along_dim_typed_handle();
    return op.call(self, indices, dim);
}

// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -> Tensor
at::Tensor take_along_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, c10::optional<int64_t> dim) {
    static auto op = create_take_along_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_out, name, "aten::index_select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_out, schema_str, "index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)")

// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_select_out::schema> create_index_select_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_select_out::name, index_select_out::overload_name)
      .typed<index_select_out::schema>();
}

// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_select_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
    static auto op = create_index_select_out_typed_handle();
    return op.call(self, dim, index, out);
}

// aten::index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_select_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, at::Tensor & out) {
    static auto op = create_index_select_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select, name, "aten::index_select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select, schema_str, "index_select(Tensor self, int dim, Tensor index) -> Tensor")

// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_select::schema> create_index_select_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_select::name, index_select::overload_name)
      .typed<index_select::schema>();
}

// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor
at::Tensor index_select::call(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
    static auto op = create_index_select_typed_handle();
    return op.call(self, dim, index);
}

// aten::index_select(Tensor self, int dim, Tensor index) -> Tensor
at::Tensor index_select::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index) {
    static auto op = create_index_select_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_dimname_out, name, "aten::index_select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_dimname_out, schema_str, "index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)")

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<index_select_dimname_out::schema> create_index_select_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_select_dimname_out::name, index_select_dimname_out::overload_name)
      .typed<index_select_dimname_out::schema>();
}

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_select_dimname_out::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
    static auto op = create_index_select_dimname_out_typed_handle();
    return op.call(self, dim, index, out);
}

// aten::index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & index_select_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, at::Tensor & out) {
    static auto op = create_index_select_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_dimname, name, "aten::index_select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_dimname, schema_str, "index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor")

// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_select_dimname::schema> create_index_select_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_select_dimname::name, index_select_dimname::overload_name)
      .typed<index_select_dimname::schema>();
}

// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
at::Tensor index_select_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
    static auto op = create_index_select_dimname_typed_handle();
    return op.call(self, dim, index);
}

// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
at::Tensor index_select_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index) {
    static auto op = create_index_select_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_backward, name, "aten::index_select_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(index_select_backward, schema_str, "index_select_backward(Tensor grad, int[] self_sizes, int dim, Tensor index) -> Tensor")

// aten::index_select_backward(Tensor grad, int[] self_sizes, int dim, Tensor index) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<index_select_backward::schema> create_index_select_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(index_select_backward::name, index_select_backward::overload_name)
      .typed<index_select_backward::schema>();
}

// aten::index_select_backward(Tensor grad, int[] self_sizes, int dim, Tensor index) -> Tensor
at::Tensor index_select_backward::call(const at::Tensor & grad, at::IntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
    static auto op = create_index_select_backward_typed_handle();
    return op.call(grad, self_sizes, dim, index);
}

// aten::index_select_backward(Tensor grad, int[] self_sizes, int dim, Tensor index) -> Tensor
at::Tensor index_select_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, at::IntArrayRef self_sizes, int64_t dim, const at::Tensor & index) {
    static auto op = create_index_select_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self_sizes, dim, index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select_out, name, "aten::masked_select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select_out, schema_str, "masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)")

// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<masked_select_out::schema> create_masked_select_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_select_out::name, masked_select_out::overload_name)
      .typed<masked_select_out::schema>();
}

// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & masked_select_out::call(const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
    static auto op = create_masked_select_out_typed_handle();
    return op.call(self, mask, out);
}

// aten::masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & masked_select_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask, at::Tensor & out) {
    static auto op = create_masked_select_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select, name, "aten::masked_select")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select, schema_str, "masked_select(Tensor self, Tensor mask) -> Tensor")

// aten::masked_select(Tensor self, Tensor mask) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<masked_select::schema> create_masked_select_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_select::name, masked_select::overload_name)
      .typed<masked_select::schema>();
}

// aten::masked_select(Tensor self, Tensor mask) -> Tensor
at::Tensor masked_select::call(const at::Tensor & self, const at::Tensor & mask) {
    static auto op = create_masked_select_typed_handle();
    return op.call(self, mask);
}

// aten::masked_select(Tensor self, Tensor mask) -> Tensor
at::Tensor masked_select::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & mask) {
    static auto op = create_masked_select_typed_handle();
    return op.redispatch(dispatchKeySet, self, mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select_backward, name, "aten::masked_select_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(masked_select_backward, schema_str, "masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor")

// aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<masked_select_backward::schema> create_masked_select_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(masked_select_backward::name, masked_select_backward::overload_name)
      .typed<masked_select_backward::schema>();
}

// aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor
at::Tensor masked_select_backward::call(const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
    static auto op = create_masked_select_backward_typed_handle();
    return op.call(grad, input, mask);
}

// aten::masked_select_backward(Tensor grad, Tensor input, Tensor mask) -> Tensor
at::Tensor masked_select_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & input, const at::Tensor & mask) {
    static auto op = create_masked_select_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, input, mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero_out, name, "aten::nonzero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero_out, schema_str, "nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nonzero_out::schema> create_nonzero_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nonzero_out::name, nonzero_out::overload_name)
      .typed<nonzero_out::schema>();
}

// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nonzero_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_nonzero_out_typed_handle();
    return op.call(self, out);
}

// aten::nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nonzero_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_nonzero_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero, name, "aten::nonzero")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero, schema_str, "nonzero(Tensor self) -> Tensor")

// aten::nonzero(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nonzero::schema> create_nonzero_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nonzero::name, nonzero::overload_name)
      .typed<nonzero::schema>();
}

// aten::nonzero(Tensor self) -> Tensor
at::Tensor nonzero::call(const at::Tensor & self) {
    static auto op = create_nonzero_typed_handle();
    return op.call(self);
}

// aten::nonzero(Tensor self) -> Tensor
at::Tensor nonzero::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_nonzero_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero_numpy, name, "aten::nonzero_numpy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero_numpy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nonzero_numpy, schema_str, "nonzero_numpy(Tensor self) -> Tensor[]")

// aten::nonzero_numpy(Tensor self) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<nonzero_numpy::schema> create_nonzero_numpy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nonzero_numpy::name, nonzero_numpy::overload_name)
      .typed<nonzero_numpy::schema>();
}

// aten::nonzero_numpy(Tensor self) -> Tensor[]
::std::vector<at::Tensor> nonzero_numpy::call(const at::Tensor & self) {
    static auto op = create_nonzero_numpy_typed_handle();
    return op.call(self);
}

// aten::nonzero_numpy(Tensor self) -> Tensor[]
::std::vector<at::Tensor> nonzero_numpy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_nonzero_numpy_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_out, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_out, schema_str, "gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)")

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gather_out::schema> create_gather_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_out::name, gather_out::overload_name)
      .typed<gather_out::schema>();
}

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_out::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    static auto op = create_gather_out_typed_handle();
    return op.call(self, dim, index, sparse_grad, out);
}

// aten::gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    static auto op = create_gather_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather, schema_str, "gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor")

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gather::schema> create_gather_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather::name, gather::overload_name)
      .typed<gather::schema>();
}

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    static auto op = create_gather_typed_handle();
    return op.call(self, dim, index, sparse_grad);
}

// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    static auto op = create_gather_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_backward, name, "aten::gather_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_backward, schema_str, "gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor")

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gather_backward::schema> create_gather_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_backward::name, gather_backward::overload_name)
      .typed<gather_backward::schema>();
}

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
at::Tensor gather_backward::call(const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    static auto op = create_gather_backward_typed_handle();
    return op.call(grad, self, dim, index, sparse_grad);
}

// aten::gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -> Tensor
at::Tensor gather_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
    static auto op = create_gather_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, self, dim, index, sparse_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname_out, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname_out, overload_name, "dimname_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname_out, schema_str, "gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)")

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<gather_dimname_out::schema> create_gather_dimname_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_dimname_out::name, gather_dimname_out::overload_name)
      .typed<gather_dimname_out::schema>();
}

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_dimname_out::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    static auto op = create_gather_dimname_out_typed_handle();
    return op.call(self, dim, index, sparse_grad, out);
}

// aten::gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & gather_dimname_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad, at::Tensor & out) {
    static auto op = create_gather_dimname_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname, name, "aten::gather")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(gather_dimname, schema_str, "gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor")

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<gather_dimname::schema> create_gather_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(gather_dimname::name, gather_dimname::overload_name)
      .typed<gather_dimname::schema>();
}

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather_dimname::call(const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
    static auto op = create_gather_dimname_typed_handle();
    return op.call(self, dim, index, sparse_grad);
}

// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
at::Tensor gather_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, const at::Tensor & index, bool sparse_grad) {
    static auto op = create_gather_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, sparse_grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_gather_sparse_backward, name, "aten::_gather_sparse_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_gather_sparse_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_gather_sparse_backward, schema_str, "_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor")

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_gather_sparse_backward::schema> create__gather_sparse_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_gather_sparse_backward::name, _gather_sparse_backward::overload_name)
      .typed<_gather_sparse_backward::schema>();
}

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
at::Tensor _gather_sparse_backward::call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
    static auto op = create__gather_sparse_backward_typed_handle();
    return op.call(self, dim, index, grad);
}

// aten::_gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -> Tensor
at::Tensor _gather_sparse_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & grad) {
    static auto op = create__gather_sparse_backward_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, grad);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul_out, name, "aten::addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul_out, schema_str, "addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addcmul_out::schema> create_addcmul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addcmul_out::name, addcmul_out::overload_name)
      .typed<addcmul_out::schema>();
}

// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addcmul_out::call(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_addcmul_out_typed_handle();
    return op.call(self, tensor1, tensor2, value, out);
}

// aten::addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addcmul_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_addcmul_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul, name, "aten::addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul, schema_str, "addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor")

// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addcmul::schema> create_addcmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addcmul::name, addcmul::overload_name)
      .typed<addcmul::schema>();
}

// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
at::Tensor addcmul::call(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcmul_typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
at::Tensor addcmul::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcmul_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul_, name, "aten::addcmul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcmul_, schema_str, "addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)")

// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addcmul_::schema> create_addcmul__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addcmul_::name, addcmul_::overload_name)
      .typed<addcmul_::schema>();
}

// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
at::Tensor & addcmul_::call(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcmul__typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
at::Tensor & addcmul_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcmul__typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv_out, name, "aten::addcdiv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv_out, schema_str, "addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)")

// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addcdiv_out::schema> create_addcdiv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addcdiv_out::name, addcdiv_out::overload_name)
      .typed<addcdiv_out::schema>();
}

// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addcdiv_out::call(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_addcdiv_out_typed_handle();
    return op.call(self, tensor1, tensor2, value, out);
}

// aten::addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
at::Tensor & addcdiv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value, at::Tensor & out) {
    static auto op = create_addcdiv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv, name, "aten::addcdiv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv, schema_str, "addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor")

// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<addcdiv::schema> create_addcdiv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addcdiv::name, addcdiv::overload_name)
      .typed<addcdiv::schema>();
}

// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
at::Tensor addcdiv::call(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcdiv_typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
at::Tensor addcdiv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcdiv_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv_, name, "aten::addcdiv_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(addcdiv_, schema_str, "addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)")

// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<addcdiv_::schema> create_addcdiv__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(addcdiv_::name, addcdiv_::overload_name)
      .typed<addcdiv_::schema>();
}

// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
at::Tensor & addcdiv_::call(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcdiv__typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
at::Tensor & addcdiv_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
    static auto op = create_addcdiv__typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_entropy_loss, name, "aten::cross_entropy_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_entropy_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cross_entropy_loss, schema_str, "cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, float label_smoothing=0.0) -> Tensor")

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, float label_smoothing=0.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cross_entropy_loss::schema> create_cross_entropy_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cross_entropy_loss::name, cross_entropy_loss::overload_name)
      .typed<cross_entropy_loss::schema>();
}

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, float label_smoothing=0.0) -> Tensor
at::Tensor cross_entropy_loss::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, double label_smoothing) {
    static auto op = create_cross_entropy_loss_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, label_smoothing);
}

// aten::cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, float label_smoothing=0.0) -> Tensor
at::Tensor cross_entropy_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, double label_smoothing) {
    static auto op = create_cross_entropy_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, label_smoothing);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstsq_X, name, "aten::lstsq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstsq_X, overload_name, "X")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstsq_X, schema_str, "lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)")

// aten::lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)
static C10_NOINLINE c10::TypedOperatorHandle<lstsq_X::schema> create_lstsq_X_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstsq_X::name, lstsq_X::overload_name)
      .typed<lstsq_X::schema>();
}

// aten::lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)
::std::tuple<at::Tensor &,at::Tensor &> lstsq_X::call(const at::Tensor & self, const at::Tensor & A, at::Tensor & X, at::Tensor & qr) {
    static auto op = create_lstsq_X_typed_handle();
    return op.call(self, A, X, qr);
}

// aten::lstsq.X(Tensor self, Tensor A, *, Tensor(a!) X, Tensor(b!) qr) -> (Tensor(a!) solution, Tensor(b!) QR)
::std::tuple<at::Tensor &,at::Tensor &> lstsq_X::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, at::Tensor & X, at::Tensor & qr) {
    static auto op = create_lstsq_X_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, X, qr);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstsq, name, "aten::lstsq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstsq, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lstsq, schema_str, "lstsq(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)")

// aten::lstsq(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)
static C10_NOINLINE c10::TypedOperatorHandle<lstsq::schema> create_lstsq_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lstsq::name, lstsq::overload_name)
      .typed<lstsq::schema>();
}

// aten::lstsq(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)
::std::tuple<at::Tensor,at::Tensor> lstsq::call(const at::Tensor & self, const at::Tensor & A) {
    static auto op = create_lstsq_typed_handle();
    return op.call(self, A);
}

// aten::lstsq(Tensor self, Tensor A) -> (Tensor solution, Tensor QR)
::std::tuple<at::Tensor,at::Tensor> lstsq::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A) {
    static auto op = create_lstsq_typed_handle();
    return op.redispatch(dispatchKeySet, self, A);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve_X, name, "aten::triangular_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve_X, overload_name, "X")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve_X, schema_str, "triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)")

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
static C10_NOINLINE c10::TypedOperatorHandle<triangular_solve_X::schema> create_triangular_solve_X_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triangular_solve_X::name, triangular_solve_X::overload_name)
      .typed<triangular_solve_X::schema>();
}

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
::std::tuple<at::Tensor &,at::Tensor &> triangular_solve_X::call(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
    static auto op = create_triangular_solve_X_typed_handle();
    return op.call(self, A, upper, transpose, unitriangular, X, M);
}

// aten::triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
::std::tuple<at::Tensor &,at::Tensor &> triangular_solve_X::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular, at::Tensor & X, at::Tensor & M) {
    static auto op = create_triangular_solve_X_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, upper, transpose, unitriangular, X, M);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve, name, "aten::triangular_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(triangular_solve, schema_str, "triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)")

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
static C10_NOINLINE c10::TypedOperatorHandle<triangular_solve::schema> create_triangular_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(triangular_solve::name, triangular_solve::overload_name)
      .typed<triangular_solve::schema>();
}

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
::std::tuple<at::Tensor,at::Tensor> triangular_solve::call(const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
    static auto op = create_triangular_solve_typed_handle();
    return op.call(self, A, upper, transpose, unitriangular);
}

// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
::std::tuple<at::Tensor,at::Tensor> triangular_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper, bool transpose, bool unitriangular) {
    static auto op = create_triangular_solve_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, upper, transpose, unitriangular);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(symeig_e, name, "aten::symeig")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(symeig_e, overload_name, "e")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(symeig_e, schema_str, "symeig.e(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)")

// aten::symeig.e(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<symeig_e::schema> create_symeig_e_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(symeig_e::name, symeig_e::overload_name)
      .typed<symeig_e::schema>();
}

// aten::symeig.e(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> symeig_e::call(const at::Tensor & self, bool eigenvectors, bool upper, at::Tensor & e, at::Tensor & V) {
    static auto op = create_symeig_e_typed_handle();
    return op.call(self, eigenvectors, upper, e, V);
}

// aten::symeig.e(Tensor self, bool eigenvectors=False, bool upper=True, *, Tensor(a!) e, Tensor(b!) V) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> symeig_e::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, bool upper, at::Tensor & e, at::Tensor & V) {
    static auto op = create_symeig_e_typed_handle();
    return op.redispatch(dispatchKeySet, self, eigenvectors, upper, e, V);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(symeig, name, "aten::symeig")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(symeig, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(symeig, schema_str, "symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)")

// aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<symeig::schema> create_symeig_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(symeig::name, symeig::overload_name)
      .typed<symeig::schema>();
}

// aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> symeig::call(const at::Tensor & self, bool eigenvectors, bool upper) {
    static auto op = create_symeig_typed_handle();
    return op.call(self, eigenvectors, upper);
}

// aten::symeig(Tensor self, bool eigenvectors=False, bool upper=True) -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> symeig::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, bool upper) {
    static auto op = create_symeig_typed_handle();
    return op.redispatch(dispatchKeySet, self, eigenvectors, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_symeig_helper, name, "aten::_symeig_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_symeig_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_symeig_helper, schema_str, "_symeig_helper(Tensor self, bool eigenvectors, bool upper) -> (Tensor, Tensor)")

// aten::_symeig_helper(Tensor self, bool eigenvectors, bool upper) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_symeig_helper::schema> create__symeig_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_symeig_helper::name, _symeig_helper::overload_name)
      .typed<_symeig_helper::schema>();
}

// aten::_symeig_helper(Tensor self, bool eigenvectors, bool upper) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _symeig_helper::call(const at::Tensor & self, bool eigenvectors, bool upper) {
    static auto op = create__symeig_helper_typed_handle();
    return op.call(self, eigenvectors, upper);
}

// aten::_symeig_helper(Tensor self, bool eigenvectors, bool upper) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _symeig_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, bool upper) {
    static auto op = create__symeig_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, eigenvectors, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eig_e, name, "aten::eig")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eig_e, overload_name, "e")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eig_e, schema_str, "eig.e(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)")

// aten::eig.e(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<eig_e::schema> create_eig_e_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eig_e::name, eig_e::overload_name)
      .typed<eig_e::schema>();
}

// aten::eig.e(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> eig_e::call(const at::Tensor & self, bool eigenvectors, at::Tensor & e, at::Tensor & v) {
    static auto op = create_eig_e_typed_handle();
    return op.call(self, eigenvectors, e, v);
}

// aten::eig.e(Tensor self, bool eigenvectors=False, *, Tensor(a!) e, Tensor(b!) v) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> eig_e::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors, at::Tensor & e, at::Tensor & v) {
    static auto op = create_eig_e_typed_handle();
    return op.redispatch(dispatchKeySet, self, eigenvectors, e, v);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eig, name, "aten::eig")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eig, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(eig, schema_str, "eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)")

// aten::eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<eig::schema> create_eig_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(eig::name, eig::overload_name)
      .typed<eig::schema>();
}

// aten::eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> eig::call(const at::Tensor & self, bool eigenvectors) {
    static auto op = create_eig_typed_handle();
    return op.call(self, eigenvectors);
}

// aten::eig(Tensor self, bool eigenvectors=False) -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> eig::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool eigenvectors) {
    static auto op = create_eig_typed_handle();
    return op.redispatch(dispatchKeySet, self, eigenvectors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(svd_U, name, "aten::svd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(svd_U, overload_name, "U")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(svd_U, schema_str, "svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)")

// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
static C10_NOINLINE c10::TypedOperatorHandle<svd_U::schema> create_svd_U_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(svd_U::name, svd_U::overload_name)
      .typed<svd_U::schema>();
}

// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_U::call(const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
    static auto op = create_svd_U_typed_handle();
    return op.call(self, some, compute_uv, U, S, V);
}

// aten::svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> svd_U::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv, at::Tensor & U, at::Tensor & S, at::Tensor & V) {
    static auto op = create_svd_U_typed_handle();
    return op.redispatch(dispatchKeySet, self, some, compute_uv, U, S, V);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(svd, name, "aten::svd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(svd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(svd, schema_str, "svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)")

// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)
static C10_NOINLINE c10::TypedOperatorHandle<svd::schema> create_svd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(svd::name, svd::overload_name)
      .typed<svd::schema>();
}

// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> svd::call(const at::Tensor & self, bool some, bool compute_uv) {
    static auto op = create_svd_typed_handle();
    return op.call(self, some, compute_uv);
}

// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor V)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> svd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv) {
    static auto op = create_svd_typed_handle();
    return op.redispatch(dispatchKeySet, self, some, compute_uv);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_svd_helper, name, "aten::_svd_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_svd_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_svd_helper, schema_str, "_svd_helper(Tensor self, bool some, bool compute_uv) -> (Tensor U, Tensor S, Tensor V)")

// aten::_svd_helper(Tensor self, bool some, bool compute_uv) -> (Tensor U, Tensor S, Tensor V)
static C10_NOINLINE c10::TypedOperatorHandle<_svd_helper::schema> create__svd_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_svd_helper::name, _svd_helper::overload_name)
      .typed<_svd_helper::schema>();
}

// aten::_svd_helper(Tensor self, bool some, bool compute_uv) -> (Tensor U, Tensor S, Tensor V)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _svd_helper::call(const at::Tensor & self, bool some, bool compute_uv) {
    static auto op = create__svd_helper_typed_handle();
    return op.call(self, some, compute_uv);
}

// aten::_svd_helper(Tensor self, bool some, bool compute_uv) -> (Tensor U, Tensor S, Tensor V)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _svd_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, bool compute_uv) {
    static auto op = create__svd_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, some, compute_uv);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes, name, "aten::swapaxes")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes, schema_str, "swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)")

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<swapaxes::schema> create_swapaxes_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(swapaxes::name, swapaxes::overload_name)
      .typed<swapaxes::schema>();
}

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
at::Tensor swapaxes::call(const at::Tensor & self, int64_t axis0, int64_t axis1) {
    static auto op = create_swapaxes_typed_handle();
    return op.call(self, axis0, axis1);
}

// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -> Tensor(a)
at::Tensor swapaxes::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t axis0, int64_t axis1) {
    static auto op = create_swapaxes_typed_handle();
    return op.redispatch(dispatchKeySet, self, axis0, axis1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes_, name, "aten::swapaxes_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapaxes_, schema_str, "swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)")

// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<swapaxes_::schema> create_swapaxes__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(swapaxes_::name, swapaxes_::overload_name)
      .typed<swapaxes_::schema>();
}

// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)
at::Tensor & swapaxes_::call(at::Tensor & self, int64_t axis0, int64_t axis1) {
    static auto op = create_swapaxes__typed_handle();
    return op.call(self, axis0, axis1);
}

// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -> Tensor(a!)
at::Tensor & swapaxes_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t axis0, int64_t axis1) {
    static auto op = create_swapaxes__typed_handle();
    return op.redispatch(dispatchKeySet, self, axis0, axis1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapdims, name, "aten::swapdims")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapdims, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapdims, schema_str, "swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)")

// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<swapdims::schema> create_swapdims_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(swapdims::name, swapdims::overload_name)
      .typed<swapdims::schema>();
}

// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
at::Tensor swapdims::call(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_swapdims_typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::swapdims(Tensor(a) self, int dim0, int dim1) -> Tensor(a)
at::Tensor swapdims::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_swapdims_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapdims_, name, "aten::swapdims_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapdims_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(swapdims_, schema_str, "swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)")

// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<swapdims_::schema> create_swapdims__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(swapdims_::name, swapdims_::overload_name)
      .typed<swapdims_::schema>();
}

// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & swapdims_::call(at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_swapdims__typed_handle();
    return op.call(self, dim0, dim1);
}

// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)
at::Tensor & swapdims_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim0, int64_t dim1) {
    static auto op = create_swapdims__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim0, dim1);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_out, name, "aten::cholesky")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_out, schema_str, "cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_out::schema> create_cholesky_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_out::name, cholesky_out::overload_name)
      .typed<cholesky_out::schema>();
}

// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_out::call(const at::Tensor & self, bool upper, at::Tensor & out) {
    static auto op = create_cholesky_out_typed_handle();
    return op.call(self, upper, out);
}

// aten::cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
    static auto op = create_cholesky_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky, name, "aten::cholesky")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky, schema_str, "cholesky(Tensor self, bool upper=False) -> Tensor")

// aten::cholesky(Tensor self, bool upper=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cholesky::schema> create_cholesky_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky::name, cholesky::overload_name)
      .typed<cholesky::schema>();
}

// aten::cholesky(Tensor self, bool upper=False) -> Tensor
at::Tensor cholesky::call(const at::Tensor & self, bool upper) {
    static auto op = create_cholesky_typed_handle();
    return op.call(self, upper);
}

// aten::cholesky(Tensor self, bool upper=False) -> Tensor
at::Tensor cholesky::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper) {
    static auto op = create_cholesky_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve_out, name, "aten::cholesky_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve_out, schema_str, "cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_solve_out::schema> create_cholesky_solve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_solve_out::name, cholesky_solve_out::overload_name)
      .typed<cholesky_solve_out::schema>();
}

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_solve_out::call(const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
    static auto op = create_cholesky_solve_out_typed_handle();
    return op.call(self, input2, upper, out);
}

// aten::cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_solve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
    static auto op = create_cholesky_solve_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, upper, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve, name, "aten::cholesky_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_solve, schema_str, "cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor")

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_solve::schema> create_cholesky_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_solve::name, cholesky_solve::overload_name)
      .typed<cholesky_solve::schema>();
}

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
at::Tensor cholesky_solve::call(const at::Tensor & self, const at::Tensor & input2, bool upper) {
    static auto op = create_cholesky_solve_typed_handle();
    return op.call(self, input2, upper);
}

// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -> Tensor
at::Tensor cholesky_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, bool upper) {
    static auto op = create_cholesky_solve_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cholesky_solve_helper, name, "aten::_cholesky_solve_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cholesky_solve_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cholesky_solve_helper, schema_str, "_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor")

// aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cholesky_solve_helper::schema> create__cholesky_solve_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cholesky_solve_helper::name, _cholesky_solve_helper::overload_name)
      .typed<_cholesky_solve_helper::schema>();
}

// aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor
at::Tensor _cholesky_solve_helper::call(const at::Tensor & self, const at::Tensor & A, bool upper) {
    static auto op = create__cholesky_solve_helper_typed_handle();
    return op.call(self, A, upper);
}

// aten::_cholesky_solve_helper(Tensor self, Tensor A, bool upper) -> Tensor
at::Tensor _cholesky_solve_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, bool upper) {
    static auto op = create__cholesky_solve_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(solve, name, "aten::solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(solve, schema_str, "solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)")

// aten::solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)
static C10_NOINLINE c10::TypedOperatorHandle<solve::schema> create_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(solve::name, solve::overload_name)
      .typed<solve::schema>();
}

// aten::solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)
::std::tuple<at::Tensor,at::Tensor> solve::call(const at::Tensor & self, const at::Tensor & A) {
    static auto op = create_solve_typed_handle();
    return op.call(self, A);
}

// aten::solve(Tensor self, Tensor A) -> (Tensor solution, Tensor LU)
::std::tuple<at::Tensor,at::Tensor> solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A) {
    static auto op = create_solve_typed_handle();
    return op.redispatch(dispatchKeySet, self, A);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(solve_solution, name, "aten::solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(solve_solution, overload_name, "solution")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(solve_solution, schema_str, "solve.solution(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)")

// aten::solve.solution(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)
static C10_NOINLINE c10::TypedOperatorHandle<solve_solution::schema> create_solve_solution_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(solve_solution::name, solve_solution::overload_name)
      .typed<solve_solution::schema>();
}

// aten::solve.solution(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)
::std::tuple<at::Tensor &,at::Tensor &> solve_solution::call(const at::Tensor & self, const at::Tensor & A, at::Tensor & solution, at::Tensor & lu) {
    static auto op = create_solve_solution_typed_handle();
    return op.call(self, A, solution, lu);
}

// aten::solve.solution(Tensor self, Tensor A, *, Tensor(a!) solution, Tensor(b!) lu) -> (Tensor(a!) solution, Tensor(b!) LU)
::std::tuple<at::Tensor &,at::Tensor &> solve_solution::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A, at::Tensor & solution, at::Tensor & lu) {
    static auto op = create_solve_solution_typed_handle();
    return op.redispatch(dispatchKeySet, self, A, solution, lu);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_solve_helper, name, "aten::_solve_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_solve_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_solve_helper, schema_str, "_solve_helper(Tensor self, Tensor A) -> (Tensor, Tensor)")

// aten::_solve_helper(Tensor self, Tensor A) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_solve_helper::schema> create__solve_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_solve_helper::name, _solve_helper::overload_name)
      .typed<_solve_helper::schema>();
}

// aten::_solve_helper(Tensor self, Tensor A) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _solve_helper::call(const at::Tensor & self, const at::Tensor & A) {
    static auto op = create__solve_helper_typed_handle();
    return op.call(self, A);
}

// aten::_solve_helper(Tensor self, Tensor A) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _solve_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & A) {
    static auto op = create__solve_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, A);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_inverse, name, "aten::cholesky_inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_inverse, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_inverse, schema_str, "cholesky_inverse(Tensor self, bool upper=False) -> Tensor")

// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_inverse::schema> create_cholesky_inverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_inverse::name, cholesky_inverse::overload_name)
      .typed<cholesky_inverse::schema>();
}

// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor
at::Tensor cholesky_inverse::call(const at::Tensor & self, bool upper) {
    static auto op = create_cholesky_inverse_typed_handle();
    return op.call(self, upper);
}

// aten::cholesky_inverse(Tensor self, bool upper=False) -> Tensor
at::Tensor cholesky_inverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper) {
    static auto op = create_cholesky_inverse_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_inverse_out, name, "aten::cholesky_inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_inverse_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(cholesky_inverse_out, schema_str, "cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<cholesky_inverse_out::schema> create_cholesky_inverse_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(cholesky_inverse_out::name, cholesky_inverse_out::overload_name)
      .typed<cholesky_inverse_out::schema>();
}

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_inverse_out::call(const at::Tensor & self, bool upper, at::Tensor & out) {
    static auto op = create_cholesky_inverse_out_typed_handle();
    return op.call(self, upper, out);
}

// aten::cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & cholesky_inverse_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
    static auto op = create_cholesky_inverse_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr_Q, name, "aten::qr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr_Q, overload_name, "Q")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr_Q, schema_str, "qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)")

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
static C10_NOINLINE c10::TypedOperatorHandle<qr_Q::schema> create_qr_Q_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(qr_Q::name, qr_Q::overload_name)
      .typed<qr_Q::schema>();
}

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
::std::tuple<at::Tensor &,at::Tensor &> qr_Q::call(const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
    static auto op = create_qr_Q_typed_handle();
    return op.call(self, some, Q, R);
}

// aten::qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
::std::tuple<at::Tensor &,at::Tensor &> qr_Q::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some, at::Tensor & Q, at::Tensor & R) {
    static auto op = create_qr_Q_typed_handle();
    return op.redispatch(dispatchKeySet, self, some, Q, R);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr, name, "aten::qr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(qr, schema_str, "qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)")

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
static C10_NOINLINE c10::TypedOperatorHandle<qr::schema> create_qr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(qr::name, qr::overload_name)
      .typed<qr::schema>();
}

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
::std::tuple<at::Tensor,at::Tensor> qr::call(const at::Tensor & self, bool some) {
    static auto op = create_qr_typed_handle();
    return op.call(self, some);
}

// aten::qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
::std::tuple<at::Tensor,at::Tensor> qr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool some) {
    static auto op = create_qr_typed_handle();
    return op.redispatch(dispatchKeySet, self, some);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geqrf_a, name, "aten::geqrf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geqrf_a, overload_name, "a")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geqrf_a, schema_str, "geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)")

// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
static C10_NOINLINE c10::TypedOperatorHandle<geqrf_a::schema> create_geqrf_a_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(geqrf_a::name, geqrf_a::overload_name)
      .typed<geqrf_a::schema>();
}

// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
::std::tuple<at::Tensor &,at::Tensor &> geqrf_a::call(const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
    static auto op = create_geqrf_a_typed_handle();
    return op.call(self, a, tau);
}

// aten::geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -> (Tensor(a!) a, Tensor(b!) tau)
::std::tuple<at::Tensor &,at::Tensor &> geqrf_a::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & a, at::Tensor & tau) {
    static auto op = create_geqrf_a_typed_handle();
    return op.redispatch(dispatchKeySet, self, a, tau);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geqrf, name, "aten::geqrf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geqrf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(geqrf, schema_str, "geqrf(Tensor self) -> (Tensor a, Tensor tau)")

// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)
static C10_NOINLINE c10::TypedOperatorHandle<geqrf::schema> create_geqrf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(geqrf::name, geqrf::overload_name)
      .typed<geqrf::schema>();
}

// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)
::std::tuple<at::Tensor,at::Tensor> geqrf::call(const at::Tensor & self) {
    static auto op = create_geqrf_typed_handle();
    return op.call(self);
}

// aten::geqrf(Tensor self) -> (Tensor a, Tensor tau)
::std::tuple<at::Tensor,at::Tensor> geqrf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_geqrf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(orgqr, name, "aten::orgqr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(orgqr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(orgqr, schema_str, "orgqr(Tensor self, Tensor input2) -> Tensor")

// aten::orgqr(Tensor self, Tensor input2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<orgqr::schema> create_orgqr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(orgqr::name, orgqr::overload_name)
      .typed<orgqr::schema>();
}

// aten::orgqr(Tensor self, Tensor input2) -> Tensor
at::Tensor orgqr::call(const at::Tensor & self, const at::Tensor & input2) {
    static auto op = create_orgqr_typed_handle();
    return op.call(self, input2);
}

// aten::orgqr(Tensor self, Tensor input2) -> Tensor
at::Tensor orgqr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2) {
    static auto op = create_orgqr_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(orgqr_out, name, "aten::orgqr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(orgqr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(orgqr_out, schema_str, "orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<orgqr_out::schema> create_orgqr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(orgqr_out::name, orgqr_out::overload_name)
      .typed<orgqr_out::schema>();
}

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & orgqr_out::call(const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
    static auto op = create_orgqr_out_typed_handle();
    return op.call(self, input2, out);
}

// aten::orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & orgqr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, at::Tensor & out) {
    static auto op = create_orgqr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr_out, name, "aten::ormqr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr_out, schema_str, "ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ormqr_out::schema> create_ormqr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ormqr_out::name, ormqr_out::overload_name)
      .typed<ormqr_out::schema>();
}

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ormqr_out::call(const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
    static auto op = create_ormqr_out_typed_handle();
    return op.call(self, input2, input3, left, transpose, out);
}

// aten::ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ormqr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose, at::Tensor & out) {
    static auto op = create_ormqr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, input3, left, transpose, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr, name, "aten::ormqr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ormqr, schema_str, "ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor")

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ormqr::schema> create_ormqr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ormqr::name, ormqr::overload_name)
      .typed<ormqr::schema>();
}

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
at::Tensor ormqr::call(const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
    static auto op = create_ormqr_typed_handle();
    return op.call(self, input2, input3, left, transpose);
}

// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -> Tensor
at::Tensor ormqr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & input2, const at::Tensor & input3, bool left, bool transpose) {
    static auto op = create_ormqr_typed_handle();
    return op.redispatch(dispatchKeySet, self, input2, input3, left, transpose);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lu_with_info, name, "aten::_lu_with_info")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lu_with_info, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_lu_with_info, schema_str, "_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor LU, Tensor pivots, Tensor info)")

// aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor LU, Tensor pivots, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<_lu_with_info::schema> create__lu_with_info_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_lu_with_info::name, _lu_with_info::overload_name)
      .typed<_lu_with_info::schema>();
}

// aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor LU, Tensor pivots, Tensor info)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _lu_with_info::call(const at::Tensor & self, bool pivot, bool check_errors) {
    static auto op = create__lu_with_info_typed_handle();
    return op.call(self, pivot, check_errors);
}

// aten::_lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -> (Tensor LU, Tensor pivots, Tensor info)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _lu_with_info::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool pivot, bool check_errors) {
    static auto op = create__lu_with_info_typed_handle();
    return op.redispatch(dispatchKeySet, self, pivot, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_solve_out, name, "aten::lu_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_solve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_solve_out, schema_str, "lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lu_solve_out::schema> create_lu_solve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lu_solve_out::name, lu_solve_out::overload_name)
      .typed<lu_solve_out::schema>();
}

// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lu_solve_out::call(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
    static auto op = create_lu_solve_out_typed_handle();
    return op.call(self, LU_data, LU_pivots, out);
}

// aten::lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lu_solve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots, at::Tensor & out) {
    static auto op = create_lu_solve_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, LU_data, LU_pivots, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_solve, name, "aten::lu_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_solve, schema_str, "lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor")

// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lu_solve::schema> create_lu_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lu_solve::name, lu_solve::overload_name)
      .typed<lu_solve::schema>();
}

// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor
at::Tensor lu_solve::call(const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
    static auto op = create_lu_solve_typed_handle();
    return op.call(self, LU_data, LU_pivots);
}

// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -> Tensor
at::Tensor lu_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & LU_data, const at::Tensor & LU_pivots) {
    static auto op = create_lu_solve_typed_handle();
    return op.redispatch(dispatchKeySet, self, LU_data, LU_pivots);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_unpack, name, "aten::lu_unpack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_unpack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_unpack, schema_str, "lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)")

// aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)
static C10_NOINLINE c10::TypedOperatorHandle<lu_unpack::schema> create_lu_unpack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lu_unpack::name, lu_unpack::overload_name)
      .typed<lu_unpack::schema>();
}

// aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lu_unpack::call(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
    static auto op = create_lu_unpack_typed_handle();
    return op.call(LU_data, LU_pivots, unpack_data, unpack_pivots);
}

// aten::lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -> (Tensor P, Tensor L, Tensor U)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> lu_unpack::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots) {
    static auto op = create_lu_unpack_typed_handle();
    return op.redispatch(dispatchKeySet, LU_data, LU_pivots, unpack_data, unpack_pivots);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_unpack_out, name, "aten::lu_unpack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_unpack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lu_unpack_out, schema_str, "lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)")

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
static C10_NOINLINE c10::TypedOperatorHandle<lu_unpack_out::schema> create_lu_unpack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lu_unpack_out::name, lu_unpack_out::overload_name)
      .typed<lu_unpack_out::schema>();
}

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> lu_unpack_out::call(const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
    static auto op = create_lu_unpack_out_typed_handle();
    return op.call(LU_data, LU_pivots, unpack_data, unpack_pivots, P, L, U);
}

// aten::lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -> (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> lu_unpack_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & LU_data, const at::Tensor & LU_pivots, bool unpack_data, bool unpack_pivots, at::Tensor & P, at::Tensor & L, at::Tensor & U) {
    static auto op = create_lu_unpack_out_typed_handle();
    return op.redispatch(dispatchKeySet, LU_data, LU_pivots, unpack_data, unpack_pivots, P, L, U);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multinomial_out, name, "aten::multinomial")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multinomial_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multinomial_out, schema_str, "multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multinomial_out::schema> create_multinomial_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multinomial_out::name, multinomial_out::overload_name)
      .typed<multinomial_out::schema>();
}

// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multinomial_out::call(const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_multinomial_out_typed_handle();
    return op.call(self, num_samples, replacement, generator, out);
}

// aten::multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multinomial_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_multinomial_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, num_samples, replacement, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multinomial, name, "aten::multinomial")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multinomial, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multinomial, schema_str, "multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor")

// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multinomial::schema> create_multinomial_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multinomial::name, multinomial::overload_name)
      .typed<multinomial::schema>();
}

// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
at::Tensor multinomial::call(const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator) {
    static auto op = create_multinomial_typed_handle();
    return op.call(self, num_samples, replacement, generator);
}

// aten::multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
at::Tensor multinomial::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t num_samples, bool replacement, c10::optional<at::Generator> generator) {
    static auto op = create_multinomial_typed_handle();
    return op.redispatch(dispatchKeySet, self, num_samples, replacement, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma_out, name, "aten::lgamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma_out, schema_str, "lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lgamma_out::schema> create_lgamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lgamma_out::name, lgamma_out::overload_name)
      .typed<lgamma_out::schema>();
}

// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lgamma_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_lgamma_out_typed_handle();
    return op.call(self, out);
}

// aten::lgamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lgamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_lgamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma_, name, "aten::lgamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma_, schema_str, "lgamma_(Tensor(a!) self) -> Tensor(a!)")

// aten::lgamma_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lgamma_::schema> create_lgamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lgamma_::name, lgamma_::overload_name)
      .typed<lgamma_::schema>();
}

// aten::lgamma_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & lgamma_::call(at::Tensor & self) {
    static auto op = create_lgamma__typed_handle();
    return op.call(self);
}

// aten::lgamma_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & lgamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_lgamma__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma, name, "aten::lgamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lgamma, schema_str, "lgamma(Tensor self) -> Tensor")

// aten::lgamma(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lgamma::schema> create_lgamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lgamma::name, lgamma::overload_name)
      .typed<lgamma::schema>();
}

// aten::lgamma(Tensor self) -> Tensor
at::Tensor lgamma::call(const at::Tensor & self) {
    static auto op = create_lgamma_typed_handle();
    return op.call(self);
}

// aten::lgamma(Tensor self) -> Tensor
at::Tensor lgamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_lgamma_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_out, name, "aten::digamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma_out, schema_str, "digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<digamma_out::schema> create_digamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(digamma_out::name, digamma_out::overload_name)
      .typed<digamma_out::schema>();
}

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & digamma_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_digamma_out_typed_handle();
    return op.call(self, out);
}

// aten::digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & digamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_digamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma, name, "aten::digamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(digamma, schema_str, "digamma(Tensor self) -> Tensor")

// aten::digamma(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<digamma::schema> create_digamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(digamma::name, digamma::overload_name)
      .typed<digamma::schema>();
}

// aten::digamma(Tensor self) -> Tensor
at::Tensor digamma::call(const at::Tensor & self) {
    static auto op = create_digamma_typed_handle();
    return op.call(self);
}

// aten::digamma(Tensor self) -> Tensor
at::Tensor digamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_digamma_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_out, name, "aten::polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_out, schema_str, "polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<polygamma_out::schema> create_polygamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polygamma_out::name, polygamma_out::overload_name)
      .typed<polygamma_out::schema>();
}

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & polygamma_out::call(int64_t n, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_polygamma_out_typed_handle();
    return op.call(n, self, out);
}

// aten::polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & polygamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_polygamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma, name, "aten::polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma, schema_str, "polygamma(int n, Tensor self) -> Tensor")

// aten::polygamma(int n, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<polygamma::schema> create_polygamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polygamma::name, polygamma::overload_name)
      .typed<polygamma::schema>();
}

// aten::polygamma(int n, Tensor self) -> Tensor
at::Tensor polygamma::call(int64_t n, const at::Tensor & self) {
    static auto op = create_polygamma_typed_handle();
    return op.call(n, self);
}

// aten::polygamma(int n, Tensor self) -> Tensor
at::Tensor polygamma::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self) {
    static auto op = create_polygamma_typed_handle();
    return op.redispatch(dispatchKeySet, n, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_, name, "aten::polygamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(polygamma_, schema_str, "polygamma_(Tensor(a!) self, int n) -> Tensor(a!)")

// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<polygamma_::schema> create_polygamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(polygamma_::name, polygamma_::overload_name)
      .typed<polygamma_::schema>();
}

// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)
at::Tensor & polygamma_::call(at::Tensor & self, int64_t n) {
    static auto op = create_polygamma__typed_handle();
    return op.call(self, n);
}

// aten::polygamma_(Tensor(a!) self, int n) -> Tensor(a!)
at::Tensor & polygamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t n) {
    static auto op = create_polygamma__typed_handle();
    return op.redispatch(dispatchKeySet, self, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv, name, "aten::erfinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv, schema_str, "erfinv(Tensor self) -> Tensor")

// aten::erfinv(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<erfinv::schema> create_erfinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erfinv::name, erfinv::overload_name)
      .typed<erfinv::schema>();
}

// aten::erfinv(Tensor self) -> Tensor
at::Tensor erfinv::call(const at::Tensor & self) {
    static auto op = create_erfinv_typed_handle();
    return op.call(self);
}

// aten::erfinv(Tensor self) -> Tensor
at::Tensor erfinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_erfinv_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv_, name, "aten::erfinv_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv_, schema_str, "erfinv_(Tensor(a!) self) -> Tensor(a!)")

// aten::erfinv_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erfinv_::schema> create_erfinv__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erfinv_::name, erfinv_::overload_name)
      .typed<erfinv_::schema>();
}

// aten::erfinv_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erfinv_::call(at::Tensor & self) {
    static auto op = create_erfinv__typed_handle();
    return op.call(self);
}

// aten::erfinv_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & erfinv_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_erfinv__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv_out, name, "aten::erfinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(erfinv_out, schema_str, "erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<erfinv_out::schema> create_erfinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(erfinv_out::name, erfinv_out::overload_name)
      .typed<erfinv_out::schema>();
}

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erfinv_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_erfinv_out_typed_handle();
    return op.call(self, out);
}

// aten::erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & erfinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_erfinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0, name, "aten::i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0, schema_str, "i0(Tensor self) -> Tensor")

// aten::i0(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<i0::schema> create_i0_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(i0::name, i0::overload_name)
      .typed<i0::schema>();
}

// aten::i0(Tensor self) -> Tensor
at::Tensor i0::call(const at::Tensor & self) {
    static auto op = create_i0_typed_handle();
    return op.call(self);
}

// aten::i0(Tensor self) -> Tensor
at::Tensor i0::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_i0_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_, name, "aten::i0_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_, schema_str, "i0_(Tensor(a!) self) -> Tensor(a!)")

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<i0_::schema> create_i0__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(i0_::name, i0_::overload_name)
      .typed<i0_::schema>();
}

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & i0_::call(at::Tensor & self) {
    static auto op = create_i0__typed_handle();
    return op.call(self);
}

// aten::i0_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & i0_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_i0__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_out, name, "aten::i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(i0_out, schema_str, "i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<i0_out::schema> create_i0_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(i0_out::name, i0_out::overload_name)
      .typed<i0_out::schema>();
}

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & i0_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_i0_out_typed_handle();
    return op.call(self, out);
}

// aten::i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & i0_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_i0_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign, name, "aten::sign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign, schema_str, "sign(Tensor self) -> Tensor")

// aten::sign(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sign::schema> create_sign_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sign::name, sign::overload_name)
      .typed<sign::schema>();
}

// aten::sign(Tensor self) -> Tensor
at::Tensor sign::call(const at::Tensor & self) {
    static auto op = create_sign_typed_handle();
    return op.call(self);
}

// aten::sign(Tensor self) -> Tensor
at::Tensor sign::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_sign_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_, name, "aten::sign_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_, schema_str, "sign_(Tensor(a!) self) -> Tensor(a!)")

// aten::sign_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sign_::schema> create_sign__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sign_::name, sign_::overload_name)
      .typed<sign_::schema>();
}

// aten::sign_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sign_::call(at::Tensor & self) {
    static auto op = create_sign__typed_handle();
    return op.call(self);
}

// aten::sign_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & sign_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_sign__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_out, name, "aten::sign")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sign_out, schema_str, "sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sign_out::schema> create_sign_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sign_out::name, sign_out::overload_name)
      .typed<sign_out::schema>();
}

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sign_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sign_out_typed_handle();
    return op.call(self, out);
}

// aten::sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & sign_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_sign_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(signbit, name, "aten::signbit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(signbit, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(signbit, schema_str, "signbit(Tensor self) -> Tensor")

// aten::signbit(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<signbit::schema> create_signbit_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(signbit::name, signbit::overload_name)
      .typed<signbit::schema>();
}

// aten::signbit(Tensor self) -> Tensor
at::Tensor signbit::call(const at::Tensor & self) {
    static auto op = create_signbit_typed_handle();
    return op.call(self);
}

// aten::signbit(Tensor self) -> Tensor
at::Tensor signbit::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_signbit_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(signbit_out, name, "aten::signbit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(signbit_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(signbit_out, schema_str, "signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<signbit_out::schema> create_signbit_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(signbit_out::name, signbit_out::overload_name)
      .typed<signbit_out::schema>();
}

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & signbit_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_signbit_out_typed_handle();
    return op.call(self, out);
}

// aten::signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & signbit_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_signbit_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dist, name, "aten::dist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(dist, schema_str, "dist(Tensor self, Tensor other, Scalar p=2) -> Tensor")

// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<dist::schema> create_dist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(dist::name, dist::overload_name)
      .typed<dist::schema>();
}

// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor
at::Tensor dist::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & p) {
    static auto op = create_dist_typed_handle();
    return op.call(self, other, p);
}

// aten::dist(Tensor self, Tensor other, Scalar p=2) -> Tensor
at::Tensor dist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & p) {
    static auto op = create_dist_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2_out, name, "aten::atan2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2_out, schema_str, "atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atan2_out::schema> create_atan2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan2_out::name, atan2_out::overload_name)
      .typed<atan2_out::schema>();
}

// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atan2_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_atan2_out_typed_handle();
    return op.call(self, other, out);
}

// aten::atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & atan2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_atan2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2_, name, "aten::atan2_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2_, schema_str, "atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<atan2_::schema> create_atan2__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan2_::name, atan2_::overload_name)
      .typed<atan2_::schema>();
}

// aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & atan2_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_atan2__typed_handle();
    return op.call(self, other);
}

// aten::atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & atan2_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_atan2__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2, name, "aten::atan2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(atan2, schema_str, "atan2(Tensor self, Tensor other) -> Tensor")

// aten::atan2(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<atan2::schema> create_atan2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(atan2::name, atan2::overload_name)
      .typed<atan2::schema>();
}

// aten::atan2(Tensor self, Tensor other) -> Tensor
at::Tensor atan2::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_atan2_typed_handle();
    return op.call(self, other);
}

// aten::atan2(Tensor self, Tensor other) -> Tensor
at::Tensor atan2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_atan2_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar_out, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar_out, schema_str, "lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Scalar_out::schema> create_lerp_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Scalar_out::name, lerp_Scalar_out::overload_name)
      .typed<lerp_Scalar_out::schema>();
}

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Scalar_out::call(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
    static auto op = create_lerp_Scalar_out_typed_handle();
    return op.call(self, end, weight, out);
}

// aten::lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight, at::Tensor & out) {
    static auto op = create_lerp_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor_out, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor_out, schema_str, "lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)")

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Tensor_out::schema> create_lerp_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Tensor_out::name, lerp_Tensor_out::overload_name)
      .typed<lerp_Tensor_out::schema>();
}

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Tensor_out::call(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
    static auto op = create_lerp_Tensor_out_typed_handle();
    return op.call(self, end, weight, out);
}

// aten::lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & lerp_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight, at::Tensor & out) {
    static auto op = create_lerp_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Scalar, schema_str, "lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor")

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Scalar::schema> create_lerp_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Scalar::name, lerp_Scalar::overload_name)
      .typed<lerp_Scalar::schema>();
}

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
at::Tensor lerp_Scalar::call(const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    static auto op = create_lerp_Scalar_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
at::Tensor lerp_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Scalar & weight) {
    static auto op = create_lerp_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor, name, "aten::lerp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(lerp_Tensor, schema_str, "lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor")

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<lerp_Tensor::schema> create_lerp_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(lerp_Tensor::name, lerp_Tensor::overload_name)
      .typed<lerp_Tensor::schema>();
}

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
at::Tensor lerp_Tensor::call(const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    static auto op = create_lerp_Tensor_typed_handle();
    return op.call(self, end, weight);
}

// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
at::Tensor lerp_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & end, const at::Tensor & weight) {
    static auto op = create_lerp_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, end, weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc_out, name, "aten::histc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc_out, schema_str, "histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<histc_out::schema> create_histc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histc_out::name, histc_out::overload_name)
      .typed<histc_out::schema>();
}

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & histc_out::call(const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
    static auto op = create_histc_out_typed_handle();
    return op.call(self, bins, min, max, out);
}

// aten::histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & histc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max, at::Tensor & out) {
    static auto op = create_histc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, min, max, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc, name, "aten::histc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histc, schema_str, "histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor")

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<histc::schema> create_histc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histc::name, histc::overload_name)
      .typed<histc::schema>();
}

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
at::Tensor histc::call(const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max) {
    static auto op = create_histc_typed_handle();
    return op.call(self, bins, min, max);
}

// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
at::Tensor histc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, const at::Scalar & min, const at::Scalar & max) {
    static auto op = create_histc_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, min, max);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bins_tensor_out, name, "aten::histogram")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bins_tensor_out, overload_name, "bins_tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bins_tensor_out, schema_str, "histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)")

// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
static C10_NOINLINE c10::TypedOperatorHandle<histogram_bins_tensor_out::schema> create_histogram_bins_tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histogram_bins_tensor_out::name, histogram_bins_tensor_out::overload_name)
      .typed<histogram_bins_tensor_out::schema>();
}

// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
::std::tuple<at::Tensor &,at::Tensor &> histogram_bins_tensor_out::call(const at::Tensor & self, const at::Tensor & bins, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & hist, at::Tensor & bin_edges) {
    static auto op = create_histogram_bins_tensor_out_typed_handle();
    return op.call(self, bins, weight, density, hist, bin_edges);
}

// aten::histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
::std::tuple<at::Tensor &,at::Tensor &> histogram_bins_tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & bins, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & hist, at::Tensor & bin_edges) {
    static auto op = create_histogram_bins_tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, weight, density, hist, bin_edges);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bins_tensor, name, "aten::histogram")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bins_tensor, overload_name, "bins_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bins_tensor, schema_str, "histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)")

// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
static C10_NOINLINE c10::TypedOperatorHandle<histogram_bins_tensor::schema> create_histogram_bins_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histogram_bins_tensor::name, histogram_bins_tensor::overload_name)
      .typed<histogram_bins_tensor::schema>();
}

// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
::std::tuple<at::Tensor,at::Tensor> histogram_bins_tensor::call(const at::Tensor & self, const at::Tensor & bins, const c10::optional<at::Tensor> & weight, bool density) {
    static auto op = create_histogram_bins_tensor_typed_handle();
    return op.call(self, bins, weight, density);
}

// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
::std::tuple<at::Tensor,at::Tensor> histogram_bins_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & bins, const c10::optional<at::Tensor> & weight, bool density) {
    static auto op = create_histogram_bins_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, weight, density);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bin_ct_out, name, "aten::histogram")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bin_ct_out, overload_name, "bin_ct_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bin_ct_out, schema_str, "histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)")

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
static C10_NOINLINE c10::TypedOperatorHandle<histogram_bin_ct_out::schema> create_histogram_bin_ct_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histogram_bin_ct_out::name, histogram_bin_ct_out::overload_name)
      .typed<histogram_bin_ct_out::schema>();
}

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
::std::tuple<at::Tensor &,at::Tensor &> histogram_bin_ct_out::call(const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & hist, at::Tensor & bin_edges) {
    static auto op = create_histogram_bin_ct_out_typed_handle();
    return op.call(self, bins, range, weight, density, hist, bin_edges);
}

// aten::histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -> (Tensor(a!) hist, Tensor(b!) bin_edges)
::std::tuple<at::Tensor &,at::Tensor &> histogram_bin_ct_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density, at::Tensor & hist, at::Tensor & bin_edges) {
    static auto op = create_histogram_bin_ct_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, range, weight, density, hist, bin_edges);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bin_ct, name, "aten::histogram")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bin_ct, overload_name, "bin_ct")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(histogram_bin_ct, schema_str, "histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)")

// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
static C10_NOINLINE c10::TypedOperatorHandle<histogram_bin_ct::schema> create_histogram_bin_ct_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(histogram_bin_ct::name, histogram_bin_ct::overload_name)
      .typed<histogram_bin_ct::schema>();
}

// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
::std::tuple<at::Tensor,at::Tensor> histogram_bin_ct::call(const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    static auto op = create_histogram_bin_ct_typed_handle();
    return op.call(self, bins, range, weight, density);
}

// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -> (Tensor hist, Tensor bin_edges)
::std::tuple<at::Tensor,at::Tensor> histogram_bin_ct::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t bins, c10::optional<at::ArrayRef<double>> range, const c10::optional<at::Tensor> & weight, bool density) {
    static auto op = create_histogram_bin_ct_typed_handle();
    return op.redispatch(dispatchKeySet, self, bins, range, weight, density);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Scalar_out, name, "aten::fmod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Scalar_out, schema_str, "fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmod_Scalar_out::schema> create_fmod_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmod_Scalar_out::name, fmod_Scalar_out::overload_name)
      .typed<fmod_Scalar_out::schema>();
}

// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmod_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_fmod_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmod_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_fmod_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Scalar, name, "aten::fmod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Scalar, schema_str, "fmod.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fmod_Scalar::schema> create_fmod_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmod_Scalar::name, fmod_Scalar::overload_name)
      .typed<fmod_Scalar::schema>();
}

// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor fmod_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_fmod_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor fmod_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_fmod_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod__Scalar, name, "aten::fmod_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod__Scalar, schema_str, "fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmod__Scalar::schema> create_fmod__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmod__Scalar::name, fmod__Scalar::overload_name)
      .typed<fmod__Scalar::schema>();
}

// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & fmod__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_fmod__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & fmod__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_fmod__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Tensor_out, name, "aten::fmod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Tensor_out, schema_str, "fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmod_Tensor_out::schema> create_fmod_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmod_Tensor_out::name, fmod_Tensor_out::overload_name)
      .typed<fmod_Tensor_out::schema>();
}

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmod_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_fmod_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmod_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_fmod_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Tensor, name, "aten::fmod")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod_Tensor, schema_str, "fmod.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fmod_Tensor::schema> create_fmod_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmod_Tensor::name, fmod_Tensor::overload_name)
      .typed<fmod_Tensor::schema>();
}

// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor fmod_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmod_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor fmod_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmod_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod__Tensor, name, "aten::fmod_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmod__Tensor, schema_str, "fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmod__Tensor::schema> create_fmod__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmod__Tensor::name, fmod__Tensor::overload_name)
      .typed<fmod__Tensor::schema>();
}

// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & fmod__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmod__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & fmod__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmod__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot_out, name, "aten::hypot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot_out, schema_str, "hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hypot_out::schema> create_hypot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hypot_out::name, hypot_out::overload_name)
      .typed<hypot_out::schema>();
}

// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hypot_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_hypot_out_typed_handle();
    return op.call(self, other, out);
}

// aten::hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hypot_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_hypot_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot, name, "aten::hypot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot, schema_str, "hypot(Tensor self, Tensor other) -> Tensor")

// aten::hypot(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hypot::schema> create_hypot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hypot::name, hypot::overload_name)
      .typed<hypot::schema>();
}

// aten::hypot(Tensor self, Tensor other) -> Tensor
at::Tensor hypot::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_hypot_typed_handle();
    return op.call(self, other);
}

// aten::hypot(Tensor self, Tensor other) -> Tensor
at::Tensor hypot::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_hypot_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot_, name, "aten::hypot_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hypot_, schema_str, "hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hypot_::schema> create_hypot__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hypot_::name, hypot_::overload_name)
      .typed<hypot_::schema>();
}

// aten::hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & hypot_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_hypot__typed_handle();
    return op.call(self, other);
}

// aten::hypot_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & hypot_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_hypot__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma_out, name, "aten::igamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma_out, schema_str, "igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<igamma_out::schema> create_igamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(igamma_out::name, igamma_out::overload_name)
      .typed<igamma_out::schema>();
}

// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & igamma_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_igamma_out_typed_handle();
    return op.call(self, other, out);
}

// aten::igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & igamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_igamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma, name, "aten::igamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma, schema_str, "igamma(Tensor self, Tensor other) -> Tensor")

// aten::igamma(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<igamma::schema> create_igamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(igamma::name, igamma::overload_name)
      .typed<igamma::schema>();
}

// aten::igamma(Tensor self, Tensor other) -> Tensor
at::Tensor igamma::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igamma_typed_handle();
    return op.call(self, other);
}

// aten::igamma(Tensor self, Tensor other) -> Tensor
at::Tensor igamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igamma_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma_, name, "aten::igamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igamma_, schema_str, "igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<igamma_::schema> create_igamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(igamma_::name, igamma_::overload_name)
      .typed<igamma_::schema>();
}

// aten::igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & igamma_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igamma__typed_handle();
    return op.call(self, other);
}

// aten::igamma_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & igamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igamma__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac_out, name, "aten::igammac")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac_out, schema_str, "igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<igammac_out::schema> create_igammac_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(igammac_out::name, igammac_out::overload_name)
      .typed<igammac_out::schema>();
}

// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & igammac_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_igammac_out_typed_handle();
    return op.call(self, other, out);
}

// aten::igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & igammac_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_igammac_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac, name, "aten::igammac")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac, schema_str, "igammac(Tensor self, Tensor other) -> Tensor")

// aten::igammac(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<igammac::schema> create_igammac_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(igammac::name, igammac::overload_name)
      .typed<igammac::schema>();
}

// aten::igammac(Tensor self, Tensor other) -> Tensor
at::Tensor igammac::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igammac_typed_handle();
    return op.call(self, other);
}

// aten::igammac(Tensor self, Tensor other) -> Tensor
at::Tensor igammac::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igammac_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac_, name, "aten::igammac_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(igammac_, schema_str, "igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<igammac_::schema> create_igammac__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(igammac_::name, igammac_::overload_name)
      .typed<igammac_::schema>();
}

// aten::igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & igammac_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igammac__typed_handle();
    return op.call(self, other);
}

// aten::igammac_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & igammac_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_igammac__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_out, name, "aten::nextafter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_out, schema_str, "nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nextafter_out::schema> create_nextafter_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nextafter_out::name, nextafter_out::overload_name)
      .typed<nextafter_out::schema>();
}

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nextafter_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_nextafter_out_typed_handle();
    return op.call(self, other, out);
}

// aten::nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nextafter_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_nextafter_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter, name, "aten::nextafter")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter, schema_str, "nextafter(Tensor self, Tensor other) -> Tensor")

// aten::nextafter(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nextafter::schema> create_nextafter_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nextafter::name, nextafter::overload_name)
      .typed<nextafter::schema>();
}

// aten::nextafter(Tensor self, Tensor other) -> Tensor
at::Tensor nextafter::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_nextafter_typed_handle();
    return op.call(self, other);
}

// aten::nextafter(Tensor self, Tensor other) -> Tensor
at::Tensor nextafter::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_nextafter_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_, name, "aten::nextafter_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nextafter_, schema_str, "nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nextafter_::schema> create_nextafter__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nextafter_::name, nextafter_::overload_name)
      .typed<nextafter_::schema>();
}

// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & nextafter_::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_nextafter__typed_handle();
    return op.call(self, other);
}

// aten::nextafter_(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & nextafter_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_nextafter__typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar_out, name, "aten::remainder")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar_out, schema_str, "remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<remainder_Scalar_out::schema> create_remainder_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder_Scalar_out::name, remainder_Scalar_out::overload_name)
      .typed<remainder_Scalar_out::schema>();
}

// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & remainder_Scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_remainder_Scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & remainder_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_remainder_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar, name, "aten::remainder")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar, schema_str, "remainder.Scalar(Tensor self, Scalar other) -> Tensor")

// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<remainder_Scalar::schema> create_remainder_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder_Scalar::name, remainder_Scalar::overload_name)
      .typed<remainder_Scalar::schema>();
}

// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor remainder_Scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_remainder_Scalar_typed_handle();
    return op.call(self, other);
}

// aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor
at::Tensor remainder_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_remainder_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder__Scalar, name, "aten::remainder_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder__Scalar, schema_str, "remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)")

// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<remainder__Scalar::schema> create_remainder__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder__Scalar::name, remainder__Scalar::overload_name)
      .typed<remainder__Scalar::schema>();
}

// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & remainder__Scalar::call(at::Tensor & self, const at::Scalar & other) {
    static auto op = create_remainder__Scalar_typed_handle();
    return op.call(self, other);
}

// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
at::Tensor & remainder__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & other) {
    static auto op = create_remainder__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Tensor_out, name, "aten::remainder")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Tensor_out, schema_str, "remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<remainder_Tensor_out::schema> create_remainder_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder_Tensor_out::name, remainder_Tensor_out::overload_name)
      .typed<remainder_Tensor_out::schema>();
}

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & remainder_Tensor_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_remainder_Tensor_out_typed_handle();
    return op.call(self, other, out);
}

// aten::remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & remainder_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_remainder_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Tensor, name, "aten::remainder")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Tensor, schema_str, "remainder.Tensor(Tensor self, Tensor other) -> Tensor")

// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<remainder_Tensor::schema> create_remainder_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder_Tensor::name, remainder_Tensor::overload_name)
      .typed<remainder_Tensor::schema>();
}

// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor remainder_Tensor::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_remainder_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor
at::Tensor remainder_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_remainder_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder__Tensor, name, "aten::remainder_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder__Tensor, schema_str, "remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)")

// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<remainder__Tensor::schema> create_remainder__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder__Tensor::name, remainder__Tensor::overload_name)
      .typed<remainder__Tensor::schema>();
}

// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & remainder__Tensor::call(at::Tensor & self, const at::Tensor & other) {
    static auto op = create_remainder__Tensor_typed_handle();
    return op.call(self, other);
}

// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
at::Tensor & remainder__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & other) {
    static auto op = create_remainder__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar_Tensor, name, "aten::remainder")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar_Tensor, overload_name, "Scalar_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(remainder_Scalar_Tensor, schema_str, "remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor")

// aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<remainder_Scalar_Tensor::schema> create_remainder_Scalar_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(remainder_Scalar_Tensor::name, remainder_Scalar_Tensor::overload_name)
      .typed<remainder_Scalar_Tensor::schema>();
}

// aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
at::Tensor remainder_Scalar_Tensor::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_remainder_Scalar_Tensor_typed_handle();
    return op.call(self, other);
}

// aten::remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
at::Tensor remainder_Scalar_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_remainder_Scalar_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min, schema_str, "min(Tensor self) -> Tensor")

// aten::min(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<min::schema> create_min_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min::name, min::overload_name)
      .typed<min::schema>();
}

// aten::min(Tensor self) -> Tensor
at::Tensor min::call(const at::Tensor & self) {
    static auto op = create_min_typed_handle();
    return op.call(self);
}

// aten::min(Tensor self) -> Tensor
at::Tensor min::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_min_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin, name, "aten::fmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin, schema_str, "fmin(Tensor self, Tensor other) -> Tensor")

// aten::fmin(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fmin::schema> create_fmin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmin::name, fmin::overload_name)
      .typed<fmin::schema>();
}

// aten::fmin(Tensor self, Tensor other) -> Tensor
at::Tensor fmin::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmin_typed_handle();
    return op.call(self, other);
}

// aten::fmin(Tensor self, Tensor other) -> Tensor
at::Tensor fmin::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmin_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin_out, name, "aten::fmin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmin_out, schema_str, "fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmin_out::schema> create_fmin_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmin_out::name, fmin_out::overload_name)
      .typed<fmin_out::schema>();
}

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmin_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_fmin_out_typed_handle();
    return op.call(self, other, out);
}

// aten::fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmin_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_fmin_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max, schema_str, "max(Tensor self) -> Tensor")

// aten::max(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max::schema> create_max_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max::name, max::overload_name)
      .typed<max::schema>();
}

// aten::max(Tensor self) -> Tensor
at::Tensor max::call(const at::Tensor & self) {
    static auto op = create_max_typed_handle();
    return op.call(self);
}

// aten::max(Tensor self) -> Tensor
at::Tensor max::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_max_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmax, name, "aten::fmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmax, schema_str, "fmax(Tensor self, Tensor other) -> Tensor")

// aten::fmax(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fmax::schema> create_fmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmax::name, fmax::overload_name)
      .typed<fmax::schema>();
}

// aten::fmax(Tensor self, Tensor other) -> Tensor
at::Tensor fmax::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmax_typed_handle();
    return op.call(self, other);
}

// aten::fmax(Tensor self, Tensor other) -> Tensor
at::Tensor fmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_fmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmax_out, name, "aten::fmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmax_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fmax_out, schema_str, "fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fmax_out::schema> create_fmax_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fmax_out::name, fmax_out::overload_name)
      .typed<fmax_out::schema>();
}

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmax_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_fmax_out_typed_handle();
    return op.call(self, other, out);
}

// aten::fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fmax_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_fmax_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum, name, "aten::maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum, schema_str, "maximum(Tensor self, Tensor other) -> Tensor")

// aten::maximum(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<maximum::schema> create_maximum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(maximum::name, maximum::overload_name)
      .typed<maximum::schema>();
}

// aten::maximum(Tensor self, Tensor other) -> Tensor
at::Tensor maximum::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_maximum_typed_handle();
    return op.call(self, other);
}

// aten::maximum(Tensor self, Tensor other) -> Tensor
at::Tensor maximum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_maximum_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum_out, name, "aten::maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(maximum_out, schema_str, "maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<maximum_out::schema> create_maximum_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(maximum_out::name, maximum_out::overload_name)
      .typed<maximum_out::schema>();
}

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & maximum_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_maximum_out_typed_handle();
    return op.call(self, other, out);
}

// aten::maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & maximum_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_maximum_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_other, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_other, overload_name, "other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_other, schema_str, "max.other(Tensor self, Tensor other) -> Tensor")

// aten::max.other(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_other::schema> create_max_other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_other::name, max_other::overload_name)
      .typed<max_other::schema>();
}

// aten::max.other(Tensor self, Tensor other) -> Tensor
at::Tensor max_other::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_max_other_typed_handle();
    return op.call(self, other);
}

// aten::max.other(Tensor self, Tensor other) -> Tensor
at::Tensor max_other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_max_other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_out, name, "aten::max")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_out, schema_str, "max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_out::schema> create_max_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_out::name, max_out::overload_name)
      .typed<max_out::schema>();
}

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_max_out_typed_handle();
    return op.call(self, other, out);
}

// aten::max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_max_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum, name, "aten::minimum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum, schema_str, "minimum(Tensor self, Tensor other) -> Tensor")

// aten::minimum(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<minimum::schema> create_minimum_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(minimum::name, minimum::overload_name)
      .typed<minimum::schema>();
}

// aten::minimum(Tensor self, Tensor other) -> Tensor
at::Tensor minimum::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_minimum_typed_handle();
    return op.call(self, other);
}

// aten::minimum(Tensor self, Tensor other) -> Tensor
at::Tensor minimum::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_minimum_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum_out, name, "aten::minimum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(minimum_out, schema_str, "minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<minimum_out::schema> create_minimum_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(minimum_out::name, minimum_out::overload_name)
      .typed<minimum_out::schema>();
}

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & minimum_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_minimum_out_typed_handle();
    return op.call(self, other, out);
}

// aten::minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & minimum_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_minimum_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_out, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_out, schema_str, "min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<min_out::schema> create_min_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_out::name, min_out::overload_name)
      .typed<min_out::schema>();
}

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & min_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_min_out_typed_handle();
    return op.call(self, other, out);
}

// aten::min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & min_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_min_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_other, name, "aten::min")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_other, overload_name, "other")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(min_other, schema_str, "min.other(Tensor self, Tensor other) -> Tensor")

// aten::min.other(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<min_other::schema> create_min_other_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(min_other::name, min_other::overload_name)
      .typed<min_other::schema>();
}

// aten::min.other(Tensor self, Tensor other) -> Tensor
at::Tensor min_other::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_min_other_typed_handle();
    return op.call(self, other);
}

// aten::min.other(Tensor self, Tensor other) -> Tensor
at::Tensor min_other::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_min_other_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar_out, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar_out, overload_name, "scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar_out, schema_str, "quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantile_scalar_out::schema> create_quantile_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_scalar_out::name, quantile_scalar_out::overload_name)
      .typed<quantile_scalar_out::schema>();
}

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_scalar_out::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_quantile_scalar_out_typed_handle();
    return op.call(self, q, dim, keepdim, out);
}

// aten::quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_quantile_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar, overload_name, "scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_scalar, schema_str, "quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor")

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantile_scalar::schema> create_quantile_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_scalar::name, quantile_scalar::overload_name)
      .typed<quantile_scalar::schema>();
}

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor quantile_scalar::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_quantile_scalar_typed_handle();
    return op.call(self, q, dim, keepdim);
}

// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor quantile_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_quantile_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_out, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_out, schema_str, "quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantile_out::schema> create_quantile_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_out::name, quantile_out::overload_name)
      .typed<quantile_out::schema>();
}

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_out::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_quantile_out_typed_handle();
    return op.call(self, q, dim, keepdim, out);
}

// aten::quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_quantile_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile, schema_str, "quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor")

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantile::schema> create_quantile_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile::name, quantile::overload_name)
      .typed<quantile::schema>();
}

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor quantile::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_quantile_typed_handle();
    return op.call(self, q, dim, keepdim);
}

// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor quantile::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_quantile_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_scalar_out, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_scalar_out, overload_name, "scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_scalar_out, schema_str, "nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_scalar_out::schema> create_nanquantile_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_scalar_out::name, nanquantile_scalar_out::overload_name)
      .typed<nanquantile_scalar_out::schema>();
}

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_scalar_out::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_nanquantile_scalar_out_typed_handle();
    return op.call(self, q, dim, keepdim, out);
}

// aten::nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_nanquantile_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_scalar, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_scalar, overload_name, "scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_scalar, schema_str, "nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor")

// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_scalar::schema> create_nanquantile_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_scalar::name, nanquantile_scalar::overload_name)
      .typed<nanquantile_scalar::schema>();
}

// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor nanquantile_scalar::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_nanquantile_scalar_typed_handle();
    return op.call(self, q, dim, keepdim);
}

// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor nanquantile_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_nanquantile_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_out, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_out, schema_str, "nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_out::schema> create_nanquantile_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_out::name, nanquantile_out::overload_name)
      .typed<nanquantile_out::schema>();
}

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_out::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_nanquantile_out_typed_handle();
    return op.call(self, q, dim, keepdim, out);
}

// aten::nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, at::Tensor & out) {
    static auto op = create_nanquantile_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile, schema_str, "nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor")

// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile::schema> create_nanquantile_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile::name, nanquantile::overload_name)
      .typed<nanquantile::schema>();
}

// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor nanquantile::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_nanquantile_typed_handle();
    return op.call(self, q, dim, keepdim);
}

// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False) -> Tensor
at::Tensor nanquantile::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim) {
    static auto op = create_nanquantile_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_scalar_out, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_scalar_out, overload_name, "new_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_scalar_out, schema_str, "quantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)")

// aten::quantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantile_new_scalar_out::schema> create_quantile_new_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_new_scalar_out::name, quantile_new_scalar_out::overload_name)
      .typed<quantile_new_scalar_out::schema>();
}

// aten::quantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_new_scalar_out::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_quantile_new_scalar_out_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation, out);
}

// aten::quantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_new_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_quantile_new_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_scalar, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_scalar, overload_name, "new_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_scalar, schema_str, "quantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor")

// aten::quantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantile_new_scalar::schema> create_quantile_new_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_new_scalar::name, quantile_new_scalar::overload_name)
      .typed<quantile_new_scalar::schema>();
}

// aten::quantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor quantile_new_scalar::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_quantile_new_scalar_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation);
}

// aten::quantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor quantile_new_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_quantile_new_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_out, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_out, overload_name, "new_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new_out, schema_str, "quantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)")

// aten::quantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<quantile_new_out::schema> create_quantile_new_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_new_out::name, quantile_new_out::overload_name)
      .typed<quantile_new_out::schema>();
}

// aten::quantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_new_out::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_quantile_new_out_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation, out);
}

// aten::quantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & quantile_new_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_quantile_new_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new, name, "aten::quantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new, overload_name, "new")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(quantile_new, schema_str, "quantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor")

// aten::quantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<quantile_new::schema> create_quantile_new_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(quantile_new::name, quantile_new::overload_name)
      .typed<quantile_new::schema>();
}

// aten::quantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor quantile_new::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_quantile_new_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation);
}

// aten::quantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor quantile_new::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_quantile_new_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_scalar_out, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_scalar_out, overload_name, "new_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_scalar_out, schema_str, "nanquantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)")

// aten::nanquantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_new_scalar_out::schema> create_nanquantile_new_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_new_scalar_out::name, nanquantile_new_scalar_out::overload_name)
      .typed<nanquantile_new_scalar_out::schema>();
}

// aten::nanquantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_new_scalar_out::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_nanquantile_new_scalar_out_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation, out);
}

// aten::nanquantile.new_scalar_out(Tensor self, float q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_new_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_nanquantile_new_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_scalar, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_scalar, overload_name, "new_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_scalar, schema_str, "nanquantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor")

// aten::nanquantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_new_scalar::schema> create_nanquantile_new_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_new_scalar::name, nanquantile_new_scalar::overload_name)
      .typed<nanquantile_new_scalar::schema>();
}

// aten::nanquantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor nanquantile_new_scalar::call(const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_nanquantile_new_scalar_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation);
}

// aten::nanquantile.new_scalar(Tensor self, float q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor nanquantile_new_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_nanquantile_new_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_out, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_out, overload_name, "new_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new_out, schema_str, "nanquantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)")

// aten::nanquantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_new_out::schema> create_nanquantile_new_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_new_out::name, nanquantile_new_out::overload_name)
      .typed<nanquantile_new_out::schema>();
}

// aten::nanquantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_new_out::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_nanquantile_new_out_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation, out);
}

// aten::nanquantile.new_out(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nanquantile_new_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation, at::Tensor & out) {
    static auto op = create_nanquantile_new_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new, name, "aten::nanquantile")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new, overload_name, "new")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nanquantile_new, schema_str, "nanquantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor")

// aten::nanquantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nanquantile_new::schema> create_nanquantile_new_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nanquantile_new::name, nanquantile_new::overload_name)
      .typed<nanquantile_new::schema>();
}

// aten::nanquantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor nanquantile_new::call(const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_nanquantile_new_typed_handle();
    return op.call(self, q, dim, keepdim, interpolation);
}

// aten::nanquantile.new(Tensor self, Tensor q, int? dim, bool keepdim, *, str interpolation) -> Tensor
at::Tensor nanquantile_new::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & q, c10::optional<int64_t> dim, bool keepdim, c10::string_view interpolation) {
    static auto op = create_nanquantile_new_typed_handle();
    return op.redispatch(dispatchKeySet, self, q, dim, keepdim, interpolation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_values, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_values, overload_name, "values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_values, schema_str, "sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_values::schema> create_sort_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_values::name, sort_values::overload_name)
      .typed<sort_values::schema>();
}

// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_values::call(const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_values_typed_handle();
    return op.call(self, dim, descending, values, indices);
}

// aten::sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_values_stable, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_values_stable, overload_name, "values_stable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_values_stable, schema_str, "sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_values_stable::schema> create_sort_values_stable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_values_stable::name, sort_values_stable::overload_name)
      .typed<sort_values_stable::schema>();
}

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_values_stable::call(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_values_stable_typed_handle();
    return op.call(self, stable, dim, descending, values, indices);
}

// aten::sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_values_stable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_values_stable_typed_handle();
    return op.redispatch(dispatchKeySet, self, stable, dim, descending, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort, schema_str, "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)")

// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort::schema> create_sort_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort::name, sort::overload_name)
      .typed<sort::schema>();
}

// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort::call(const at::Tensor & self, int64_t dim, bool descending) {
    static auto op = create_sort_typed_handle();
    return op.call(self, dim, descending);
}

// aten::sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending) {
    static auto op = create_sort_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_stable, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_stable, overload_name, "stable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_stable, schema_str, "sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)")

// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_stable::schema> create_sort_stable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_stable::name, sort_stable::overload_name)
      .typed<sort_stable::schema>();
}

// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort_stable::call(const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending) {
    static auto op = create_sort_stable_typed_handle();
    return op.call(self, stable, dim, descending);
}

// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort_stable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, int64_t dim, bool descending) {
    static auto op = create_sort_stable_typed_handle();
    return op.redispatch(dispatchKeySet, self, stable, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_values, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_values, overload_name, "dimname_values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_values, schema_str, "sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_dimname_values::schema> create_sort_dimname_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_dimname_values::name, sort_dimname_values::overload_name)
      .typed<sort_dimname_values::schema>();
}

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_dimname_values::call(const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_dimname_values_typed_handle();
    return op.call(self, dim, descending, values, indices);
}

// aten::sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_dimname_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_dimname_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_values_stable, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_values_stable, overload_name, "dimname_values_stable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_values_stable, schema_str, "sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_dimname_values_stable::schema> create_sort_dimname_values_stable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_dimname_values_stable::name, sort_dimname_values_stable::overload_name)
      .typed<sort_dimname_values_stable::schema>();
}

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_dimname_values_stable::call(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_dimname_values_stable_typed_handle();
    return op.call(self, stable, dim, descending, values, indices);
}

// aten::sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> sort_dimname_values_stable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_sort_dimname_values_stable_typed_handle();
    return op.redispatch(dispatchKeySet, self, stable, dim, descending, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname, schema_str, "sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)")

// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_dimname::schema> create_sort_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_dimname::name, sort_dimname::overload_name)
      .typed<sort_dimname::schema>();
}

// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort_dimname::call(const at::Tensor & self, at::Dimname dim, bool descending) {
    static auto op = create_sort_dimname_typed_handle();
    return op.call(self, dim, descending);
}

// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending) {
    static auto op = create_sort_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_stable, name, "aten::sort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_stable, overload_name, "dimname_stable")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sort_dimname_stable, schema_str, "sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)")

// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<sort_dimname_stable::schema> create_sort_dimname_stable_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sort_dimname_stable::name, sort_dimname_stable::overload_name)
      .typed<sort_dimname_stable::schema>();
}

// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort_dimname_stable::call(const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
    static auto op = create_sort_dimname_stable_typed_handle();
    return op.call(self, stable, dim, descending);
}

// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> sort_dimname_stable::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<bool> stable, at::Dimname dim, bool descending) {
    static auto op = create_sort_dimname_stable_typed_handle();
    return op.redispatch(dispatchKeySet, self, stable, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort_out, name, "aten::msort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort_out, schema_str, "msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<msort_out::schema> create_msort_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(msort_out::name, msort_out::overload_name)
      .typed<msort_out::schema>();
}

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & msort_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_msort_out_typed_handle();
    return op.call(self, out);
}

// aten::msort.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & msort_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_msort_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort, name, "aten::msort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(msort, schema_str, "msort(Tensor self) -> Tensor")

// aten::msort(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<msort::schema> create_msort_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(msort::name, msort::overload_name)
      .typed<msort::schema>();
}

// aten::msort(Tensor self) -> Tensor
at::Tensor msort::call(const at::Tensor & self) {
    static auto op = create_msort_typed_handle();
    return op.call(self);
}

// aten::msort(Tensor self) -> Tensor
at::Tensor msort::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_msort_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort, name, "aten::argsort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort, schema_str, "argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor")

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argsort::schema> create_argsort_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argsort::name, argsort::overload_name)
      .typed<argsort::schema>();
}

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
at::Tensor argsort::call(const at::Tensor & self, int64_t dim, bool descending) {
    static auto op = create_argsort_typed_handle();
    return op.call(self, dim, descending);
}

// aten::argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
at::Tensor argsort::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending) {
    static auto op = create_argsort_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_dimname, name, "aten::argsort")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_dimname, overload_name, "dimname")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(argsort_dimname, schema_str, "argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor")

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<argsort_dimname::schema> create_argsort_dimname_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(argsort_dimname::name, argsort_dimname::overload_name)
      .typed<argsort_dimname::schema>();
}

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
at::Tensor argsort_dimname::call(const at::Tensor & self, at::Dimname dim, bool descending) {
    static auto op = create_argsort_dimname_typed_handle();
    return op.call(self, dim, descending);
}

// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
at::Tensor argsort_dimname::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Dimname dim, bool descending) {
    static auto op = create_argsort_dimname_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, descending);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk_values, name, "aten::topk")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk_values, overload_name, "values")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk_values, schema_str, "topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)")

// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
static C10_NOINLINE c10::TypedOperatorHandle<topk_values::schema> create_topk_values_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(topk_values::name, topk_values::overload_name)
      .typed<topk_values::schema>();
}

// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> topk_values::call(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_topk_values_typed_handle();
    return op.call(self, k, dim, largest, sorted, values, indices);
}

// aten::topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
::std::tuple<at::Tensor &,at::Tensor &> topk_values::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted, at::Tensor & values, at::Tensor & indices) {
    static auto op = create_topk_values_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, largest, sorted, values, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk, name, "aten::topk")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(topk, schema_str, "topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)")

// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
static C10_NOINLINE c10::TypedOperatorHandle<topk::schema> create_topk_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(topk::name, topk::overload_name)
      .typed<topk::schema>();
}

// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> topk::call(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
    static auto op = create_topk_typed_handle();
    return op.call(self, k, dim, largest, sorted);
}

// aten::topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
::std::tuple<at::Tensor,at::Tensor> topk::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
    static auto op = create_topk_typed_handle();
    return op.redispatch(dispatchKeySet, self, k, dim, largest, sorted);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all, name, "aten::all")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all, schema_str, "all(Tensor self) -> Tensor")

// aten::all(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<all::schema> create_all_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(all::name, all::overload_name)
      .typed<all::schema>();
}

// aten::all(Tensor self) -> Tensor
at::Tensor all::call(const at::Tensor & self) {
    static auto op = create_all_typed_handle();
    return op.call(self);
}

// aten::all(Tensor self) -> Tensor
at::Tensor all::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_all_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_all_out, name, "aten::all")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_all_out, overload_name, "all_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(all_all_out, schema_str, "all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<all_all_out::schema> create_all_all_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(all_all_out::name, all_all_out::overload_name)
      .typed<all_all_out::schema>();
}

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & all_all_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_all_all_out_typed_handle();
    return op.call(self, out);
}

// aten::all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & all_all_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_all_all_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any, name, "aten::any")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any, schema_str, "any(Tensor self) -> Tensor")

// aten::any(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<any::schema> create_any_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(any::name, any::overload_name)
      .typed<any::schema>();
}

// aten::any(Tensor self) -> Tensor
at::Tensor any::call(const at::Tensor & self) {
    static auto op = create_any_typed_handle();
    return op.call(self);
}

// aten::any(Tensor self) -> Tensor
at::Tensor any::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_any_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_all_out, name, "aten::any")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_all_out, overload_name, "all_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(any_all_out, schema_str, "any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<any_all_out::schema> create_any_all_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(any_all_out::name, any_all_out::overload_name)
      .typed<any_all_out::schema>();
}

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & any_all_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_any_all_out_typed_handle();
    return op.call(self, out);
}

// aten::any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & any_all_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_any_all_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm_out, name, "aten::renorm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm_out, schema_str, "renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)")

// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<renorm_out::schema> create_renorm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(renorm_out::name, renorm_out::overload_name)
      .typed<renorm_out::schema>();
}

// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & renorm_out::call(const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm, at::Tensor & out) {
    static auto op = create_renorm_out_typed_handle();
    return op.call(self, p, dim, maxnorm, out);
}

// aten::renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & renorm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm, at::Tensor & out) {
    static auto op = create_renorm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, maxnorm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm, name, "aten::renorm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm, schema_str, "renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor")

// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<renorm::schema> create_renorm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(renorm::name, renorm::overload_name)
      .typed<renorm::schema>();
}

// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
at::Tensor renorm::call(const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
    static auto op = create_renorm_typed_handle();
    return op.call(self, p, dim, maxnorm);
}

// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
at::Tensor renorm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
    static auto op = create_renorm_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, maxnorm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm_, name, "aten::renorm_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(renorm_, schema_str, "renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)")

// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<renorm_::schema> create_renorm__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(renorm_::name, renorm_::overload_name)
      .typed<renorm_::schema>();
}

// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)
at::Tensor & renorm_::call(at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
    static auto op = create_renorm__typed_handle();
    return op.call(self, p, dim, maxnorm);
}

// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)
at::Tensor & renorm_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
    static auto op = create_renorm__typed_handle();
    return op.redispatch(dispatchKeySet, self, p, dim, maxnorm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold, name, "aten::unfold")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold, schema_str, "unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)")

// aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<unfold::schema> create_unfold_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unfold::name, unfold::overload_name)
      .typed<unfold::schema>();
}

// aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)
at::Tensor unfold::call(const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
    static auto op = create_unfold_typed_handle();
    return op.call(self, dimension, size, step);
}

// aten::unfold(Tensor(a) self, int dimension, int size, int step) -> Tensor(a)
at::Tensor unfold::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dimension, int64_t size, int64_t step) {
    static auto op = create_unfold_typed_handle();
    return op.redispatch(dispatchKeySet, self, dimension, size, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward, name, "aten::unfold_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unfold_backward, schema_str, "unfold_backward(Tensor grad_in, int[] input_sizes, int dim, int size, int step) -> Tensor")

// aten::unfold_backward(Tensor grad_in, int[] input_sizes, int dim, int size, int step) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<unfold_backward::schema> create_unfold_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unfold_backward::name, unfold_backward::overload_name)
      .typed<unfold_backward::schema>();
}

// aten::unfold_backward(Tensor grad_in, int[] input_sizes, int dim, int size, int step) -> Tensor
at::Tensor unfold_backward::call(const at::Tensor & grad_in, at::IntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step) {
    static auto op = create_unfold_backward_typed_handle();
    return op.call(grad_in, input_sizes, dim, size, step);
}

// aten::unfold_backward(Tensor grad_in, int[] input_sizes, int dim, int size, int step) -> Tensor
at::Tensor unfold_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_in, at::IntArrayRef input_sizes, int64_t dim, int64_t size, int64_t step) {
    static auto op = create_unfold_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_in, input_sizes, dim, size, step);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(equal, name, "aten::equal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(equal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(equal, schema_str, "equal(Tensor self, Tensor other) -> bool")

// aten::equal(Tensor self, Tensor other) -> bool
static C10_NOINLINE c10::TypedOperatorHandle<equal::schema> create_equal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(equal::name, equal::overload_name)
      .typed<equal::schema>();
}

// aten::equal(Tensor self, Tensor other) -> bool
bool equal::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_equal_typed_handle();
    return op.call(self, other);
}

// aten::equal(Tensor self, Tensor other) -> bool
bool equal::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_equal_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Tensor_out, name, "aten::pow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Tensor_out, overload_name, "Tensor_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Tensor_out, schema_str, "pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)")

// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<pow_Tensor_Tensor_out::schema> create_pow_Tensor_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow_Tensor_Tensor_out::name, pow_Tensor_Tensor_out::overload_name)
      .typed<pow_Tensor_Tensor_out::schema>();
}

// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pow_Tensor_Tensor_out::call(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_pow_Tensor_Tensor_out_typed_handle();
    return op.call(self, exponent, out);
}

// aten::pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pow_Tensor_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_pow_Tensor_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Tensor, name, "aten::pow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Tensor, overload_name, "Tensor_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Tensor, schema_str, "pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor")

// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pow_Tensor_Tensor::schema> create_pow_Tensor_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow_Tensor_Tensor::name, pow_Tensor_Tensor::overload_name)
      .typed<pow_Tensor_Tensor::schema>();
}

// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
at::Tensor pow_Tensor_Tensor::call(const at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_pow_Tensor_Tensor_typed_handle();
    return op.call(self, exponent);
}

// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
at::Tensor pow_Tensor_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_pow_Tensor_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Scalar_out, name, "aten::pow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Scalar_out, schema_str, "pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)")

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<pow_Scalar_out::schema> create_pow_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow_Scalar_out::name, pow_Scalar_out::overload_name)
      .typed<pow_Scalar_out::schema>();
}

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pow_Scalar_out::call(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_pow_Scalar_out_typed_handle();
    return op.call(self, exponent, out);
}

// aten::pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pow_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_pow_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Scalar, name, "aten::pow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Scalar, schema_str, "pow.Scalar(Scalar self, Tensor exponent) -> Tensor")

// aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pow_Scalar::schema> create_pow_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow_Scalar::name, pow_Scalar::overload_name)
      .typed<pow_Scalar::schema>();
}

// aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor
at::Tensor pow_Scalar::call(const at::Scalar & self, const at::Tensor & exponent) {
    static auto op = create_pow_Scalar_typed_handle();
    return op.call(self, exponent);
}

// aten::pow.Scalar(Scalar self, Tensor exponent) -> Tensor
at::Tensor pow_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent) {
    static auto op = create_pow_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Scalar_out, name, "aten::pow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Scalar_out, overload_name, "Tensor_Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Scalar_out, schema_str, "pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)")

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<pow_Tensor_Scalar_out::schema> create_pow_Tensor_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow_Tensor_Scalar_out::name, pow_Tensor_Scalar_out::overload_name)
      .typed<pow_Tensor_Scalar_out::schema>();
}

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pow_Tensor_Scalar_out::call(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    static auto op = create_pow_Tensor_Scalar_out_typed_handle();
    return op.call(self, exponent, out);
}

// aten::pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & pow_Tensor_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    static auto op = create_pow_Tensor_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Scalar, name, "aten::pow")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow_Tensor_Scalar, schema_str, "pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor")

// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pow_Tensor_Scalar::schema> create_pow_Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow_Tensor_Scalar::name, pow_Tensor_Scalar::overload_name)
      .typed<pow_Tensor_Scalar::schema>();
}

// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
at::Tensor pow_Tensor_Scalar::call(const at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_pow_Tensor_Scalar_typed_handle();
    return op.call(self, exponent);
}

// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
at::Tensor pow_Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_pow_Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow__Scalar, name, "aten::pow_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow__Scalar, schema_str, "pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)")

// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<pow__Scalar::schema> create_pow__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow__Scalar::name, pow__Scalar::overload_name)
      .typed<pow__Scalar::schema>();
}

// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
at::Tensor & pow__Scalar::call(at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_pow__Scalar_typed_handle();
    return op.call(self, exponent);
}

// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
at::Tensor & pow__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_pow__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow__Tensor, name, "aten::pow_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pow__Tensor, schema_str, "pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)")

// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<pow__Tensor::schema> create_pow__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pow__Tensor::name, pow__Tensor::overload_name)
      .typed<pow__Tensor::schema>();
}

// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
at::Tensor & pow__Tensor::call(at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_pow__Tensor_typed_handle();
    return op.call(self, exponent);
}

// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
at::Tensor & pow__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_pow__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Tensor_out, name, "aten::float_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Tensor_out, overload_name, "Tensor_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Tensor_out, schema_str, "float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)")

// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<float_power_Tensor_Tensor_out::schema> create_float_power_Tensor_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power_Tensor_Tensor_out::name, float_power_Tensor_Tensor_out::overload_name)
      .typed<float_power_Tensor_Tensor_out::schema>();
}

// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & float_power_Tensor_Tensor_out::call(const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_float_power_Tensor_Tensor_out_typed_handle();
    return op.call(self, exponent, out);
}

// aten::float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & float_power_Tensor_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_float_power_Tensor_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Tensor, name, "aten::float_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Tensor, overload_name, "Tensor_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Tensor, schema_str, "float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor")

// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<float_power_Tensor_Tensor::schema> create_float_power_Tensor_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power_Tensor_Tensor::name, float_power_Tensor_Tensor::overload_name)
      .typed<float_power_Tensor_Tensor::schema>();
}

// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
at::Tensor float_power_Tensor_Tensor::call(const at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_float_power_Tensor_Tensor_typed_handle();
    return op.call(self, exponent);
}

// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
at::Tensor float_power_Tensor_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_float_power_Tensor_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Scalar_out, name, "aten::float_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Scalar_out, overload_name, "Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Scalar_out, schema_str, "float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)")

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<float_power_Scalar_out::schema> create_float_power_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power_Scalar_out::name, float_power_Scalar_out::overload_name)
      .typed<float_power_Scalar_out::schema>();
}

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & float_power_Scalar_out::call(const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_float_power_Scalar_out_typed_handle();
    return op.call(self, exponent, out);
}

// aten::float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & float_power_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent, at::Tensor & out) {
    static auto op = create_float_power_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Scalar, name, "aten::float_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Scalar, schema_str, "float_power.Scalar(Scalar self, Tensor exponent) -> Tensor")

// aten::float_power.Scalar(Scalar self, Tensor exponent) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<float_power_Scalar::schema> create_float_power_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power_Scalar::name, float_power_Scalar::overload_name)
      .typed<float_power_Scalar::schema>();
}

// aten::float_power.Scalar(Scalar self, Tensor exponent) -> Tensor
at::Tensor float_power_Scalar::call(const at::Scalar & self, const at::Tensor & exponent) {
    static auto op = create_float_power_Scalar_typed_handle();
    return op.call(self, exponent);
}

// aten::float_power.Scalar(Scalar self, Tensor exponent) -> Tensor
at::Tensor float_power_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & exponent) {
    static auto op = create_float_power_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Scalar_out, name, "aten::float_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Scalar_out, overload_name, "Tensor_Scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Scalar_out, schema_str, "float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)")

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<float_power_Tensor_Scalar_out::schema> create_float_power_Tensor_Scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power_Tensor_Scalar_out::name, float_power_Tensor_Scalar_out::overload_name)
      .typed<float_power_Tensor_Scalar_out::schema>();
}

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & float_power_Tensor_Scalar_out::call(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    static auto op = create_float_power_Tensor_Scalar_out_typed_handle();
    return op.call(self, exponent, out);
}

// aten::float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & float_power_Tensor_Scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    static auto op = create_float_power_Tensor_Scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Scalar, name, "aten::float_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Scalar, overload_name, "Tensor_Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power_Tensor_Scalar, schema_str, "float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor")

// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<float_power_Tensor_Scalar::schema> create_float_power_Tensor_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power_Tensor_Scalar::name, float_power_Tensor_Scalar::overload_name)
      .typed<float_power_Tensor_Scalar::schema>();
}

// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
at::Tensor float_power_Tensor_Scalar::call(const at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_float_power_Tensor_Scalar_typed_handle();
    return op.call(self, exponent);
}

// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
at::Tensor float_power_Tensor_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_float_power_Tensor_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power__Scalar, name, "aten::float_power_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power__Scalar, schema_str, "float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)")

// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<float_power__Scalar::schema> create_float_power__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power__Scalar::name, float_power__Scalar::overload_name)
      .typed<float_power__Scalar::schema>();
}

// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
at::Tensor & float_power__Scalar::call(at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_float_power__Scalar_typed_handle();
    return op.call(self, exponent);
}

// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
at::Tensor & float_power__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & exponent) {
    static auto op = create_float_power__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power__Tensor, name, "aten::float_power_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power__Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(float_power__Tensor, schema_str, "float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)")

// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<float_power__Tensor::schema> create_float_power__Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(float_power__Tensor::name, float_power__Tensor::overload_name)
      .typed<float_power__Tensor::schema>();
}

// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
at::Tensor & float_power__Tensor::call(at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_float_power__Tensor_typed_handle();
    return op.call(self, exponent);
}

// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
at::Tensor & float_power__Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & exponent) {
    static auto op = create_float_power__Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, exponent);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_, name, "aten::normal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_, schema_str, "normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)")

// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_::schema> create_normal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_::name, normal_::overload_name)
      .typed<normal_::schema>();
}

// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & normal_::call(at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    static auto op = create_normal__typed_handle();
    return op.call(self, mean, std, generator);
}

// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
at::Tensor & normal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, double mean, double std, c10::optional<at::Generator> generator) {
    static auto op = create_normal__typed_handle();
    return op.redispatch(dispatchKeySet, self, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float_out, overload_name, "Tensor_float_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float_out, schema_str, "normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_float_out::schema> create_normal_Tensor_float_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_float_out::name, normal_Tensor_float_out::overload_name)
      .typed<normal_Tensor_float_out::schema>();
}

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_float_out::call(const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_Tensor_float_out_typed_handle();
    return op.call(mean, std, generator, out);
}

// aten::normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_float_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_Tensor_float_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float, overload_name, "Tensor_float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_float, schema_str, "normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor")

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_float::schema> create_normal_Tensor_float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_float::name, normal_Tensor_float::overload_name)
      .typed<normal_Tensor_float::schema>();
}

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_float::call(const at::Tensor & mean, double std, c10::optional<at::Generator> generator) {
    static auto op = create_normal_Tensor_float_typed_handle();
    return op.call(mean, std, generator);
}

// aten::normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_float::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, double std, c10::optional<at::Generator> generator) {
    static auto op = create_normal_Tensor_float_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor_out, overload_name, "float_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor_out, schema_str, "normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_Tensor_out::schema> create_normal_float_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_Tensor_out::name, normal_float_Tensor_out::overload_name)
      .typed<normal_float_Tensor_out::schema>();
}

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_Tensor_out::call(double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_float_Tensor_out_typed_handle();
    return op.call(mean, std, generator, out);
}

// aten::normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_float_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor, overload_name, "float_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_Tensor, schema_str, "normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor")

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_Tensor::schema> create_normal_float_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_Tensor::name, normal_float_Tensor::overload_name)
      .typed<normal_float_Tensor::schema>();
}

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_float_Tensor::call(double mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    static auto op = create_normal_float_Tensor_typed_handle();
    return op.call(mean, std, generator);
}

// aten::normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_float_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    static auto op = create_normal_float_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor_out, overload_name, "Tensor_Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor_out, schema_str, "normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_Tensor_out::schema> create_normal_Tensor_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_Tensor_out::name, normal_Tensor_Tensor_out::overload_name)
      .typed<normal_Tensor_Tensor_out::schema>();
}

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_Tensor_out::call(const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_Tensor_Tensor_out_typed_handle();
    return op.call(mean, std, generator, out);
}

// aten::normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_Tensor_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_Tensor_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor, overload_name, "Tensor_Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_Tensor_Tensor, schema_str, "normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor")

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_Tensor_Tensor::schema> create_normal_Tensor_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_Tensor_Tensor::name, normal_Tensor_Tensor::overload_name)
      .typed<normal_Tensor_Tensor::schema>();
}

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_Tensor::call(const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    static auto op = create_normal_Tensor_Tensor_typed_handle();
    return op.call(mean, std, generator);
}

// aten::normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
at::Tensor normal_Tensor_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & mean, const at::Tensor & std, c10::optional<at::Generator> generator) {
    static auto op = create_normal_Tensor_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float, overload_name, "float_float")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float, schema_str, "normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_float::schema> create_normal_float_float_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_float::name, normal_float_float::overload_name)
      .typed<normal_float_float::schema>();
}

// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor normal_float_float::call(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_normal_float_float_typed_handle();
    return op.call(mean, std, size, generator, dtype, layout, device, pin_memory);
}

// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor normal_float_float::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_normal_float_float_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, size, generator, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float_out, name, "aten::normal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float_out, overload_name, "float_float_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(normal_float_float_out, schema_str, "normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)")

// aten::normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<normal_float_float_out::schema> create_normal_float_float_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(normal_float_float_out::name, normal_float_float_out::overload_name)
      .typed<normal_float_float_out::schema>();
}

// aten::normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_float_out::call(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_float_float_out_typed_handle();
    return op.call(mean, std, size, generator, out);
}

// aten::normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & normal_float_float_out::redispatch(c10::DispatchKeySet dispatchKeySet, double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_normal_float_float_out_typed_handle();
    return op.redispatch(dispatchKeySet, mean, std, size, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alias, name, "aten::alias")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alias, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(alias, schema_str, "alias(Tensor(a) self) -> Tensor(a)")

// aten::alias(Tensor(a) self) -> Tensor(a)
static C10_NOINLINE c10::TypedOperatorHandle<alias::schema> create_alias_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(alias::name, alias::overload_name)
      .typed<alias::schema>();
}

// aten::alias(Tensor(a) self) -> Tensor(a)
at::Tensor alias::call(const at::Tensor & self) {
    static auto op = create_alias_typed_handle();
    return op.call(self);
}

// aten::alias(Tensor(a) self) -> Tensor(a)
at::Tensor alias::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_alias_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_index_copy_, name, "aten::_index_copy_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_index_copy_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_index_copy_, schema_str, "_index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)")

// aten::_index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_index_copy_::schema> create__index_copy__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_index_copy_::name, _index_copy_::overload_name)
      .typed<_index_copy_::schema>();
}

// aten::_index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & _index_copy_::call(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create__index_copy__typed_handle();
    return op.call(self, dim, index, source);
}

// aten::_index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
at::Tensor & _index_copy_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    static auto op = create__index_copy__typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, index, source);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_amp_foreach_non_finite_check_and_unscale_, name, "aten::_amp_foreach_non_finite_check_and_unscale_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_amp_foreach_non_finite_check_and_unscale_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_amp_foreach_non_finite_check_and_unscale_, schema_str, "_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()")

// aten::_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_amp_foreach_non_finite_check_and_unscale_::schema> create__amp_foreach_non_finite_check_and_unscale__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_amp_foreach_non_finite_check_and_unscale_::name, _amp_foreach_non_finite_check_and_unscale_::overload_name)
      .typed<_amp_foreach_non_finite_check_and_unscale_::schema>();
}

// aten::_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()
void _amp_foreach_non_finite_check_and_unscale_::call(at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
    static auto op = create__amp_foreach_non_finite_check_and_unscale__typed_handle();
    return op.call(self, found_inf, inv_scale);
}

// aten::_amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()
void _amp_foreach_non_finite_check_and_unscale_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::Tensor & found_inf, const at::Tensor & inv_scale) {
    static auto op = create__amp_foreach_non_finite_check_and_unscale__typed_handle();
    return op.redispatch(dispatchKeySet, self, found_inf, inv_scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_amp_update_scale_, name, "aten::_amp_update_scale_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_amp_update_scale_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_amp_update_scale_, schema_str, "_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)")

// aten::_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_amp_update_scale_::schema> create__amp_update_scale__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_amp_update_scale_::name, _amp_update_scale_::overload_name)
      .typed<_amp_update_scale_::schema>();
}

// aten::_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)
at::Tensor & _amp_update_scale_::call(at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
    static auto op = create__amp_update_scale__typed_handle();
    return op.call(self, growth_tracker, found_inf, scale_growth_factor, scale_backoff_factor, growth_interval);
}

// aten::_amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -> Tensor(a!)
at::Tensor & _amp_update_scale_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Tensor & growth_tracker, const at::Tensor & found_inf, double scale_growth_factor, double scale_backoff_factor, int64_t growth_interval) {
    static auto op = create__amp_update_scale__typed_handle();
    return op.redispatch(dispatchKeySet, self, growth_tracker, found_inf, scale_growth_factor, scale_backoff_factor, growth_interval);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cat, name, "aten::_cat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cat, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cat, schema_str, "_cat(Tensor[] tensors, int dim=0) -> Tensor")

// aten::_cat(Tensor[] tensors, int dim=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_cat::schema> create__cat_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cat::name, _cat::overload_name)
      .typed<_cat::schema>();
}

// aten::_cat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor _cat::call(at::TensorList tensors, int64_t dim) {
    static auto op = create__cat_typed_handle();
    return op.call(tensors, dim);
}

// aten::_cat(Tensor[] tensors, int dim=0) -> Tensor
at::Tensor _cat::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim) {
    static auto op = create__cat_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cat_out, name, "aten::_cat")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cat_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_cat_out, schema_str, "_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_cat_out::schema> create__cat_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_cat_out::name, _cat_out::overload_name)
      .typed<_cat_out::schema>();
}

// aten::_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _cat_out::call(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create__cat_out_typed_handle();
    return op.call(tensors, dim, out);
}

// aten::_cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _cat_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, int64_t dim, at::Tensor & out) {
    static auto op = create__cat_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_Scalar, name, "aten::_foreach_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_Scalar, schema_str, "_foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]")

// aten::_foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_add_Scalar::schema> create__foreach_add_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_add_Scalar::name, _foreach_add_Scalar::overload_name)
      .typed<_foreach_add_Scalar::schema>();
}

// aten::_foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_add_Scalar::call(at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_add_Scalar_typed_handle();
    return op.call(tensors, scalar);
}

// aten::_foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_add_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_add_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__Scalar, name, "aten::_foreach_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__Scalar, schema_str, "_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_add__Scalar::schema> create__foreach_add__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_add__Scalar::name, _foreach_add__Scalar::overload_name)
      .typed<_foreach_add__Scalar::schema>();
}

// aten::_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_add__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_add__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_add__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_add__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_Scalar, schema_str, "_foreach_sub.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]")

// aten::_foreach_sub.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_Scalar::schema> create__foreach_sub_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_Scalar::name, _foreach_sub_Scalar::overload_name)
      .typed<_foreach_sub_Scalar::schema>();
}

// aten::_foreach_sub.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_Scalar::call(at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_sub_Scalar_typed_handle();
    return op.call(tensors, scalar);
}

// aten::_foreach_sub.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_sub_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__Scalar, name, "aten::_foreach_sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__Scalar, schema_str, "_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub__Scalar::schema> create__foreach_sub__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub__Scalar::name, _foreach_sub__Scalar::overload_name)
      .typed<_foreach_sub__Scalar::schema>();
}

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_sub__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_sub__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_sub__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_sub__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_Scalar, schema_str, "_foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]")

// aten::_foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_Scalar::schema> create__foreach_mul_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_Scalar::name, _foreach_mul_Scalar::overload_name)
      .typed<_foreach_mul_Scalar::schema>();
}

// aten::_foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_Scalar::call(at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_mul_Scalar_typed_handle();
    return op.call(tensors, scalar);
}

// aten::_foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_mul_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Scalar, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__Scalar, schema_str, "_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__Scalar::schema> create__foreach_mul__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__Scalar::name, _foreach_mul__Scalar::overload_name)
      .typed<_foreach_mul__Scalar::schema>();
}

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_mul__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_mul__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_mul__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_mul__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_Scalar, schema_str, "_foreach_div.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]")

// aten::_foreach_div.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_Scalar::schema> create__foreach_div_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_Scalar::name, _foreach_div_Scalar::overload_name)
      .typed<_foreach_div_Scalar::schema>();
}

// aten::_foreach_div.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_Scalar::call(at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_div_Scalar_typed_handle();
    return op.call(tensors, scalar);
}

// aten::_foreach_div.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, const at::Scalar & scalar) {
    static auto op = create__foreach_div_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Scalar, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__Scalar, schema_str, "_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()")

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__Scalar::schema> create__foreach_div__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__Scalar::name, _foreach_div__Scalar::overload_name)
      .typed<_foreach_div__Scalar::schema>();
}

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_div__Scalar::call(at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_div__Scalar_typed_handle();
    return op.call(self, scalar);
}

// aten::_foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
void _foreach_div__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Scalar & scalar) {
    static auto op = create__foreach_div__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalar);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_List, name, "aten::_foreach_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_List, schema_str, "_foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]")

// aten::_foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_add_List::schema> create__foreach_add_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_add_List::name, _foreach_add_List::overload_name)
      .typed<_foreach_add_List::schema>();
}

// aten::_foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_add_List::call(at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
    static auto op = create__foreach_add_List_typed_handle();
    return op.call(tensors1, tensors2, alpha);
}

// aten::_foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_add_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
    static auto op = create__foreach_add_List_typed_handle();
    return op.redispatch(dispatchKeySet, tensors1, tensors2, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__List, name, "aten::_foreach_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__List, schema_str, "_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()")

// aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_add__List::schema> create__foreach_add__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_add__List::name, _foreach_add__List::overload_name)
      .typed<_foreach_add__List::schema>();
}

// aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
void _foreach_add__List::call(at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    static auto op = create__foreach_add__List_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
void _foreach_add__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    static auto op = create__foreach_add__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_List, schema_str, "_foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]")

// aten::_foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_List::schema> create__foreach_sub_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_List::name, _foreach_sub_List::overload_name)
      .typed<_foreach_sub_List::schema>();
}

// aten::_foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_List::call(at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
    static auto op = create__foreach_sub_List_typed_handle();
    return op.call(tensors1, tensors2, alpha);
}

// aten::_foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2, const at::Scalar & alpha) {
    static auto op = create__foreach_sub_List_typed_handle();
    return op.redispatch(dispatchKeySet, tensors1, tensors2, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__List, name, "aten::_foreach_sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__List, schema_str, "_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()")

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub__List::schema> create__foreach_sub__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub__List::name, _foreach_sub__List::overload_name)
      .typed<_foreach_sub__List::schema>();
}

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
void _foreach_sub__List::call(at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    static auto op = create__foreach_sub__List_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
void _foreach_sub__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other, const at::Scalar & alpha) {
    static auto op = create__foreach_sub__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_List, schema_str, "_foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]")

// aten::_foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_List::schema> create__foreach_mul_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_List::name, _foreach_mul_List::overload_name)
      .typed<_foreach_mul_List::schema>();
}

// aten::_foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_List::call(at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_mul_List_typed_handle();
    return op.call(tensors1, tensors2);
}

// aten::_foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_mul_List_typed_handle();
    return op.redispatch(dispatchKeySet, tensors1, tensors2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__List, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__List, schema_str, "_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()")

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__List::schema> create__foreach_mul__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__List::name, _foreach_mul__List::overload_name)
      .typed<_foreach_mul__List::schema>();
}

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_mul__List::call(at::TensorList self, at::TensorList other) {
    static auto op = create__foreach_mul__List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_mul__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    static auto op = create__foreach_mul__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_List, schema_str, "_foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]")

// aten::_foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_List::schema> create__foreach_div_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_List::name, _foreach_div_List::overload_name)
      .typed<_foreach_div_List::schema>();
}

// aten::_foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_List::call(at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_div_List_typed_handle();
    return op.call(tensors1, tensors2);
}

// aten::_foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_div_List_typed_handle();
    return op.redispatch(dispatchKeySet, tensors1, tensors2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__List, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__List, schema_str, "_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()")

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__List::schema> create__foreach_div__List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__List::name, _foreach_div__List::overload_name)
      .typed<_foreach_div__List::schema>();
}

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_div__List::call(at::TensorList self, at::TensorList other) {
    static auto op = create__foreach_div__List_typed_handle();
    return op.call(self, other);
}

// aten::_foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
void _foreach_div__List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList other) {
    static auto op = create__foreach_div__List_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_ScalarList, name, "aten::_foreach_add")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add_ScalarList, schema_str, "_foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_add_ScalarList::schema> create__foreach_add_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_add_ScalarList::name, _foreach_add_ScalarList::overload_name)
      .typed<_foreach_add_ScalarList::schema>();
}

// aten::_foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_add_ScalarList::call(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_add_ScalarList_typed_handle();
    return op.call(tensors, scalars);
}

// aten::_foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_add_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_add_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__ScalarList, name, "aten::_foreach_add_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_add__ScalarList, schema_str, "_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_add__ScalarList::schema> create__foreach_add__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_add__ScalarList::name, _foreach_add__ScalarList::overload_name)
      .typed<_foreach_add__ScalarList::schema>();
}

// aten::_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_add__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_add__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_add__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_add__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList, name, "aten::_foreach_sub")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub_ScalarList, schema_str, "_foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub_ScalarList::schema> create__foreach_sub_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub_ScalarList::name, _foreach_sub_ScalarList::overload_name)
      .typed<_foreach_sub_ScalarList::schema>();
}

// aten::_foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_ScalarList::call(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_sub_ScalarList_typed_handle();
    return op.call(tensors, scalars);
}

// aten::_foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_sub_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_sub_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__ScalarList, name, "aten::_foreach_sub_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sub__ScalarList, schema_str, "_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sub__ScalarList::schema> create__foreach_sub__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sub__ScalarList::name, _foreach_sub__ScalarList::overload_name)
      .typed<_foreach_sub__ScalarList::schema>();
}

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_sub__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_sub__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_sub__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_sub__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList, name, "aten::_foreach_div")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div_ScalarList, schema_str, "_foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div_ScalarList::schema> create__foreach_div_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div_ScalarList::name, _foreach_div_ScalarList::overload_name)
      .typed<_foreach_div_ScalarList::schema>();
}

// aten::_foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_ScalarList::call(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_div_ScalarList_typed_handle();
    return op.call(tensors, scalars);
}

// aten::_foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_div_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_div_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__ScalarList, name, "aten::_foreach_div_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_div__ScalarList, schema_str, "_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_div__ScalarList::schema> create__foreach_div__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_div__ScalarList::name, _foreach_div__ScalarList::overload_name)
      .typed<_foreach_div__ScalarList::schema>();
}

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_div__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_div__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_div__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_div__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList, name, "aten::_foreach_mul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul_ScalarList, schema_str, "_foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul_ScalarList::schema> create__foreach_mul_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul_ScalarList::name, _foreach_mul_ScalarList::overload_name)
      .typed<_foreach_mul_ScalarList::schema>();
}

// aten::_foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_ScalarList::call(at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_mul_ScalarList_typed_handle();
    return op.call(tensors, scalars);
}

// aten::_foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_mul_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_mul_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__ScalarList, name, "aten::_foreach_mul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_mul__ScalarList, schema_str, "_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()")

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_mul__ScalarList::schema> create__foreach_mul__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_mul__ScalarList::name, _foreach_mul__ScalarList::overload_name)
      .typed<_foreach_mul__ScalarList::schema>();
}

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_mul__ScalarList::call(at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_mul__ScalarList_typed_handle();
    return op.call(self, scalars);
}

// aten::_foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
void _foreach_mul__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_mul__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_exp, name, "aten::_foreach_exp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_exp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_exp, schema_str, "_foreach_exp(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_exp(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_exp::schema> create__foreach_exp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_exp::name, _foreach_exp::overload_name)
      .typed<_foreach_exp::schema>();
}

// aten::_foreach_exp(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_exp::call(at::TensorList tensors) {
    static auto op = create__foreach_exp_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_exp(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_exp::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_exp_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_, name, "aten::_foreach_zero_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_zero_, schema_str, "_foreach_zero_(Tensor(a!)[] self) -> ()")

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_zero_::schema> create__foreach_zero__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_zero_::name, _foreach_zero_::overload_name)
      .typed<_foreach_zero_::schema>();
}

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
void _foreach_zero_::call(at::TensorList self) {
    static auto op = create__foreach_zero__typed_handle();
    return op.call(self);
}

// aten::_foreach_zero_(Tensor(a!)[] self) -> ()
void _foreach_zero_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_zero__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_exp_, name, "aten::_foreach_exp_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_exp_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_exp_, schema_str, "_foreach_exp_(Tensor(a!)[] self) -> ()")

// aten::_foreach_exp_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_exp_::schema> create__foreach_exp__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_exp_::name, _foreach_exp_::overload_name)
      .typed<_foreach_exp_::schema>();
}

// aten::_foreach_exp_(Tensor(a!)[] self) -> ()
void _foreach_exp_::call(at::TensorList self) {
    static auto op = create__foreach_exp__typed_handle();
    return op.call(self);
}

// aten::_foreach_exp_(Tensor(a!)[] self) -> ()
void _foreach_exp_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_exp__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sqrt, name, "aten::_foreach_sqrt")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sqrt, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sqrt, schema_str, "_foreach_sqrt(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_sqrt(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sqrt::schema> create__foreach_sqrt_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sqrt::name, _foreach_sqrt::overload_name)
      .typed<_foreach_sqrt::schema>();
}

// aten::_foreach_sqrt(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sqrt::call(at::TensorList tensors) {
    static auto op = create__foreach_sqrt_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_sqrt(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sqrt::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_sqrt_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sqrt_, name, "aten::_foreach_sqrt_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sqrt_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sqrt_, schema_str, "_foreach_sqrt_(Tensor(a!)[] self) -> ()")

// aten::_foreach_sqrt_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sqrt_::schema> create__foreach_sqrt__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sqrt_::name, _foreach_sqrt_::overload_name)
      .typed<_foreach_sqrt_::schema>();
}

// aten::_foreach_sqrt_(Tensor(a!)[] self) -> ()
void _foreach_sqrt_::call(at::TensorList self) {
    static auto op = create__foreach_sqrt__typed_handle();
    return op.call(self);
}

// aten::_foreach_sqrt_(Tensor(a!)[] self) -> ()
void _foreach_sqrt_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_sqrt__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_abs, name, "aten::_foreach_abs")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_abs, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_abs, schema_str, "_foreach_abs(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_abs(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_abs::schema> create__foreach_abs_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_abs::name, _foreach_abs::overload_name)
      .typed<_foreach_abs::schema>();
}

// aten::_foreach_abs(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_abs::call(at::TensorList tensors) {
    static auto op = create__foreach_abs_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_abs(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_abs::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_abs_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_abs_, name, "aten::_foreach_abs_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_abs_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_abs_, schema_str, "_foreach_abs_(Tensor(a!)[] self) -> ()")

// aten::_foreach_abs_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_abs_::schema> create__foreach_abs__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_abs_::name, _foreach_abs_::overload_name)
      .typed<_foreach_abs_::schema>();
}

// aten::_foreach_abs_(Tensor(a!)[] self) -> ()
void _foreach_abs_::call(at::TensorList self) {
    static auto op = create__foreach_abs__typed_handle();
    return op.call(self);
}

// aten::_foreach_abs_(Tensor(a!)[] self) -> ()
void _foreach_abs_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_abs__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos, name, "aten::_foreach_acos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos, schema_str, "_foreach_acos(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_acos(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_acos::schema> create__foreach_acos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_acos::name, _foreach_acos::overload_name)
      .typed<_foreach_acos::schema>();
}

// aten::_foreach_acos(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_acos::call(at::TensorList tensors) {
    static auto op = create__foreach_acos_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_acos(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_acos::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_acos_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_, name, "aten::_foreach_acos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_acos_, schema_str, "_foreach_acos_(Tensor(a!)[] self) -> ()")

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_acos_::schema> create__foreach_acos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_acos_::name, _foreach_acos_::overload_name)
      .typed<_foreach_acos_::schema>();
}

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
void _foreach_acos_::call(at::TensorList self) {
    static auto op = create__foreach_acos__typed_handle();
    return op.call(self);
}

// aten::_foreach_acos_(Tensor(a!)[] self) -> ()
void _foreach_acos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_acos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin, name, "aten::_foreach_asin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin, schema_str, "_foreach_asin(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_asin(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_asin::schema> create__foreach_asin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_asin::name, _foreach_asin::overload_name)
      .typed<_foreach_asin::schema>();
}

// aten::_foreach_asin(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_asin::call(at::TensorList tensors) {
    static auto op = create__foreach_asin_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_asin(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_asin::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_asin_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_, name, "aten::_foreach_asin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_asin_, schema_str, "_foreach_asin_(Tensor(a!)[] self) -> ()")

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_asin_::schema> create__foreach_asin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_asin_::name, _foreach_asin_::overload_name)
      .typed<_foreach_asin_::schema>();
}

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
void _foreach_asin_::call(at::TensorList self) {
    static auto op = create__foreach_asin__typed_handle();
    return op.call(self);
}

// aten::_foreach_asin_(Tensor(a!)[] self) -> ()
void _foreach_asin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_asin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan, name, "aten::_foreach_atan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan, schema_str, "_foreach_atan(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_atan(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_atan::schema> create__foreach_atan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_atan::name, _foreach_atan::overload_name)
      .typed<_foreach_atan::schema>();
}

// aten::_foreach_atan(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_atan::call(at::TensorList tensors) {
    static auto op = create__foreach_atan_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_atan(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_atan::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_atan_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_, name, "aten::_foreach_atan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_atan_, schema_str, "_foreach_atan_(Tensor(a!)[] self) -> ()")

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_atan_::schema> create__foreach_atan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_atan_::name, _foreach_atan_::overload_name)
      .typed<_foreach_atan_::schema>();
}

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
void _foreach_atan_::call(at::TensorList self) {
    static auto op = create__foreach_atan__typed_handle();
    return op.call(self);
}

// aten::_foreach_atan_(Tensor(a!)[] self) -> ()
void _foreach_atan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_atan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil, name, "aten::_foreach_ceil")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil, schema_str, "_foreach_ceil(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_ceil(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_ceil::schema> create__foreach_ceil_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_ceil::name, _foreach_ceil::overload_name)
      .typed<_foreach_ceil::schema>();
}

// aten::_foreach_ceil(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_ceil::call(at::TensorList tensors) {
    static auto op = create__foreach_ceil_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_ceil(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_ceil::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_ceil_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_, name, "aten::_foreach_ceil_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_ceil_, schema_str, "_foreach_ceil_(Tensor(a!)[] self) -> ()")

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_ceil_::schema> create__foreach_ceil__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_ceil_::name, _foreach_ceil_::overload_name)
      .typed<_foreach_ceil_::schema>();
}

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
void _foreach_ceil_::call(at::TensorList self) {
    static auto op = create__foreach_ceil__typed_handle();
    return op.call(self);
}

// aten::_foreach_ceil_(Tensor(a!)[] self) -> ()
void _foreach_ceil_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_ceil__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos, name, "aten::_foreach_cos")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos, schema_str, "_foreach_cos(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_cos(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cos::schema> create__foreach_cos_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cos::name, _foreach_cos::overload_name)
      .typed<_foreach_cos::schema>();
}

// aten::_foreach_cos(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_cos::call(at::TensorList tensors) {
    static auto op = create__foreach_cos_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_cos(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_cos::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_cos_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_, name, "aten::_foreach_cos_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cos_, schema_str, "_foreach_cos_(Tensor(a!)[] self) -> ()")

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cos_::schema> create__foreach_cos__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cos_::name, _foreach_cos_::overload_name)
      .typed<_foreach_cos_::schema>();
}

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
void _foreach_cos_::call(at::TensorList self) {
    static auto op = create__foreach_cos__typed_handle();
    return op.call(self);
}

// aten::_foreach_cos_(Tensor(a!)[] self) -> ()
void _foreach_cos_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_cos__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cosh, name, "aten::_foreach_cosh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cosh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cosh, schema_str, "_foreach_cosh(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_cosh(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cosh::schema> create__foreach_cosh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cosh::name, _foreach_cosh::overload_name)
      .typed<_foreach_cosh::schema>();
}

// aten::_foreach_cosh(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_cosh::call(at::TensorList tensors) {
    static auto op = create__foreach_cosh_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_cosh(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_cosh::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_cosh_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cosh_, name, "aten::_foreach_cosh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cosh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_cosh_, schema_str, "_foreach_cosh_(Tensor(a!)[] self) -> ()")

// aten::_foreach_cosh_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_cosh_::schema> create__foreach_cosh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_cosh_::name, _foreach_cosh_::overload_name)
      .typed<_foreach_cosh_::schema>();
}

// aten::_foreach_cosh_(Tensor(a!)[] self) -> ()
void _foreach_cosh_::call(at::TensorList self) {
    static auto op = create__foreach_cosh__typed_handle();
    return op.call(self);
}

// aten::_foreach_cosh_(Tensor(a!)[] self) -> ()
void _foreach_cosh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_cosh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf, name, "aten::_foreach_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf, schema_str, "_foreach_erf(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_erf(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erf::schema> create__foreach_erf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erf::name, _foreach_erf::overload_name)
      .typed<_foreach_erf::schema>();
}

// aten::_foreach_erf(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_erf::call(at::TensorList tensors) {
    static auto op = create__foreach_erf_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_erf(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_erf::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_erf_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_, name, "aten::_foreach_erf_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erf_, schema_str, "_foreach_erf_(Tensor(a!)[] self) -> ()")

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erf_::schema> create__foreach_erf__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erf_::name, _foreach_erf_::overload_name)
      .typed<_foreach_erf_::schema>();
}

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
void _foreach_erf_::call(at::TensorList self) {
    static auto op = create__foreach_erf__typed_handle();
    return op.call(self);
}

// aten::_foreach_erf_(Tensor(a!)[] self) -> ()
void _foreach_erf_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_erf__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erfc, name, "aten::_foreach_erfc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erfc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erfc, schema_str, "_foreach_erfc(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_erfc(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erfc::schema> create__foreach_erfc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erfc::name, _foreach_erfc::overload_name)
      .typed<_foreach_erfc::schema>();
}

// aten::_foreach_erfc(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_erfc::call(at::TensorList tensors) {
    static auto op = create__foreach_erfc_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_erfc(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_erfc::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_erfc_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erfc_, name, "aten::_foreach_erfc_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erfc_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_erfc_, schema_str, "_foreach_erfc_(Tensor(a!)[] self) -> ()")

// aten::_foreach_erfc_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_erfc_::schema> create__foreach_erfc__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_erfc_::name, _foreach_erfc_::overload_name)
      .typed<_foreach_erfc_::schema>();
}

// aten::_foreach_erfc_(Tensor(a!)[] self) -> ()
void _foreach_erfc_::call(at::TensorList self) {
    static auto op = create__foreach_erfc__typed_handle();
    return op.call(self);
}

// aten::_foreach_erfc_(Tensor(a!)[] self) -> ()
void _foreach_erfc_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_erfc__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_expm1, name, "aten::_foreach_expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_expm1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_expm1, schema_str, "_foreach_expm1(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_expm1(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_expm1::schema> create__foreach_expm1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_expm1::name, _foreach_expm1::overload_name)
      .typed<_foreach_expm1::schema>();
}

// aten::_foreach_expm1(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_expm1::call(at::TensorList tensors) {
    static auto op = create__foreach_expm1_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_expm1(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_expm1::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_expm1_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_expm1_, name, "aten::_foreach_expm1_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_expm1_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_expm1_, schema_str, "_foreach_expm1_(Tensor(a!)[] self) -> ()")

// aten::_foreach_expm1_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_expm1_::schema> create__foreach_expm1__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_expm1_::name, _foreach_expm1_::overload_name)
      .typed<_foreach_expm1_::schema>();
}

// aten::_foreach_expm1_(Tensor(a!)[] self) -> ()
void _foreach_expm1_::call(at::TensorList self) {
    static auto op = create__foreach_expm1__typed_handle();
    return op.call(self);
}

// aten::_foreach_expm1_(Tensor(a!)[] self) -> ()
void _foreach_expm1_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_expm1__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor, name, "aten::_foreach_floor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor, schema_str, "_foreach_floor(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_floor(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_floor::schema> create__foreach_floor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_floor::name, _foreach_floor::overload_name)
      .typed<_foreach_floor::schema>();
}

// aten::_foreach_floor(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_floor::call(at::TensorList tensors) {
    static auto op = create__foreach_floor_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_floor(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_floor::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_floor_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_, name, "aten::_foreach_floor_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_floor_, schema_str, "_foreach_floor_(Tensor(a!)[] self) -> ()")

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_floor_::schema> create__foreach_floor__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_floor_::name, _foreach_floor_::overload_name)
      .typed<_foreach_floor_::schema>();
}

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
void _foreach_floor_::call(at::TensorList self) {
    static auto op = create__foreach_floor__typed_handle();
    return op.call(self);
}

// aten::_foreach_floor_(Tensor(a!)[] self) -> ()
void _foreach_floor_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_floor__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log, name, "aten::_foreach_log")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log, schema_str, "_foreach_log(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_log(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log::schema> create__foreach_log_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log::name, _foreach_log::overload_name)
      .typed<_foreach_log::schema>();
}

// aten::_foreach_log(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log::call(at::TensorList tensors) {
    static auto op = create__foreach_log_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_log(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_log_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log_, name, "aten::_foreach_log_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log_, schema_str, "_foreach_log_(Tensor(a!)[] self) -> ()")

// aten::_foreach_log_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log_::schema> create__foreach_log__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log_::name, _foreach_log_::overload_name)
      .typed<_foreach_log_::schema>();
}

// aten::_foreach_log_(Tensor(a!)[] self) -> ()
void _foreach_log_::call(at::TensorList self) {
    static auto op = create__foreach_log__typed_handle();
    return op.call(self);
}

// aten::_foreach_log_(Tensor(a!)[] self) -> ()
void _foreach_log_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_log__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log10, name, "aten::_foreach_log10")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log10, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log10, schema_str, "_foreach_log10(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_log10(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log10::schema> create__foreach_log10_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log10::name, _foreach_log10::overload_name)
      .typed<_foreach_log10::schema>();
}

// aten::_foreach_log10(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log10::call(at::TensorList tensors) {
    static auto op = create__foreach_log10_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_log10(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log10::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_log10_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log10_, name, "aten::_foreach_log10_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log10_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log10_, schema_str, "_foreach_log10_(Tensor(a!)[] self) -> ()")

// aten::_foreach_log10_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log10_::schema> create__foreach_log10__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log10_::name, _foreach_log10_::overload_name)
      .typed<_foreach_log10_::schema>();
}

// aten::_foreach_log10_(Tensor(a!)[] self) -> ()
void _foreach_log10_::call(at::TensorList self) {
    static auto op = create__foreach_log10__typed_handle();
    return op.call(self);
}

// aten::_foreach_log10_(Tensor(a!)[] self) -> ()
void _foreach_log10_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_log10__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log1p, name, "aten::_foreach_log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log1p, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log1p, schema_str, "_foreach_log1p(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_log1p(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log1p::schema> create__foreach_log1p_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log1p::name, _foreach_log1p::overload_name)
      .typed<_foreach_log1p::schema>();
}

// aten::_foreach_log1p(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log1p::call(at::TensorList tensors) {
    static auto op = create__foreach_log1p_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_log1p(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log1p::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_log1p_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log1p_, name, "aten::_foreach_log1p_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log1p_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log1p_, schema_str, "_foreach_log1p_(Tensor(a!)[] self) -> ()")

// aten::_foreach_log1p_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log1p_::schema> create__foreach_log1p__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log1p_::name, _foreach_log1p_::overload_name)
      .typed<_foreach_log1p_::schema>();
}

// aten::_foreach_log1p_(Tensor(a!)[] self) -> ()
void _foreach_log1p_::call(at::TensorList self) {
    static auto op = create__foreach_log1p__typed_handle();
    return op.call(self);
}

// aten::_foreach_log1p_(Tensor(a!)[] self) -> ()
void _foreach_log1p_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_log1p__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2, name, "aten::_foreach_log2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2, schema_str, "_foreach_log2(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_log2(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log2::schema> create__foreach_log2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log2::name, _foreach_log2::overload_name)
      .typed<_foreach_log2::schema>();
}

// aten::_foreach_log2(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log2::call(at::TensorList tensors) {
    static auto op = create__foreach_log2_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_log2(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_log2::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_log2_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_, name, "aten::_foreach_log2_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_log2_, schema_str, "_foreach_log2_(Tensor(a!)[] self) -> ()")

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_log2_::schema> create__foreach_log2__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_log2_::name, _foreach_log2_::overload_name)
      .typed<_foreach_log2_::schema>();
}

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
void _foreach_log2_::call(at::TensorList self) {
    static auto op = create__foreach_log2__typed_handle();
    return op.call(self);
}

// aten::_foreach_log2_(Tensor(a!)[] self) -> ()
void _foreach_log2_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_log2__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_neg, name, "aten::_foreach_neg")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_neg, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_neg, schema_str, "_foreach_neg(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_neg(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_neg::schema> create__foreach_neg_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_neg::name, _foreach_neg::overload_name)
      .typed<_foreach_neg::schema>();
}

// aten::_foreach_neg(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_neg::call(at::TensorList tensors) {
    static auto op = create__foreach_neg_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_neg(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_neg::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_neg_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_neg_, name, "aten::_foreach_neg_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_neg_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_neg_, schema_str, "_foreach_neg_(Tensor(a!)[] self) -> ()")

// aten::_foreach_neg_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_neg_::schema> create__foreach_neg__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_neg_::name, _foreach_neg_::overload_name)
      .typed<_foreach_neg_::schema>();
}

// aten::_foreach_neg_(Tensor(a!)[] self) -> ()
void _foreach_neg_::call(at::TensorList self) {
    static auto op = create__foreach_neg__typed_handle();
    return op.call(self);
}

// aten::_foreach_neg_(Tensor(a!)[] self) -> ()
void _foreach_neg_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_neg__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tan, name, "aten::_foreach_tan")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tan, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tan, schema_str, "_foreach_tan(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_tan(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tan::schema> create__foreach_tan_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tan::name, _foreach_tan::overload_name)
      .typed<_foreach_tan::schema>();
}

// aten::_foreach_tan(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_tan::call(at::TensorList tensors) {
    static auto op = create__foreach_tan_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_tan(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_tan::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_tan_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tan_, name, "aten::_foreach_tan_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tan_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tan_, schema_str, "_foreach_tan_(Tensor(a!)[] self) -> ()")

// aten::_foreach_tan_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tan_::schema> create__foreach_tan__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tan_::name, _foreach_tan_::overload_name)
      .typed<_foreach_tan_::schema>();
}

// aten::_foreach_tan_(Tensor(a!)[] self) -> ()
void _foreach_tan_::call(at::TensorList self) {
    static auto op = create__foreach_tan__typed_handle();
    return op.call(self);
}

// aten::_foreach_tan_(Tensor(a!)[] self) -> ()
void _foreach_tan_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_tan__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh, name, "aten::_foreach_tanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh, schema_str, "_foreach_tanh(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_tanh(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tanh::schema> create__foreach_tanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tanh::name, _foreach_tanh::overload_name)
      .typed<_foreach_tanh::schema>();
}

// aten::_foreach_tanh(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_tanh::call(at::TensorList tensors) {
    static auto op = create__foreach_tanh_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_tanh(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_tanh::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_tanh_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_, name, "aten::_foreach_tanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_tanh_, schema_str, "_foreach_tanh_(Tensor(a!)[] self) -> ()")

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_tanh_::schema> create__foreach_tanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_tanh_::name, _foreach_tanh_::overload_name)
      .typed<_foreach_tanh_::schema>();
}

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
void _foreach_tanh_::call(at::TensorList self) {
    static auto op = create__foreach_tanh__typed_handle();
    return op.call(self);
}

// aten::_foreach_tanh_(Tensor(a!)[] self) -> ()
void _foreach_tanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_tanh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sin, name, "aten::_foreach_sin")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sin, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sin, schema_str, "_foreach_sin(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_sin(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sin::schema> create__foreach_sin_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sin::name, _foreach_sin::overload_name)
      .typed<_foreach_sin::schema>();
}

// aten::_foreach_sin(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sin::call(at::TensorList tensors) {
    static auto op = create__foreach_sin_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_sin(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sin::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_sin_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sin_, name, "aten::_foreach_sin_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sin_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sin_, schema_str, "_foreach_sin_(Tensor(a!)[] self) -> ()")

// aten::_foreach_sin_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sin_::schema> create__foreach_sin__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sin_::name, _foreach_sin_::overload_name)
      .typed<_foreach_sin_::schema>();
}

// aten::_foreach_sin_(Tensor(a!)[] self) -> ()
void _foreach_sin_::call(at::TensorList self) {
    static auto op = create__foreach_sin__typed_handle();
    return op.call(self);
}

// aten::_foreach_sin_(Tensor(a!)[] self) -> ()
void _foreach_sin_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_sin__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sinh, name, "aten::_foreach_sinh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sinh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sinh, schema_str, "_foreach_sinh(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_sinh(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sinh::schema> create__foreach_sinh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sinh::name, _foreach_sinh::overload_name)
      .typed<_foreach_sinh::schema>();
}

// aten::_foreach_sinh(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sinh::call(at::TensorList tensors) {
    static auto op = create__foreach_sinh_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_sinh(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sinh::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_sinh_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sinh_, name, "aten::_foreach_sinh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sinh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sinh_, schema_str, "_foreach_sinh_(Tensor(a!)[] self) -> ()")

// aten::_foreach_sinh_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sinh_::schema> create__foreach_sinh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sinh_::name, _foreach_sinh_::overload_name)
      .typed<_foreach_sinh_::schema>();
}

// aten::_foreach_sinh_(Tensor(a!)[] self) -> ()
void _foreach_sinh_::call(at::TensorList self) {
    static auto op = create__foreach_sinh__typed_handle();
    return op.call(self);
}

// aten::_foreach_sinh_(Tensor(a!)[] self) -> ()
void _foreach_sinh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_sinh__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_round, name, "aten::_foreach_round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_round, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_round, schema_str, "_foreach_round(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_round(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_round::schema> create__foreach_round_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_round::name, _foreach_round::overload_name)
      .typed<_foreach_round::schema>();
}

// aten::_foreach_round(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_round::call(at::TensorList tensors) {
    static auto op = create__foreach_round_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_round(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_round::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_round_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_round_, name, "aten::_foreach_round_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_round_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_round_, schema_str, "_foreach_round_(Tensor(a!)[] self) -> ()")

// aten::_foreach_round_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_round_::schema> create__foreach_round__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_round_::name, _foreach_round_::overload_name)
      .typed<_foreach_round_::schema>();
}

// aten::_foreach_round_(Tensor(a!)[] self) -> ()
void _foreach_round_::call(at::TensorList self) {
    static auto op = create__foreach_round__typed_handle();
    return op.call(self);
}

// aten::_foreach_round_(Tensor(a!)[] self) -> ()
void _foreach_round_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_round__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_lgamma, name, "aten::_foreach_lgamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_lgamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_lgamma, schema_str, "_foreach_lgamma(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_lgamma(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_lgamma::schema> create__foreach_lgamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_lgamma::name, _foreach_lgamma::overload_name)
      .typed<_foreach_lgamma::schema>();
}

// aten::_foreach_lgamma(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_lgamma::call(at::TensorList tensors) {
    static auto op = create__foreach_lgamma_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_lgamma(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_lgamma::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_lgamma_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_lgamma_, name, "aten::_foreach_lgamma_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_lgamma_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_lgamma_, schema_str, "_foreach_lgamma_(Tensor(a!)[] self) -> ()")

// aten::_foreach_lgamma_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_lgamma_::schema> create__foreach_lgamma__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_lgamma_::name, _foreach_lgamma_::overload_name)
      .typed<_foreach_lgamma_::schema>();
}

// aten::_foreach_lgamma_(Tensor(a!)[] self) -> ()
void _foreach_lgamma_::call(at::TensorList self) {
    static auto op = create__foreach_lgamma__typed_handle();
    return op.call(self);
}

// aten::_foreach_lgamma_(Tensor(a!)[] self) -> ()
void _foreach_lgamma_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_lgamma__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_frac, name, "aten::_foreach_frac")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_frac, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_frac, schema_str, "_foreach_frac(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_frac(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_frac::schema> create__foreach_frac_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_frac::name, _foreach_frac::overload_name)
      .typed<_foreach_frac::schema>();
}

// aten::_foreach_frac(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_frac::call(at::TensorList tensors) {
    static auto op = create__foreach_frac_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_frac(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_frac::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_frac_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_frac_, name, "aten::_foreach_frac_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_frac_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_frac_, schema_str, "_foreach_frac_(Tensor(a!)[] self) -> ()")

// aten::_foreach_frac_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_frac_::schema> create__foreach_frac__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_frac_::name, _foreach_frac_::overload_name)
      .typed<_foreach_frac_::schema>();
}

// aten::_foreach_frac_(Tensor(a!)[] self) -> ()
void _foreach_frac_::call(at::TensorList self) {
    static auto op = create__foreach_frac__typed_handle();
    return op.call(self);
}

// aten::_foreach_frac_(Tensor(a!)[] self) -> ()
void _foreach_frac_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_frac__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_reciprocal, name, "aten::_foreach_reciprocal")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_reciprocal, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_reciprocal, schema_str, "_foreach_reciprocal(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_reciprocal(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_reciprocal::schema> create__foreach_reciprocal_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_reciprocal::name, _foreach_reciprocal::overload_name)
      .typed<_foreach_reciprocal::schema>();
}

// aten::_foreach_reciprocal(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_reciprocal::call(at::TensorList tensors) {
    static auto op = create__foreach_reciprocal_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_reciprocal(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_reciprocal::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_reciprocal_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_reciprocal_, name, "aten::_foreach_reciprocal_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_reciprocal_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_reciprocal_, schema_str, "_foreach_reciprocal_(Tensor(a!)[] self) -> ()")

// aten::_foreach_reciprocal_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_reciprocal_::schema> create__foreach_reciprocal__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_reciprocal_::name, _foreach_reciprocal_::overload_name)
      .typed<_foreach_reciprocal_::schema>();
}

// aten::_foreach_reciprocal_(Tensor(a!)[] self) -> ()
void _foreach_reciprocal_::call(at::TensorList self) {
    static auto op = create__foreach_reciprocal__typed_handle();
    return op.call(self);
}

// aten::_foreach_reciprocal_(Tensor(a!)[] self) -> ()
void _foreach_reciprocal_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_reciprocal__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sigmoid, name, "aten::_foreach_sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sigmoid, schema_str, "_foreach_sigmoid(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_sigmoid(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sigmoid::schema> create__foreach_sigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sigmoid::name, _foreach_sigmoid::overload_name)
      .typed<_foreach_sigmoid::schema>();
}

// aten::_foreach_sigmoid(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sigmoid::call(at::TensorList tensors) {
    static auto op = create__foreach_sigmoid_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_sigmoid(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_sigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_sigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sigmoid_, name, "aten::_foreach_sigmoid_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sigmoid_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_sigmoid_, schema_str, "_foreach_sigmoid_(Tensor(a!)[] self) -> ()")

// aten::_foreach_sigmoid_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_sigmoid_::schema> create__foreach_sigmoid__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_sigmoid_::name, _foreach_sigmoid_::overload_name)
      .typed<_foreach_sigmoid_::schema>();
}

// aten::_foreach_sigmoid_(Tensor(a!)[] self) -> ()
void _foreach_sigmoid_::call(at::TensorList self) {
    static auto op = create__foreach_sigmoid__typed_handle();
    return op.call(self);
}

// aten::_foreach_sigmoid_(Tensor(a!)[] self) -> ()
void _foreach_sigmoid_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_sigmoid__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_trunc, name, "aten::_foreach_trunc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_trunc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_trunc, schema_str, "_foreach_trunc(Tensor[] tensors) -> Tensor[]")

// aten::_foreach_trunc(Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_trunc::schema> create__foreach_trunc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_trunc::name, _foreach_trunc::overload_name)
      .typed<_foreach_trunc::schema>();
}

// aten::_foreach_trunc(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_trunc::call(at::TensorList tensors) {
    static auto op = create__foreach_trunc_typed_handle();
    return op.call(tensors);
}

// aten::_foreach_trunc(Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> _foreach_trunc::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create__foreach_trunc_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_trunc_, name, "aten::_foreach_trunc_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_trunc_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_trunc_, schema_str, "_foreach_trunc_(Tensor(a!)[] self) -> ()")

// aten::_foreach_trunc_(Tensor(a!)[] self) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_trunc_::schema> create__foreach_trunc__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_trunc_::name, _foreach_trunc_::overload_name)
      .typed<_foreach_trunc_::schema>();
}

// aten::_foreach_trunc_(Tensor(a!)[] self) -> ()
void _foreach_trunc_::call(at::TensorList self) {
    static auto op = create__foreach_trunc__typed_handle();
    return op.call(self);
}

// aten::_foreach_trunc_(Tensor(a!)[] self) -> ()
void _foreach_trunc_::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self) {
    static auto op = create__foreach_trunc__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv__Scalar, name, "aten::_foreach_addcdiv_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv__Scalar, schema_str, "_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()")

// aten::_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcdiv__Scalar::schema> create__foreach_addcdiv__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcdiv__Scalar::name, _foreach_addcdiv__Scalar::overload_name)
      .typed<_foreach_addcdiv__Scalar::schema>();
}

// aten::_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
void _foreach_addcdiv__Scalar::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcdiv__Scalar_typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::_foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
void _foreach_addcdiv__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcdiv__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Scalar, name, "aten::_foreach_addcmul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__Scalar, schema_str, "_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()")

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul__Scalar::schema> create__foreach_addcmul__Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul__Scalar::name, _foreach_addcmul__Scalar::overload_name)
      .typed<_foreach_addcmul__Scalar::schema>();
}

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
void _foreach_addcmul__Scalar::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcmul__Scalar_typed_handle();
    return op.call(self, tensor1, tensor2, value);
}

// aten::_foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
void _foreach_addcmul__Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcmul__Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv__ScalarList, name, "aten::_foreach_addcdiv_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv__ScalarList, schema_str, "_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()")

// aten::_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcdiv__ScalarList::schema> create__foreach_addcdiv__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcdiv__ScalarList::name, _foreach_addcdiv__ScalarList::overload_name)
      .typed<_foreach_addcdiv__ScalarList::schema>();
}

// aten::_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
void _foreach_addcdiv__ScalarList::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcdiv__ScalarList_typed_handle();
    return op.call(self, tensor1, tensor2, scalars);
}

// aten::_foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
void _foreach_addcdiv__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcdiv__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__ScalarList, name, "aten::_foreach_addcmul_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul__ScalarList, schema_str, "_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()")

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul__ScalarList::schema> create__foreach_addcmul__ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul__ScalarList::name, _foreach_addcmul__ScalarList::overload_name)
      .typed<_foreach_addcmul__ScalarList::schema>();
}

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
void _foreach_addcmul__ScalarList::call(at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcmul__ScalarList_typed_handle();
    return op.call(self, tensor1, tensor2, scalars);
}

// aten::_foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
void _foreach_addcmul__ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList self, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcmul__ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, self, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv_Scalar, name, "aten::_foreach_addcdiv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv_Scalar, schema_str, "_foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]")

// aten::_foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcdiv_Scalar::schema> create__foreach_addcdiv_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcdiv_Scalar::name, _foreach_addcdiv_Scalar::overload_name)
      .typed<_foreach_addcdiv_Scalar::schema>();
}

// aten::_foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcdiv_Scalar::call(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcdiv_Scalar_typed_handle();
    return op.call(input, tensor1, tensor2, value);
}

// aten::_foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcdiv_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcdiv_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, input, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_Scalar, schema_str, "_foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]")

// aten::_foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_Scalar::schema> create__foreach_addcmul_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_Scalar::name, _foreach_addcmul_Scalar::overload_name)
      .typed<_foreach_addcmul_Scalar::schema>();
}

// aten::_foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_Scalar::call(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcmul_Scalar_typed_handle();
    return op.call(input, tensor1, tensor2, value);
}

// aten::_foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, const at::Scalar & value) {
    static auto op = create__foreach_addcmul_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, input, tensor1, tensor2, value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv_ScalarList, name, "aten::_foreach_addcdiv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcdiv_ScalarList, schema_str, "_foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcdiv_ScalarList::schema> create__foreach_addcdiv_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcdiv_ScalarList::name, _foreach_addcdiv_ScalarList::overload_name)
      .typed<_foreach_addcdiv_ScalarList::schema>();
}

// aten::_foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcdiv_ScalarList::call(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcdiv_ScalarList_typed_handle();
    return op.call(input, tensor1, tensor2, scalars);
}

// aten::_foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcdiv_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcdiv_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, input, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList, name, "aten::_foreach_addcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList, overload_name, "ScalarList")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_addcmul_ScalarList, schema_str, "_foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]")

// aten::_foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_addcmul_ScalarList::schema> create__foreach_addcmul_ScalarList_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_addcmul_ScalarList::name, _foreach_addcmul_ScalarList::overload_name)
      .typed<_foreach_addcmul_ScalarList::schema>();
}

// aten::_foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_ScalarList::call(at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcmul_ScalarList_typed_handle();
    return op.call(input, tensor1, tensor2, scalars);
}

// aten::_foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
::std::vector<at::Tensor> _foreach_addcmul_ScalarList::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList input, at::TensorList tensor1, at::TensorList tensor2, at::ArrayRef<at::Scalar> scalars) {
    static auto op = create__foreach_addcmul_ScalarList_typed_handle();
    return op.redispatch(dispatchKeySet, input, tensor1, tensor2, scalars);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List, name, "aten::_foreach_maximum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_maximum_List, schema_str, "_foreach_maximum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]")

// aten::_foreach_maximum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_maximum_List::schema> create__foreach_maximum_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_maximum_List::name, _foreach_maximum_List::overload_name)
      .typed<_foreach_maximum_List::schema>();
}

// aten::_foreach_maximum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_List::call(at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_maximum_List_typed_handle();
    return op.call(tensors1, tensors2);
}

// aten::_foreach_maximum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_maximum_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_maximum_List_typed_handle();
    return op.redispatch(dispatchKeySet, tensors1, tensors2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_minimum_List, name, "aten::_foreach_minimum")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_minimum_List, overload_name, "List")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_foreach_minimum_List, schema_str, "_foreach_minimum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]")

// aten::_foreach_minimum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<_foreach_minimum_List::schema> create__foreach_minimum_List_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_foreach_minimum_List::name, _foreach_minimum_List::overload_name)
      .typed<_foreach_minimum_List::schema>();
}

// aten::_foreach_minimum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_minimum_List::call(at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_minimum_List_typed_handle();
    return op.call(tensors1, tensors2);
}

// aten::_foreach_minimum.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
::std::vector<at::Tensor> _foreach_minimum_List::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors1, at::TensorList tensors2) {
    static auto op = create__foreach_minimum_List_typed_handle();
    return op.redispatch(dispatchKeySet, tensors1, tensors2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor, schema_str, "bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor")

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Tensor::schema> create_bucketize_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Tensor::name, bucketize_Tensor::overload_name)
      .typed<bucketize_Tensor::schema>();
}

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Tensor::call(const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    static auto op = create_bucketize_Tensor_typed_handle();
    return op.call(self, boundaries, out_int32, right);
}

// aten::bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    static auto op = create_bucketize_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor_out, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Tensor_out, schema_str, "bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)")

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Tensor_out::schema> create_bucketize_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Tensor_out::name, bucketize_Tensor_out::overload_name)
      .typed<bucketize_Tensor_out::schema>();
}

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bucketize_Tensor_out::call(const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
    static auto op = create_bucketize_Tensor_out_typed_handle();
    return op.call(self, boundaries, out_int32, right, out);
}

// aten::bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & bucketize_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & boundaries, bool out_int32, bool right, at::Tensor & out) {
    static auto op = create_bucketize_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar, name, "aten::bucketize")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(bucketize_Scalar, schema_str, "bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor")

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<bucketize_Scalar::schema> create_bucketize_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(bucketize_Scalar::name, bucketize_Scalar::overload_name)
      .typed<bucketize_Scalar::schema>();
}

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Scalar::call(const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    static auto op = create_bucketize_Scalar_typed_handle();
    return op.call(self, boundaries, out_int32, right);
}

// aten::bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor bucketize_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & boundaries, bool out_int32, bool right) {
    static auto op = create_bucketize_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, boundaries, out_int32, right);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Tensor, name, "aten::searchsorted")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Tensor, overload_name, "Tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Tensor, schema_str, "searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False) -> Tensor")

// aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<searchsorted_Tensor::schema> create_searchsorted_Tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(searchsorted_Tensor::name, searchsorted_Tensor::overload_name)
      .typed<searchsorted_Tensor::schema>();
}

// aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor searchsorted_Tensor::call(const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right) {
    static auto op = create_searchsorted_Tensor_typed_handle();
    return op.call(sorted_sequence, self, out_int32, right);
}

// aten::searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor searchsorted_Tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right) {
    static auto op = create_searchsorted_Tensor_typed_handle();
    return op.redispatch(dispatchKeySet, sorted_sequence, self, out_int32, right);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Tensor_out, name, "aten::searchsorted")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Tensor_out, overload_name, "Tensor_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Tensor_out, schema_str, "searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)")

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<searchsorted_Tensor_out::schema> create_searchsorted_Tensor_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(searchsorted_Tensor_out::name, searchsorted_Tensor_out::overload_name)
      .typed<searchsorted_Tensor_out::schema>();
}

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & searchsorted_Tensor_out::call(const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, at::Tensor & out) {
    static auto op = create_searchsorted_Tensor_out_typed_handle();
    return op.call(sorted_sequence, self, out_int32, right, out);
}

// aten::searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & searchsorted_Tensor_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Tensor & self, bool out_int32, bool right, at::Tensor & out) {
    static auto op = create_searchsorted_Tensor_out_typed_handle();
    return op.redispatch(dispatchKeySet, sorted_sequence, self, out_int32, right, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Scalar, name, "aten::searchsorted")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Scalar, overload_name, "Scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(searchsorted_Scalar, schema_str, "searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False) -> Tensor")

// aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<searchsorted_Scalar::schema> create_searchsorted_Scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(searchsorted_Scalar::name, searchsorted_Scalar::overload_name)
      .typed<searchsorted_Scalar::schema>();
}

// aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor searchsorted_Scalar::call(const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right) {
    static auto op = create_searchsorted_Scalar_typed_handle();
    return op.call(sorted_sequence, self, out_int32, right);
}

// aten::searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False) -> Tensor
at::Tensor searchsorted_Scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & sorted_sequence, const at::Scalar & self, bool out_int32, bool right) {
    static auto op = create_searchsorted_Scalar_typed_handle();
    return op.redispatch(dispatchKeySet, sorted_sequence, self, out_int32, right);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_coo_to_csr, name, "aten::_convert_indices_from_coo_to_csr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_coo_to_csr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_coo_to_csr, schema_str, "_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -> Tensor")

// aten::_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_convert_indices_from_coo_to_csr::schema> create__convert_indices_from_coo_to_csr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convert_indices_from_coo_to_csr::name, _convert_indices_from_coo_to_csr::overload_name)
      .typed<_convert_indices_from_coo_to_csr::schema>();
}

// aten::_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -> Tensor
at::Tensor _convert_indices_from_coo_to_csr::call(const at::Tensor & self, int64_t size, bool out_int32) {
    static auto op = create__convert_indices_from_coo_to_csr_typed_handle();
    return op.call(self, size, out_int32);
}

// aten::_convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -> Tensor
at::Tensor _convert_indices_from_coo_to_csr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t size, bool out_int32) {
    static auto op = create__convert_indices_from_coo_to_csr_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, out_int32);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_coo_to_csr_out, name, "aten::_convert_indices_from_coo_to_csr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_coo_to_csr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_convert_indices_from_coo_to_csr_out, schema_str, "_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)")

// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_convert_indices_from_coo_to_csr_out::schema> create__convert_indices_from_coo_to_csr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_convert_indices_from_coo_to_csr_out::name, _convert_indices_from_coo_to_csr_out::overload_name)
      .typed<_convert_indices_from_coo_to_csr_out::schema>();
}

// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _convert_indices_from_coo_to_csr_out::call(const at::Tensor & self, int64_t size, bool out_int32, at::Tensor & out) {
    static auto op = create__convert_indices_from_coo_to_csr_out_typed_handle();
    return op.call(self, size, out_int32, out);
}

// aten::_convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & _convert_indices_from_coo_to_csr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t size, bool out_int32, at::Tensor & out) {
    static auto op = create__convert_indices_from_coo_to_csr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, size, out_int32, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_out, name, "aten::mse_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_out, schema_str, "mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mse_loss_out::schema> create_mse_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mse_loss_out::name, mse_loss_out::overload_name)
      .typed<mse_loss_out::schema>();
}

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mse_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_mse_loss_out_typed_handle();
    return op.call(self, target, reduction, out);
}

// aten::mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & mse_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_mse_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss, name, "aten::mse_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss, schema_str, "mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mse_loss::schema> create_mse_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mse_loss::name, mse_loss::overload_name)
      .typed<mse_loss::schema>();
}

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor mse_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_mse_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor mse_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_mse_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_backward_grad_input, name, "aten::mse_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_backward_grad_input, schema_str, "mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<mse_loss_backward_grad_input::schema> create_mse_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mse_loss_backward_grad_input::name, mse_loss_backward_grad_input::overload_name)
      .typed<mse_loss_backward_grad_input::schema>();
}

// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & mse_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_mse_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, reduction, grad_input);
}

// aten::mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & mse_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_mse_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_backward, name, "aten::mse_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mse_loss_backward, schema_str, "mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor")

// aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mse_loss_backward::schema> create_mse_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mse_loss_backward::name, mse_loss_backward::overload_name)
      .typed<mse_loss_backward::schema>();
}

// aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
at::Tensor mse_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_mse_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction);
}

// aten::mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
at::Tensor mse_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_mse_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_out, name, "aten::l1_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_out, schema_str, "l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<l1_loss_out::schema> create_l1_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(l1_loss_out::name, l1_loss_out::overload_name)
      .typed<l1_loss_out::schema>();
}

// aten::l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & l1_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_l1_loss_out_typed_handle();
    return op.call(self, target, reduction, out);
}

// aten::l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & l1_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_l1_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss, name, "aten::l1_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss, schema_str, "l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<l1_loss::schema> create_l1_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(l1_loss::name, l1_loss::overload_name)
      .typed<l1_loss::schema>();
}

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor l1_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_l1_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor l1_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_l1_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_backward_grad_input, name, "aten::l1_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_backward_grad_input, schema_str, "l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<l1_loss_backward_grad_input::schema> create_l1_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(l1_loss_backward_grad_input::name, l1_loss_backward_grad_input::overload_name)
      .typed<l1_loss_backward_grad_input::schema>();
}

// aten::l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & l1_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_l1_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, reduction, grad_input);
}

// aten::l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & l1_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_l1_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_backward, name, "aten::l1_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(l1_loss_backward, schema_str, "l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor")

// aten::l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<l1_loss_backward::schema> create_l1_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(l1_loss_backward::name, l1_loss_backward::overload_name)
      .typed<l1_loss_backward::schema>();
}

// aten::l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
at::Tensor l1_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_l1_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction);
}

// aten::l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
at::Tensor l1_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_l1_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_out, name, "aten::multi_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_out, schema_str, "multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multi_margin_loss_out::schema> create_multi_margin_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multi_margin_loss_out::name, multi_margin_loss_out::overload_name)
      .typed<multi_margin_loss_out::schema>();
}

// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multi_margin_loss_out::call(const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
    static auto op = create_multi_margin_loss_out_typed_handle();
    return op.call(self, target, p, margin, weight, reduction, out);
}

// aten::multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multi_margin_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & out) {
    static auto op = create_multi_margin_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, p, margin, weight, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss, name, "aten::multi_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss, schema_str, "multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor")

// aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multi_margin_loss::schema> create_multi_margin_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multi_margin_loss::name, multi_margin_loss::overload_name)
      .typed<multi_margin_loss::schema>();
}

// aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor multi_margin_loss::call(const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_multi_margin_loss_typed_handle();
    return op.call(self, target, p, margin, weight, reduction);
}

// aten::multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor multi_margin_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_multi_margin_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, p, margin, weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_backward_grad_input, name, "aten::multi_margin_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_backward_grad_input, schema_str, "multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multi_margin_loss_backward_grad_input::schema> create_multi_margin_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multi_margin_loss_backward_grad_input::name, multi_margin_loss_backward_grad_input::overload_name)
      .typed<multi_margin_loss_backward_grad_input::schema>();
}

// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & multi_margin_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_multi_margin_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, p, margin, weight, reduction, grad_input);
}

// aten::multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & multi_margin_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_multi_margin_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, p, margin, weight, reduction, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_backward, name, "aten::multi_margin_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multi_margin_loss_backward, schema_str, "multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor")

// aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multi_margin_loss_backward::schema> create_multi_margin_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multi_margin_loss_backward::name, multi_margin_loss_backward::overload_name)
      .typed<multi_margin_loss_backward::schema>();
}

// aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor multi_margin_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_multi_margin_loss_backward_typed_handle();
    return op.call(grad_output, self, target, p, margin, weight, reduction);
}

// aten::multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -> Tensor
at::Tensor multi_margin_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const at::Scalar & p, const at::Scalar & margin, const c10::optional<at::Tensor> & weight, int64_t reduction) {
    static auto op = create_multi_margin_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, p, margin, weight, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_out, name, "aten::multilabel_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_out, schema_str, "multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multilabel_margin_loss_out::schema> create_multilabel_margin_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multilabel_margin_loss_out::name, multilabel_margin_loss_out::overload_name)
      .typed<multilabel_margin_loss_out::schema>();
}

// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multilabel_margin_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_multilabel_margin_loss_out_typed_handle();
    return op.call(self, target, reduction, out);
}

// aten::multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & multilabel_margin_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_multilabel_margin_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss, name, "aten::multilabel_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss, schema_str, "multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multilabel_margin_loss::schema> create_multilabel_margin_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multilabel_margin_loss::name, multilabel_margin_loss::overload_name)
      .typed<multilabel_margin_loss::schema>();
}

// aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor multilabel_margin_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_multilabel_margin_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor multilabel_margin_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_multilabel_margin_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_forward_output, name, "aten::multilabel_margin_loss_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_forward_output, schema_str, "multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))")

// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<multilabel_margin_loss_forward_output::schema> create_multilabel_margin_loss_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multilabel_margin_loss_forward_output::name, multilabel_margin_loss_forward_output::overload_name)
      .typed<multilabel_margin_loss_forward_output::schema>();
}

// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> multilabel_margin_loss_forward_output::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & output, at::Tensor & is_target) {
    static auto op = create_multilabel_margin_loss_forward_output_typed_handle();
    return op.call(self, target, reduction, output, is_target);
}

// aten::multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> multilabel_margin_loss_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & output, at::Tensor & is_target) {
    static auto op = create_multilabel_margin_loss_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, output, is_target);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_forward, name, "aten::multilabel_margin_loss_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_forward, schema_str, "multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)")

// aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
static C10_NOINLINE c10::TypedOperatorHandle<multilabel_margin_loss_forward::schema> create_multilabel_margin_loss_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multilabel_margin_loss_forward::name, multilabel_margin_loss_forward::overload_name)
      .typed<multilabel_margin_loss_forward::schema>();
}

// aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
::std::tuple<at::Tensor,at::Tensor> multilabel_margin_loss_forward::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_multilabel_margin_loss_forward_typed_handle();
    return op.call(self, target, reduction);
}

// aten::multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
::std::tuple<at::Tensor,at::Tensor> multilabel_margin_loss_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_multilabel_margin_loss_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_backward_grad_input, name, "aten::multilabel_margin_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_backward_grad_input, schema_str, "multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<multilabel_margin_loss_backward_grad_input::schema> create_multilabel_margin_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multilabel_margin_loss_backward_grad_input::name, multilabel_margin_loss_backward_grad_input::overload_name)
      .typed<multilabel_margin_loss_backward_grad_input::schema>();
}

// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & multilabel_margin_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target, at::Tensor & grad_input) {
    static auto op = create_multilabel_margin_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, reduction, is_target, grad_input);
}

// aten::multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & multilabel_margin_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target, at::Tensor & grad_input) {
    static auto op = create_multilabel_margin_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, is_target, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_backward, name, "aten::multilabel_margin_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(multilabel_margin_loss_backward, schema_str, "multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor")

// aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<multilabel_margin_loss_backward::schema> create_multilabel_margin_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(multilabel_margin_loss_backward::name, multilabel_margin_loss_backward::overload_name)
      .typed<multilabel_margin_loss_backward::schema>();
}

// aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor
at::Tensor multilabel_margin_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target) {
    static auto op = create_multilabel_margin_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction, is_target);
}

// aten::multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -> Tensor
at::Tensor multilabel_margin_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, const at::Tensor & is_target) {
    static auto op = create_multilabel_margin_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, is_target);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_out, name, "aten::nll_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_out, schema_str, "nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_out::schema> create_nll_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_out::name, nll_loss_out::overload_name)
      .typed<nll_loss_out::schema>();
}

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss_out::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
    static auto op = create_nll_loss_out_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, out);
}

// aten::nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
    static auto op = create_nll_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_nd, name, "aten::nll_loss_nd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_nd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_nd, schema_str, "nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor")

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_nd::schema> create_nll_loss_nd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_nd::name, nll_loss_nd::overload_name)
      .typed<nll_loss_nd::schema>();
}

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
at::Tensor nll_loss_nd::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss_nd_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
at::Tensor nll_loss_nd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss_nd_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss, name, "aten::nll_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss, schema_str, "nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor")

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss::schema> create_nll_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss::name, nll_loss::overload_name)
      .typed<nll_loss::schema>();
}

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
at::Tensor nll_loss::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
at::Tensor nll_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_forward_output, name, "aten::nll_loss_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_forward_output, schema_str, "nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))")

// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_forward_output::schema> create_nll_loss_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_forward_output::name, nll_loss_forward_output::overload_name)
      .typed<nll_loss_forward_output::schema>();
}

// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> nll_loss_forward_output::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
    static auto op = create_nll_loss_forward_output_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, output, total_weight);
}

// aten::nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> nll_loss_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
    static auto op = create_nll_loss_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, output, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_forward, name, "aten::nll_loss_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_forward, schema_str, "nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)")

// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_forward::schema> create_nll_loss_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_forward::name, nll_loss_forward::overload_name)
      .typed<nll_loss_forward::schema>();
}

// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
::std::tuple<at::Tensor,at::Tensor> nll_loss_forward::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss_forward_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
::std::tuple<at::Tensor,at::Tensor> nll_loss_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward_grad_input, name, "aten::nll_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward_grad_input, schema_str, "nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_backward_grad_input::schema> create_nll_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_backward_grad_input::name, nll_loss_backward_grad_input::overload_name)
      .typed<nll_loss_backward_grad_input::schema>();
}

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    static auto op = create_nll_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

// aten::nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    static auto op = create_nll_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward, name, "aten::nll_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss_backward, schema_str, "nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor")

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss_backward::schema> create_nll_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss_backward::name, nll_loss_backward::overload_name)
      .typed<nll_loss_backward::schema>();
}

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
    static auto op = create_nll_loss_backward_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

// aten::nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
    static auto op = create_nll_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_out, name, "aten::nll_loss2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_out, schema_str, "nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)")

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_out::schema> create_nll_loss2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_out::name, nll_loss2d_out::overload_name)
      .typed<nll_loss2d_out::schema>();
}

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss2d_out::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
    static auto op = create_nll_loss2d_out_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, out);
}

// aten::nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & nll_loss2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & out) {
    static auto op = create_nll_loss2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d, name, "aten::nll_loss2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d, schema_str, "nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor")

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d::schema> create_nll_loss2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d::name, nll_loss2d::overload_name)
      .typed<nll_loss2d::schema>();
}

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
at::Tensor nll_loss2d::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss2d_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
at::Tensor nll_loss2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward_output, name, "aten::nll_loss2d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward_output, schema_str, "nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))")

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_forward_output::schema> create_nll_loss2d_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_forward_output::name, nll_loss2d_forward_output::overload_name)
      .typed<nll_loss2d_forward_output::schema>();
}

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> nll_loss2d_forward_output::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
    static auto op = create_nll_loss2d_forward_output_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index, output, total_weight);
}

// aten::nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> nll_loss2d_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, at::Tensor & output, at::Tensor & total_weight) {
    static auto op = create_nll_loss2d_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index, output, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward, name, "aten::nll_loss2d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_forward, schema_str, "nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)")

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_forward::schema> create_nll_loss2d_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_forward::name, nll_loss2d_forward::overload_name)
      .typed<nll_loss2d_forward::schema>();
}

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
::std::tuple<at::Tensor,at::Tensor> nll_loss2d_forward::call(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss2d_forward_typed_handle();
    return op.call(self, target, weight, reduction, ignore_index);
}

// aten::nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
::std::tuple<at::Tensor,at::Tensor> nll_loss2d_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
    static auto op = create_nll_loss2d_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, weight, reduction, ignore_index);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward_grad_input, name, "aten::nll_loss2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward_grad_input, schema_str, "nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_backward_grad_input::schema> create_nll_loss2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_backward_grad_input::name, nll_loss2d_backward_grad_input::overload_name)
      .typed<nll_loss2d_backward_grad_input::schema>();
}

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    static auto op = create_nll_loss2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

// aten::nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & nll_loss2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight, at::Tensor & grad_input) {
    static auto op = create_nll_loss2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward, name, "aten::nll_loss2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(nll_loss2d_backward, schema_str, "nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor")

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<nll_loss2d_backward::schema> create_nll_loss2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(nll_loss2d_backward::name, nll_loss2d_backward::overload_name)
      .typed<nll_loss2d_backward::schema>();
}

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
    static auto op = create_nll_loss2d_backward_typed_handle();
    return op.call(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

// aten::nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
at::Tensor nll_loss2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
    static auto op = create_nll_loss2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_out, name, "aten::smooth_l1_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_out, schema_str, "smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<smooth_l1_loss_out::schema> create_smooth_l1_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smooth_l1_loss_out::name, smooth_l1_loss_out::overload_name)
      .typed<smooth_l1_loss_out::schema>();
}

// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & smooth_l1_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
    static auto op = create_smooth_l1_loss_out_typed_handle();
    return op.call(self, target, reduction, beta, out);
}

// aten::smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & smooth_l1_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & out) {
    static auto op = create_smooth_l1_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, beta, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss, name, "aten::smooth_l1_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss, schema_str, "smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor")

// aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<smooth_l1_loss::schema> create_smooth_l1_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smooth_l1_loss::name, smooth_l1_loss::overload_name)
      .typed<smooth_l1_loss::schema>();
}

// aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
at::Tensor smooth_l1_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    static auto op = create_smooth_l1_loss_typed_handle();
    return op.call(self, target, reduction, beta);
}

// aten::smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
at::Tensor smooth_l1_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    static auto op = create_smooth_l1_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, beta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward_grad_input, name, "aten::smooth_l1_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward_grad_input, schema_str, "smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<smooth_l1_loss_backward_grad_input::schema> create_smooth_l1_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smooth_l1_loss_backward_grad_input::name, smooth_l1_loss_backward_grad_input::overload_name)
      .typed<smooth_l1_loss_backward_grad_input::schema>();
}

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & smooth_l1_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
    static auto op = create_smooth_l1_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, reduction, beta, grad_input);
}

// aten::smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & smooth_l1_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta, at::Tensor & grad_input) {
    static auto op = create_smooth_l1_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, beta, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward, name, "aten::smooth_l1_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(smooth_l1_loss_backward, schema_str, "smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor")

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<smooth_l1_loss_backward::schema> create_smooth_l1_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(smooth_l1_loss_backward::name, smooth_l1_loss_backward::overload_name)
      .typed<smooth_l1_loss_backward::schema>();
}

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
at::Tensor smooth_l1_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    static auto op = create_smooth_l1_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction, beta);
}

// aten::smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
at::Tensor smooth_l1_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    static auto op = create_smooth_l1_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, beta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_out, name, "aten::huber_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_out, schema_str, "huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss_out::schema> create_huber_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss_out::name, huber_loss_out::overload_name)
      .typed<huber_loss_out::schema>();
}

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & huber_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
    static auto op = create_huber_loss_out_typed_handle();
    return op.call(self, target, reduction, delta, out);
}

// aten::huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & huber_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & out) {
    static auto op = create_huber_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, delta, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss, name, "aten::huber_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss, schema_str, "huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor")

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss::schema> create_huber_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss::name, huber_loss::overload_name)
      .typed<huber_loss::schema>();
}

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
at::Tensor huber_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    static auto op = create_huber_loss_typed_handle();
    return op.call(self, target, reduction, delta);
}

// aten::huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -> Tensor
at::Tensor huber_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    static auto op = create_huber_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, delta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward_out, name, "aten::huber_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward_out, schema_str, "huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss_backward_out::schema> create_huber_loss_backward_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss_backward_out::name, huber_loss_backward_out::overload_name)
      .typed<huber_loss_backward_out::schema>();
}

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & huber_loss_backward_out::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
    static auto op = create_huber_loss_backward_out_typed_handle();
    return op.call(grad_output, self, target, reduction, delta, grad_input);
}

// aten::huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & huber_loss_backward_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta, at::Tensor & grad_input) {
    static auto op = create_huber_loss_backward_out_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, delta, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward, name, "aten::huber_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(huber_loss_backward, schema_str, "huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor")

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<huber_loss_backward::schema> create_huber_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(huber_loss_backward::name, huber_loss_backward::overload_name)
      .typed<huber_loss_backward::schema>();
}

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
at::Tensor huber_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    static auto op = create_huber_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction, delta);
}

// aten::huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -> Tensor
at::Tensor huber_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    static auto op = create_huber_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, delta);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_out, name, "aten::soft_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_out, schema_str, "soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)")

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<soft_margin_loss_out::schema> create_soft_margin_loss_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(soft_margin_loss_out::name, soft_margin_loss_out::overload_name)
      .typed<soft_margin_loss_out::schema>();
}

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & soft_margin_loss_out::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_soft_margin_loss_out_typed_handle();
    return op.call(self, target, reduction, out);
}

// aten::soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & soft_margin_loss_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    static auto op = create_soft_margin_loss_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss, name, "aten::soft_margin_loss")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss, schema_str, "soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor")

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<soft_margin_loss::schema> create_soft_margin_loss_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(soft_margin_loss::name, soft_margin_loss::overload_name)
      .typed<soft_margin_loss::schema>();
}

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor soft_margin_loss::call(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_soft_margin_loss_typed_handle();
    return op.call(self, target, reduction);
}

// aten::soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
at::Tensor soft_margin_loss::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_soft_margin_loss_typed_handle();
    return op.redispatch(dispatchKeySet, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_backward_grad_input, name, "aten::soft_margin_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_backward_grad_input, schema_str, "soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<soft_margin_loss_backward_grad_input::schema> create_soft_margin_loss_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(soft_margin_loss_backward_grad_input::name, soft_margin_loss_backward_grad_input::overload_name)
      .typed<soft_margin_loss_backward_grad_input::schema>();
}

// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & soft_margin_loss_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_soft_margin_loss_backward_grad_input_typed_handle();
    return op.call(grad_output, self, target, reduction, grad_input);
}

// aten::soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & soft_margin_loss_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    static auto op = create_soft_margin_loss_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_backward, name, "aten::soft_margin_loss_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(soft_margin_loss_backward, schema_str, "soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor")

// aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<soft_margin_loss_backward::schema> create_soft_margin_loss_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(soft_margin_loss_backward::name, soft_margin_loss_backward::overload_name)
      .typed<soft_margin_loss_backward::schema>();
}

// aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
at::Tensor soft_margin_loss_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_soft_margin_loss_backward_typed_handle();
    return op.call(grad_output, self, target, reduction);
}

// aten::soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
at::Tensor soft_margin_loss_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    static auto op = create_soft_margin_loss_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, target, reduction);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_out, name, "aten::elu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_out, schema_str, "elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<elu_out::schema> create_elu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(elu_out::name, elu_out::overload_name)
      .typed<elu_out::schema>();
}

// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & elu_out::call(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
    static auto op = create_elu_out_typed_handle();
    return op.call(self, alpha, scale, input_scale, out);
}

// aten::elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & elu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, at::Tensor & out) {
    static auto op = create_elu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha, scale, input_scale, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu, name, "aten::elu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu, schema_str, "elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor")

// aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<elu::schema> create_elu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(elu::name, elu::overload_name)
      .typed<elu::schema>();
}

// aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
at::Tensor elu::call(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
    static auto op = create_elu_typed_handle();
    return op.call(self, alpha, scale, input_scale);
}

// aten::elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
at::Tensor elu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
    static auto op = create_elu_typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha, scale, input_scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_backward_grad_input, name, "aten::elu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_backward_grad_input, schema_str, "elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<elu_backward_grad_input::schema> create_elu_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(elu_backward_grad_input::name, elu_backward_grad_input::overload_name)
      .typed<elu_backward_grad_input::schema>();
}

// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & elu_backward_grad_input::call(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
    static auto op = create_elu_backward_grad_input_typed_handle();
    return op.call(grad_output, alpha, scale, input_scale, is_result, self_or_result, grad_input);
}

// aten::elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & elu_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result, at::Tensor & grad_input) {
    static auto op = create_elu_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, alpha, scale, input_scale, is_result, self_or_result, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_backward, name, "aten::elu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_backward, schema_str, "elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor")

// aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<elu_backward::schema> create_elu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(elu_backward::name, elu_backward::overload_name)
      .typed<elu_backward::schema>();
}

// aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
at::Tensor elu_backward::call(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
    static auto op = create_elu_backward_typed_handle();
    return op.call(grad_output, alpha, scale, input_scale, is_result, self_or_result);
}

// aten::elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
at::Tensor elu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
    static auto op = create_elu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, alpha, scale, input_scale, is_result, self_or_result);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_, name, "aten::elu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(elu_, schema_str, "elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)")

// aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<elu_::schema> create_elu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(elu_::name, elu_::overload_name)
      .typed<elu_::schema>();
}

// aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
at::Tensor & elu_::call(at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
    static auto op = create_elu__typed_handle();
    return op.call(self, alpha, scale, input_scale);
}

// aten::elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
at::Tensor & elu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
    static auto op = create_elu__typed_handle();
    return op.redispatch(dispatchKeySet, self, alpha, scale, input_scale);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_out, name, "aten::glu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_out, schema_str, "glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<glu_out::schema> create_glu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu_out::name, glu_out::overload_name)
      .typed<glu_out::schema>();
}

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & glu_out::call(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    static auto op = create_glu_out_typed_handle();
    return op.call(self, dim, out);
}

// aten::glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & glu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, at::Tensor & out) {
    static auto op = create_glu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu, name, "aten::glu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu, schema_str, "glu(Tensor self, int dim=-1) -> Tensor")

// aten::glu(Tensor self, int dim=-1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<glu::schema> create_glu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu::name, glu::overload_name)
      .typed<glu::schema>();
}

// aten::glu(Tensor self, int dim=-1) -> Tensor
at::Tensor glu::call(const at::Tensor & self, int64_t dim) {
    static auto op = create_glu_typed_handle();
    return op.call(self, dim);
}

// aten::glu(Tensor self, int dim=-1) -> Tensor
at::Tensor glu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim) {
    static auto op = create_glu_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_grad_input, name, "aten::glu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward_grad_input, schema_str, "glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<glu_backward_grad_input::schema> create_glu_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu_backward_grad_input::name, glu_backward_grad_input::overload_name)
      .typed<glu_backward_grad_input::schema>();
}

// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & glu_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
    static auto op = create_glu_backward_grad_input_typed_handle();
    return op.call(grad_output, self, dim, grad_input);
}

// aten::glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & glu_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim, at::Tensor & grad_input) {
    static auto op = create_glu_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, dim, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward, name, "aten::glu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(glu_backward, schema_str, "glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor")

// aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<glu_backward::schema> create_glu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(glu_backward::name, glu_backward::overload_name)
      .typed<glu_backward::schema>();
}

// aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
at::Tensor glu_backward::call(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
    static auto op = create_glu_backward_typed_handle();
    return op.call(grad_output, self, dim);
}

// aten::glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
at::Tensor glu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
    static auto op = create_glu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_out, name, "aten::hardsigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_out, schema_str, "hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid_out::schema> create_hardsigmoid_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid_out::name, hardsigmoid_out::overload_name)
      .typed<hardsigmoid_out::schema>();
}

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardsigmoid_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_hardsigmoid_out_typed_handle();
    return op.call(self, out);
}

// aten::hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardsigmoid_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_hardsigmoid_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid, name, "aten::hardsigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid, schema_str, "hardsigmoid(Tensor self) -> Tensor")

// aten::hardsigmoid(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid::schema> create_hardsigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid::name, hardsigmoid::overload_name)
      .typed<hardsigmoid::schema>();
}

// aten::hardsigmoid(Tensor self) -> Tensor
at::Tensor hardsigmoid::call(const at::Tensor & self) {
    static auto op = create_hardsigmoid_typed_handle();
    return op.call(self);
}

// aten::hardsigmoid(Tensor self) -> Tensor
at::Tensor hardsigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_hardsigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_, name, "aten::hardsigmoid_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_, schema_str, "hardsigmoid_(Tensor(a!) self) -> Tensor(a!)")

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid_::schema> create_hardsigmoid__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid_::name, hardsigmoid_::overload_name)
      .typed<hardsigmoid_::schema>();
}

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & hardsigmoid_::call(at::Tensor & self) {
    static auto op = create_hardsigmoid__typed_handle();
    return op.call(self);
}

// aten::hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & hardsigmoid_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_hardsigmoid__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_backward_grad_input, name, "aten::hardsigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_backward_grad_input, schema_str, "hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid_backward_grad_input::schema> create_hardsigmoid_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid_backward_grad_input::name, hardsigmoid_backward_grad_input::overload_name)
      .typed<hardsigmoid_backward_grad_input::schema>();
}

// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & hardsigmoid_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_hardsigmoid_backward_grad_input_typed_handle();
    return op.call(grad_output, self, grad_input);
}

// aten::hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & hardsigmoid_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_hardsigmoid_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_backward, name, "aten::hardsigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardsigmoid_backward, schema_str, "hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardsigmoid_backward::schema> create_hardsigmoid_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardsigmoid_backward::name, hardsigmoid_backward::overload_name)
      .typed<hardsigmoid_backward::schema>();
}

// aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor hardsigmoid_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_hardsigmoid_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor hardsigmoid_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_hardsigmoid_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_out, name, "aten::hardtanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_out, schema_str, "hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh_out::schema> create_hardtanh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh_out::name, hardtanh_out::overload_name)
      .typed<hardtanh_out::schema>();
}

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardtanh_out::call(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
    static auto op = create_hardtanh_out_typed_handle();
    return op.call(self, min_val, max_val, out);
}

// aten::hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardtanh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
    static auto op = create_hardtanh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, min_val, max_val, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh, name, "aten::hardtanh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh, schema_str, "hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor")

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh::schema> create_hardtanh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh::name, hardtanh::overload_name)
      .typed<hardtanh::schema>();
}

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
at::Tensor hardtanh::call(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    static auto op = create_hardtanh_typed_handle();
    return op.call(self, min_val, max_val);
}

// aten::hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
at::Tensor hardtanh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    static auto op = create_hardtanh_typed_handle();
    return op.redispatch(dispatchKeySet, self, min_val, max_val);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_backward_grad_input, name, "aten::hardtanh_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_backward_grad_input, schema_str, "hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh_backward_grad_input::schema> create_hardtanh_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh_backward_grad_input::name, hardtanh_backward_grad_input::overload_name)
      .typed<hardtanh_backward_grad_input::schema>();
}

// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & hardtanh_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & grad_input) {
    static auto op = create_hardtanh_backward_grad_input_typed_handle();
    return op.call(grad_output, self, min_val, max_val, grad_input);
}

// aten::hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & hardtanh_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & grad_input) {
    static auto op = create_hardtanh_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, min_val, max_val, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_backward, name, "aten::hardtanh_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_backward, schema_str, "hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor")

// aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh_backward::schema> create_hardtanh_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh_backward::name, hardtanh_backward::overload_name)
      .typed<hardtanh_backward::schema>();
}

// aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
at::Tensor hardtanh_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    static auto op = create_hardtanh_backward_typed_handle();
    return op.call(grad_output, self, min_val, max_val);
}

// aten::hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
at::Tensor hardtanh_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    static auto op = create_hardtanh_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, min_val, max_val);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_, name, "aten::hardtanh_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardtanh_, schema_str, "hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)")

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardtanh_::schema> create_hardtanh__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardtanh_::name, hardtanh_::overload_name)
      .typed<hardtanh_::schema>();
}

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
at::Tensor & hardtanh_::call(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    static auto op = create_hardtanh__typed_handle();
    return op.call(self, min_val, max_val);
}

// aten::hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
at::Tensor & hardtanh_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    static auto op = create_hardtanh__typed_handle();
    return op.redispatch(dispatchKeySet, self, min_val, max_val);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_out, name, "aten::hardswish")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_out, schema_str, "hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardswish_out::schema> create_hardswish_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardswish_out::name, hardswish_out::overload_name)
      .typed<hardswish_out::schema>();
}

// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardswish_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_hardswish_out_typed_handle();
    return op.call(self, out);
}

// aten::hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & hardswish_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_hardswish_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish, name, "aten::hardswish")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish, schema_str, "hardswish(Tensor self) -> Tensor")

// aten::hardswish(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardswish::schema> create_hardswish_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardswish::name, hardswish::overload_name)
      .typed<hardswish::schema>();
}

// aten::hardswish(Tensor self) -> Tensor
at::Tensor hardswish::call(const at::Tensor & self) {
    static auto op = create_hardswish_typed_handle();
    return op.call(self);
}

// aten::hardswish(Tensor self) -> Tensor
at::Tensor hardswish::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_hardswish_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_, name, "aten::hardswish_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_, schema_str, "hardswish_(Tensor(a!) self) -> Tensor(a!)")

// aten::hardswish_(Tensor(a!) self) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<hardswish_::schema> create_hardswish__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardswish_::name, hardswish_::overload_name)
      .typed<hardswish_::schema>();
}

// aten::hardswish_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & hardswish_::call(at::Tensor & self) {
    static auto op = create_hardswish__typed_handle();
    return op.call(self);
}

// aten::hardswish_(Tensor(a!) self) -> Tensor(a!)
at::Tensor & hardswish_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    static auto op = create_hardswish__typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward, name, "aten::hardswish_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(hardswish_backward, schema_str, "hardswish_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<hardswish_backward::schema> create_hardswish_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(hardswish_backward::name, hardswish_backward::overload_name)
      .typed<hardswish_backward::schema>();
}

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor hardswish_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_hardswish_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor hardswish_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_hardswish_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_out, name, "aten::leaky_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_out, schema_str, "leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)")

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu_out::schema> create_leaky_relu_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu_out::name, leaky_relu_out::overload_name)
      .typed<leaky_relu_out::schema>();
}

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & leaky_relu_out::call(const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
    static auto op = create_leaky_relu_out_typed_handle();
    return op.call(self, negative_slope, out);
}

// aten::leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & leaky_relu_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope, at::Tensor & out) {
    static auto op = create_leaky_relu_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, negative_slope, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu, name, "aten::leaky_relu")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu, schema_str, "leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor")

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu::schema> create_leaky_relu_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu::name, leaky_relu::overload_name)
      .typed<leaky_relu::schema>();
}

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
at::Tensor leaky_relu::call(const at::Tensor & self, const at::Scalar & negative_slope) {
    static auto op = create_leaky_relu_typed_handle();
    return op.call(self, negative_slope);
}

// aten::leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
at::Tensor leaky_relu::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & negative_slope) {
    static auto op = create_leaky_relu_typed_handle();
    return op.redispatch(dispatchKeySet, self, negative_slope);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_backward_grad_input, name, "aten::leaky_relu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_backward_grad_input, schema_str, "leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu_backward_grad_input::schema> create_leaky_relu_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu_backward_grad_input::name, leaky_relu_backward_grad_input::overload_name)
      .typed<leaky_relu_backward_grad_input::schema>();
}

// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & leaky_relu_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
    static auto op = create_leaky_relu_backward_grad_input_typed_handle();
    return op.call(grad_output, self, negative_slope, self_is_result, grad_input);
}

// aten::leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & leaky_relu_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result, at::Tensor & grad_input) {
    static auto op = create_leaky_relu_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, negative_slope, self_is_result, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_backward, name, "aten::leaky_relu_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_backward, schema_str, "leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor")

// aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu_backward::schema> create_leaky_relu_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu_backward::name, leaky_relu_backward::overload_name)
      .typed<leaky_relu_backward::schema>();
}

// aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
at::Tensor leaky_relu_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
    static auto op = create_leaky_relu_backward_typed_handle();
    return op.call(grad_output, self, negative_slope, self_is_result);
}

// aten::leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
at::Tensor leaky_relu_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
    static auto op = create_leaky_relu_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, negative_slope, self_is_result);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_, name, "aten::leaky_relu_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(leaky_relu_, schema_str, "leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)")

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<leaky_relu_::schema> create_leaky_relu__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(leaky_relu_::name, leaky_relu_::overload_name)
      .typed<leaky_relu_::schema>();
}

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
at::Tensor & leaky_relu_::call(at::Tensor & self, const at::Scalar & negative_slope) {
    static auto op = create_leaky_relu__typed_handle();
    return op.call(self, negative_slope);
}

// aten::leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
at::Tensor & leaky_relu_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Scalar & negative_slope) {
    static auto op = create_leaky_relu__typed_handle();
    return op.redispatch(dispatchKeySet, self, negative_slope);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_out, name, "aten::log_sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_out, schema_str, "log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_out::schema> create_log_sigmoid_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_out::name, log_sigmoid_out::overload_name)
      .typed<log_sigmoid_out::schema>();
}

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_sigmoid_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log_sigmoid_out_typed_handle();
    return op.call(self, out);
}

// aten::log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & log_sigmoid_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_log_sigmoid_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid, name, "aten::log_sigmoid")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid, schema_str, "log_sigmoid(Tensor self) -> Tensor")

// aten::log_sigmoid(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid::schema> create_log_sigmoid_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid::name, log_sigmoid::overload_name)
      .typed<log_sigmoid::schema>();
}

// aten::log_sigmoid(Tensor self) -> Tensor
at::Tensor log_sigmoid::call(const at::Tensor & self) {
    static auto op = create_log_sigmoid_typed_handle();
    return op.call(self);
}

// aten::log_sigmoid(Tensor self) -> Tensor
at::Tensor log_sigmoid::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_log_sigmoid_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward_output, name, "aten::log_sigmoid_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward_output, schema_str, "log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))")

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_forward_output::schema> create_log_sigmoid_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_forward_output::name, log_sigmoid_forward_output::overload_name)
      .typed<log_sigmoid_forward_output::schema>();
}

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> log_sigmoid_forward_output::call(const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
    static auto op = create_log_sigmoid_forward_output_typed_handle();
    return op.call(self, output, buffer);
}

// aten::log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> log_sigmoid_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & output, at::Tensor & buffer) {
    static auto op = create_log_sigmoid_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, output, buffer);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward, name, "aten::log_sigmoid_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_forward, schema_str, "log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)")

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_forward::schema> create_log_sigmoid_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_forward::name, log_sigmoid_forward::overload_name)
      .typed<log_sigmoid_forward::schema>();
}

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
::std::tuple<at::Tensor,at::Tensor> log_sigmoid_forward::call(const at::Tensor & self) {
    static auto op = create_log_sigmoid_forward_typed_handle();
    return op.call(self);
}

// aten::log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
::std::tuple<at::Tensor,at::Tensor> log_sigmoid_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_log_sigmoid_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward_grad_input, name, "aten::log_sigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward_grad_input, schema_str, "log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_backward_grad_input::schema> create_log_sigmoid_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_backward_grad_input::name, log_sigmoid_backward_grad_input::overload_name)
      .typed<log_sigmoid_backward_grad_input::schema>();
}

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & log_sigmoid_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
    static auto op = create_log_sigmoid_backward_grad_input_typed_handle();
    return op.call(grad_output, self, buffer, grad_input);
}

// aten::log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & log_sigmoid_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer, at::Tensor & grad_input) {
    static auto op = create_log_sigmoid_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, buffer, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward, name, "aten::log_sigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(log_sigmoid_backward, schema_str, "log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor")

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<log_sigmoid_backward::schema> create_log_sigmoid_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(log_sigmoid_backward::name, log_sigmoid_backward::overload_name)
      .typed<log_sigmoid_backward::schema>();
}

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
at::Tensor log_sigmoid_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
    static auto op = create_log_sigmoid_backward_typed_handle();
    return op.call(grad_output, self, buffer);
}

// aten::log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
at::Tensor log_sigmoid_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
    static auto op = create_log_sigmoid_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, buffer);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_out, name, "aten::rrelu_with_noise")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_out, schema_str, "rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rrelu_with_noise_out::schema> create_rrelu_with_noise_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rrelu_with_noise_out::name, rrelu_with_noise_out::overload_name)
      .typed<rrelu_with_noise_out::schema>();
}

// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rrelu_with_noise_out::call(const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_rrelu_with_noise_out_typed_handle();
    return op.call(self, noise, lower, upper, training, generator, out);
}

// aten::rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & rrelu_with_noise_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator, at::Tensor & out) {
    static auto op = create_rrelu_with_noise_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, noise, lower, upper, training, generator, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise, name, "aten::rrelu_with_noise")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise, schema_str, "rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor")

// aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rrelu_with_noise::schema> create_rrelu_with_noise_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rrelu_with_noise::name, rrelu_with_noise::overload_name)
      .typed<rrelu_with_noise::schema>();
}

// aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
at::Tensor rrelu_with_noise::call(const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu_with_noise_typed_handle();
    return op.call(self, noise, lower, upper, training, generator);
}

// aten::rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
at::Tensor rrelu_with_noise::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu_with_noise_typed_handle();
    return op.redispatch(dispatchKeySet, self, noise, lower, upper, training, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_backward, name, "aten::rrelu_with_noise_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_backward, schema_str, "rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor")

// aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<rrelu_with_noise_backward::schema> create_rrelu_with_noise_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rrelu_with_noise_backward::name, rrelu_with_noise_backward::overload_name)
      .typed<rrelu_with_noise_backward::schema>();
}

// aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor
at::Tensor rrelu_with_noise_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
    static auto op = create_rrelu_with_noise_backward_typed_handle();
    return op.call(grad_output, self, noise, lower, upper, training, self_is_result);
}

// aten::rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -> Tensor
at::Tensor rrelu_with_noise_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
    static auto op = create_rrelu_with_noise_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, noise, lower, upper, training, self_is_result);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_, name, "aten::rrelu_with_noise_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(rrelu_with_noise_, schema_str, "rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)")

// aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<rrelu_with_noise_::schema> create_rrelu_with_noise__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(rrelu_with_noise_::name, rrelu_with_noise_::overload_name)
      .typed<rrelu_with_noise_::schema>();
}

// aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
at::Tensor & rrelu_with_noise_::call(at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu_with_noise__typed_handle();
    return op.call(self, noise, lower, upper, training, generator);
}

// aten::rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
at::Tensor & rrelu_with_noise_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, c10::optional<at::Generator> generator) {
    static auto op = create_rrelu_with_noise__typed_handle();
    return op.redispatch(dispatchKeySet, self, noise, lower, upper, training, generator);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_out, name, "aten::softplus")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_out, schema_str, "softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)")

// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<softplus_out::schema> create_softplus_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softplus_out::name, softplus_out::overload_name)
      .typed<softplus_out::schema>();
}

// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softplus_out::call(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
    static auto op = create_softplus_out_typed_handle();
    return op.call(self, beta, threshold, out);
}

// aten::softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softplus_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, at::Tensor & out) {
    static auto op = create_softplus_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, beta, threshold, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus, name, "aten::softplus")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus, schema_str, "softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor")

// aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softplus::schema> create_softplus_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softplus::name, softplus::overload_name)
      .typed<softplus::schema>();
}

// aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
at::Tensor softplus::call(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
    static auto op = create_softplus_typed_handle();
    return op.call(self, beta, threshold);
}

// aten::softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
at::Tensor softplus::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
    static auto op = create_softplus_typed_handle();
    return op.redispatch(dispatchKeySet, self, beta, threshold);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_backward_grad_input, name, "aten::softplus_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_backward_grad_input, schema_str, "softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<softplus_backward_grad_input::schema> create_softplus_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softplus_backward_grad_input::name, softplus_backward_grad_input::overload_name)
      .typed<softplus_backward_grad_input::schema>();
}

// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & softplus_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output, at::Tensor & grad_input) {
    static auto op = create_softplus_backward_grad_input_typed_handle();
    return op.call(grad_output, self, beta, threshold, output, grad_input);
}

// aten::softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & softplus_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output, at::Tensor & grad_input) {
    static auto op = create_softplus_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, beta, threshold, output, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_backward, name, "aten::softplus_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softplus_backward, schema_str, "softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output) -> Tensor")

// aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softplus_backward::schema> create_softplus_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softplus_backward::name, softplus_backward::overload_name)
      .typed<softplus_backward::schema>();
}

// aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output) -> Tensor
at::Tensor softplus_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output) {
    static auto op = create_softplus_backward_typed_handle();
    return op.call(grad_output, self, beta, threshold, output);
}

// aten::softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, Tensor output) -> Tensor
at::Tensor softplus_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output) {
    static auto op = create_softplus_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, beta, threshold, output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_out, name, "aten::softshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_out, schema_str, "softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)")

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<softshrink_out::schema> create_softshrink_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softshrink_out::name, softshrink_out::overload_name)
      .typed<softshrink_out::schema>();
}

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softshrink_out::call(const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    static auto op = create_softshrink_out_typed_handle();
    return op.call(self, lambd, out);
}

// aten::softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & softshrink_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & out) {
    static auto op = create_softshrink_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink, name, "aten::softshrink")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink, schema_str, "softshrink(Tensor self, Scalar lambd=0.5) -> Tensor")

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softshrink::schema> create_softshrink_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softshrink::name, softshrink::overload_name)
      .typed<softshrink::schema>();
}

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor softshrink::call(const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_softshrink_typed_handle();
    return op.call(self, lambd);
}

// aten::softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
at::Tensor softshrink::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_softshrink_typed_handle();
    return op.redispatch(dispatchKeySet, self, lambd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_backward_grad_input, name, "aten::softshrink_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_backward_grad_input, schema_str, "softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<softshrink_backward_grad_input::schema> create_softshrink_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softshrink_backward_grad_input::name, softshrink_backward_grad_input::overload_name)
      .typed<softshrink_backward_grad_input::schema>();
}

// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & softshrink_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
    static auto op = create_softshrink_backward_grad_input_typed_handle();
    return op.call(grad_output, self, lambd, grad_input);
}

// aten::softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & softshrink_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd, at::Tensor & grad_input) {
    static auto op = create_softshrink_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, lambd, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_backward, name, "aten::softshrink_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(softshrink_backward, schema_str, "softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor")

// aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<softshrink_backward::schema> create_softshrink_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(softshrink_backward::name, softshrink_backward::overload_name)
      .typed<softshrink_backward::schema>();
}

// aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
at::Tensor softshrink_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_softshrink_backward_typed_handle();
    return op.call(grad_output, self, lambd);
}

// aten::softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
at::Tensor softshrink_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
    static auto op = create_softshrink_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, lambd);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d_out, name, "aten::adaptive_avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d_out, schema_str, "adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool2d_out::schema> create_adaptive_avg_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool2d_out::name, adaptive_avg_pool2d_out::overload_name)
      .typed<adaptive_avg_pool2d_out::schema>();
}

// aten::adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool2d_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
    static auto op = create_adaptive_avg_pool2d_out_typed_handle();
    return op.call(self, output_size, out);
}

// aten::adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
    static auto op = create_adaptive_avg_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d, name, "aten::adaptive_avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool2d, schema_str, "adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor")

// aten::adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool2d::schema> create_adaptive_avg_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool2d::name, adaptive_avg_pool2d::overload_name)
      .typed<adaptive_avg_pool2d::schema>();
}

// aten::adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
at::Tensor adaptive_avg_pool2d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_avg_pool2d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
at::Tensor adaptive_avg_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_avg_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_adaptive_avg_pool2d, name, "aten::mkldnn_adaptive_avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_adaptive_avg_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_adaptive_avg_pool2d, schema_str, "mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor")

// aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_adaptive_avg_pool2d::schema> create_mkldnn_adaptive_avg_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_adaptive_avg_pool2d::name, mkldnn_adaptive_avg_pool2d::overload_name)
      .typed<mkldnn_adaptive_avg_pool2d::schema>();
}

// aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
at::Tensor mkldnn_adaptive_avg_pool2d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_mkldnn_adaptive_avg_pool2d_typed_handle();
    return op.call(self, output_size);
}

// aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
at::Tensor mkldnn_adaptive_avg_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_mkldnn_adaptive_avg_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_adaptive_avg_pool2d_backward, name, "aten::mkldnn_adaptive_avg_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_adaptive_avg_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(mkldnn_adaptive_avg_pool2d_backward, schema_str, "mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<mkldnn_adaptive_avg_pool2d_backward::schema> create_mkldnn_adaptive_avg_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(mkldnn_adaptive_avg_pool2d_backward::name, mkldnn_adaptive_avg_pool2d_backward::overload_name)
      .typed<mkldnn_adaptive_avg_pool2d_backward::schema>();
}

// aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor mkldnn_adaptive_avg_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_mkldnn_adaptive_avg_pool2d_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor mkldnn_adaptive_avg_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create_mkldnn_adaptive_avg_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool2d, name, "aten::_adaptive_avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool2d, schema_str, "_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor")

// aten::_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool2d::schema> create__adaptive_avg_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool2d::name, _adaptive_avg_pool2d::overload_name)
      .typed<_adaptive_avg_pool2d::schema>();
}

// aten::_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
at::Tensor _adaptive_avg_pool2d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create__adaptive_avg_pool2d_typed_handle();
    return op.call(self, output_size);
}

// aten::_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
at::Tensor _adaptive_avg_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create__adaptive_avg_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool2d_backward, name, "aten::_adaptive_avg_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool2d_backward, schema_str, "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool2d_backward::schema> create__adaptive_avg_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool2d_backward::name, _adaptive_avg_pool2d_backward::overload_name)
      .typed<_adaptive_avg_pool2d_backward::schema>();
}

// aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor _adaptive_avg_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create__adaptive_avg_pool2d_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor _adaptive_avg_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create__adaptive_avg_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_out, name, "aten::adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_out, schema_str, "adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool3d_out::schema> create_adaptive_avg_pool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool3d_out::name, adaptive_avg_pool3d_out::overload_name)
      .typed<adaptive_avg_pool3d_out::schema>();
}

// aten::adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
    static auto op = create_adaptive_avg_pool3d_out_typed_handle();
    return op.call(self, output_size, out);
}

// aten::adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out) {
    static auto op = create_adaptive_avg_pool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d, name, "aten::adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d, schema_str, "adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor")

// aten::adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool3d::schema> create_adaptive_avg_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool3d::name, adaptive_avg_pool3d::overload_name)
      .typed<adaptive_avg_pool3d::schema>();
}

// aten::adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
at::Tensor adaptive_avg_pool3d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_avg_pool3d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
at::Tensor adaptive_avg_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_avg_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d, name, "aten::_adaptive_avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d, schema_str, "_adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor")

// aten::_adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool3d::schema> create__adaptive_avg_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool3d::name, _adaptive_avg_pool3d::overload_name)
      .typed<_adaptive_avg_pool3d::schema>();
}

// aten::_adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
at::Tensor _adaptive_avg_pool3d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create__adaptive_avg_pool3d_typed_handle();
    return op.call(self, output_size);
}

// aten::_adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
at::Tensor _adaptive_avg_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create__adaptive_avg_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_backward_grad_input, name, "aten::adaptive_avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_avg_pool3d_backward_grad_input, schema_str, "adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_avg_pool3d_backward_grad_input::schema> create_adaptive_avg_pool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_avg_pool3d_backward_grad_input::name, adaptive_avg_pool3d_backward_grad_input::overload_name)
      .typed<adaptive_avg_pool3d_backward_grad_input::schema>();
}

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_adaptive_avg_pool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, grad_input);
}

// aten::adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_avg_pool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::Tensor & grad_input) {
    static auto op = create_adaptive_avg_pool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward, name, "aten::_adaptive_avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_adaptive_avg_pool3d_backward, schema_str, "_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor")

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_adaptive_avg_pool3d_backward::schema> create__adaptive_avg_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_adaptive_avg_pool3d_backward::name, _adaptive_avg_pool3d_backward::overload_name)
      .typed<_adaptive_avg_pool3d_backward::schema>();
}

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor _adaptive_avg_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create__adaptive_avg_pool3d_backward_typed_handle();
    return op.call(grad_output, self);
}

// aten::_adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
at::Tensor _adaptive_avg_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self) {
    static auto op = create__adaptive_avg_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_out, name, "aten::adaptive_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_out, schema_str, "adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d_out::schema> create_adaptive_max_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d_out::name, adaptive_max_pool2d_out::overload_name)
      .typed<adaptive_max_pool2d_out::schema>();
}

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool2d_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_adaptive_max_pool2d_out_typed_handle();
    return op.call(self, output_size, out, indices);
}

// aten::adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_adaptive_max_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d, name, "aten::adaptive_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d, schema_str, "adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)")

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d::schema> create_adaptive_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d::name, adaptive_max_pool2d::overload_name)
      .typed<adaptive_max_pool2d::schema>();
}

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool2d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_max_pool2d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward_grad_input, name, "aten::adaptive_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward_grad_input, schema_str, "adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d_backward_grad_input::schema> create_adaptive_max_pool2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d_backward_grad_input::name, adaptive_max_pool2d_backward_grad_input::overload_name)
      .typed<adaptive_max_pool2d_backward_grad_input::schema>();
}

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_adaptive_max_pool2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, indices, grad_input);
}

// aten::adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_adaptive_max_pool2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward, name, "aten::adaptive_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool2d_backward, schema_str, "adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor")

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool2d_backward::schema> create_adaptive_max_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool2d_backward::name, adaptive_max_pool2d_backward::overload_name)
      .typed<adaptive_max_pool2d_backward::schema>();
}

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    static auto op = create_adaptive_max_pool2d_backward_typed_handle();
    return op.call(grad_output, self, indices);
}

// aten::adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    static auto op = create_adaptive_max_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_out, name, "aten::adaptive_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_out, schema_str, "adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d_out::schema> create_adaptive_max_pool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d_out::name, adaptive_max_pool3d_out::overload_name)
      .typed<adaptive_max_pool3d_out::schema>();
}

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool3d_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_adaptive_max_pool3d_out_typed_handle();
    return op.call(self, output_size, out, indices);
}

// aten::adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> adaptive_max_pool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_adaptive_max_pool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, out, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d, name, "aten::adaptive_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d, schema_str, "adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)")

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d::schema> create_adaptive_max_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d::name, adaptive_max_pool3d::overload_name)
      .typed<adaptive_max_pool3d::schema>();
}

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool3d::call(const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_max_pool3d_typed_handle();
    return op.call(self, output_size);
}

// aten::adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size) {
    static auto op = create_adaptive_max_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward_grad_input, name, "aten::adaptive_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward_grad_input, schema_str, "adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d_backward_grad_input::schema> create_adaptive_max_pool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d_backward_grad_input::name, adaptive_max_pool3d_backward_grad_input::overload_name)
      .typed<adaptive_max_pool3d_backward_grad_input::schema>();
}

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_adaptive_max_pool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, indices, grad_input);
}

// aten::adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & adaptive_max_pool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_adaptive_max_pool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward, name, "aten::adaptive_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(adaptive_max_pool3d_backward, schema_str, "adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor")

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<adaptive_max_pool3d_backward::schema> create_adaptive_max_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(adaptive_max_pool3d_backward::name, adaptive_max_pool3d_backward::overload_name)
      .typed<adaptive_max_pool3d_backward::schema>();
}

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    static auto op = create_adaptive_max_pool3d_backward_typed_handle();
    return op.call(grad_output, self, indices);
}

// aten::adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
at::Tensor adaptive_max_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
    static auto op = create_adaptive_max_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_out, name, "aten::avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_out, schema_str, "avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool2d_out::schema> create_avg_pool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool2d_out::name, avg_pool2d_out::overload_name)
      .typed<avg_pool2d_out::schema>();
}

// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & avg_pool2d_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
    static auto op = create_avg_pool2d_out_typed_handle();
    return op.call(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}

// aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & avg_pool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
    static auto op = create_avg_pool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d, name, "aten::avg_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d, schema_str, "avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor")

// aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool2d::schema> create_avg_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool2d::name, avg_pool2d::overload_name)
      .typed<avg_pool2d::schema>();
}

// aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
at::Tensor avg_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool2d_typed_handle();
    return op.call(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

// aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
at::Tensor avg_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward_grad_input, name, "aten::avg_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward_grad_input, schema_str, "avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool2d_backward_grad_input::schema> create_avg_pool2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool2d_backward_grad_input::name, avg_pool2d_backward_grad_input::overload_name)
      .typed<avg_pool2d_backward_grad_input::schema>();
}

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & avg_pool2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    static auto op = create_avg_pool2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}

// aten::avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & avg_pool2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    static auto op = create_avg_pool2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward, name, "aten::avg_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool2d_backward, schema_str, "avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor")

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool2d_backward::schema> create_avg_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool2d_backward::name, avg_pool2d_backward::overload_name)
      .typed<avg_pool2d_backward::schema>();
}

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
at::Tensor avg_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool2d_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

// aten::avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
at::Tensor avg_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_out, name, "aten::avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_out, schema_str, "avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool3d_out::schema> create_avg_pool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool3d_out::name, avg_pool3d_out::overload_name)
      .typed<avg_pool3d_out::schema>();
}

// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & avg_pool3d_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
    static auto op = create_avg_pool3d_out_typed_handle();
    return op.call(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}

// aten::avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & avg_pool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & out) {
    static auto op = create_avg_pool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d, name, "aten::avg_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d, schema_str, "avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor")

// aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool3d::schema> create_avg_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool3d::name, avg_pool3d::overload_name)
      .typed<avg_pool3d::schema>();
}

// aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
at::Tensor avg_pool3d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool3d_typed_handle();
    return op.call(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

// aten::avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
at::Tensor avg_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_backward_grad_input, name, "aten::avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_backward_grad_input, schema_str, "avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool3d_backward_grad_input::schema> create_avg_pool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool3d_backward_grad_input::name, avg_pool3d_backward_grad_input::overload_name)
      .typed<avg_pool3d_backward_grad_input::schema>();
}

// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & avg_pool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    static auto op = create_avg_pool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}

// aten::avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & avg_pool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override, at::Tensor & grad_input) {
    static auto op = create_avg_pool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_backward, name, "aten::avg_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(avg_pool3d_backward, schema_str, "avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor")

// aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<avg_pool3d_backward::schema> create_avg_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(avg_pool3d_backward::name, avg_pool3d_backward::overload_name)
      .typed<avg_pool3d_backward::schema>();
}

// aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
at::Tensor avg_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool3d_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

// aten::avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
at::Tensor avg_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    static auto op = create_avg_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_output, name, "aten::fractional_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_output, schema_str, "fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool2d_output::schema> create_fractional_max_pool2d_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool2d_output::name, fractional_max_pool2d_output::overload_name)
      .typed<fractional_max_pool2d_output::schema>();
}

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool2d_output::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    static auto op = create_fractional_max_pool2d_output_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples, output, indices);
}

// aten::fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool2d_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    static auto op = create_fractional_max_pool2d_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples, output, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d, name, "aten::fractional_max_pool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d, schema_str, "fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)")

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool2d::schema> create_fractional_max_pool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool2d::name, fractional_max_pool2d::overload_name)
      .typed<fractional_max_pool2d::schema>();
}

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool2d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    static auto op = create_fractional_max_pool2d_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples);
}

// aten::fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    static auto op = create_fractional_max_pool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_backward_grad_input, name, "aten::fractional_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_backward_grad_input, schema_str, "fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool2d_backward_grad_input::schema> create_fractional_max_pool2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool2d_backward_grad_input::name, fractional_max_pool2d_backward_grad_input::overload_name)
      .typed<fractional_max_pool2d_backward_grad_input::schema>();
}

// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & fractional_max_pool2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_fractional_max_pool2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, output_size, indices, grad_input);
}

// aten::fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & fractional_max_pool2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_fractional_max_pool2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, output_size, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_backward, name, "aten::fractional_max_pool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool2d_backward, schema_str, "fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor")

// aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool2d_backward::schema> create_fractional_max_pool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool2d_backward::name, fractional_max_pool2d_backward::overload_name)
      .typed<fractional_max_pool2d_backward::schema>();
}

// aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor
at::Tensor fractional_max_pool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
    static auto op = create_fractional_max_pool2d_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, output_size, indices);
}

// aten::fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -> Tensor
at::Tensor fractional_max_pool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
    static auto op = create_fractional_max_pool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, output_size, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_output, name, "aten::fractional_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_output, schema_str, "fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool3d_output::schema> create_fractional_max_pool3d_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool3d_output::name, fractional_max_pool3d_output::overload_name)
      .typed<fractional_max_pool3d_output::schema>();
}

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool3d_output::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    static auto op = create_fractional_max_pool3d_output_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples, output, indices);
}

// aten::fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> fractional_max_pool3d_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples, at::Tensor & output, at::Tensor & indices) {
    static auto op = create_fractional_max_pool3d_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples, output, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d, name, "aten::fractional_max_pool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d, schema_str, "fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)")

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool3d::schema> create_fractional_max_pool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool3d::name, fractional_max_pool3d::overload_name)
      .typed<fractional_max_pool3d::schema>();
}

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool3d::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    static auto op = create_fractional_max_pool3d_typed_handle();
    return op.call(self, kernel_size, output_size, random_samples);
}

// aten::fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> fractional_max_pool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
    static auto op = create_fractional_max_pool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, output_size, random_samples);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_backward_grad_input, name, "aten::fractional_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_backward_grad_input, schema_str, "fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool3d_backward_grad_input::schema> create_fractional_max_pool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool3d_backward_grad_input::name, fractional_max_pool3d_backward_grad_input::overload_name)
      .typed<fractional_max_pool3d_backward_grad_input::schema>();
}

// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & fractional_max_pool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_fractional_max_pool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, output_size, indices, grad_input);
}

// aten::fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & fractional_max_pool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_fractional_max_pool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, output_size, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_backward, name, "aten::fractional_max_pool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fractional_max_pool3d_backward, schema_str, "fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor")

// aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fractional_max_pool3d_backward::schema> create_fractional_max_pool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fractional_max_pool3d_backward::name, fractional_max_pool3d_backward::overload_name)
      .typed<fractional_max_pool3d_backward::schema>();
}

// aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor
at::Tensor fractional_max_pool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
    static auto op = create_fractional_max_pool3d_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, output_size, indices);
}

// aten::fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -> Tensor
at::Tensor fractional_max_pool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & indices) {
    static auto op = create_fractional_max_pool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, output_size, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_out, name, "aten::max_pool2d_with_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_out, schema_str, "max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<max_pool2d_with_indices_out::schema> create_max_pool2d_with_indices_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool2d_with_indices_out::name, max_pool2d_with_indices_out::overload_name)
      .typed<max_pool2d_with_indices_out::schema>();
}

// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> max_pool2d_with_indices_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_max_pool2d_with_indices_out_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode, out, indices);
}

// aten::max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> max_pool2d_with_indices_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_max_pool2d_with_indices_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode, out, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices, name, "aten::max_pool2d_with_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices, schema_str, "max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)")

// aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<max_pool2d_with_indices::schema> create_max_pool2d_with_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool2d_with_indices::name, max_pool2d_with_indices::overload_name)
      .typed<max_pool2d_with_indices::schema>();
}

// aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> max_pool2d_with_indices::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool2d_with_indices_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> max_pool2d_with_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool2d_with_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_backward_grad_input, name, "aten::max_pool2d_with_indices_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_backward_grad_input, schema_str, "max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_pool2d_with_indices_backward_grad_input::schema> create_max_pool2d_with_indices_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool2d_with_indices_backward_grad_input::name, max_pool2d_with_indices_backward_grad_input::overload_name)
      .typed<max_pool2d_with_indices_backward_grad_input::schema>();
}

// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_pool2d_with_indices_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_max_pool2d_with_indices_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices, grad_input);
}

// aten::max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_pool2d_with_indices_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_max_pool2d_with_indices_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_backward, name, "aten::max_pool2d_with_indices_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool2d_with_indices_backward, schema_str, "max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor")

// aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool2d_with_indices_backward::schema> create_max_pool2d_with_indices_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool2d_with_indices_backward::name, max_pool2d_with_indices_backward::overload_name)
      .typed<max_pool2d_with_indices_backward::schema>();
}

// aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
at::Tensor max_pool2d_with_indices_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    static auto op = create_max_pool2d_with_indices_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

// aten::max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
at::Tensor max_pool2d_with_indices_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    static auto op = create_max_pool2d_with_indices_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_out, name, "aten::max_pool3d_with_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_out, schema_str, "max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))")

// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<max_pool3d_with_indices_out::schema> create_max_pool3d_with_indices_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool3d_with_indices_out::name, max_pool3d_with_indices_out::overload_name)
      .typed<max_pool3d_with_indices_out::schema>();
}

// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> max_pool3d_with_indices_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_max_pool3d_with_indices_out_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode, out, indices);
}

// aten::max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> max_pool3d_with_indices_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, at::Tensor & out, at::Tensor & indices) {
    static auto op = create_max_pool3d_with_indices_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode, out, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices, name, "aten::max_pool3d_with_indices")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices, schema_str, "max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)")

// aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<max_pool3d_with_indices::schema> create_max_pool3d_with_indices_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool3d_with_indices::name, max_pool3d_with_indices::overload_name)
      .typed<max_pool3d_with_indices::schema>();
}

// aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> max_pool3d_with_indices::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool3d_with_indices_typed_handle();
    return op.call(self, kernel_size, stride, padding, dilation, ceil_mode);
}

// aten::max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> max_pool3d_with_indices::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
    static auto op = create_max_pool3d_with_indices_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, stride, padding, dilation, ceil_mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_backward_grad_input, name, "aten::max_pool3d_with_indices_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_backward_grad_input, schema_str, "max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_pool3d_with_indices_backward_grad_input::schema> create_max_pool3d_with_indices_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool3d_with_indices_backward_grad_input::name, max_pool3d_with_indices_backward_grad_input::overload_name)
      .typed<max_pool3d_with_indices_backward_grad_input::schema>();
}

// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_pool3d_with_indices_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_max_pool3d_with_indices_backward_grad_input_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices, grad_input);
}

// aten::max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_pool3d_with_indices_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices, at::Tensor & grad_input) {
    static auto op = create_max_pool3d_with_indices_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_backward, name, "aten::max_pool3d_with_indices_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_pool3d_with_indices_backward, schema_str, "max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor")

// aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_pool3d_with_indices_backward::schema> create_max_pool3d_with_indices_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_pool3d_with_indices_backward::name, max_pool3d_with_indices_backward::overload_name)
      .typed<max_pool3d_with_indices_backward::schema>();
}

// aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
at::Tensor max_pool3d_with_indices_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    static auto op = create_max_pool3d_with_indices_backward_typed_handle();
    return op.call(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

// aten::max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
at::Tensor max_pool3d_with_indices_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
    static auto op = create_max_pool3d_with_indices_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_out, name, "aten::max_unpool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_out, schema_str, "max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)")

// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool2d_out::schema> create_max_unpool2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool2d_out::name, max_unpool2d_out::overload_name)
      .typed<max_unpool2d_out::schema>();
}

// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool2d_out::call(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & out) {
    static auto op = create_max_unpool2d_out_typed_handle();
    return op.call(self, indices, output_size, out);
}

// aten::max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & out) {
    static auto op = create_max_unpool2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d, name, "aten::max_unpool2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d, schema_str, "max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor")

// aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool2d::schema> create_max_unpool2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool2d::name, max_unpool2d::overload_name)
      .typed<max_unpool2d::schema>();
}

// aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor
at::Tensor max_unpool2d::call(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
    static auto op = create_max_unpool2d_typed_handle();
    return op.call(self, indices, output_size);
}

// aten::max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor
at::Tensor max_unpool2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
    static auto op = create_max_unpool2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_backward_grad_input, name, "aten::max_unpool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_backward_grad_input, schema_str, "max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool2d_backward_grad_input::schema> create_max_unpool2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool2d_backward_grad_input::name, max_unpool2d_backward_grad_input::overload_name)
      .typed<max_unpool2d_backward_grad_input::schema>();
}

// aten::max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_unpool2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & grad_input) {
    static auto op = create_max_unpool2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, indices, output_size, grad_input);
}

// aten::max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_unpool2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::Tensor & grad_input) {
    static auto op = create_max_unpool2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, output_size, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_backward, name, "aten::max_unpool2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool2d_backward, schema_str, "max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor")

// aten::max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool2d_backward::schema> create_max_unpool2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool2d_backward::name, max_unpool2d_backward::overload_name)
      .typed<max_unpool2d_backward::schema>();
}

// aten::max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor
at::Tensor max_unpool2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
    static auto op = create_max_unpool2d_backward_typed_handle();
    return op.call(grad_output, self, indices, output_size);
}

// aten::max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor
at::Tensor max_unpool2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size) {
    static auto op = create_max_unpool2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, output_size);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_out, name, "aten::max_unpool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_out, schema_str, "max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool3d_out::schema> create_max_unpool3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool3d_out::name, max_unpool3d_out::overload_name)
      .typed<max_unpool3d_out::schema>();
}

// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool3d_out::call(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_max_unpool3d_out_typed_handle();
    return op.call(self, indices, output_size, stride, padding, out);
}

// aten::max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & max_unpool3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_max_unpool3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size, stride, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d, name, "aten::max_unpool3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d, schema_str, "max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor")

// aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool3d::schema> create_max_unpool3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool3d::name, max_unpool3d::overload_name)
      .typed<max_unpool3d::schema>();
}

// aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
at::Tensor max_unpool3d::call(const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_max_unpool3d_typed_handle();
    return op.call(self, indices, output_size, stride, padding);
}

// aten::max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
at::Tensor max_unpool3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_max_unpool3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, indices, output_size, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_backward_grad_input, name, "aten::max_unpool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_backward_grad_input, schema_str, "max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool3d_backward_grad_input::schema> create_max_unpool3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool3d_backward_grad_input::name, max_unpool3d_backward_grad_input::overload_name)
      .typed<max_unpool3d_backward_grad_input::schema>();
}

// aten::max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_unpool3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_max_unpool3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, indices, output_size, stride, padding, grad_input);
}

// aten::max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & max_unpool3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_max_unpool3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, output_size, stride, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_backward, name, "aten::max_unpool3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(max_unpool3d_backward, schema_str, "max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor")

// aten::max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<max_unpool3d_backward::schema> create_max_unpool3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(max_unpool3d_backward::name, max_unpool3d_backward::overload_name)
      .typed<max_unpool3d_backward::schema>();
}

// aten::max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
at::Tensor max_unpool3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_max_unpool3d_backward_typed_handle();
    return op.call(grad_output, self, indices, output_size, stride, padding);
}

// aten::max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
at::Tensor max_unpool3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices, at::IntArrayRef output_size, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_max_unpool3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, indices, output_size, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_out, name, "aten::reflection_pad1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_out, schema_str, "reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad1d_out::schema> create_reflection_pad1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad1d_out::name, reflection_pad1d_out::overload_name)
      .typed<reflection_pad1d_out::schema>();
}

// aten::reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad1d_out::call(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_reflection_pad1d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_reflection_pad1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d, name, "aten::reflection_pad1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d, schema_str, "reflection_pad1d(Tensor self, int[2] padding) -> Tensor")

// aten::reflection_pad1d(Tensor self, int[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad1d::schema> create_reflection_pad1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad1d::name, reflection_pad1d::overload_name)
      .typed<reflection_pad1d::schema>();
}

// aten::reflection_pad1d(Tensor self, int[2] padding) -> Tensor
at::Tensor reflection_pad1d::call(const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad1d_typed_handle();
    return op.call(self, padding);
}

// aten::reflection_pad1d(Tensor self, int[2] padding) -> Tensor
at::Tensor reflection_pad1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_backward_grad_input, name, "aten::reflection_pad1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_backward_grad_input, schema_str, "reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad1d_backward_grad_input::schema> create_reflection_pad1d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad1d_backward_grad_input::name, reflection_pad1d_backward_grad_input::overload_name)
      .typed<reflection_pad1d_backward_grad_input::schema>();
}

// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad1d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_reflection_pad1d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad1d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_reflection_pad1d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_backward, name, "aten::reflection_pad1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad1d_backward, schema_str, "reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor")

// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad1d_backward::schema> create_reflection_pad1d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad1d_backward::name, reflection_pad1d_backward::overload_name)
      .typed<reflection_pad1d_backward::schema>();
}

// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
at::Tensor reflection_pad1d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad1d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
at::Tensor reflection_pad1d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad1d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_out, name, "aten::reflection_pad2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_out, schema_str, "reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad2d_out::schema> create_reflection_pad2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad2d_out::name, reflection_pad2d_out::overload_name)
      .typed<reflection_pad2d_out::schema>();
}

// aten::reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad2d_out::call(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_reflection_pad2d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_reflection_pad2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d, name, "aten::reflection_pad2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d, schema_str, "reflection_pad2d(Tensor self, int[4] padding) -> Tensor")

// aten::reflection_pad2d(Tensor self, int[4] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad2d::schema> create_reflection_pad2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad2d::name, reflection_pad2d::overload_name)
      .typed<reflection_pad2d::schema>();
}

// aten::reflection_pad2d(Tensor self, int[4] padding) -> Tensor
at::Tensor reflection_pad2d::call(const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad2d_typed_handle();
    return op.call(self, padding);
}

// aten::reflection_pad2d(Tensor self, int[4] padding) -> Tensor
at::Tensor reflection_pad2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_backward_grad_input, name, "aten::reflection_pad2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_backward_grad_input, schema_str, "reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad2d_backward_grad_input::schema> create_reflection_pad2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad2d_backward_grad_input::name, reflection_pad2d_backward_grad_input::overload_name)
      .typed<reflection_pad2d_backward_grad_input::schema>();
}

// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_reflection_pad2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_reflection_pad2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_backward, name, "aten::reflection_pad2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad2d_backward, schema_str, "reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor")

// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad2d_backward::schema> create_reflection_pad2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad2d_backward::name, reflection_pad2d_backward::overload_name)
      .typed<reflection_pad2d_backward::schema>();
}

// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
at::Tensor reflection_pad2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad2d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
at::Tensor reflection_pad2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_out, name, "aten::reflection_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_out, schema_str, "reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d_out::schema> create_reflection_pad3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d_out::name, reflection_pad3d_out::overload_name)
      .typed<reflection_pad3d_out::schema>();
}

// aten::reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad3d_out::call(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_reflection_pad3d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & reflection_pad3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_reflection_pad3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d, name, "aten::reflection_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d, schema_str, "reflection_pad3d(Tensor self, int[6] padding) -> Tensor")

// aten::reflection_pad3d(Tensor self, int[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d::schema> create_reflection_pad3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d::name, reflection_pad3d::overload_name)
      .typed<reflection_pad3d::schema>();
}

// aten::reflection_pad3d(Tensor self, int[6] padding) -> Tensor
at::Tensor reflection_pad3d::call(const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad3d_typed_handle();
    return op.call(self, padding);
}

// aten::reflection_pad3d(Tensor self, int[6] padding) -> Tensor
at::Tensor reflection_pad3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward_grad_input, name, "aten::reflection_pad3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward_grad_input, schema_str, "reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d_backward_grad_input::schema> create_reflection_pad3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d_backward_grad_input::name, reflection_pad3d_backward_grad_input::overload_name)
      .typed<reflection_pad3d_backward_grad_input::schema>();
}

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_reflection_pad3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & reflection_pad3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_reflection_pad3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward, name, "aten::reflection_pad3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(reflection_pad3d_backward, schema_str, "reflection_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor")

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<reflection_pad3d_backward::schema> create_reflection_pad3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(reflection_pad3d_backward::name, reflection_pad3d_backward::overload_name)
      .typed<reflection_pad3d_backward::schema>();
}

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
at::Tensor reflection_pad3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad3d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::reflection_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
at::Tensor reflection_pad3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_reflection_pad3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_out, name, "aten::replication_pad1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_out, schema_str, "replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad1d_out::schema> create_replication_pad1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad1d_out::name, replication_pad1d_out::overload_name)
      .typed<replication_pad1d_out::schema>();
}

// aten::replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad1d_out::call(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_replication_pad1d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_replication_pad1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d, name, "aten::replication_pad1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d, schema_str, "replication_pad1d(Tensor self, int[2] padding) -> Tensor")

// aten::replication_pad1d(Tensor self, int[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad1d::schema> create_replication_pad1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad1d::name, replication_pad1d::overload_name)
      .typed<replication_pad1d::schema>();
}

// aten::replication_pad1d(Tensor self, int[2] padding) -> Tensor
at::Tensor replication_pad1d::call(const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad1d_typed_handle();
    return op.call(self, padding);
}

// aten::replication_pad1d(Tensor self, int[2] padding) -> Tensor
at::Tensor replication_pad1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_backward_grad_input, name, "aten::replication_pad1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_backward_grad_input, schema_str, "replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad1d_backward_grad_input::schema> create_replication_pad1d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad1d_backward_grad_input::name, replication_pad1d_backward_grad_input::overload_name)
      .typed<replication_pad1d_backward_grad_input::schema>();
}

// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad1d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_replication_pad1d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad1d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_replication_pad1d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_backward, name, "aten::replication_pad1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad1d_backward, schema_str, "replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor")

// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad1d_backward::schema> create_replication_pad1d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad1d_backward::name, replication_pad1d_backward::overload_name)
      .typed<replication_pad1d_backward::schema>();
}

// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
at::Tensor replication_pad1d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad1d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
at::Tensor replication_pad1d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad1d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_out, name, "aten::replication_pad2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_out, schema_str, "replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d_out::schema> create_replication_pad2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d_out::name, replication_pad2d_out::overload_name)
      .typed<replication_pad2d_out::schema>();
}

// aten::replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad2d_out::call(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_replication_pad2d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_replication_pad2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d, name, "aten::replication_pad2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d, schema_str, "replication_pad2d(Tensor self, int[4] padding) -> Tensor")

// aten::replication_pad2d(Tensor self, int[4] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d::schema> create_replication_pad2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d::name, replication_pad2d::overload_name)
      .typed<replication_pad2d::schema>();
}

// aten::replication_pad2d(Tensor self, int[4] padding) -> Tensor
at::Tensor replication_pad2d::call(const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad2d_typed_handle();
    return op.call(self, padding);
}

// aten::replication_pad2d(Tensor self, int[4] padding) -> Tensor
at::Tensor replication_pad2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward_grad_input, name, "aten::replication_pad2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward_grad_input, schema_str, "replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d_backward_grad_input::schema> create_replication_pad2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d_backward_grad_input::name, replication_pad2d_backward_grad_input::overload_name)
      .typed<replication_pad2d_backward_grad_input::schema>();
}

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_replication_pad2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_replication_pad2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward, name, "aten::replication_pad2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad2d_backward, schema_str, "replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor")

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad2d_backward::schema> create_replication_pad2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad2d_backward::name, replication_pad2d_backward::overload_name)
      .typed<replication_pad2d_backward::schema>();
}

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
at::Tensor replication_pad2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad2d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
at::Tensor replication_pad2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_out, name, "aten::replication_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_out, schema_str, "replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)")

// aten::replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad3d_out::schema> create_replication_pad3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad3d_out::name, replication_pad3d_out::overload_name)
      .typed<replication_pad3d_out::schema>();
}

// aten::replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad3d_out::call(const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_replication_pad3d_out_typed_handle();
    return op.call(self, padding, out);
}

// aten::replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & replication_pad3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_replication_pad3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d, name, "aten::replication_pad3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d, schema_str, "replication_pad3d(Tensor self, int[6] padding) -> Tensor")

// aten::replication_pad3d(Tensor self, int[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad3d::schema> create_replication_pad3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad3d::name, replication_pad3d::overload_name)
      .typed<replication_pad3d::schema>();
}

// aten::replication_pad3d(Tensor self, int[6] padding) -> Tensor
at::Tensor replication_pad3d::call(const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad3d_typed_handle();
    return op.call(self, padding);
}

// aten::replication_pad3d(Tensor self, int[6] padding) -> Tensor
at::Tensor replication_pad3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_backward_grad_input, name, "aten::replication_pad3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_backward_grad_input, schema_str, "replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad3d_backward_grad_input::schema> create_replication_pad3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad3d_backward_grad_input::name, replication_pad3d_backward_grad_input::overload_name)
      .typed<replication_pad3d_backward_grad_input::schema>();
}

// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_replication_pad3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, padding, grad_input);
}

// aten::replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & replication_pad3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding, at::Tensor & grad_input) {
    static auto op = create_replication_pad3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_backward, name, "aten::replication_pad3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(replication_pad3d_backward, schema_str, "replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor")

// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<replication_pad3d_backward::schema> create_replication_pad3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(replication_pad3d_backward::name, replication_pad3d_backward::overload_name)
      .typed<replication_pad3d_backward::schema>();
}

// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
at::Tensor replication_pad3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad3d_backward_typed_handle();
    return op.call(grad_output, self, padding);
}

// aten::replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
at::Tensor replication_pad3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
    static auto op = create_replication_pad3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_vec, name, "aten::upsample_linear1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_vec, schema_str, "upsample_linear1d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_linear1d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_vec::schema> create_upsample_linear1d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_vec::name, upsample_linear1d_vec::overload_name)
      .typed<upsample_linear1d_vec::schema>();
}

// aten::upsample_linear1d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_linear1d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_linear1d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_linear1d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_linear1d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_linear1d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward_vec, name, "aten::upsample_linear1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward_vec, schema_str, "upsample_linear1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_linear1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_backward_vec::schema> create_upsample_linear1d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_backward_vec::name, upsample_linear1d_backward_vec::overload_name)
      .typed<upsample_linear1d_backward_vec::schema>();
}

// aten::upsample_linear1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_linear1d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_linear1d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scale_factors);
}

// aten::upsample_linear1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_linear1d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_linear1d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_vec, name, "aten::upsample_bilinear2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_vec, schema_str, "upsample_bilinear2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_bilinear2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_vec::schema> create_upsample_bilinear2d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_vec::name, upsample_bilinear2d_vec::overload_name)
      .typed<upsample_bilinear2d_vec::schema>();
}

// aten::upsample_bilinear2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bilinear2d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bilinear2d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_bilinear2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bilinear2d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bilinear2d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward_vec, name, "aten::upsample_bilinear2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward_vec, schema_str, "upsample_bilinear2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_bilinear2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_backward_vec::schema> create_upsample_bilinear2d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_backward_vec::name, upsample_bilinear2d_backward_vec::overload_name)
      .typed<upsample_bilinear2d_backward_vec::schema>();
}

// aten::upsample_bilinear2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bilinear2d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bilinear2d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scale_factors);
}

// aten::upsample_bilinear2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bilinear2d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bilinear2d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_vec, name, "aten::upsample_trilinear3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_vec, schema_str, "upsample_trilinear3d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_trilinear3d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_vec::schema> create_upsample_trilinear3d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_vec::name, upsample_trilinear3d_vec::overload_name)
      .typed<upsample_trilinear3d_vec::schema>();
}

// aten::upsample_trilinear3d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_trilinear3d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_trilinear3d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_trilinear3d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_trilinear3d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_trilinear3d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_vec, name, "aten::upsample_trilinear3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_vec, schema_str, "upsample_trilinear3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_trilinear3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_backward_vec::schema> create_upsample_trilinear3d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_backward_vec::name, upsample_trilinear3d_backward_vec::overload_name)
      .typed<upsample_trilinear3d_backward_vec::schema>();
}

// aten::upsample_trilinear3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_trilinear3d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_trilinear3d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scale_factors);
}

// aten::upsample_trilinear3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_trilinear3d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_trilinear3d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_vec, name, "aten::upsample_bicubic2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_vec, schema_str, "upsample_bicubic2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_bicubic2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_vec::schema> create_upsample_bicubic2d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_vec::name, upsample_bicubic2d_vec::overload_name)
      .typed<upsample_bicubic2d_vec::schema>();
}

// aten::upsample_bicubic2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bicubic2d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bicubic2d_vec_typed_handle();
    return op.call(input, output_size, align_corners, scale_factors);
}

// aten::upsample_bicubic2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bicubic2d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bicubic2d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_vec, name, "aten::upsample_bicubic2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_vec, schema_str, "upsample_bicubic2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor")

// aten::upsample_bicubic2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_backward_vec::schema> create_upsample_bicubic2d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_backward_vec::name, upsample_bicubic2d_backward_vec::overload_name)
      .typed<upsample_bicubic2d_backward_vec::schema>();
}

// aten::upsample_bicubic2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bicubic2d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bicubic2d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scale_factors);
}

// aten::upsample_bicubic2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
at::Tensor upsample_bicubic2d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_bicubic2d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_vec, name, "aten::upsample_nearest1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_vec, schema_str, "upsample_nearest1d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest1d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_vec::schema> create_upsample_nearest1d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_vec::name, upsample_nearest1d_vec::overload_name)
      .typed<upsample_nearest1d_vec::schema>();
}

// aten::upsample_nearest1d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest1d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest1d_vec_typed_handle();
    return op.call(input, output_size, scale_factors);
}

// aten::upsample_nearest1d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest1d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest1d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward_vec, name, "aten::upsample_nearest1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward_vec, schema_str, "upsample_nearest1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_backward_vec::schema> create_upsample_nearest1d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_backward_vec::name, upsample_nearest1d_backward_vec::overload_name)
      .typed<upsample_nearest1d_backward_vec::schema>();
}

// aten::upsample_nearest1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest1d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest1d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, scale_factors);
}

// aten::upsample_nearest1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest1d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest1d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_vec, name, "aten::upsample_nearest2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_vec, schema_str, "upsample_nearest2d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest2d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_vec::schema> create_upsample_nearest2d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_vec::name, upsample_nearest2d_vec::overload_name)
      .typed<upsample_nearest2d_vec::schema>();
}

// aten::upsample_nearest2d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest2d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest2d_vec_typed_handle();
    return op.call(input, output_size, scale_factors);
}

// aten::upsample_nearest2d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest2d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest2d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward_vec, name, "aten::upsample_nearest2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward_vec, schema_str, "upsample_nearest2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_backward_vec::schema> create_upsample_nearest2d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_backward_vec::name, upsample_nearest2d_backward_vec::overload_name)
      .typed<upsample_nearest2d_backward_vec::schema>();
}

// aten::upsample_nearest2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest2d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest2d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, scale_factors);
}

// aten::upsample_nearest2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest2d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest2d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_vec, name, "aten::upsample_nearest3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_vec, schema_str, "upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_vec::schema> create_upsample_nearest3d_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_vec::name, upsample_nearest3d_vec::overload_name)
      .typed<upsample_nearest3d_vec::schema>();
}

// aten::upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest3d_vec::call(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest3d_vec_typed_handle();
    return op.call(input, output_size, scale_factors);
}

// aten::upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest3d_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest3d_vec_typed_handle();
    return op.redispatch(dispatchKeySet, input, output_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_vec, name, "aten::upsample_nearest3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_vec, overload_name, "vec")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_vec, schema_str, "upsample_nearest3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor")

// aten::upsample_nearest3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_backward_vec::schema> create_upsample_nearest3d_backward_vec_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_backward_vec::name, upsample_nearest3d_backward_vec::overload_name)
      .typed<upsample_nearest3d_backward_vec::schema>();
}

// aten::upsample_nearest3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest3d_backward_vec::call(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest3d_backward_vec_typed_handle();
    return op.call(grad_output, output_size, input_size, scale_factors);
}

// aten::upsample_nearest3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
at::Tensor upsample_nearest3d_backward_vec::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    static auto op = create_upsample_nearest3d_backward_vec_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scale_factors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_out, name, "aten::upsample_linear1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_out, schema_str, "upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_out::schema> create_upsample_linear1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_out::name, upsample_linear1d_out::overload_name)
      .typed<upsample_linear1d_out::schema>();
}

// aten::upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_linear1d_out::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
    static auto op = create_upsample_linear1d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales, out);
}

// aten::upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_linear1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales, at::Tensor & out) {
    static auto op = create_upsample_linear1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d, name, "aten::upsample_linear1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d, schema_str, "upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, float? scales=None) -> Tensor")

// aten::upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d::schema> create_upsample_linear1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d::name, upsample_linear1d::overload_name)
      .typed<upsample_linear1d::schema>();
}

// aten::upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, float? scales=None) -> Tensor
at::Tensor upsample_linear1d::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
    static auto op = create_upsample_linear1d_typed_handle();
    return op.call(self, output_size, align_corners, scales);
}

// aten::upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, float? scales=None) -> Tensor
at::Tensor upsample_linear1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
    static auto op = create_upsample_linear1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward_grad_input, name, "aten::upsample_linear1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward_grad_input, schema_str, "upsample_linear1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_backward_grad_input::schema> create_upsample_linear1d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_backward_grad_input::name, upsample_linear1d_backward_grad_input::overload_name)
      .typed<upsample_linear1d_backward_grad_input::schema>();
}

// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_linear1d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales, at::Tensor & grad_input) {
    static auto op = create_upsample_linear1d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales, grad_input);
}

// aten::upsample_linear1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_linear1d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales, at::Tensor & grad_input) {
    static auto op = create_upsample_linear1d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward, name, "aten::upsample_linear1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_linear1d_backward, schema_str, "upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None) -> Tensor")

// aten::upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_linear1d_backward::schema> create_upsample_linear1d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_linear1d_backward::name, upsample_linear1d_backward::overload_name)
      .typed<upsample_linear1d_backward::schema>();
}

// aten::upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None) -> Tensor
at::Tensor upsample_linear1d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
    static auto op = create_upsample_linear1d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales);
}

// aten::upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None) -> Tensor
at::Tensor upsample_linear1d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
    static auto op = create_upsample_linear1d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_out, name, "aten::upsample_bilinear2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_out, schema_str, "upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_out::schema> create_upsample_bilinear2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_out::name, upsample_bilinear2d_out::overload_name)
      .typed<upsample_bilinear2d_out::schema>();
}

// aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bilinear2d_out::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_bilinear2d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w, out);
}

// aten::upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bilinear2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_bilinear2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d, name, "aten::upsample_bilinear2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d, schema_str, "upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d::schema> create_upsample_bilinear2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d::name, upsample_bilinear2d::overload_name)
      .typed<upsample_bilinear2d::schema>();
}

// aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bilinear2d::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bilinear2d_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bilinear2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bilinear2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward_grad_input, name, "aten::upsample_bilinear2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward_grad_input, schema_str, "upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_backward_grad_input::schema> create_upsample_bilinear2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_backward_grad_input::name, upsample_bilinear2d_backward_grad_input::overload_name)
      .typed<upsample_bilinear2d_backward_grad_input::schema>();
}

// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_bilinear2d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_bilinear2d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_h, scales_w, grad_input);
}

// aten::upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_bilinear2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_bilinear2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward, name, "aten::upsample_bilinear2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bilinear2d_backward, schema_str, "upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bilinear2d_backward::schema> create_upsample_bilinear2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bilinear2d_backward::name, upsample_bilinear2d_backward::overload_name)
      .typed<upsample_bilinear2d_backward::schema>();
}

// aten::upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bilinear2d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bilinear2d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bilinear2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bilinear2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_out, name, "aten::upsample_bicubic2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_out, schema_str, "upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_out::schema> create_upsample_bicubic2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_out::name, upsample_bicubic2d_out::overload_name)
      .typed<upsample_bicubic2d_out::schema>();
}

// aten::upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_out::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_bicubic2d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w, out);
}

// aten::upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_bicubic2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d, name, "aten::upsample_bicubic2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d, schema_str, "upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d::schema> create_upsample_bicubic2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d::name, upsample_bicubic2d::overload_name)
      .typed<upsample_bicubic2d::schema>();
}

// aten::upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bicubic2d_typed_handle();
    return op.call(self, output_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bicubic2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_grad_input, name, "aten::upsample_bicubic2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward_grad_input, schema_str, "upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_backward_grad_input::schema> create_upsample_bicubic2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_backward_grad_input::name, upsample_bicubic2d_backward_grad_input::overload_name)
      .typed<upsample_bicubic2d_backward_grad_input::schema>();
}

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_bicubic2d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_h, scales_w, grad_input);
}

// aten::upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_bicubic2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_bicubic2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward, name, "aten::upsample_bicubic2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_bicubic2d_backward, schema_str, "upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_bicubic2d_backward::schema> create_upsample_bicubic2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_bicubic2d_backward::name, upsample_bicubic2d_backward::overload_name)
      .typed<upsample_bicubic2d_backward::schema>();
}

// aten::upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bicubic2d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

// aten::upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_bicubic2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_bicubic2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_out, name, "aten::upsample_trilinear3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_out, schema_str, "upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_out::schema> create_upsample_trilinear3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_out::name, upsample_trilinear3d_out::overload_name)
      .typed<upsample_trilinear3d_out::schema>();
}

// aten::upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_trilinear3d_out::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_trilinear3d_out_typed_handle();
    return op.call(self, output_size, align_corners, scales_d, scales_h, scales_w, out);
}

// aten::upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_trilinear3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_trilinear3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_d, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d, name, "aten::upsample_trilinear3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d, schema_str, "upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d::schema> create_upsample_trilinear3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d::name, upsample_trilinear3d::overload_name)
      .typed<upsample_trilinear3d::schema>();
}

// aten::upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_trilinear3d::call(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_trilinear3d_typed_handle();
    return op.call(self, output_size, align_corners, scales_d, scales_h, scales_w);
}

// aten::upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_trilinear3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_trilinear3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, align_corners, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_grad_input, name, "aten::upsample_trilinear3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward_grad_input, schema_str, "upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_backward_grad_input::schema> create_upsample_trilinear3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_backward_grad_input::name, upsample_trilinear3d_backward_grad_input::overload_name)
      .typed<upsample_trilinear3d_backward_grad_input::schema>();
}

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_trilinear3d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_trilinear3d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input);
}

// aten::upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_trilinear3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_trilinear3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward, name, "aten::upsample_trilinear3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_trilinear3d_backward, schema_str, "upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_trilinear3d_backward::schema> create_upsample_trilinear3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_trilinear3d_backward::name, upsample_trilinear3d_backward::overload_name)
      .typed<upsample_trilinear3d_backward::schema>();
}

// aten::upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_trilinear3d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_trilinear3d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}

// aten::upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_trilinear3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_trilinear3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_out, name, "aten::upsample_nearest1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_out, schema_str, "upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_out::schema> create_upsample_nearest1d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_out::name, upsample_nearest1d_out::overload_name)
      .typed<upsample_nearest1d_out::schema>();
}

// aten::upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest1d_out::call(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
    static auto op = create_upsample_nearest1d_out_typed_handle();
    return op.call(self, output_size, scales, out);
}

// aten::upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest1d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales, at::Tensor & out) {
    static auto op = create_upsample_nearest1d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d, name, "aten::upsample_nearest1d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d, schema_str, "upsample_nearest1d(Tensor self, int[1] output_size, float? scales=None) -> Tensor")

// aten::upsample_nearest1d(Tensor self, int[1] output_size, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d::schema> create_upsample_nearest1d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d::name, upsample_nearest1d::overload_name)
      .typed<upsample_nearest1d::schema>();
}

// aten::upsample_nearest1d(Tensor self, int[1] output_size, float? scales=None) -> Tensor
at::Tensor upsample_nearest1d::call(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales) {
    static auto op = create_upsample_nearest1d_typed_handle();
    return op.call(self, output_size, scales);
}

// aten::upsample_nearest1d(Tensor self, int[1] output_size, float? scales=None) -> Tensor
at::Tensor upsample_nearest1d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales) {
    static auto op = create_upsample_nearest1d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward_grad_input, name, "aten::upsample_nearest1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward_grad_input, schema_str, "upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_backward_grad_input::schema> create_upsample_nearest1d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_backward_grad_input::name, upsample_nearest1d_backward_grad_input::overload_name)
      .typed<upsample_nearest1d_backward_grad_input::schema>();
}

// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest1d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales, at::Tensor & grad_input) {
    static auto op = create_upsample_nearest1d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, scales, grad_input);
}

// aten::upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest1d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales, at::Tensor & grad_input) {
    static auto op = create_upsample_nearest1d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward, name, "aten::upsample_nearest1d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest1d_backward, schema_str, "upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor")

// aten::upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest1d_backward::schema> create_upsample_nearest1d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest1d_backward::name, upsample_nearest1d_backward::overload_name)
      .typed<upsample_nearest1d_backward::schema>();
}

// aten::upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor
at::Tensor upsample_nearest1d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales) {
    static auto op = create_upsample_nearest1d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, scales);
}

// aten::upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor
at::Tensor upsample_nearest1d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales) {
    static auto op = create_upsample_nearest1d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_out, name, "aten::upsample_nearest2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_out, schema_str, "upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_out::schema> create_upsample_nearest2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_out::name, upsample_nearest2d_out::overload_name)
      .typed<upsample_nearest2d_out::schema>();
}

// aten::upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest2d_out::call(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_nearest2d_out_typed_handle();
    return op.call(self, output_size, scales_h, scales_w, out);
}

// aten::upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_nearest2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d, name, "aten::upsample_nearest2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d, schema_str, "upsample_nearest2d(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_nearest2d(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d::schema> create_upsample_nearest2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d::name, upsample_nearest2d::overload_name)
      .typed<upsample_nearest2d::schema>();
}

// aten::upsample_nearest2d(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest2d::call(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest2d_typed_handle();
    return op.call(self, output_size, scales_h, scales_w);
}

// aten::upsample_nearest2d(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward_grad_input, name, "aten::upsample_nearest2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward_grad_input, schema_str, "upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_backward_grad_input::schema> create_upsample_nearest2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_backward_grad_input::name, upsample_nearest2d_backward_grad_input::overload_name)
      .typed<upsample_nearest2d_backward_grad_input::schema>();
}

// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest2d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_nearest2d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_h, scales_w, grad_input);
}

// aten::upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_nearest2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward, name, "aten::upsample_nearest2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest2d_backward, schema_str, "upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest2d_backward::schema> create_upsample_nearest2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest2d_backward::name, upsample_nearest2d_backward::overload_name)
      .typed<upsample_nearest2d_backward::schema>();
}

// aten::upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest2d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest2d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_h, scales_w);
}

// aten::upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_out, name, "aten::upsample_nearest3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_out, schema_str, "upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_out::schema> create_upsample_nearest3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_out::name, upsample_nearest3d_out::overload_name)
      .typed<upsample_nearest3d_out::schema>();
}

// aten::upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest3d_out::call(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_nearest3d_out_typed_handle();
    return op.call(self, output_size, scales_d, scales_h, scales_w, out);
}

// aten::upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & upsample_nearest3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & out) {
    static auto op = create_upsample_nearest3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales_d, scales_h, scales_w, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d, name, "aten::upsample_nearest3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d, schema_str, "upsample_nearest3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_nearest3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d::schema> create_upsample_nearest3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d::name, upsample_nearest3d::overload_name)
      .typed<upsample_nearest3d::schema>();
}

// aten::upsample_nearest3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest3d::call(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest3d_typed_handle();
    return op.call(self, output_size, scales_d, scales_h, scales_w);
}

// aten::upsample_nearest3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_grad_input, name, "aten::upsample_nearest3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward_grad_input, schema_str, "upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_backward_grad_input::schema> create_upsample_nearest3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_backward_grad_input::name, upsample_nearest3d_backward_grad_input::overload_name)
      .typed<upsample_nearest3d_backward_grad_input::schema>();
}

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest3d_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_nearest3d_backward_grad_input_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_d, scales_h, scales_w, grad_input);
}

// aten::upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & upsample_nearest3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w, at::Tensor & grad_input) {
    static auto op = create_upsample_nearest3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_d, scales_h, scales_w, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward, name, "aten::upsample_nearest3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(upsample_nearest3d_backward, schema_str, "upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor")

// aten::upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<upsample_nearest3d_backward::schema> create_upsample_nearest3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(upsample_nearest3d_backward::name, upsample_nearest3d_backward::overload_name)
      .typed<upsample_nearest3d_backward::schema>();
}

// aten::upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest3d_backward::call(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest3d_backward_typed_handle();
    return op.call(grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

// aten::upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
at::Tensor upsample_nearest3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
    static auto op = create_upsample_nearest3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_backward_grad_input, name, "aten::sigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_backward_grad_input, schema_str, "sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid_backward_grad_input::schema> create_sigmoid_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid_backward_grad_input::name, sigmoid_backward_grad_input::overload_name)
      .typed<sigmoid_backward_grad_input::schema>();
}

// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & sigmoid_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
    static auto op = create_sigmoid_backward_grad_input_typed_handle();
    return op.call(grad_output, output, grad_input);
}

// aten::sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & sigmoid_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
    static auto op = create_sigmoid_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_backward, name, "aten::sigmoid_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(sigmoid_backward, schema_str, "sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor")

// aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<sigmoid_backward::schema> create_sigmoid_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(sigmoid_backward::name, sigmoid_backward::overload_name)
      .typed<sigmoid_backward::schema>();
}

// aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
at::Tensor sigmoid_backward::call(const at::Tensor & grad_output, const at::Tensor & output) {
    static auto op = create_sigmoid_backward_typed_handle();
    return op.call(grad_output, output);
}

// aten::sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
at::Tensor sigmoid_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output) {
    static auto op = create_sigmoid_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward_grad_input, name, "aten::logit_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward_grad_input, schema_str, "logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<logit_backward_grad_input::schema> create_logit_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_backward_grad_input::name, logit_backward_grad_input::overload_name)
      .typed<logit_backward_grad_input::schema>();
}

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & logit_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
    static auto op = create_logit_backward_grad_input_typed_handle();
    return op.call(grad_output, self, eps, grad_input);
}

// aten::logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & logit_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps, at::Tensor & grad_input) {
    static auto op = create_logit_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, eps, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward, name, "aten::logit_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(logit_backward, schema_str, "logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor")

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<logit_backward::schema> create_logit_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(logit_backward::name, logit_backward::overload_name)
      .typed<logit_backward::schema>();
}

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
at::Tensor logit_backward::call(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_logit_backward_typed_handle();
    return op.call(grad_output, self, eps);
}

// aten::logit_backward(Tensor grad_output, Tensor self, float? eps=None) -> Tensor
at::Tensor logit_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_logit_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_backward_grad_input, name, "aten::tanh_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_backward_grad_input, schema_str, "tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<tanh_backward_grad_input::schema> create_tanh_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh_backward_grad_input::name, tanh_backward_grad_input::overload_name)
      .typed<tanh_backward_grad_input::schema>();
}

// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & tanh_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
    static auto op = create_tanh_backward_grad_input_typed_handle();
    return op.call(grad_output, output, grad_input);
}

// aten::tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & tanh_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output, at::Tensor & grad_input) {
    static auto op = create_tanh_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_backward, name, "aten::tanh_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(tanh_backward, schema_str, "tanh_backward(Tensor grad_output, Tensor output) -> Tensor")

// aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<tanh_backward::schema> create_tanh_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(tanh_backward::name, tanh_backward::overload_name)
      .typed<tanh_backward::schema>();
}

// aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor
at::Tensor tanh_backward::call(const at::Tensor & grad_output, const at::Tensor & output) {
    static auto op = create_tanh_backward_typed_handle();
    return op.call(grad_output, output);
}

// aten::tanh_backward(Tensor grad_output, Tensor output) -> Tensor
at::Tensor tanh_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & output) {
    static auto op = create_tanh_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, output);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_out, name, "aten::slow_conv_transpose2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_out, schema_str, "slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose2d_out::schema> create_slow_conv_transpose2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose2d_out::name, slow_conv_transpose2d_out::overload_name)
      .typed<slow_conv_transpose2d_out::schema>();
}

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_transpose2d_out::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
    static auto op = create_slow_conv_transpose2d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output_padding, dilation, out);
}

// aten::slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_transpose2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
    static auto op = create_slow_conv_transpose2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output_padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d, name, "aten::slow_conv_transpose2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d, schema_str, "slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor")

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose2d::schema> create_slow_conv_transpose2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose2d::name, slow_conv_transpose2d::overload_name)
      .typed<slow_conv_transpose2d::schema>();
}

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor
at::Tensor slow_conv_transpose2d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_transpose2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

// aten::slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor
at::Tensor slow_conv_transpose2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_transpose2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_backward_grad_output, name, "aten::slow_conv_transpose2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_backward_grad_output, overload_name, "grad_output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_backward_grad_output, schema_str, "slow_conv_transpose2d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::slow_conv_transpose2d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose2d_backward_grad_output::schema> create_slow_conv_transpose2d_backward_grad_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose2d_backward_grad_output::name, slow_conv_transpose2d_backward_grad_output::overload_name)
      .typed<slow_conv_transpose2d_backward_grad_output::schema>();
}

// aten::slow_conv_transpose2d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv_transpose2d_backward_grad_output::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_slow_conv_transpose2d_backward_grad_output_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones, grad_input, grad_weight, grad_bias);
}

// aten::slow_conv_transpose2d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv_transpose2d_backward_grad_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_slow_conv_transpose2d_backward_grad_output_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones, grad_input, grad_weight, grad_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_backward_output_mask, name, "aten::slow_conv_transpose2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose2d_backward_output_mask, schema_str, "slow_conv_transpose2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::slow_conv_transpose2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose2d_backward_output_mask::schema> create_slow_conv_transpose2d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose2d_backward_output_mask::name, slow_conv_transpose2d_backward_output_mask::overload_name)
      .typed<slow_conv_transpose2d_backward_output_mask::schema>();
}

// aten::slow_conv_transpose2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_transpose2d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_transpose2d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones, output_mask);
}

// aten::slow_conv_transpose2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, Tensor columns, Tensor ones, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_transpose2d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & columns, const at::Tensor & ones, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_transpose2d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_out, name, "aten::slow_conv_transpose3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_out, schema_str, "slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose3d_out::schema> create_slow_conv_transpose3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose3d_out::name, slow_conv_transpose3d_out::overload_name)
      .typed<slow_conv_transpose3d_out::schema>();
}

// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_transpose3d_out::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
    static auto op = create_slow_conv_transpose3d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output_padding, dilation, out);
}

// aten::slow_conv_transpose3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv_transpose3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, at::Tensor & out) {
    static auto op = create_slow_conv_transpose3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output_padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d, name, "aten::slow_conv_transpose3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d, schema_str, "slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1) -> Tensor")

// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose3d::schema> create_slow_conv_transpose3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose3d::name, slow_conv_transpose3d::overload_name)
      .typed<slow_conv_transpose3d::schema>();
}

// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1) -> Tensor
at::Tensor slow_conv_transpose3d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_transpose3d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

// aten::slow_conv_transpose3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int[3] dilation=1) -> Tensor
at::Tensor slow_conv_transpose3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_transpose3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_backward_grad_output, name, "aten::slow_conv_transpose3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_backward_grad_output, overload_name, "grad_output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_backward_grad_output, schema_str, "slow_conv_transpose3d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::slow_conv_transpose3d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose3d_backward_grad_output::schema> create_slow_conv_transpose3d_backward_grad_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose3d_backward_grad_output::name, slow_conv_transpose3d_backward_grad_output::overload_name)
      .typed<slow_conv_transpose3d_backward_grad_output::schema>();
}

// aten::slow_conv_transpose3d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv_transpose3d_backward_grad_output::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_slow_conv_transpose3d_backward_grad_output_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

// aten::slow_conv_transpose3d_backward.grad_output(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv_transpose3d_backward_grad_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_slow_conv_transpose3d_backward_grad_output_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_backward_output_mask, name, "aten::slow_conv_transpose3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_transpose3d_backward_output_mask, schema_str, "slow_conv_transpose3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::slow_conv_transpose3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_transpose3d_backward_output_mask::schema> create_slow_conv_transpose3d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_transpose3d_backward_output_mask::name, slow_conv_transpose3d_backward_output_mask::overload_name)
      .typed<slow_conv_transpose3d_backward_output_mask::schema>();
}

// aten::slow_conv_transpose3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_transpose3d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_transpose3d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input, output_mask);
}

// aten::slow_conv_transpose3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] output_padding, int[3] dilation, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_transpose3d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation, const at::Tensor & finput, const at::Tensor & fgrad_input, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_transpose3d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_out, name, "aten::thnn_conv2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_out, schema_str, "thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<thnn_conv2d_out::schema> create_thnn_conv2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(thnn_conv2d_out::name, thnn_conv2d_out::overload_name)
      .typed<thnn_conv2d_out::schema>();
}

// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & thnn_conv2d_out::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_thnn_conv2d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, out);
}

// aten::thnn_conv2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & thnn_conv2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_thnn_conv2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d, name, "aten::thnn_conv2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d, schema_str, "thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor")

// aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<thnn_conv2d::schema> create_thnn_conv2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(thnn_conv2d::name, thnn_conv2d::overload_name)
      .typed<thnn_conv2d::schema>();
}

// aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor
at::Tensor thnn_conv2d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_thnn_conv2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding);
}

// aten::thnn_conv2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0) -> Tensor
at::Tensor thnn_conv2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_thnn_conv2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_forward_output, name, "aten::thnn_conv2d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_forward_output, schema_str, "thnn_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::thnn_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<thnn_conv2d_forward_output::schema> create_thnn_conv2d_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(thnn_conv2d_forward_output::name, thnn_conv2d_forward_output::overload_name)
      .typed<thnn_conv2d_forward_output::schema>();
}

// aten::thnn_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> thnn_conv2d_forward_output::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
    static auto op = create_thnn_conv2d_forward_output_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output, finput, fgrad_input);
}

// aten::thnn_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> thnn_conv2d_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
    static auto op = create_thnn_conv2d_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output, finput, fgrad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_forward, name, "aten::thnn_conv2d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_forward, schema_str, "thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)")

// aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
static C10_NOINLINE c10::TypedOperatorHandle<thnn_conv2d_forward::schema> create_thnn_conv2d_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(thnn_conv2d_forward::name, thnn_conv2d_forward::overload_name)
      .typed<thnn_conv2d_forward::schema>();
}

// aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> thnn_conv2d_forward::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_thnn_conv2d_forward_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding);
}

// aten::thnn_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> thnn_conv2d_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_thnn_conv2d_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_backward_grad_input, name, "aten::thnn_conv2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_backward_grad_input, schema_str, "thnn_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::thnn_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<thnn_conv2d_backward_grad_input::schema> create_thnn_conv2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(thnn_conv2d_backward_grad_input::name, thnn_conv2d_backward_grad_input::overload_name)
      .typed<thnn_conv2d_backward_grad_input::schema>();
}

// aten::thnn_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> thnn_conv2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_thnn_conv2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

// aten::thnn_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> thnn_conv2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_thnn_conv2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_backward_output_mask, name, "aten::thnn_conv2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(thnn_conv2d_backward_output_mask, schema_str, "thnn_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::thnn_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<thnn_conv2d_backward_output_mask::schema> create_thnn_conv2d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(thnn_conv2d_backward_output_mask::name, thnn_conv2d_backward_output_mask::overload_name)
      .typed<thnn_conv2d_backward_output_mask::schema>();
}

// aten::thnn_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> thnn_conv2d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, ::std::array<bool,3> output_mask) {
    static auto op = create_thnn_conv2d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, output_mask);
}

// aten::thnn_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> thnn_conv2d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, ::std::array<bool,3> output_mask) {
    static auto op = create_thnn_conv2d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_out, name, "aten::_conv_depthwise2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_out, schema_str, "_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)")

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_conv_depthwise2d_out::schema> create__conv_depthwise2d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conv_depthwise2d_out::name, _conv_depthwise2d_out::overload_name)
      .typed<_conv_depthwise2d_out::schema>();
}

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & _conv_depthwise2d_out::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, const at::Tensor & out) {
    static auto op = create__conv_depthwise2d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation, out);
}

// aten::_conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
const at::Tensor & _conv_depthwise2d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, const at::Tensor & out) {
    static auto op = create__conv_depthwise2d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d, name, "aten::_conv_depthwise2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d, schema_str, "_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor")

// aten::_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_conv_depthwise2d::schema> create__conv_depthwise2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conv_depthwise2d::name, _conv_depthwise2d::overload_name)
      .typed<_conv_depthwise2d::schema>();
}

// aten::_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor
at::Tensor _conv_depthwise2d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create__conv_depthwise2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::_conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor
at::Tensor _conv_depthwise2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create__conv_depthwise2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_backward_grad_input, name, "aten::_conv_depthwise2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_backward_grad_input, schema_str, "_conv_depthwise2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight) -> (Tensor(a!), Tensor(b!))")

// aten::_conv_depthwise2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight) -> (Tensor(a!), Tensor(b!))
static C10_NOINLINE c10::TypedOperatorHandle<_conv_depthwise2d_backward_grad_input::schema> create__conv_depthwise2d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conv_depthwise2d_backward_grad_input::name, _conv_depthwise2d_backward_grad_input::overload_name)
      .typed<_conv_depthwise2d_backward_grad_input::schema>();
}

// aten::_conv_depthwise2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _conv_depthwise2d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight) {
    static auto op = create__conv_depthwise2d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, dilation, grad_input, grad_weight);
}

// aten::_conv_depthwise2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight) -> (Tensor(a!), Tensor(b!))
::std::tuple<at::Tensor &,at::Tensor &> _conv_depthwise2d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight) {
    static auto op = create__conv_depthwise2d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, dilation, grad_input, grad_weight);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_backward_output_mask, name, "aten::_conv_depthwise2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_conv_depthwise2d_backward_output_mask, schema_str, "_conv_depthwise2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)")

// aten::_conv_depthwise2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
static C10_NOINLINE c10::TypedOperatorHandle<_conv_depthwise2d_backward_output_mask::schema> create__conv_depthwise2d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_conv_depthwise2d_backward_output_mask::name, _conv_depthwise2d_backward_output_mask::overload_name)
      .typed<_conv_depthwise2d_backward_output_mask::schema>();
}

// aten::_conv_depthwise2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
::std::tuple<at::Tensor,at::Tensor> _conv_depthwise2d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,2> output_mask) {
    static auto op = create__conv_depthwise2d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

// aten::_conv_depthwise2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
::std::tuple<at::Tensor,at::Tensor> _conv_depthwise2d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,2> output_mask) {
    static auto op = create__conv_depthwise2d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d, name, "aten::conv_depthwise3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d, schema_str, "conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation) -> Tensor")

// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<conv_depthwise3d::schema> create_conv_depthwise3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_depthwise3d::name, conv_depthwise3d::overload_name)
      .typed<conv_depthwise3d::schema>();
}

// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation) -> Tensor
at::Tensor conv_depthwise3d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create_conv_depthwise3d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::conv_depthwise3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, int[3] dilation) -> Tensor
at::Tensor conv_depthwise3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create_conv_depthwise3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_backward_grad_input, name, "aten::conv_depthwise3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_backward_grad_input, schema_str, "conv_depthwise3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::conv_depthwise3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<conv_depthwise3d_backward_grad_input::schema> create_conv_depthwise3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_depthwise3d_backward_grad_input::name, conv_depthwise3d_backward_grad_input::overload_name)
      .typed<conv_depthwise3d_backward_grad_input::schema>();
}

// aten::conv_depthwise3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> conv_depthwise3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_conv_depthwise3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, dilation, grad_input, grad_weight, grad_bias);
}

// aten::conv_depthwise3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> conv_depthwise3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_conv_depthwise3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, dilation, grad_input, grad_weight, grad_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_backward_output_mask, name, "aten::conv_depthwise3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(conv_depthwise3d_backward_output_mask, schema_str, "conv_depthwise3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::conv_depthwise3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<conv_depthwise3d_backward_output_mask::schema> create_conv_depthwise3d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(conv_depthwise3d_backward_output_mask::name, conv_depthwise3d_backward_output_mask::overload_name)
      .typed<conv_depthwise3d_backward_output_mask::schema>();
}

// aten::conv_depthwise3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_depthwise3d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,3> output_mask) {
    static auto op = create_conv_depthwise3d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

// aten::conv_depthwise3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> conv_depthwise3d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,3> output_mask) {
    static auto op = create_conv_depthwise3d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_out, name, "aten::slow_conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_out, schema_str, "slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d_out::schema> create_slow_conv3d_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d_out::name, slow_conv3d_out::overload_name)
      .typed<slow_conv3d_out::schema>();
}

// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv3d_out::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_slow_conv3d_out_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, out);
}

// aten::slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & slow_conv3d_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & out) {
    static auto op = create_slow_conv3d_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d, name, "aten::slow_conv3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d, schema_str, "slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor")

// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d::schema> create_slow_conv3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d::name, slow_conv3d::overload_name)
      .typed<slow_conv3d::schema>();
}

// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor
at::Tensor slow_conv3d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_slow_conv3d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding);
}

// aten::slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor
at::Tensor slow_conv3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_slow_conv3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_forward_output, name, "aten::slow_conv3d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_forward_output, overload_name, "output")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_forward_output, schema_str, "slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d_forward_output::schema> create_slow_conv3d_forward_output_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d_forward_output::name, slow_conv3d_forward_output::overload_name)
      .typed<slow_conv3d_forward_output::schema>();
}

// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv3d_forward_output::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
    static auto op = create_slow_conv3d_forward_output_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, output, finput, fgrad_input);
}

// aten::slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output, Tensor(b!) finput, Tensor(c!) fgrad_input) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv3d_forward_output::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::Tensor & output, at::Tensor & finput, at::Tensor & fgrad_input) {
    static auto op = create_slow_conv3d_forward_output_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, output, finput, fgrad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_forward, name, "aten::slow_conv3d_forward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_forward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_forward, schema_str, "slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)")

// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d_forward::schema> create_slow_conv3d_forward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d_forward::name, slow_conv3d_forward::overload_name)
      .typed<slow_conv3d_forward::schema>();
}

// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv3d_forward::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_slow_conv3d_forward_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding);
}

// aten::slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> (Tensor output, Tensor finput, Tensor fgrad_input)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv3d_forward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding) {
    static auto op = create_slow_conv3d_forward_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_backward_grad_input, name, "aten::slow_conv3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_backward_grad_input, schema_str, "slow_conv3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))")

// aten::slow_conv3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d_backward_grad_input::schema> create_slow_conv3d_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d_backward_grad_input::name, slow_conv3d_backward_grad_input::overload_name)
      .typed<slow_conv3d_backward_grad_input::schema>();
}

// aten::slow_conv3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv3d_backward_grad_input::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_slow_conv3d_backward_grad_input_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

// aten::slow_conv3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -> (Tensor(a!), Tensor(b!), Tensor(c!))
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> slow_conv3d_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, at::Tensor & grad_input, at::Tensor & grad_weight, at::Tensor & grad_bias) {
    static auto op = create_slow_conv3d_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, grad_input, grad_weight, grad_bias);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_backward_output_mask, name, "aten::slow_conv3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_backward_output_mask, overload_name, "output_mask")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv3d_backward_output_mask, schema_str, "slow_conv3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::slow_conv3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv3d_backward_output_mask::schema> create_slow_conv3d_backward_output_mask_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv3d_backward_output_mask::name, slow_conv3d_backward_output_mask::overload_name)
      .typed<slow_conv3d_backward_output_mask::schema>();
}

// aten::slow_conv3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv3d_backward_output_mask::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv3d_backward_output_mask_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, output_mask);
}

// aten::slow_conv3d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, Tensor finput, Tensor fgrad_input, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv3d_backward_output_mask::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, const at::Tensor & finput, const at::Tensor & fgrad_input, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv3d_backward_output_mask_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d, name, "aten::slow_conv_dilated2d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d, schema_str, "slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor")

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_dilated2d::schema> create_slow_conv_dilated2d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_dilated2d::name, slow_conv_dilated2d::overload_name)
      .typed<slow_conv_dilated2d::schema>();
}

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor
at::Tensor slow_conv_dilated2d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_dilated2d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor
at::Tensor slow_conv_dilated2d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_dilated2d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d_backward, name, "aten::slow_conv_dilated2d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated2d_backward, schema_str, "slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_dilated2d_backward::schema> create_slow_conv_dilated2d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_dilated2d_backward::name, slow_conv_dilated2d_backward::overload_name)
      .typed<slow_conv_dilated2d_backward::schema>();
}

// aten::slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_dilated2d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_dilated2d_backward_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

// aten::slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_dilated2d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_dilated2d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated3d, name, "aten::slow_conv_dilated3d")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated3d, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated3d, schema_str, "slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1) -> Tensor")

// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_dilated3d::schema> create_slow_conv_dilated3d_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_dilated3d::name, slow_conv_dilated3d::overload_name)
      .typed<slow_conv_dilated3d::schema>();
}

// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1) -> Tensor
at::Tensor slow_conv_dilated3d::call(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_dilated3d_typed_handle();
    return op.call(self, weight, kernel_size, bias, stride, padding, dilation);
}

// aten::slow_conv_dilated3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] dilation=1) -> Tensor
at::Tensor slow_conv_dilated3d::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation) {
    static auto op = create_slow_conv_dilated3d_typed_handle();
    return op.redispatch(dispatchKeySet, self, weight, kernel_size, bias, stride, padding, dilation);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated3d_backward, name, "aten::slow_conv_dilated3d_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated3d_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(slow_conv_dilated3d_backward, schema_str, "slow_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)")

// aten::slow_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
static C10_NOINLINE c10::TypedOperatorHandle<slow_conv_dilated3d_backward::schema> create_slow_conv_dilated3d_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(slow_conv_dilated3d_backward::name, slow_conv_dilated3d_backward::overload_name)
      .typed<slow_conv_dilated3d_backward::schema>();
}

// aten::slow_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_dilated3d_backward::call(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_dilated3d_backward_typed_handle();
    return op.call(grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

// aten::slow_conv_dilated3d_backward(Tensor grad_output, Tensor self, Tensor weight, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> slow_conv_dilated3d_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, ::std::array<bool,3> output_mask) {
    static auto op = create_slow_conv_dilated3d_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_out, name, "aten::col2im")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_out, schema_str, "col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)")

// aten::col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<col2im_out::schema> create_col2im_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col2im_out::name, col2im_out::overload_name)
      .typed<col2im_out::schema>();
}

// aten::col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & col2im_out::call(const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
    static auto op = create_col2im_out_typed_handle();
    return op.call(self, output_size, kernel_size, dilation, padding, stride, out);
}

// aten::col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & col2im_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
    static auto op = create_col2im_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, kernel_size, dilation, padding, stride, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im, name, "aten::col2im")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im, schema_str, "col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor")

// aten::col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<col2im::schema> create_col2im_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col2im::name, col2im::overload_name)
      .typed<col2im::schema>();
}

// aten::col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor col2im::call(const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_col2im_typed_handle();
    return op.call(self, output_size, kernel_size, dilation, padding, stride);
}

// aten::col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor col2im::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef output_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_col2im_typed_handle();
    return op.redispatch(dispatchKeySet, self, output_size, kernel_size, dilation, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_backward_grad_input, name, "aten::col2im_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_backward_grad_input, schema_str, "col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<col2im_backward_grad_input::schema> create_col2im_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col2im_backward_grad_input::name, col2im_backward_grad_input::overload_name)
      .typed<col2im_backward_grad_input::schema>();
}

// aten::col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & col2im_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
    static auto op = create_col2im_backward_grad_input_typed_handle();
    return op.call(grad_output, kernel_size, dilation, padding, stride, grad_input);
}

// aten::col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & col2im_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
    static auto op = create_col2im_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, kernel_size, dilation, padding, stride, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_backward, name, "aten::col2im_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(col2im_backward, schema_str, "col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor")

// aten::col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<col2im_backward::schema> create_col2im_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(col2im_backward::name, col2im_backward::overload_name)
      .typed<col2im_backward::schema>();
}

// aten::col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor col2im_backward::call(const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_col2im_backward_typed_handle();
    return op.call(grad_output, kernel_size, dilation, padding, stride);
}

// aten::col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor col2im_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_col2im_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, kernel_size, dilation, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(column_stack, name, "aten::column_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(column_stack, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(column_stack, schema_str, "column_stack(Tensor[] tensors) -> Tensor")

// aten::column_stack(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<column_stack::schema> create_column_stack_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(column_stack::name, column_stack::overload_name)
      .typed<column_stack::schema>();
}

// aten::column_stack(Tensor[] tensors) -> Tensor
at::Tensor column_stack::call(at::TensorList tensors) {
    static auto op = create_column_stack_typed_handle();
    return op.call(tensors);
}

// aten::column_stack(Tensor[] tensors) -> Tensor
at::Tensor column_stack::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_column_stack_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(column_stack_out, name, "aten::column_stack")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(column_stack_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(column_stack_out, schema_str, "column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<column_stack_out::schema> create_column_stack_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(column_stack_out::name, column_stack_out::overload_name)
      .typed<column_stack_out::schema>();
}

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & column_stack_out::call(at::TensorList tensors, at::Tensor & out) {
    static auto op = create_column_stack_out_typed_handle();
    return op.call(tensors, out);
}

// aten::column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & column_stack_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    static auto op = create_column_stack_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_out, name, "aten::im2col")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_out, schema_str, "im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)")

// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<im2col_out::schema> create_im2col_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(im2col_out::name, im2col_out::overload_name)
      .typed<im2col_out::schema>();
}

// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & im2col_out::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
    static auto op = create_im2col_out_typed_handle();
    return op.call(self, kernel_size, dilation, padding, stride, out);
}

// aten::im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & im2col_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & out) {
    static auto op = create_im2col_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, dilation, padding, stride, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col, name, "aten::im2col")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col, schema_str, "im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor")

// aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<im2col::schema> create_im2col_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(im2col::name, im2col::overload_name)
      .typed<im2col::schema>();
}

// aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor im2col::call(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_im2col_typed_handle();
    return op.call(self, kernel_size, dilation, padding, stride);
}

// aten::im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor im2col::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_im2col_typed_handle();
    return op.redispatch(dispatchKeySet, self, kernel_size, dilation, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_backward_grad_input, name, "aten::im2col_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_backward_grad_input, overload_name, "grad_input")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_backward_grad_input, schema_str, "im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)")

// aten::im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<im2col_backward_grad_input::schema> create_im2col_backward_grad_input_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(im2col_backward_grad_input::name, im2col_backward_grad_input::overload_name)
      .typed<im2col_backward_grad_input::schema>();
}

// aten::im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & im2col_backward_grad_input::call(const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
    static auto op = create_im2col_backward_grad_input_typed_handle();
    return op.call(grad_output, input_size, kernel_size, dilation, padding, stride, grad_input);
}

// aten::im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
at::Tensor & im2col_backward_grad_input::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride, at::Tensor & grad_input) {
    static auto op = create_im2col_backward_grad_input_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_size, kernel_size, dilation, padding, stride, grad_input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_backward, name, "aten::im2col_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(im2col_backward, schema_str, "im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor")

// aten::im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<im2col_backward::schema> create_im2col_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(im2col_backward::name, im2col_backward::overload_name)
      .typed<im2col_backward::schema>();
}

// aten::im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor im2col_backward::call(const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_im2col_backward_typed_handle();
    return op.call(grad_output, input_size, kernel_size, dilation, padding, stride);
}

// aten::im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
at::Tensor im2col_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad_output, at::IntArrayRef input_size, at::IntArrayRef kernel_size, at::IntArrayRef dilation, at::IntArrayRef padding, at::IntArrayRef stride) {
    static auto op = create_im2col_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad_output, input_size, kernel_size, dilation, padding, stride);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isfinite, name, "aten::isfinite")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isfinite, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isfinite, schema_str, "isfinite(Tensor self) -> Tensor")

// aten::isfinite(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isfinite::schema> create_isfinite_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isfinite::name, isfinite::overload_name)
      .typed<isfinite::schema>();
}

// aten::isfinite(Tensor self) -> Tensor
at::Tensor isfinite::call(const at::Tensor & self) {
    static auto op = create_isfinite_typed_handle();
    return op.call(self);
}

// aten::isfinite(Tensor self) -> Tensor
at::Tensor isfinite::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_isfinite_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isinf, name, "aten::isinf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isinf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isinf, schema_str, "isinf(Tensor self) -> Tensor")

// aten::isinf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isinf::schema> create_isinf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isinf::name, isinf::overload_name)
      .typed<isinf::schema>();
}

// aten::isinf(Tensor self) -> Tensor
at::Tensor isinf::call(const at::Tensor & self) {
    static auto op = create_isinf_typed_handle();
    return op.call(self);
}

// aten::isinf(Tensor self) -> Tensor
at::Tensor isinf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_isinf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(record_stream, name, "aten::record_stream")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(record_stream, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(record_stream, schema_str, "record_stream(Tensor(a!) self, Stream s) -> ()")

// aten::record_stream(Tensor(a!) self, Stream s) -> ()
static C10_NOINLINE c10::TypedOperatorHandle<record_stream::schema> create_record_stream_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(record_stream::name, record_stream::overload_name)
      .typed<record_stream::schema>();
}

// aten::record_stream(Tensor(a!) self, Stream s) -> ()
void record_stream::call(at::Tensor & self, at::Stream s) {
    static auto op = create_record_stream_typed_handle();
    return op.call(self, s);
}

// aten::record_stream(Tensor(a!) self, Stream s) -> ()
void record_stream::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Stream s) {
    static auto op = create_record_stream_typed_handle();
    return op.redispatch(dispatchKeySet, self, s);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf, name, "aten::isposinf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf, schema_str, "isposinf(Tensor self) -> Tensor")

// aten::isposinf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isposinf::schema> create_isposinf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isposinf::name, isposinf::overload_name)
      .typed<isposinf::schema>();
}

// aten::isposinf(Tensor self) -> Tensor
at::Tensor isposinf::call(const at::Tensor & self) {
    static auto op = create_isposinf_typed_handle();
    return op.call(self);
}

// aten::isposinf(Tensor self) -> Tensor
at::Tensor isposinf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_isposinf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf_out, name, "aten::isposinf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isposinf_out, schema_str, "isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isposinf_out::schema> create_isposinf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isposinf_out::name, isposinf_out::overload_name)
      .typed<isposinf_out::schema>();
}

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isposinf_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_isposinf_out_typed_handle();
    return op.call(self, out);
}

// aten::isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isposinf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_isposinf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isneginf, name, "aten::isneginf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isneginf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isneginf, schema_str, "isneginf(Tensor self) -> Tensor")

// aten::isneginf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<isneginf::schema> create_isneginf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isneginf::name, isneginf::overload_name)
      .typed<isneginf::schema>();
}

// aten::isneginf(Tensor self) -> Tensor
at::Tensor isneginf::call(const at::Tensor & self) {
    static auto op = create_isneginf_typed_handle();
    return op.call(self);
}

// aten::isneginf(Tensor self) -> Tensor
at::Tensor isneginf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_isneginf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isneginf_out, name, "aten::isneginf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isneginf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(isneginf_out, schema_str, "isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<isneginf_out::schema> create_isneginf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(isneginf_out::name, isneginf_out::overload_name)
      .typed<isneginf_out::schema>();
}

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isneginf_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_isneginf_out_typed_handle();
    return op.call(self, out);
}

// aten::isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & isneginf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_isneginf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_batch_dim, name, "aten::_add_batch_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_batch_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_add_batch_dim, schema_str, "_add_batch_dim(Tensor self, int batch_dim, int level) -> Tensor")

// aten::_add_batch_dim(Tensor self, int batch_dim, int level) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_add_batch_dim::schema> create__add_batch_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_add_batch_dim::name, _add_batch_dim::overload_name)
      .typed<_add_batch_dim::schema>();
}

// aten::_add_batch_dim(Tensor self, int batch_dim, int level) -> Tensor
at::Tensor _add_batch_dim::call(const at::Tensor & self, int64_t batch_dim, int64_t level) {
    static auto op = create__add_batch_dim_typed_handle();
    return op.call(self, batch_dim, level);
}

// aten::_add_batch_dim(Tensor self, int batch_dim, int level) -> Tensor
at::Tensor _add_batch_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t batch_dim, int64_t level) {
    static auto op = create__add_batch_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, batch_dim, level);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_remove_batch_dim, name, "aten::_remove_batch_dim")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_remove_batch_dim, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_remove_batch_dim, schema_str, "_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor")

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_remove_batch_dim::schema> create__remove_batch_dim_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_remove_batch_dim::name, _remove_batch_dim::overload_name)
      .typed<_remove_batch_dim::schema>();
}

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
at::Tensor _remove_batch_dim::call(const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
    static auto op = create__remove_batch_dim_typed_handle();
    return op.call(self, level, batch_size, out_dim);
}

// aten::_remove_batch_dim(Tensor self, int level, int batch_size, int out_dim) -> Tensor
at::Tensor _remove_batch_dim::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t level, int64_t batch_size, int64_t out_dim) {
    static auto op = create__remove_batch_dim_typed_handle();
    return op.redispatch(dispatchKeySet, self, level, batch_size, out_dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_entr, name, "aten::special_entr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_entr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_entr, schema_str, "special_entr(Tensor self) -> Tensor")

// aten::special_entr(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_entr::schema> create_special_entr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_entr::name, special_entr::overload_name)
      .typed<special_entr::schema>();
}

// aten::special_entr(Tensor self) -> Tensor
at::Tensor special_entr::call(const at::Tensor & self) {
    static auto op = create_special_entr_typed_handle();
    return op.call(self);
}

// aten::special_entr(Tensor self) -> Tensor
at::Tensor special_entr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_entr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_entr_out, name, "aten::special_entr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_entr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_entr_out, schema_str, "special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_entr_out::schema> create_special_entr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_entr_out::name, special_entr_out::overload_name)
      .typed<special_entr_out::schema>();
}

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_entr_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_entr_out_typed_handle();
    return op.call(self, out);
}

// aten::special_entr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_entr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_entr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtri, name, "aten::special_ndtri")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtri, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtri, schema_str, "special_ndtri(Tensor self) -> Tensor")

// aten::special_ndtri(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_ndtri::schema> create_special_ndtri_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_ndtri::name, special_ndtri::overload_name)
      .typed<special_ndtri::schema>();
}

// aten::special_ndtri(Tensor self) -> Tensor
at::Tensor special_ndtri::call(const at::Tensor & self) {
    static auto op = create_special_ndtri_typed_handle();
    return op.call(self);
}

// aten::special_ndtri(Tensor self) -> Tensor
at::Tensor special_ndtri::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_ndtri_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtri_out, name, "aten::special_ndtri")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtri_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtri_out, schema_str, "special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_ndtri_out::schema> create_special_ndtri_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_ndtri_out::name, special_ndtri_out::overload_name)
      .typed<special_ndtri_out::schema>();
}

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_ndtri_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_ndtri_out_typed_handle();
    return op.call(self, out);
}

// aten::special_ndtri.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_ndtri_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_ndtri_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1, name, "aten::special_expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1, schema_str, "special_expm1(Tensor self) -> Tensor")

// aten::special_expm1(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_expm1::schema> create_special_expm1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expm1::name, special_expm1::overload_name)
      .typed<special_expm1::schema>();
}

// aten::special_expm1(Tensor self) -> Tensor
at::Tensor special_expm1::call(const at::Tensor & self) {
    static auto op = create_special_expm1_typed_handle();
    return op.call(self);
}

// aten::special_expm1(Tensor self) -> Tensor
at::Tensor special_expm1::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_expm1_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1_out, name, "aten::special_expm1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expm1_out, schema_str, "special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_expm1_out::schema> create_special_expm1_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expm1_out::name, special_expm1_out::overload_name)
      .typed<special_expm1_out::schema>();
}

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expm1_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_expm1_out_typed_handle();
    return op.call(self, out);
}

// aten::special_expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expm1_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_expm1_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2, name, "aten::special_exp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2, schema_str, "special_exp2(Tensor self) -> Tensor")

// aten::special_exp2(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_exp2::schema> create_special_exp2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_exp2::name, special_exp2::overload_name)
      .typed<special_exp2::schema>();
}

// aten::special_exp2(Tensor self) -> Tensor
at::Tensor special_exp2::call(const at::Tensor & self) {
    static auto op = create_special_exp2_typed_handle();
    return op.call(self);
}

// aten::special_exp2(Tensor self) -> Tensor
at::Tensor special_exp2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_exp2_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2_out, name, "aten::special_exp2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_exp2_out, schema_str, "special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_exp2_out::schema> create_special_exp2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_exp2_out::name, special_exp2_out::overload_name)
      .typed<special_exp2_out::schema>();
}

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_exp2_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_exp2_out_typed_handle();
    return op.call(self, out);
}

// aten::special_exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_exp2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_exp2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_psi, name, "aten::special_psi")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_psi, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_psi, schema_str, "special_psi(Tensor self) -> Tensor")

// aten::special_psi(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_psi::schema> create_special_psi_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_psi::name, special_psi::overload_name)
      .typed<special_psi::schema>();
}

// aten::special_psi(Tensor self) -> Tensor
at::Tensor special_psi::call(const at::Tensor & self) {
    static auto op = create_special_psi_typed_handle();
    return op.call(self);
}

// aten::special_psi(Tensor self) -> Tensor
at::Tensor special_psi::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_psi_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_psi_out, name, "aten::special_psi")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_psi_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_psi_out, schema_str, "special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_psi_out::schema> create_special_psi_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_psi_out::name, special_psi_out::overload_name)
      .typed<special_psi_out::schema>();
}

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_psi_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_psi_out_typed_handle();
    return op.call(self, out);
}

// aten::special_psi.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_psi_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_psi_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_digamma, name, "aten::special_digamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_digamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_digamma, schema_str, "special_digamma(Tensor self) -> Tensor")

// aten::special_digamma(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_digamma::schema> create_special_digamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_digamma::name, special_digamma::overload_name)
      .typed<special_digamma::schema>();
}

// aten::special_digamma(Tensor self) -> Tensor
at::Tensor special_digamma::call(const at::Tensor & self) {
    static auto op = create_special_digamma_typed_handle();
    return op.call(self);
}

// aten::special_digamma(Tensor self) -> Tensor
at::Tensor special_digamma::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_digamma_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_digamma_out, name, "aten::special_digamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_digamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_digamma_out, schema_str, "special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_digamma_out::schema> create_special_digamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_digamma_out::name, special_digamma_out::overload_name)
      .typed<special_digamma_out::schema>();
}

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_digamma_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_digamma_out_typed_handle();
    return op.call(self, out);
}

// aten::special_digamma.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_digamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_digamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln, name, "aten::special_gammaln")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln, schema_str, "special_gammaln(Tensor self) -> Tensor")

// aten::special_gammaln(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_gammaln::schema> create_special_gammaln_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammaln::name, special_gammaln::overload_name)
      .typed<special_gammaln::schema>();
}

// aten::special_gammaln(Tensor self) -> Tensor
at::Tensor special_gammaln::call(const at::Tensor & self) {
    static auto op = create_special_gammaln_typed_handle();
    return op.call(self);
}

// aten::special_gammaln(Tensor self) -> Tensor
at::Tensor special_gammaln::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_gammaln_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln_out, name, "aten::special_gammaln")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaln_out, schema_str, "special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_gammaln_out::schema> create_special_gammaln_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammaln_out::name, special_gammaln_out::overload_name)
      .typed<special_gammaln_out::schema>();
}

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammaln_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_gammaln_out_typed_handle();
    return op.call(self, out);
}

// aten::special_gammaln.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammaln_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_gammaln_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf, name, "aten::special_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf, schema_str, "special_erf(Tensor self) -> Tensor")

// aten::special_erf(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_erf::schema> create_special_erf_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erf::name, special_erf::overload_name)
      .typed<special_erf::schema>();
}

// aten::special_erf(Tensor self) -> Tensor
at::Tensor special_erf::call(const at::Tensor & self) {
    static auto op = create_special_erf_typed_handle();
    return op.call(self);
}

// aten::special_erf(Tensor self) -> Tensor
at::Tensor special_erf::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_erf_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf_out, name, "aten::special_erf")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erf_out, schema_str, "special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_erf_out::schema> create_special_erf_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erf_out::name, special_erf_out::overload_name)
      .typed<special_erf_out::schema>();
}

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erf_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erf_out_typed_handle();
    return op.call(self, out);
}

// aten::special_erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erf_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erf_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfc, name, "aten::special_erfc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfc, schema_str, "special_erfc(Tensor self) -> Tensor")

// aten::special_erfc(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_erfc::schema> create_special_erfc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfc::name, special_erfc::overload_name)
      .typed<special_erfc::schema>();
}

// aten::special_erfc(Tensor self) -> Tensor
at::Tensor special_erfc::call(const at::Tensor & self) {
    static auto op = create_special_erfc_typed_handle();
    return op.call(self);
}

// aten::special_erfc(Tensor self) -> Tensor
at::Tensor special_erfc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_erfc_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfc_out, name, "aten::special_erfc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfc_out, schema_str, "special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_erfc_out::schema> create_special_erfc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfc_out::name, special_erfc_out::overload_name)
      .typed<special_erfc_out::schema>();
}

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfc_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erfc_out_typed_handle();
    return op.call(self, out);
}

// aten::special_erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erfc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfcx, name, "aten::special_erfcx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfcx, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfcx, schema_str, "special_erfcx(Tensor self) -> Tensor")

// aten::special_erfcx(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_erfcx::schema> create_special_erfcx_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfcx::name, special_erfcx::overload_name)
      .typed<special_erfcx::schema>();
}

// aten::special_erfcx(Tensor self) -> Tensor
at::Tensor special_erfcx::call(const at::Tensor & self) {
    static auto op = create_special_erfcx_typed_handle();
    return op.call(self);
}

// aten::special_erfcx(Tensor self) -> Tensor
at::Tensor special_erfcx::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_erfcx_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfcx_out, name, "aten::special_erfcx")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfcx_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfcx_out, schema_str, "special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_erfcx_out::schema> create_special_erfcx_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfcx_out::name, special_erfcx_out::overload_name)
      .typed<special_erfcx_out::schema>();
}

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfcx_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erfcx_out_typed_handle();
    return op.call(self, out);
}

// aten::special_erfcx.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfcx_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erfcx_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv, name, "aten::special_erfinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv, schema_str, "special_erfinv(Tensor self) -> Tensor")

// aten::special_erfinv(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_erfinv::schema> create_special_erfinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfinv::name, special_erfinv::overload_name)
      .typed<special_erfinv::schema>();
}

// aten::special_erfinv(Tensor self) -> Tensor
at::Tensor special_erfinv::call(const at::Tensor & self) {
    static auto op = create_special_erfinv_typed_handle();
    return op.call(self);
}

// aten::special_erfinv(Tensor self) -> Tensor
at::Tensor special_erfinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_erfinv_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv_out, name, "aten::special_erfinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_erfinv_out, schema_str, "special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_erfinv_out::schema> create_special_erfinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_erfinv_out::name, special_erfinv_out::overload_name)
      .typed<special_erfinv_out::schema>();
}

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfinv_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erfinv_out_typed_handle();
    return op.call(self, out);
}

// aten::special_erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_erfinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_erfinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtr, name, "aten::special_ndtr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtr, schema_str, "special_ndtr(Tensor self) -> Tensor")

// aten::special_ndtr(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_ndtr::schema> create_special_ndtr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_ndtr::name, special_ndtr::overload_name)
      .typed<special_ndtr::schema>();
}

// aten::special_ndtr(Tensor self) -> Tensor
at::Tensor special_ndtr::call(const at::Tensor & self) {
    static auto op = create_special_ndtr_typed_handle();
    return op.call(self);
}

// aten::special_ndtr(Tensor self) -> Tensor
at::Tensor special_ndtr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_ndtr_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtr_out, name, "aten::special_ndtr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_ndtr_out, schema_str, "special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_ndtr_out::schema> create_special_ndtr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_ndtr_out::name, special_ndtr_out::overload_name)
      .typed<special_ndtr_out::schema>();
}

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_ndtr_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_ndtr_out_typed_handle();
    return op.call(self, out);
}

// aten::special_ndtr.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_ndtr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_ndtr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py, schema_str, "special_xlog1py(Tensor self, Tensor other) -> Tensor")

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py::schema> create_special_xlog1py_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py::name, special_xlog1py::overload_name)
      .typed<special_xlog1py::schema>();
}

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlog1py::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_xlog1py_typed_handle();
    return op.call(self, other);
}

// aten::special_xlog1py(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlog1py::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_xlog1py_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar, overload_name, "self_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar, schema_str, "special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor")

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_self_scalar::schema> create_special_xlog1py_self_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_self_scalar::name, special_xlog1py_self_scalar::overload_name)
      .typed<special_xlog1py_self_scalar::schema>();
}

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlog1py_self_scalar::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_special_xlog1py_self_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlog1py.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlog1py_self_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_special_xlog1py_self_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar, overload_name, "other_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar, schema_str, "special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor")

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_other_scalar::schema> create_special_xlog1py_other_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_other_scalar::name, special_xlog1py_other_scalar::overload_name)
      .typed<special_xlog1py_other_scalar::schema>();
}

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlog1py_other_scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_special_xlog1py_other_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlog1py.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlog1py_other_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_special_xlog1py_other_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_out, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_out, schema_str, "special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_out::schema> create_special_xlog1py_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_out::name, special_xlog1py_out::overload_name)
      .typed<special_xlog1py_out::schema>();
}

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlog1py_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlog1py_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar_out, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar_out, overload_name, "self_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_self_scalar_out, schema_str, "special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_self_scalar_out::schema> create_special_xlog1py_self_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_self_scalar_out::name, special_xlog1py_self_scalar_out::overload_name)
      .typed<special_xlog1py_self_scalar_out::schema>();
}

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_self_scalar_out::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlog1py_self_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_self_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlog1py_self_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar_out, name, "aten::special_xlog1py")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar_out, overload_name, "other_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlog1py_other_scalar_out, schema_str, "special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlog1py_other_scalar_out::schema> create_special_xlog1py_other_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlog1py_other_scalar_out::name, special_xlog1py_other_scalar_out::overload_name)
      .typed<special_xlog1py_other_scalar_out::schema>();
}

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_other_scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_special_xlog1py_other_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlog1py_other_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_special_xlog1py_other_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy, schema_str, "special_xlogy(Tensor self, Tensor other) -> Tensor")

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy::schema> create_special_xlogy_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy::name, special_xlogy::overload_name)
      .typed<special_xlogy::schema>();
}

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlogy::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_xlogy_typed_handle();
    return op.call(self, other);
}

// aten::special_xlogy(Tensor self, Tensor other) -> Tensor
at::Tensor special_xlogy::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_xlogy_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar, overload_name, "self_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar, schema_str, "special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor")

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_self_scalar::schema> create_special_xlogy_self_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_self_scalar::name, special_xlogy_self_scalar::overload_name)
      .typed<special_xlogy_self_scalar::schema>();
}

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlogy_self_scalar::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_special_xlogy_self_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlogy.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_xlogy_self_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_special_xlogy_self_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar, overload_name, "other_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar, schema_str, "special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor")

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_other_scalar::schema> create_special_xlogy_other_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_other_scalar::name, special_xlogy_other_scalar::overload_name)
      .typed<special_xlogy_other_scalar::schema>();
}

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlogy_other_scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_special_xlogy_other_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_xlogy.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_xlogy_other_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_special_xlogy_other_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_out, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_out, schema_str, "special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_out::schema> create_special_xlogy_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_out::name, special_xlogy_out::overload_name)
      .typed<special_xlogy_out::schema>();
}

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlogy_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlogy_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar_out, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar_out, overload_name, "self_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_self_scalar_out, schema_str, "special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_self_scalar_out::schema> create_special_xlogy_self_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_self_scalar_out::name, special_xlogy_self_scalar_out::overload_name)
      .typed<special_xlogy_self_scalar_out::schema>();
}

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_self_scalar_out::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlogy_self_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_self_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_xlogy_self_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar_out, name, "aten::special_xlogy")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar_out, overload_name, "other_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_xlogy_other_scalar_out, schema_str, "special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_xlogy_other_scalar_out::schema> create_special_xlogy_other_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_xlogy_other_scalar_out::name, special_xlogy_other_scalar_out::overload_name)
      .typed<special_xlogy_other_scalar_out::schema>();
}

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_other_scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_special_xlogy_other_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_xlogy_other_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_special_xlogy_other_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta, name, "aten::special_zeta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta, schema_str, "special_zeta(Tensor self, Tensor other) -> Tensor")

// aten::special_zeta(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_zeta::schema> create_special_zeta_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_zeta::name, special_zeta::overload_name)
      .typed<special_zeta::schema>();
}

// aten::special_zeta(Tensor self, Tensor other) -> Tensor
at::Tensor special_zeta::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_zeta_typed_handle();
    return op.call(self, other);
}

// aten::special_zeta(Tensor self, Tensor other) -> Tensor
at::Tensor special_zeta::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_zeta_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_self_scalar, name, "aten::special_zeta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_self_scalar, overload_name, "self_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_self_scalar, schema_str, "special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor")

// aten::special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_zeta_self_scalar::schema> create_special_zeta_self_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_zeta_self_scalar::name, special_zeta_self_scalar::overload_name)
      .typed<special_zeta_self_scalar::schema>();
}

// aten::special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_zeta_self_scalar::call(const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_special_zeta_self_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_zeta.self_scalar(Scalar self, Tensor other) -> Tensor
at::Tensor special_zeta_self_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other) {
    static auto op = create_special_zeta_self_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_other_scalar, name, "aten::special_zeta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_other_scalar, overload_name, "other_scalar")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_other_scalar, schema_str, "special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor")

// aten::special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_zeta_other_scalar::schema> create_special_zeta_other_scalar_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_zeta_other_scalar::name, special_zeta_other_scalar::overload_name)
      .typed<special_zeta_other_scalar::schema>();
}

// aten::special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_zeta_other_scalar::call(const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_special_zeta_other_scalar_typed_handle();
    return op.call(self, other);
}

// aten::special_zeta.other_scalar(Tensor self, Scalar other) -> Tensor
at::Tensor special_zeta_other_scalar::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other) {
    static auto op = create_special_zeta_other_scalar_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_out, name, "aten::special_zeta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_out, schema_str, "special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_zeta_out::schema> create_special_zeta_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_zeta_out::name, special_zeta_out::overload_name)
      .typed<special_zeta_out::schema>();
}

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_zeta_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_zeta_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_zeta_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_zeta_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_self_scalar_out, name, "aten::special_zeta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_self_scalar_out, overload_name, "self_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_self_scalar_out, schema_str, "special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_zeta_self_scalar_out::schema> create_special_zeta_self_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_zeta_self_scalar_out::name, special_zeta_self_scalar_out::overload_name)
      .typed<special_zeta_self_scalar_out::schema>();
}

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_zeta_self_scalar_out::call(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_zeta_self_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_zeta_self_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_zeta_self_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_other_scalar_out, name, "aten::special_zeta")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_other_scalar_out, overload_name, "other_scalar_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_zeta_other_scalar_out, schema_str, "special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_zeta_other_scalar_out::schema> create_special_zeta_other_scalar_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_zeta_other_scalar_out::name, special_zeta_other_scalar_out::overload_name)
      .typed<special_zeta_other_scalar_out::schema>();
}

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_zeta_other_scalar_out::call(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_special_zeta_other_scalar_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_zeta_other_scalar_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    static auto op = create_special_zeta_other_scalar_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0, name, "aten::special_i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0, schema_str, "special_i0(Tensor self) -> Tensor")

// aten::special_i0(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_i0::schema> create_special_i0_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i0::name, special_i0::overload_name)
      .typed<special_i0::schema>();
}

// aten::special_i0(Tensor self) -> Tensor
at::Tensor special_i0::call(const at::Tensor & self) {
    static auto op = create_special_i0_typed_handle();
    return op.call(self);
}

// aten::special_i0(Tensor self) -> Tensor
at::Tensor special_i0::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_i0_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0_out, name, "aten::special_i0")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0_out, schema_str, "special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_i0_out::schema> create_special_i0_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i0_out::name, special_i0_out::overload_name)
      .typed<special_i0_out::schema>();
}

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i0_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i0_out_typed_handle();
    return op.call(self, out);
}

// aten::special_i0.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i0_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i0_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0e, name, "aten::special_i0e")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0e, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0e, schema_str, "special_i0e(Tensor self) -> Tensor")

// aten::special_i0e(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_i0e::schema> create_special_i0e_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i0e::name, special_i0e::overload_name)
      .typed<special_i0e::schema>();
}

// aten::special_i0e(Tensor self) -> Tensor
at::Tensor special_i0e::call(const at::Tensor & self) {
    static auto op = create_special_i0e_typed_handle();
    return op.call(self);
}

// aten::special_i0e(Tensor self) -> Tensor
at::Tensor special_i0e::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_i0e_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0e_out, name, "aten::special_i0e")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0e_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i0e_out, schema_str, "special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_i0e_out::schema> create_special_i0e_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i0e_out::name, special_i0e_out::overload_name)
      .typed<special_i0e_out::schema>();
}

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i0e_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i0e_out_typed_handle();
    return op.call(self, out);
}

// aten::special_i0e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i0e_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i0e_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1, name, "aten::special_i1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1, schema_str, "special_i1(Tensor self) -> Tensor")

// aten::special_i1(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_i1::schema> create_special_i1_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i1::name, special_i1::overload_name)
      .typed<special_i1::schema>();
}

// aten::special_i1(Tensor self) -> Tensor
at::Tensor special_i1::call(const at::Tensor & self) {
    static auto op = create_special_i1_typed_handle();
    return op.call(self);
}

// aten::special_i1(Tensor self) -> Tensor
at::Tensor special_i1::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_i1_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1_out, name, "aten::special_i1")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1_out, schema_str, "special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_i1_out::schema> create_special_i1_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i1_out::name, special_i1_out::overload_name)
      .typed<special_i1_out::schema>();
}

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i1_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i1_out_typed_handle();
    return op.call(self, out);
}

// aten::special_i1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i1_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i1_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1e, name, "aten::special_i1e")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1e, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1e, schema_str, "special_i1e(Tensor self) -> Tensor")

// aten::special_i1e(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_i1e::schema> create_special_i1e_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i1e::name, special_i1e::overload_name)
      .typed<special_i1e::schema>();
}

// aten::special_i1e(Tensor self) -> Tensor
at::Tensor special_i1e::call(const at::Tensor & self) {
    static auto op = create_special_i1e_typed_handle();
    return op.call(self);
}

// aten::special_i1e(Tensor self) -> Tensor
at::Tensor special_i1e::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_i1e_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1e_out, name, "aten::special_i1e")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1e_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_i1e_out, schema_str, "special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_i1e_out::schema> create_special_i1e_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_i1e_out::name, special_i1e_out::overload_name)
      .typed<special_i1e_out::schema>();
}

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i1e_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i1e_out_typed_handle();
    return op.call(self, out);
}

// aten::special_i1e.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_i1e_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_i1e_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logit, name, "aten::special_logit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logit, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logit, schema_str, "special_logit(Tensor self, float? eps=None) -> Tensor")

// aten::special_logit(Tensor self, float? eps=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_logit::schema> create_special_logit_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_logit::name, special_logit::overload_name)
      .typed<special_logit::schema>();
}

// aten::special_logit(Tensor self, float? eps=None) -> Tensor
at::Tensor special_logit::call(const at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_special_logit_typed_handle();
    return op.call(self, eps);
}

// aten::special_logit(Tensor self, float? eps=None) -> Tensor
at::Tensor special_logit::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps) {
    static auto op = create_special_logit_typed_handle();
    return op.redispatch(dispatchKeySet, self, eps);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logit_out, name, "aten::special_logit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logit_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logit_out, schema_str, "special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_logit_out::schema> create_special_logit_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_logit_out::name, special_logit_out::overload_name)
      .typed<special_logit_out::schema>();
}

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_logit_out::call(const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    static auto op = create_special_logit_out_typed_handle();
    return op.call(self, eps, out);
}

// aten::special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_logit_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> eps, at::Tensor & out) {
    static auto op = create_special_logit_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, eps, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma, name, "aten::special_polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma, schema_str, "special_polygamma(int n, Tensor self) -> Tensor")

// aten::special_polygamma(int n, Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_polygamma::schema> create_special_polygamma_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_polygamma::name, special_polygamma::overload_name)
      .typed<special_polygamma::schema>();
}

// aten::special_polygamma(int n, Tensor self) -> Tensor
at::Tensor special_polygamma::call(int64_t n, const at::Tensor & self) {
    static auto op = create_special_polygamma_typed_handle();
    return op.call(n, self);
}

// aten::special_polygamma(int n, Tensor self) -> Tensor
at::Tensor special_polygamma::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self) {
    static auto op = create_special_polygamma_typed_handle();
    return op.redispatch(dispatchKeySet, n, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma_out, name, "aten::special_polygamma")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_polygamma_out, schema_str, "special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_polygamma_out::schema> create_special_polygamma_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_polygamma_out::name, special_polygamma_out::overload_name)
      .typed<special_polygamma_out::schema>();
}

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_polygamma_out::call(int64_t n, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_polygamma_out_typed_handle();
    return op.call(n, self, out);
}

// aten::special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_polygamma_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_polygamma_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logsumexp, name, "aten::special_logsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logsumexp, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logsumexp, schema_str, "special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor")

// aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_logsumexp::schema> create_special_logsumexp_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_logsumexp::name, special_logsumexp::overload_name)
      .typed<special_logsumexp::schema>();
}

// aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor special_logsumexp::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_special_logsumexp_typed_handle();
    return op.call(self, dim, keepdim);
}

// aten::special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
at::Tensor special_logsumexp::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    static auto op = create_special_logsumexp_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logsumexp_out, name, "aten::special_logsumexp")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logsumexp_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_logsumexp_out, schema_str, "special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_logsumexp_out::schema> create_special_logsumexp_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_logsumexp_out::name, special_logsumexp_out::overload_name)
      .typed<special_logsumexp_out::schema>();
}

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_logsumexp_out::call(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_special_logsumexp_out_typed_handle();
    return op.call(self, dim, keepdim, out);
}

// aten::special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_logsumexp_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    static auto op = create_special_logsumexp_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, keepdim, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit, name, "aten::special_expit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit, schema_str, "special_expit(Tensor self) -> Tensor")

// aten::special_expit(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_expit::schema> create_special_expit_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expit::name, special_expit::overload_name)
      .typed<special_expit::schema>();
}

// aten::special_expit(Tensor self) -> Tensor
at::Tensor special_expit::call(const at::Tensor & self) {
    static auto op = create_special_expit_typed_handle();
    return op.call(self);
}

// aten::special_expit(Tensor self) -> Tensor
at::Tensor special_expit::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_expit_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit_out, name, "aten::special_expit")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_expit_out, schema_str, "special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_expit_out::schema> create_special_expit_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_expit_out::name, special_expit_out::overload_name)
      .typed<special_expit_out::schema>();
}

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expit_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_expit_out_typed_handle();
    return op.call(self, out);
}

// aten::special_expit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_expit_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_expit_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc, name, "aten::special_sinc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc, schema_str, "special_sinc(Tensor self) -> Tensor")

// aten::special_sinc(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_sinc::schema> create_special_sinc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_sinc::name, special_sinc::overload_name)
      .typed<special_sinc::schema>();
}

// aten::special_sinc(Tensor self) -> Tensor
at::Tensor special_sinc::call(const at::Tensor & self) {
    static auto op = create_special_sinc_typed_handle();
    return op.call(self);
}

// aten::special_sinc(Tensor self) -> Tensor
at::Tensor special_sinc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_sinc_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc_out, name, "aten::special_sinc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_sinc_out, schema_str, "special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_sinc_out::schema> create_special_sinc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_sinc_out::name, special_sinc_out::overload_name)
      .typed<special_sinc_out::schema>();
}

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_sinc_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_sinc_out_typed_handle();
    return op.call(self, out);
}

// aten::special_sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_sinc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_sinc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_round, name, "aten::special_round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_round, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_round, schema_str, "special_round(Tensor self) -> Tensor")

// aten::special_round(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_round::schema> create_special_round_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_round::name, special_round::overload_name)
      .typed<special_round::schema>();
}

// aten::special_round(Tensor self) -> Tensor
at::Tensor special_round::call(const at::Tensor & self) {
    static auto op = create_special_round_typed_handle();
    return op.call(self);
}

// aten::special_round(Tensor self) -> Tensor
at::Tensor special_round::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_round_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_round_out, name, "aten::special_round")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_round_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_round_out, schema_str, "special_round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_round_out::schema> create_special_round_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_round_out::name, special_round_out::overload_name)
      .typed<special_round_out::schema>();
}

// aten::special_round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_round_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_round_out_typed_handle();
    return op.call(self, out);
}

// aten::special_round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_round_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_round_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p, name, "aten::special_log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p, schema_str, "special_log1p(Tensor self) -> Tensor")

// aten::special_log1p(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_log1p::schema> create_special_log1p_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log1p::name, special_log1p::overload_name)
      .typed<special_log1p::schema>();
}

// aten::special_log1p(Tensor self) -> Tensor
at::Tensor special_log1p::call(const at::Tensor & self) {
    static auto op = create_special_log1p_typed_handle();
    return op.call(self);
}

// aten::special_log1p(Tensor self) -> Tensor
at::Tensor special_log1p::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_special_log1p_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p_out, name, "aten::special_log1p")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log1p_out, schema_str, "special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_log1p_out::schema> create_special_log1p_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log1p_out::name, special_log1p_out::overload_name)
      .typed<special_log1p_out::schema>();
}

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_log1p_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_log1p_out_typed_handle();
    return op.call(self, out);
}

// aten::special_log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_log1p_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_special_log1p_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_softmax, name, "aten::special_log_softmax")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_softmax, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_log_softmax, schema_str, "special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor")

// aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_log_softmax::schema> create_special_log_softmax_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_log_softmax::name, special_log_softmax::overload_name)
      .typed<special_log_softmax::schema>();
}

// aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor special_log_softmax::call(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_special_log_softmax_typed_handle();
    return op.call(self, dim, dtype);
}

// aten::special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
at::Tensor special_log_softmax::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_special_log_softmax_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammainc_out, name, "aten::special_gammainc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammainc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammainc_out, schema_str, "special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_gammainc_out::schema> create_special_gammainc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammainc_out::name, special_gammainc_out::overload_name)
      .typed<special_gammainc_out::schema>();
}

// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammainc_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_gammainc_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammainc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_gammainc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammainc, name, "aten::special_gammainc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammainc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammainc, schema_str, "special_gammainc(Tensor self, Tensor other) -> Tensor")

// aten::special_gammainc(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_gammainc::schema> create_special_gammainc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammainc::name, special_gammainc::overload_name)
      .typed<special_gammainc::schema>();
}

// aten::special_gammainc(Tensor self, Tensor other) -> Tensor
at::Tensor special_gammainc::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_gammainc_typed_handle();
    return op.call(self, other);
}

// aten::special_gammainc(Tensor self, Tensor other) -> Tensor
at::Tensor special_gammainc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_gammainc_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaincc_out, name, "aten::special_gammaincc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaincc_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaincc_out, schema_str, "special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_gammaincc_out::schema> create_special_gammaincc_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammaincc_out::name, special_gammaincc_out::overload_name)
      .typed<special_gammaincc_out::schema>();
}

// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammaincc_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_gammaincc_out_typed_handle();
    return op.call(self, other, out);
}

// aten::special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_gammaincc_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_special_gammaincc_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaincc, name, "aten::special_gammaincc")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaincc, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_gammaincc, schema_str, "special_gammaincc(Tensor self, Tensor other) -> Tensor")

// aten::special_gammaincc(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_gammaincc::schema> create_special_gammaincc_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_gammaincc::name, special_gammaincc::overload_name)
      .typed<special_gammaincc::schema>();
}

// aten::special_gammaincc(Tensor self, Tensor other) -> Tensor
at::Tensor special_gammaincc::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_gammaincc_typed_handle();
    return op.call(self, other);
}

// aten::special_gammaincc(Tensor self, Tensor other) -> Tensor
at::Tensor special_gammaincc::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_special_gammaincc_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_multigammaln, name, "aten::special_multigammaln")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_multigammaln, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_multigammaln, schema_str, "special_multigammaln(Tensor self, int p) -> Tensor")

// aten::special_multigammaln(Tensor self, int p) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<special_multigammaln::schema> create_special_multigammaln_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_multigammaln::name, special_multigammaln::overload_name)
      .typed<special_multigammaln::schema>();
}

// aten::special_multigammaln(Tensor self, int p) -> Tensor
at::Tensor special_multigammaln::call(const at::Tensor & self, int64_t p) {
    static auto op = create_special_multigammaln_typed_handle();
    return op.call(self, p);
}

// aten::special_multigammaln(Tensor self, int p) -> Tensor
at::Tensor special_multigammaln::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p) {
    static auto op = create_special_multigammaln_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_multigammaln_out, name, "aten::special_multigammaln")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_multigammaln_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(special_multigammaln_out, schema_str, "special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)")

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<special_multigammaln_out::schema> create_special_multigammaln_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(special_multigammaln_out::name, special_multigammaln_out::overload_name)
      .typed<special_multigammaln_out::schema>();
}

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_multigammaln_out::call(const at::Tensor & self, int64_t p, at::Tensor & out) {
    static auto op = create_special_multigammaln_out_typed_handle();
    return op.call(self, p, out);
}

// aten::special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & special_multigammaln_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t p, at::Tensor & out) {
    static auto op = create_special_multigammaln_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft, name, "aten::fft_fft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft, schema_str, "fft_fft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_fft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fft::schema> create_fft_fft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fft::name, fft_fft::overload_name)
      .typed<fft_fft::schema>();
}

// aten::fft_fft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_fft::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_fft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_fft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_fft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_fft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft_out, name, "aten::fft_fft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft_out, schema_str, "fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_fft_out::schema> create_fft_fft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fft_out::name, fft_fft_out::overload_name)
      .typed<fft_fft_out::schema>();
}

// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fft_out::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_fft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_fft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_fft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft, name, "aten::fft_ifft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft, schema_str, "fft_ifft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_ifft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifft::schema> create_fft_ifft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifft::name, fft_ifft::overload_name)
      .typed<fft_ifft::schema>();
}

// aten::fft_ifft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_ifft::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ifft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_ifft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_ifft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ifft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft_out, name, "aten::fft_ifft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft_out, schema_str, "fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifft_out::schema> create_fft_ifft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifft_out::name, fft_ifft_out::overload_name)
      .typed<fft_ifft_out::schema>();
}

// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifft_out::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ifft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_ifft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ifft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft, name, "aten::fft_rfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft, schema_str, "fft_rfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_rfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfft::schema> create_fft_rfft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfft::name, fft_rfft::overload_name)
      .typed<fft_rfft::schema>();
}

// aten::fft_rfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_rfft::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_rfft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_rfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_rfft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_rfft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft_out, name, "aten::fft_rfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft_out, schema_str, "fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfft_out::schema> create_fft_rfft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfft_out::name, fft_rfft_out::overload_name)
      .typed<fft_rfft_out::schema>();
}

// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfft_out::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_rfft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_rfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_rfft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft, name, "aten::fft_irfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft, schema_str, "fft_irfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_irfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft::schema> create_fft_irfft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft::name, fft_irfft::overload_name)
      .typed<fft_irfft::schema>();
}

// aten::fft_irfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_irfft::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_irfft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_irfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_irfft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_irfft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft_out, name, "aten::fft_irfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft_out, schema_str, "fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft_out::schema> create_fft_irfft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft_out::name, fft_irfft_out::overload_name)
      .typed<fft_irfft_out::schema>();
}

// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft_out::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_irfft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_irfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_irfft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft, name, "aten::fft_hfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft, schema_str, "fft_hfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_hfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_hfft::schema> create_fft_hfft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_hfft::name, fft_hfft::overload_name)
      .typed<fft_hfft::schema>();
}

// aten::fft_hfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_hfft::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_hfft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_hfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_hfft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_hfft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft_out, name, "aten::fft_hfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_hfft_out, schema_str, "fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_hfft_out::schema> create_fft_hfft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_hfft_out::name, fft_hfft_out::overload_name)
      .typed<fft_hfft_out::schema>();
}

// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_hfft_out::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_hfft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_hfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_hfft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_hfft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfft, name, "aten::fft_ihfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfft, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfft, schema_str, "fft_ihfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor")

// aten::fft_ihfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ihfft::schema> create_fft_ihfft_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ihfft::name, fft_ihfft::overload_name)
      .typed<fft_ihfft::schema>();
}

// aten::fft_ihfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_ihfft::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ihfft_typed_handle();
    return op.call(self, n, dim, norm);
}

// aten::fft_ihfft(Tensor self, int? n=None, int dim=-1, str? norm=None) -> Tensor
at::Tensor fft_ihfft::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ihfft_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfft_out, name, "aten::fft_ihfft")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfft_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ihfft_out, schema_str, "fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ihfft_out::schema> create_fft_ihfft_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ihfft_out::name, fft_ihfft_out::overload_name)
      .typed<fft_ihfft_out::schema>();
}

// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ihfft_out::call(const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ihfft_out_typed_handle();
    return op.call(self, n, dim, norm, out);
}

// aten::fft_ihfft.out(Tensor self, int? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ihfft_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<int64_t> n, int64_t dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ihfft_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft2, name, "aten::fft_fft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft2, schema_str, "fft_fft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_fft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fft2::schema> create_fft_fft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fft2::name, fft_fft2::overload_name)
      .typed<fft_fft2::schema>();
}

// aten::fft_fft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_fft2::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_fft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_fft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_fft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_fft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft2_out, name, "aten::fft_fft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fft2_out, schema_str, "fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_fft2_out::schema> create_fft_fft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fft2_out::name, fft_fft2_out::overload_name)
      .typed<fft_fft2_out::schema>();
}

// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fft2_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_fft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_fft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_fft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2, name, "aten::fft_ifft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2, schema_str, "fft_ifft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_ifft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifft2::schema> create_fft_ifft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifft2::name, fft_ifft2::overload_name)
      .typed<fft_ifft2::schema>();
}

// aten::fft_ifft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_ifft2::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ifft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_ifft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_ifft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ifft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2_out, name, "aten::fft_ifft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifft2_out, schema_str, "fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifft2_out::schema> create_fft_ifft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifft2_out::name, fft_ifft2_out::overload_name)
      .typed<fft_ifft2_out::schema>();
}

// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifft2_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ifft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_ifft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ifft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft2, name, "aten::fft_rfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft2, schema_str, "fft_rfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_rfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfft2::schema> create_fft_rfft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfft2::name, fft_rfft2::overload_name)
      .typed<fft_rfft2::schema>();
}

// aten::fft_rfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_rfft2::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_rfft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_rfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_rfft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_rfft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft2_out, name, "aten::fft_rfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfft2_out, schema_str, "fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfft2_out::schema> create_fft_rfft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfft2_out::name, fft_rfft2_out::overload_name)
      .typed<fft_rfft2_out::schema>();
}

// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfft2_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_rfft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_rfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_rfft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2, name, "aten::fft_irfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2, schema_str, "fft_irfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor")

// aten::fft_irfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft2::schema> create_fft_irfft2_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft2::name, fft_irfft2::overload_name)
      .typed<fft_irfft2::schema>();
}

// aten::fft_irfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_irfft2::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_irfft2_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_irfft2(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -> Tensor
at::Tensor fft_irfft2::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_irfft2_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2_out, name, "aten::fft_irfft2")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfft2_out, schema_str, "fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfft2_out::schema> create_fft_irfft2_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfft2_out::name, fft_irfft2_out::overload_name)
      .typed<fft_irfft2_out::schema>();
}

// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft2_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_irfft2_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_irfft2.out(Tensor self, int[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfft2_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, at::IntArrayRef dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_irfft2_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftn, name, "aten::fft_fftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftn, schema_str, "fft_fftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_fftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftn::schema> create_fft_fftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftn::name, fft_fftn::overload_name)
      .typed<fft_fftn::schema>();
}

// aten::fft_fftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_fftn::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_fftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_fftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_fftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_fftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftn_out, name, "aten::fft_fftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftn_out, schema_str, "fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftn_out::schema> create_fft_fftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftn_out::name, fft_fftn_out::overload_name)
      .typed<fft_fftn_out::schema>();
}

// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fftn_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_fftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_fftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_fftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn, name, "aten::fft_ifftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn, schema_str, "fft_ifftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_ifftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifftn::schema> create_fft_ifftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifftn::name, fft_ifftn::overload_name)
      .typed<fft_ifftn::schema>();
}

// aten::fft_ifftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_ifftn::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ifftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_ifftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_ifftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_ifftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn_out, name, "aten::fft_ifftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftn_out, schema_str, "fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifftn_out::schema> create_fft_ifftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifftn_out::name, fft_ifftn_out::overload_name)
      .typed<fft_ifftn_out::schema>();
}

// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifftn_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ifftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_ifftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_ifftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_ifftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn, name, "aten::fft_rfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn, schema_str, "fft_rfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_rfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftn::schema> create_fft_rfftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftn::name, fft_rfftn::overload_name)
      .typed<fft_rfftn::schema>();
}

// aten::fft_rfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_rfftn::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_rfftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_rfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_rfftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_rfftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn_out, name, "aten::fft_rfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftn_out, schema_str, "fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftn_out::schema> create_fft_rfftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftn_out::name, fft_rfftn_out::overload_name)
      .typed<fft_rfftn_out::schema>();
}

// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftn_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_rfftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_rfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_rfftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfftn, name, "aten::fft_irfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfftn, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfftn, schema_str, "fft_irfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor")

// aten::fft_irfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfftn::schema> create_fft_irfftn_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfftn::name, fft_irfftn::overload_name)
      .typed<fft_irfftn::schema>();
}

// aten::fft_irfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_irfftn::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_irfftn_typed_handle();
    return op.call(self, s, dim, norm);
}

// aten::fft_irfftn(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None) -> Tensor
at::Tensor fft_irfftn::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm) {
    static auto op = create_fft_irfftn_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfftn_out, name, "aten::fft_irfftn")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfftn_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_irfftn_out, schema_str, "fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_irfftn_out::schema> create_fft_irfftn_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_irfftn_out::name, fft_irfftn_out::overload_name)
      .typed<fft_irfftn_out::schema>();
}

// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfftn_out::call(const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_irfftn_out_typed_handle();
    return op.call(self, s, dim, norm, out);
}

// aten::fft_irfftn.out(Tensor self, int[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_irfftn_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> s, c10::optional<at::IntArrayRef> dim, c10::optional<c10::string_view> norm, at::Tensor & out) {
    static auto op = create_fft_irfftn_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, s, dim, norm, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq, name, "aten::fft_fftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq, schema_str, "fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftfreq::schema> create_fft_fftfreq_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftfreq::name, fft_fftfreq::overload_name)
      .typed<fft_fftfreq::schema>();
}

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_fftfreq::call(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_fft_fftfreq_typed_handle();
    return op.call(n, d, dtype, layout, device, pin_memory);
}

// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_fftfreq::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_fft_fftfreq_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq_out, name, "aten::fft_fftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftfreq_out, schema_str, "fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftfreq_out::schema> create_fft_fftfreq_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftfreq_out::name, fft_fftfreq_out::overload_name)
      .typed<fft_fftfreq_out::schema>();
}

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fftfreq_out::call(int64_t n, double d, at::Tensor & out) {
    static auto op = create_fft_fftfreq_out_typed_handle();
    return op.call(n, d, out);
}

// aten::fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_fftfreq_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
    static auto op = create_fft_fftfreq_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq, name, "aten::fft_rfftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq, schema_str, "fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor")

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftfreq::schema> create_fft_rfftfreq_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftfreq::name, fft_rfftfreq::overload_name)
      .typed<fft_rfftfreq::schema>();
}

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_rfftfreq::call(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_fft_rfftfreq_typed_handle();
    return op.call(n, d, dtype, layout, device, pin_memory);
}

// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
at::Tensor fft_rfftfreq::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
    static auto op = create_fft_rfftfreq_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, dtype, layout, device, pin_memory);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq_out, name, "aten::fft_rfftfreq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_rfftfreq_out, schema_str, "fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)")

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<fft_rfftfreq_out::schema> create_fft_rfftfreq_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_rfftfreq_out::name, fft_rfftfreq_out::overload_name)
      .typed<fft_rfftfreq_out::schema>();
}

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftfreq_out::call(int64_t n, double d, at::Tensor & out) {
    static auto op = create_fft_rfftfreq_out_typed_handle();
    return op.call(n, d, out);
}

// aten::fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & fft_rfftfreq_out::redispatch(c10::DispatchKeySet dispatchKeySet, int64_t n, double d, at::Tensor & out) {
    static auto op = create_fft_rfftfreq_out_typed_handle();
    return op.redispatch(dispatchKeySet, n, d, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftshift, name, "aten::fft_fftshift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftshift, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_fftshift, schema_str, "fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor")

// aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_fftshift::schema> create_fft_fftshift_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_fftshift::name, fft_fftshift::overload_name)
      .typed<fft_fftshift::schema>();
}

// aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
at::Tensor fft_fftshift::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
    static auto op = create_fft_fftshift_typed_handle();
    return op.call(self, dim);
}

// aten::fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
at::Tensor fft_fftshift::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
    static auto op = create_fft_fftshift_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftshift, name, "aten::fft_ifftshift")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftshift, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(fft_ifftshift, schema_str, "fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor")

// aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<fft_ifftshift::schema> create_fft_ifftshift_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(fft_ifftshift::name, fft_ifftshift::overload_name)
      .typed<fft_ifftshift::schema>();
}

// aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
at::Tensor fft_ifftshift::call(const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
    static auto op = create_fft_ifftshift_typed_handle();
    return op.call(self, dim);
}

// aten::fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
at::Tensor fft_ifftshift::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<at::IntArrayRef> dim) {
    static auto op = create_fft_ifftshift_typed_handle();
    return op.redispatch(dispatchKeySet, self, dim);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex, name, "aten::linalg_cholesky_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex, schema_str, "linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)")

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky_ex::schema> create_linalg_cholesky_ex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky_ex::name, linalg_cholesky_ex::overload_name)
      .typed<linalg_cholesky_ex::schema>();
}

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_cholesky_ex::call(const at::Tensor & self, bool upper, bool check_errors) {
    static auto op = create_linalg_cholesky_ex_typed_handle();
    return op.call(self, upper, check_errors);
}

// aten::linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -> (Tensor L, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_cholesky_ex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, bool check_errors) {
    static auto op = create_linalg_cholesky_ex_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex_L, name, "aten::linalg_cholesky_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex_L, overload_name, "L")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_ex_L, schema_str, "linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)")

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky_ex_L::schema> create_linalg_cholesky_ex_L_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky_ex_L::name, linalg_cholesky_ex_L::overload_name)
      .typed<linalg_cholesky_ex_L::schema>();
}

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_L::call(const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
    static auto op = create_linalg_cholesky_ex_L_typed_handle();
    return op.call(self, upper, check_errors, L, info);
}

// aten::linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -> (Tensor(a!) L, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_cholesky_ex_L::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, bool check_errors, at::Tensor & L, at::Tensor & info) {
    static auto op = create_linalg_cholesky_ex_L_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, check_errors, L, info);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky, name, "aten::linalg_cholesky")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky, schema_str, "linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor")

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky::schema> create_linalg_cholesky_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky::name, linalg_cholesky::overload_name)
      .typed<linalg_cholesky::schema>();
}

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
at::Tensor linalg_cholesky::call(const at::Tensor & self, bool upper) {
    static auto op = create_linalg_cholesky_typed_handle();
    return op.call(self, upper);
}

// aten::linalg_cholesky(Tensor self, *, bool upper=False) -> Tensor
at::Tensor linalg_cholesky::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper) {
    static auto op = create_linalg_cholesky_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_out, name, "aten::linalg_cholesky")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cholesky_out, schema_str, "linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cholesky_out::schema> create_linalg_cholesky_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cholesky_out::name, linalg_cholesky_out::overload_name)
      .typed<linalg_cholesky_out::schema>();
}

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cholesky_out::call(const at::Tensor & self, bool upper, at::Tensor & out) {
    static auto op = create_linalg_cholesky_out_typed_handle();
    return op.call(self, upper, out);
}

// aten::linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cholesky_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool upper, at::Tensor & out) {
    static auto op = create_linalg_cholesky_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, upper, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_det, name, "aten::linalg_det")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_det, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_det, schema_str, "linalg_det(Tensor self) -> Tensor")

// aten::linalg_det(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_det::schema> create_linalg_det_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_det::name, linalg_det::overload_name)
      .typed<linalg_det::schema>();
}

// aten::linalg_det(Tensor self) -> Tensor
at::Tensor linalg_det::call(const at::Tensor & self) {
    static auto op = create_linalg_det_typed_handle();
    return op.call(self);
}

// aten::linalg_det(Tensor self) -> Tensor
at::Tensor linalg_det::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_linalg_det_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_det_out, name, "aten::linalg_det")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_det_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_det_out, schema_str, "linalg_det.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_det.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_det_out::schema> create_linalg_det_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_det_out::name, linalg_det_out::overload_name)
      .typed<linalg_det_out::schema>();
}

// aten::linalg_det.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_det_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_linalg_det_out_typed_handle();
    return op.call(self, out);
}

// aten::linalg_det.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_det_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_linalg_det_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(det, name, "aten::det")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(det, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(det, schema_str, "det(Tensor self) -> Tensor")

// aten::det(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<det::schema> create_det_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(det::name, det::overload_name)
      .typed<det::schema>();
}

// aten::det(Tensor self) -> Tensor
at::Tensor det::call(const at::Tensor & self) {
    static auto op = create_det_typed_handle();
    return op.call(self);
}

// aten::det(Tensor self) -> Tensor
at::Tensor det::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_det_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_det_lu_based_helper, name, "aten::_det_lu_based_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_det_lu_based_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_det_lu_based_helper, schema_str, "_det_lu_based_helper(Tensor self) -> (Tensor det, Tensor lu, Tensor pivs)")

// aten::_det_lu_based_helper(Tensor self) -> (Tensor det, Tensor lu, Tensor pivs)
static C10_NOINLINE c10::TypedOperatorHandle<_det_lu_based_helper::schema> create__det_lu_based_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_det_lu_based_helper::name, _det_lu_based_helper::overload_name)
      .typed<_det_lu_based_helper::schema>();
}

// aten::_det_lu_based_helper(Tensor self) -> (Tensor det, Tensor lu, Tensor pivs)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _det_lu_based_helper::call(const at::Tensor & self) {
    static auto op = create__det_lu_based_helper_typed_handle();
    return op.call(self);
}

// aten::_det_lu_based_helper(Tensor self) -> (Tensor det, Tensor lu, Tensor pivs)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> _det_lu_based_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create__det_lu_based_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_det_lu_based_helper_backward_helper, name, "aten::_det_lu_based_helper_backward_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_det_lu_based_helper_backward_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_det_lu_based_helper_backward_helper, schema_str, "_det_lu_based_helper_backward_helper(Tensor det_grad, Tensor det, Tensor self, Tensor lu, Tensor pivs) -> Tensor")

// aten::_det_lu_based_helper_backward_helper(Tensor det_grad, Tensor det, Tensor self, Tensor lu, Tensor pivs) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_det_lu_based_helper_backward_helper::schema> create__det_lu_based_helper_backward_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_det_lu_based_helper_backward_helper::name, _det_lu_based_helper_backward_helper::overload_name)
      .typed<_det_lu_based_helper_backward_helper::schema>();
}

// aten::_det_lu_based_helper_backward_helper(Tensor det_grad, Tensor det, Tensor self, Tensor lu, Tensor pivs) -> Tensor
at::Tensor _det_lu_based_helper_backward_helper::call(const at::Tensor & det_grad, const at::Tensor & det, const at::Tensor & self, const at::Tensor & lu, const at::Tensor & pivs) {
    static auto op = create__det_lu_based_helper_backward_helper_typed_handle();
    return op.call(det_grad, det, self, lu, pivs);
}

// aten::_det_lu_based_helper_backward_helper(Tensor det_grad, Tensor det, Tensor self, Tensor lu, Tensor pivs) -> Tensor
at::Tensor _det_lu_based_helper_backward_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & det_grad, const at::Tensor & det, const at::Tensor & self, const at::Tensor & lu, const at::Tensor & pivs) {
    static auto op = create__det_lu_based_helper_backward_helper_typed_handle();
    return op.redispatch(dispatchKeySet, det_grad, det, self, lu, pivs);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lstsq, name, "aten::linalg_lstsq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lstsq, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lstsq, schema_str, "linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)")

// aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_lstsq::schema> create_linalg_lstsq_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_lstsq::name, linalg_lstsq::overload_name)
      .typed<linalg_lstsq::schema>();
}

// aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> linalg_lstsq::call(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
    static auto op = create_linalg_lstsq_typed_handle();
    return op.call(self, b, rcond, driver);
}

// aten::linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -> (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)
::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> linalg_lstsq::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
    static auto op = create_linalg_lstsq_typed_handle();
    return op.redispatch(dispatchKeySet, self, b, rcond, driver);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lstsq_out, name, "aten::linalg_lstsq")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lstsq_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_lstsq_out, schema_str, "linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)")

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_lstsq_out::schema> create_linalg_lstsq_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_lstsq_out::name, linalg_lstsq_out::overload_name)
      .typed<linalg_lstsq_out::schema>();
}

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> linalg_lstsq_out::call(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
    static auto op = create_linalg_lstsq_out_typed_handle();
    return op.call(self, b, rcond, driver, solution, residuals, rank, singular_values);
}

// aten::linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -> (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &,at::Tensor &> linalg_lstsq_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver, at::Tensor & solution, at::Tensor & residuals, at::Tensor & rank, at::Tensor & singular_values) {
    static auto op = create_linalg_lstsq_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, b, rcond, driver, solution, residuals, rank, singular_values);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul, name, "aten::linalg_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul, schema_str, "linalg_matmul(Tensor self, Tensor other) -> Tensor")

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matmul::schema> create_linalg_matmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matmul::name, linalg_matmul::overload_name)
      .typed<linalg_matmul::schema>();
}

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
at::Tensor linalg_matmul::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_linalg_matmul_typed_handle();
    return op.call(self, other);
}

// aten::linalg_matmul(Tensor self, Tensor other) -> Tensor
at::Tensor linalg_matmul::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_linalg_matmul_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul_out, name, "aten::linalg_matmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matmul_out, schema_str, "linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matmul_out::schema> create_linalg_matmul_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matmul_out::name, linalg_matmul_out::overload_name)
      .typed<linalg_matmul_out::schema>();
}

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matmul_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_linalg_matmul_out_typed_handle();
    return op.call(self, other, out);
}

// aten::linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matmul_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_linalg_matmul_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet, name, "aten::linalg_slogdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet, schema_str, "linalg_slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)")

// aten::linalg_slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_slogdet::schema> create_linalg_slogdet_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_slogdet::name, linalg_slogdet::overload_name)
      .typed<linalg_slogdet::schema>();
}

// aten::linalg_slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
::std::tuple<at::Tensor,at::Tensor> linalg_slogdet::call(const at::Tensor & self) {
    static auto op = create_linalg_slogdet_typed_handle();
    return op.call(self);
}

// aten::linalg_slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
::std::tuple<at::Tensor,at::Tensor> linalg_slogdet::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_linalg_slogdet_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet_out, name, "aten::linalg_slogdet")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_slogdet_out, schema_str, "linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)")

// aten::linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_slogdet_out::schema> create_linalg_slogdet_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_slogdet_out::name, linalg_slogdet_out::overload_name)
      .typed<linalg_slogdet_out::schema>();
}

// aten::linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_out::call(const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
    static auto op = create_linalg_slogdet_out_typed_handle();
    return op.call(self, sign, logabsdet);
}

// aten::linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
::std::tuple<at::Tensor &,at::Tensor &> linalg_slogdet_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & sign, at::Tensor & logabsdet) {
    static auto op = create_linalg_slogdet_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, sign, logabsdet);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eig, name, "aten::linalg_eig")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eig, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eig, schema_str, "linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)")

// aten::linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eig::schema> create_linalg_eig_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eig::name, linalg_eig::overload_name)
      .typed<linalg_eig::schema>();
}

// aten::linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> linalg_eig::call(const at::Tensor & self) {
    static auto op = create_linalg_eig_typed_handle();
    return op.call(self);
}

// aten::linalg_eig(Tensor self) -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> linalg_eig::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_linalg_eig_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eig_out, name, "aten::linalg_eig")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eig_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eig_out, schema_str, "linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)")

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eig_out::schema> create_linalg_eig_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eig_out::name, linalg_eig_out::overload_name)
      .typed<linalg_eig_out::schema>();
}

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> linalg_eig_out::call(const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
    static auto op = create_linalg_eig_out_typed_handle();
    return op.call(self, eigenvalues, eigenvectors);
}

// aten::linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> linalg_eig_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & eigenvalues, at::Tensor & eigenvectors) {
    static auto op = create_linalg_eig_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, eigenvalues, eigenvectors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals, name, "aten::linalg_eigvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals, schema_str, "linalg_eigvals(Tensor self) -> Tensor")

// aten::linalg_eigvals(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigvals::schema> create_linalg_eigvals_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigvals::name, linalg_eigvals::overload_name)
      .typed<linalg_eigvals::schema>();
}

// aten::linalg_eigvals(Tensor self) -> Tensor
at::Tensor linalg_eigvals::call(const at::Tensor & self) {
    static auto op = create_linalg_eigvals_typed_handle();
    return op.call(self);
}

// aten::linalg_eigvals(Tensor self) -> Tensor
at::Tensor linalg_eigvals::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_linalg_eigvals_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals_out, name, "aten::linalg_eigvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvals_out, schema_str, "linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigvals_out::schema> create_linalg_eigvals_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigvals_out::name, linalg_eigvals_out::overload_name)
      .typed<linalg_eigvals_out::schema>();
}

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_eigvals_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_linalg_eigvals_out_typed_handle();
    return op.call(self, out);
}

// aten::linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_eigvals_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_linalg_eigvals_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigh, name, "aten::linalg_eigh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigh, schema_str, "linalg_eigh(Tensor self, str UPLO=\"L\") -> (Tensor eigenvalues, Tensor eigenvectors)")

// aten::linalg_eigh(Tensor self, str UPLO="L") -> (Tensor eigenvalues, Tensor eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigh::schema> create_linalg_eigh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigh::name, linalg_eigh::overload_name)
      .typed<linalg_eigh::schema>();
}

// aten::linalg_eigh(Tensor self, str UPLO="L") -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> linalg_eigh::call(const at::Tensor & self, c10::string_view UPLO) {
    static auto op = create_linalg_eigh_typed_handle();
    return op.call(self, UPLO);
}

// aten::linalg_eigh(Tensor self, str UPLO="L") -> (Tensor eigenvalues, Tensor eigenvectors)
::std::tuple<at::Tensor,at::Tensor> linalg_eigh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO) {
    static auto op = create_linalg_eigh_typed_handle();
    return op.redispatch(dispatchKeySet, self, UPLO);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigh_eigvals, name, "aten::linalg_eigh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigh_eigvals, overload_name, "eigvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigh_eigvals, schema_str, "linalg_eigh.eigvals(Tensor self, str UPLO=\"L\", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)")

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigh_eigvals::schema> create_linalg_eigh_eigvals_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigh_eigvals::name, linalg_eigh_eigvals::overload_name)
      .typed<linalg_eigh_eigvals::schema>();
}

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_eigvals::call(const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
    static auto op = create_linalg_eigh_eigvals_typed_handle();
    return op.call(self, UPLO, eigvals, eigvecs);
}

// aten::linalg_eigh.eigvals(Tensor self, str UPLO="L", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -> (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)
::std::tuple<at::Tensor &,at::Tensor &> linalg_eigh_eigvals::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO, at::Tensor & eigvals, at::Tensor & eigvecs) {
    static auto op = create_linalg_eigh_eigvals_typed_handle();
    return op.redispatch(dispatchKeySet, self, UPLO, eigvals, eigvecs);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvalsh, name, "aten::linalg_eigvalsh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvalsh, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvalsh, schema_str, "linalg_eigvalsh(Tensor self, str UPLO=\"L\") -> Tensor")

// aten::linalg_eigvalsh(Tensor self, str UPLO="L") -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigvalsh::schema> create_linalg_eigvalsh_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigvalsh::name, linalg_eigvalsh::overload_name)
      .typed<linalg_eigvalsh::schema>();
}

// aten::linalg_eigvalsh(Tensor self, str UPLO="L") -> Tensor
at::Tensor linalg_eigvalsh::call(const at::Tensor & self, c10::string_view UPLO) {
    static auto op = create_linalg_eigvalsh_typed_handle();
    return op.call(self, UPLO);
}

// aten::linalg_eigvalsh(Tensor self, str UPLO="L") -> Tensor
at::Tensor linalg_eigvalsh::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO) {
    static auto op = create_linalg_eigvalsh_typed_handle();
    return op.redispatch(dispatchKeySet, self, UPLO);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvalsh_out, name, "aten::linalg_eigvalsh")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvalsh_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_eigvalsh_out, schema_str, "linalg_eigvalsh.out(Tensor self, str UPLO='L', *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_eigvalsh.out(Tensor self, str UPLO='L', *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_eigvalsh_out::schema> create_linalg_eigvalsh_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_eigvalsh_out::name, linalg_eigvalsh_out::overload_name)
      .typed<linalg_eigvalsh_out::schema>();
}

// aten::linalg_eigvalsh.out(Tensor self, str UPLO='L', *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_eigvalsh_out::call(const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
    static auto op = create_linalg_eigvalsh_out_typed_handle();
    return op.call(self, UPLO, out);
}

// aten::linalg_eigvalsh.out(Tensor self, str UPLO='L', *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_eigvalsh_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view UPLO, at::Tensor & out) {
    static auto op = create_linalg_eigvalsh_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, UPLO, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_householder_product, name, "aten::linalg_householder_product")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_householder_product, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_householder_product, schema_str, "linalg_householder_product(Tensor input, Tensor tau) -> Tensor")

// aten::linalg_householder_product(Tensor input, Tensor tau) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_householder_product::schema> create_linalg_householder_product_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_householder_product::name, linalg_householder_product::overload_name)
      .typed<linalg_householder_product::schema>();
}

// aten::linalg_householder_product(Tensor input, Tensor tau) -> Tensor
at::Tensor linalg_householder_product::call(const at::Tensor & input, const at::Tensor & tau) {
    static auto op = create_linalg_householder_product_typed_handle();
    return op.call(input, tau);
}

// aten::linalg_householder_product(Tensor input, Tensor tau) -> Tensor
at::Tensor linalg_householder_product::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tau) {
    static auto op = create_linalg_householder_product_typed_handle();
    return op.redispatch(dispatchKeySet, input, tau);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_householder_product_out, name, "aten::linalg_householder_product")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_householder_product_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_householder_product_out, schema_str, "linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_householder_product_out::schema> create_linalg_householder_product_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_householder_product_out::name, linalg_householder_product_out::overload_name)
      .typed<linalg_householder_product_out::schema>();
}

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_householder_product_out::call(const at::Tensor & input, const at::Tensor & tau, at::Tensor & out) {
    static auto op = create_linalg_householder_product_out_typed_handle();
    return op.call(input, tau, out);
}

// aten::linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_householder_product_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tau, at::Tensor & out) {
    static auto op = create_linalg_householder_product_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, tau, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_inv_out_helper_, name, "aten::_linalg_inv_out_helper_")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_inv_out_helper_, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_inv_out_helper_, schema_str, "_linalg_inv_out_helper_(Tensor(a!) self, Tensor(b!) infos_lu, Tensor(c!) infos_getri) -> Tensor(a!)")

// aten::_linalg_inv_out_helper_(Tensor(a!) self, Tensor(b!) infos_lu, Tensor(c!) infos_getri) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<_linalg_inv_out_helper_::schema> create__linalg_inv_out_helper__typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_linalg_inv_out_helper_::name, _linalg_inv_out_helper_::overload_name)
      .typed<_linalg_inv_out_helper_::schema>();
}

// aten::_linalg_inv_out_helper_(Tensor(a!) self, Tensor(b!) infos_lu, Tensor(c!) infos_getri) -> Tensor(a!)
at::Tensor & _linalg_inv_out_helper_::call(at::Tensor & self, at::Tensor & infos_lu, at::Tensor & infos_getri) {
    static auto op = create__linalg_inv_out_helper__typed_handle();
    return op.call(self, infos_lu, infos_getri);
}

// aten::_linalg_inv_out_helper_(Tensor(a!) self, Tensor(b!) infos_lu, Tensor(c!) infos_getri) -> Tensor(a!)
at::Tensor & _linalg_inv_out_helper_::redispatch(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, at::Tensor & infos_lu, at::Tensor & infos_getri) {
    static auto op = create__linalg_inv_out_helper__typed_handle();
    return op.redispatch(dispatchKeySet, self, infos_lu, infos_getri);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex, name, "aten::linalg_inv_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex, schema_str, "linalg_inv_ex(Tensor self, *, bool check_errors=False) -> (Tensor inverse, Tensor info)")

// aten::linalg_inv_ex(Tensor self, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_inv_ex::schema> create_linalg_inv_ex_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_inv_ex::name, linalg_inv_ex::overload_name)
      .typed<linalg_inv_ex::schema>();
}

// aten::linalg_inv_ex(Tensor self, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_inv_ex::call(const at::Tensor & self, bool check_errors) {
    static auto op = create_linalg_inv_ex_typed_handle();
    return op.call(self, check_errors);
}

// aten::linalg_inv_ex(Tensor self, *, bool check_errors=False) -> (Tensor inverse, Tensor info)
::std::tuple<at::Tensor,at::Tensor> linalg_inv_ex::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool check_errors) {
    static auto op = create_linalg_inv_ex_typed_handle();
    return op.redispatch(dispatchKeySet, self, check_errors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex_inverse, name, "aten::linalg_inv_ex")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex_inverse, overload_name, "inverse")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_ex_inverse, schema_str, "linalg_inv_ex.inverse(Tensor self, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)")

// aten::linalg_inv_ex.inverse(Tensor self, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_inv_ex_inverse::schema> create_linalg_inv_ex_inverse_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_inv_ex_inverse::name, linalg_inv_ex_inverse::overload_name)
      .typed<linalg_inv_ex_inverse::schema>();
}

// aten::linalg_inv_ex.inverse(Tensor self, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_inverse::call(const at::Tensor & self, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
    static auto op = create_linalg_inv_ex_inverse_typed_handle();
    return op.call(self, check_errors, inverse, info);
}

// aten::linalg_inv_ex.inverse(Tensor self, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -> (Tensor(a!) inverse, Tensor(b!) info)
::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_inverse::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
    static auto op = create_linalg_inv_ex_inverse_typed_handle();
    return op.redispatch(dispatchKeySet, self, check_errors, inverse, info);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv, name, "aten::linalg_inv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv, schema_str, "linalg_inv(Tensor self) -> Tensor")

// aten::linalg_inv(Tensor self) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_inv::schema> create_linalg_inv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_inv::name, linalg_inv::overload_name)
      .typed<linalg_inv::schema>();
}

// aten::linalg_inv(Tensor self) -> Tensor
at::Tensor linalg_inv::call(const at::Tensor & self) {
    static auto op = create_linalg_inv_typed_handle();
    return op.call(self);
}

// aten::linalg_inv(Tensor self) -> Tensor
at::Tensor linalg_inv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self) {
    static auto op = create_linalg_inv_typed_handle();
    return op.redispatch(dispatchKeySet, self);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_out, name, "aten::linalg_inv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_inv_out, schema_str, "linalg_inv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_inv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_inv_out::schema> create_linalg_inv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_inv_out::name, linalg_inv_out::overload_name)
      .typed<linalg_inv_out::schema>();
}

// aten::linalg_inv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_inv_out::call(const at::Tensor & self, at::Tensor & out) {
    static auto op = create_linalg_inv_out_typed_handle();
    return op.call(self, out);
}

// aten::linalg_inv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_inv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::Tensor & out) {
    static auto op = create_linalg_inv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner, name, "aten::inner")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner, schema_str, "inner(Tensor self, Tensor other) -> Tensor")

// aten::inner(Tensor self, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<inner::schema> create_inner_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inner::name, inner::overload_name)
      .typed<inner::schema>();
}

// aten::inner(Tensor self, Tensor other) -> Tensor
at::Tensor inner::call(const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_inner_typed_handle();
    return op.call(self, other);
}

// aten::inner(Tensor self, Tensor other) -> Tensor
at::Tensor inner::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other) {
    static auto op = create_inner_typed_handle();
    return op.redispatch(dispatchKeySet, self, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner_out, name, "aten::inner")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(inner_out, schema_str, "inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<inner_out::schema> create_inner_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(inner_out::name, inner_out::overload_name)
      .typed<inner_out::schema>();
}

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inner_out::call(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_inner_out_typed_handle();
    return op.call(self, other, out);
}

// aten::inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & inner_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_inner_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(outer, name, "aten::outer")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(outer, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(outer, schema_str, "outer(Tensor self, Tensor vec2) -> Tensor")

// aten::outer(Tensor self, Tensor vec2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<outer::schema> create_outer_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(outer::name, outer::overload_name)
      .typed<outer::schema>();
}

// aten::outer(Tensor self, Tensor vec2) -> Tensor
at::Tensor outer::call(const at::Tensor & self, const at::Tensor & vec2) {
    static auto op = create_outer_typed_handle();
    return op.call(self, vec2);
}

// aten::outer(Tensor self, Tensor vec2) -> Tensor
at::Tensor outer::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2) {
    static auto op = create_outer_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(outer_out, name, "aten::outer")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(outer_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(outer_out, schema_str, "outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<outer_out::schema> create_outer_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(outer_out::name, outer_out::overload_name)
      .typed<outer_out::schema>();
}

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & outer_out::call(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    static auto op = create_outer_out_typed_handle();
    return op.call(self, vec2, out);
}

// aten::outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & outer_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    static auto op = create_outer_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ger, name, "aten::ger")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ger, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ger, schema_str, "ger(Tensor self, Tensor vec2) -> Tensor")

// aten::ger(Tensor self, Tensor vec2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<ger::schema> create_ger_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ger::name, ger::overload_name)
      .typed<ger::schema>();
}

// aten::ger(Tensor self, Tensor vec2) -> Tensor
at::Tensor ger::call(const at::Tensor & self, const at::Tensor & vec2) {
    static auto op = create_ger_typed_handle();
    return op.call(self, vec2);
}

// aten::ger(Tensor self, Tensor vec2) -> Tensor
at::Tensor ger::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2) {
    static auto op = create_ger_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec2);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ger_out, name, "aten::ger")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ger_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(ger_out, schema_str, "ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<ger_out::schema> create_ger_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(ger_out::name, ger_out::overload_name)
      .typed<ger_out::schema>();
}

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ger_out::call(const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    static auto op = create_ger_out_typed_handle();
    return op.call(self, vec2, out);
}

// aten::ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & ger_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & vec2, at::Tensor & out) {
    static auto op = create_ger_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, vec2, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm, name, "aten::linalg_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm, schema_str, "linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_norm::schema> create_linalg_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_norm::name, linalg_norm::overload_name)
      .typed<linalg_norm::schema>();
}

// aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_norm::call(const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_norm_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype);
}

// aten::linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_ord_str, name, "aten::linalg_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_ord_str, overload_name, "ord_str")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_ord_str, schema_str, "linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_norm_ord_str::schema> create_linalg_norm_ord_str_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_norm_ord_str::name, linalg_norm_ord_str::overload_name)
      .typed<linalg_norm_ord_str::schema>();
}

// aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_norm_ord_str::call(const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_norm_ord_str_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype);
}

// aten::linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_norm_ord_str::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_norm_ord_str_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_out, name, "aten::linalg_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_out, schema_str, "linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_norm_out::schema> create_linalg_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_norm_out::name, linalg_norm_out::overload_name)
      .typed<linalg_norm_out::schema>();
}

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_norm_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_norm_out_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype, out);
}

// aten::linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_ord_str_out, name, "aten::linalg_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_ord_str_out, overload_name, "ord_str_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_norm_ord_str_out, schema_str, "linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_norm_ord_str_out::schema> create_linalg_norm_ord_str_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_norm_ord_str_out::name, linalg_norm_ord_str_out::overload_name)
      .typed<linalg_norm_ord_str_out::schema>();
}

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_norm_ord_str_out::call(const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_norm_ord_str_out_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype, out);
}

// aten::linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_norm_ord_str_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_norm_ord_str_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm, name, "aten::linalg_vector_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm, schema_str, "linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_vector_norm::schema> create_linalg_vector_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_vector_norm::name, linalg_vector_norm::overload_name)
      .typed<linalg_vector_norm::schema>();
}

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_vector_norm::call(const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_vector_norm_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype);
}

// aten::linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_vector_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_vector_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm_out, name, "aten::linalg_vector_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_vector_norm_out, schema_str, "linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_vector_norm_out::schema> create_linalg_vector_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_vector_norm_out::name, linalg_vector_norm_out::overload_name)
      .typed<linalg_vector_norm_out::schema>();
}

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_vector_norm_out::call(const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_vector_norm_out_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype, out);
}

// aten::linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_vector_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, c10::optional<at::IntArrayRef> dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_vector_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm, name, "aten::linalg_matrix_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm, schema_str, "linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_norm::schema> create_linalg_matrix_norm_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_norm::name, linalg_matrix_norm::overload_name)
      .typed<linalg_matrix_norm::schema>();
}

// aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_matrix_norm::call(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_matrix_norm_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype);
}

// aten::linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_matrix_norm::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_matrix_norm_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_out, name, "aten::linalg_matrix_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_out, schema_str, "linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_norm_out::schema> create_linalg_matrix_norm_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_norm_out::name, linalg_matrix_norm_out::overload_name)
      .typed<linalg_matrix_norm_out::schema>();
}

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_norm_out::call(const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_matrix_norm_out_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype, out);
}

// aten::linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_norm_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Scalar & ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_matrix_norm_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_str_ord, name, "aten::linalg_matrix_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_str_ord, overload_name, "str_ord")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_str_ord, schema_str, "linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor")

// aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_norm_str_ord::schema> create_linalg_matrix_norm_str_ord_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_norm_str_ord::name, linalg_matrix_norm_str_ord::overload_name)
      .typed<linalg_matrix_norm_str_ord::schema>();
}

// aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_matrix_norm_str_ord::call(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_matrix_norm_str_ord_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype);
}

// aten::linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
at::Tensor linalg_matrix_norm_str_ord::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
    static auto op = create_linalg_matrix_norm_str_ord_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_str_ord_out, name, "aten::linalg_matrix_norm")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_str_ord_out, overload_name, "str_ord_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_norm_str_ord_out, schema_str, "linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_norm_str_ord_out::schema> create_linalg_matrix_norm_str_ord_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_norm_str_ord_out::name, linalg_matrix_norm_str_ord_out::overload_name)
      .typed<linalg_matrix_norm_str_ord_out::schema>();
}

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_norm_str_ord_out::call(const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_matrix_norm_str_ord_out_typed_handle();
    return op.call(self, ord, dim, keepdim, dtype, out);
}

// aten::linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_norm_str_ord_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view ord, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype, at::Tensor & out) {
    static auto op = create_linalg_matrix_norm_str_ord_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ord, dim, keepdim, dtype, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svd_U, name, "aten::linalg_svd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svd_U, overload_name, "U")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svd_U, schema_str, "linalg_svd.U(Tensor self, bool full_matrices=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)")

// aten::linalg_svd.U(Tensor self, bool full_matrices=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_svd_U::schema> create_linalg_svd_U_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_svd_U::name, linalg_svd_U::overload_name)
      .typed<linalg_svd_U::schema>();
}

// aten::linalg_svd.U(Tensor self, bool full_matrices=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_U::call(const at::Tensor & self, bool full_matrices, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
    static auto op = create_linalg_svd_U_typed_handle();
    return op.call(self, full_matrices, U, S, Vh);
}

// aten::linalg_svd.U(Tensor self, bool full_matrices=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
::std::tuple<at::Tensor &,at::Tensor &,at::Tensor &> linalg_svd_U::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool full_matrices, at::Tensor & U, at::Tensor & S, at::Tensor & Vh) {
    static auto op = create_linalg_svd_U_typed_handle();
    return op.redispatch(dispatchKeySet, self, full_matrices, U, S, Vh);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svd, name, "aten::linalg_svd")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svd, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svd, schema_str, "linalg_svd(Tensor self, bool full_matrices=True) -> (Tensor U, Tensor S, Tensor Vh)")

// aten::linalg_svd(Tensor self, bool full_matrices=True) -> (Tensor U, Tensor S, Tensor Vh)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_svd::schema> create_linalg_svd_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_svd::name, linalg_svd::overload_name)
      .typed<linalg_svd::schema>();
}

// aten::linalg_svd(Tensor self, bool full_matrices=True) -> (Tensor U, Tensor S, Tensor Vh)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_svd::call(const at::Tensor & self, bool full_matrices) {
    static auto op = create_linalg_svd_typed_handle();
    return op.call(self, full_matrices);
}

// aten::linalg_svd(Tensor self, bool full_matrices=True) -> (Tensor U, Tensor S, Tensor Vh)
::std::tuple<at::Tensor,at::Tensor,at::Tensor> linalg_svd::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, bool full_matrices) {
    static auto op = create_linalg_svd_typed_handle();
    return op.redispatch(dispatchKeySet, self, full_matrices);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svdvals, name, "aten::linalg_svdvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svdvals, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svdvals, schema_str, "linalg_svdvals(Tensor input) -> Tensor")

// aten::linalg_svdvals(Tensor input) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_svdvals::schema> create_linalg_svdvals_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_svdvals::name, linalg_svdvals::overload_name)
      .typed<linalg_svdvals::schema>();
}

// aten::linalg_svdvals(Tensor input) -> Tensor
at::Tensor linalg_svdvals::call(const at::Tensor & input) {
    static auto op = create_linalg_svdvals_typed_handle();
    return op.call(input);
}

// aten::linalg_svdvals(Tensor input) -> Tensor
at::Tensor linalg_svdvals::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input) {
    static auto op = create_linalg_svdvals_typed_handle();
    return op.redispatch(dispatchKeySet, input);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svdvals_out, name, "aten::linalg_svdvals")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svdvals_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_svdvals_out, schema_str, "linalg_svdvals.out(Tensor input, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_svdvals.out(Tensor input, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_svdvals_out::schema> create_linalg_svdvals_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_svdvals_out::name, linalg_svdvals_out::overload_name)
      .typed<linalg_svdvals_out::schema>();
}

// aten::linalg_svdvals.out(Tensor input, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_svdvals_out::call(const at::Tensor & input, at::Tensor & out) {
    static auto op = create_linalg_svdvals_out_typed_handle();
    return op.call(input, out);
}

// aten::linalg_svdvals.out(Tensor input, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_svdvals_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, at::Tensor & out) {
    static auto op = create_linalg_svdvals_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond, schema_str, "linalg_cond(Tensor self, Scalar? p=None) -> Tensor")

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond::schema> create_linalg_cond_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond::name, linalg_cond::overload_name)
      .typed<linalg_cond::schema>();
}

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
at::Tensor linalg_cond::call(const at::Tensor & self, const c10::optional<at::Scalar> & p) {
    static auto op = create_linalg_cond_typed_handle();
    return op.call(self, p);
}

// aten::linalg_cond(Tensor self, Scalar? p=None) -> Tensor
at::Tensor linalg_cond::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p) {
    static auto op = create_linalg_cond_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_out, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_out, schema_str, "linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond_out::schema> create_linalg_cond_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond_out::name, linalg_cond_out::overload_name)
      .typed<linalg_cond_out::schema>();
}

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_out::call(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
    static auto op = create_linalg_cond_out_typed_handle();
    return op.call(self, p, out);
}

// aten::linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const c10::optional<at::Scalar> & p, at::Tensor & out) {
    static auto op = create_linalg_cond_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str, overload_name, "p_str")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str, schema_str, "linalg_cond.p_str(Tensor self, str p) -> Tensor")

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond_p_str::schema> create_linalg_cond_p_str_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond_p_str::name, linalg_cond_p_str::overload_name)
      .typed<linalg_cond_p_str::schema>();
}

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
at::Tensor linalg_cond_p_str::call(const at::Tensor & self, c10::string_view p) {
    static auto op = create_linalg_cond_p_str_typed_handle();
    return op.call(self, p);
}

// aten::linalg_cond.p_str(Tensor self, str p) -> Tensor
at::Tensor linalg_cond_p_str::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p) {
    static auto op = create_linalg_cond_p_str_typed_handle();
    return op.redispatch(dispatchKeySet, self, p);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str_out, name, "aten::linalg_cond")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str_out, overload_name, "p_str_out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_cond_p_str_out, schema_str, "linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_cond_p_str_out::schema> create_linalg_cond_p_str_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_cond_p_str_out::name, linalg_cond_p_str_out::overload_name)
      .typed<linalg_cond_p_str_out::schema>();
}

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_p_str_out::call(const at::Tensor & self, c10::string_view p, at::Tensor & out) {
    static auto op = create_linalg_cond_p_str_out_typed_handle();
    return op.call(self, p, out);
}

// aten::linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_cond_p_str_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view p, at::Tensor & out) {
    static auto op = create_linalg_cond_p_str_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, p, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv, schema_str, "linalg_pinv(Tensor self, float rcond=1e-15, bool hermitian=False) -> Tensor")

// aten::linalg_pinv(Tensor self, float rcond=1e-15, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv::schema> create_linalg_pinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv::name, linalg_pinv::overload_name)
      .typed<linalg_pinv::schema>();
}

// aten::linalg_pinv(Tensor self, float rcond=1e-15, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv::call(const at::Tensor & self, double rcond, bool hermitian) {
    static auto op = create_linalg_pinv_typed_handle();
    return op.call(self, rcond, hermitian);
}

// aten::linalg_pinv(Tensor self, float rcond=1e-15, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian) {
    static auto op = create_linalg_pinv_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_rcond_tensor, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_rcond_tensor, overload_name, "rcond_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_rcond_tensor, schema_str, "linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor")

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_rcond_tensor::schema> create_linalg_pinv_rcond_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_rcond_tensor::name, linalg_pinv_rcond_tensor::overload_name)
      .typed<linalg_pinv_rcond_tensor::schema>();
}

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_rcond_tensor::call(const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
    static auto op = create_linalg_pinv_rcond_tensor_typed_handle();
    return op.call(self, rcond, hermitian);
}

// aten::linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -> Tensor
at::Tensor linalg_pinv_rcond_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian) {
    static auto op = create_linalg_pinv_rcond_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out, schema_str, "linalg_pinv.out(Tensor self, float rcond=1e-15, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_pinv.out(Tensor self, float rcond=1e-15, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_out::schema> create_linalg_pinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_out::name, linalg_pinv_out::overload_name)
      .typed<linalg_pinv_out::schema>();
}

// aten::linalg_pinv.out(Tensor self, float rcond=1e-15, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out::call(const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_pinv_out_typed_handle();
    return op.call(self, rcond, hermitian, out);
}

// aten::linalg_pinv.out(Tensor self, float rcond=1e-15, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, double rcond, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_pinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out_rcond_tensor, name, "aten::linalg_pinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out_rcond_tensor, overload_name, "out_rcond_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_pinv_out_rcond_tensor, schema_str, "linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_pinv_out_rcond_tensor::schema> create_linalg_pinv_out_rcond_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_pinv_out_rcond_tensor::name, linalg_pinv_out_rcond_tensor::overload_name)
      .typed<linalg_pinv_out_rcond_tensor::schema>();
}

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out_rcond_tensor::call(const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_pinv_out_rcond_tensor_typed_handle();
    return op.call(self, rcond, hermitian, out);
}

// aten::linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_pinv_out_rcond_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & rcond, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_pinv_out_rcond_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, self, rcond, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve, name, "aten::linalg_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve, schema_str, "linalg_solve(Tensor input, Tensor other) -> Tensor")

// aten::linalg_solve(Tensor input, Tensor other) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve::schema> create_linalg_solve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve::name, linalg_solve::overload_name)
      .typed<linalg_solve::schema>();
}

// aten::linalg_solve(Tensor input, Tensor other) -> Tensor
at::Tensor linalg_solve::call(const at::Tensor & input, const at::Tensor & other) {
    static auto op = create_linalg_solve_typed_handle();
    return op.call(input, other);
}

// aten::linalg_solve(Tensor input, Tensor other) -> Tensor
at::Tensor linalg_solve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & other) {
    static auto op = create_linalg_solve_typed_handle();
    return op.redispatch(dispatchKeySet, input, other);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_out, name, "aten::linalg_solve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_solve_out, schema_str, "linalg_solve.out(Tensor input, Tensor other, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_solve.out(Tensor input, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_solve_out::schema> create_linalg_solve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_solve_out::name, linalg_solve_out::overload_name)
      .typed<linalg_solve_out::schema>();
}

// aten::linalg_solve.out(Tensor input, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_solve_out::call(const at::Tensor & input, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_linalg_solve_out_typed_handle();
    return op.call(input, other, out);
}

// aten::linalg_solve.out(Tensor input, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_solve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & other, at::Tensor & out) {
    static auto op = create_linalg_solve_out_typed_handle();
    return op.redispatch(dispatchKeySet, input, other, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv, name, "aten::linalg_tensorinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv, schema_str, "linalg_tensorinv(Tensor self, int ind=2) -> Tensor")

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorinv::schema> create_linalg_tensorinv_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorinv::name, linalg_tensorinv::overload_name)
      .typed<linalg_tensorinv::schema>();
}

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
at::Tensor linalg_tensorinv::call(const at::Tensor & self, int64_t ind) {
    static auto op = create_linalg_tensorinv_typed_handle();
    return op.call(self, ind);
}

// aten::linalg_tensorinv(Tensor self, int ind=2) -> Tensor
at::Tensor linalg_tensorinv::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind) {
    static auto op = create_linalg_tensorinv_typed_handle();
    return op.redispatch(dispatchKeySet, self, ind);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv_out, name, "aten::linalg_tensorinv")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorinv_out, schema_str, "linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorinv_out::schema> create_linalg_tensorinv_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorinv_out::name, linalg_tensorinv_out::overload_name)
      .typed<linalg_tensorinv_out::schema>();
}

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorinv_out::call(const at::Tensor & self, int64_t ind, at::Tensor & out) {
    static auto op = create_linalg_tensorinv_out_typed_handle();
    return op.call(self, ind, out);
}

// aten::linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorinv_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t ind, at::Tensor & out) {
    static auto op = create_linalg_tensorinv_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, ind, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve, name, "aten::linalg_tensorsolve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve, schema_str, "linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor")

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorsolve::schema> create_linalg_tensorsolve_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorsolve::name, linalg_tensorsolve::overload_name)
      .typed<linalg_tensorsolve::schema>();
}

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
at::Tensor linalg_tensorsolve::call(const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims) {
    static auto op = create_linalg_tensorsolve_typed_handle();
    return op.call(self, other, dims);
}

// aten::linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -> Tensor
at::Tensor linalg_tensorsolve::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims) {
    static auto op = create_linalg_tensorsolve_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve_out, name, "aten::linalg_tensorsolve")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_tensorsolve_out, schema_str, "linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_tensorsolve_out::schema> create_linalg_tensorsolve_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_tensorsolve_out::name, linalg_tensorsolve_out::overload_name)
      .typed<linalg_tensorsolve_out::schema>();
}

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorsolve_out::call(const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims, at::Tensor & out) {
    static auto op = create_linalg_tensorsolve_out_typed_handle();
    return op.call(self, other, dims, out);
}

// aten::linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_tensorsolve_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, c10::optional<at::IntArrayRef> dims, at::Tensor & out) {
    static auto op = create_linalg_tensorsolve_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, dims, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_qr, name, "aten::linalg_qr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_qr, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_qr, schema_str, "linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)")

// aten::linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_qr::schema> create_linalg_qr_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_qr::name, linalg_qr::overload_name)
      .typed<linalg_qr::schema>();
}

// aten::linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)
::std::tuple<at::Tensor,at::Tensor> linalg_qr::call(const at::Tensor & self, c10::string_view mode) {
    static auto op = create_linalg_qr_typed_handle();
    return op.call(self, mode);
}

// aten::linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)
::std::tuple<at::Tensor,at::Tensor> linalg_qr::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view mode) {
    static auto op = create_linalg_qr_typed_handle();
    return op.redispatch(dispatchKeySet, self, mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_qr_out, name, "aten::linalg_qr")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_qr_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_qr_out, schema_str, "linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)")

// aten::linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_qr_out::schema> create_linalg_qr_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_qr_out::name, linalg_qr_out::overload_name)
      .typed<linalg_qr_out::schema>();
}

// aten::linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
::std::tuple<at::Tensor &,at::Tensor &> linalg_qr_out::call(const at::Tensor & self, c10::string_view mode, at::Tensor & Q, at::Tensor & R) {
    static auto op = create_linalg_qr_out_typed_handle();
    return op.call(self, mode, Q, R);
}

// aten::linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
::std::tuple<at::Tensor &,at::Tensor &> linalg_qr_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view mode, at::Tensor & Q, at::Tensor & R) {
    static auto op = create_linalg_qr_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, mode, Q, R);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_qr_helper, name, "aten::_linalg_qr_helper")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_qr_helper, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_linalg_qr_helper, schema_str, "_linalg_qr_helper(Tensor self, str mode) -> (Tensor, Tensor)")

// aten::_linalg_qr_helper(Tensor self, str mode) -> (Tensor, Tensor)
static C10_NOINLINE c10::TypedOperatorHandle<_linalg_qr_helper::schema> create__linalg_qr_helper_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_linalg_qr_helper::name, _linalg_qr_helper::overload_name)
      .typed<_linalg_qr_helper::schema>();
}

// aten::_linalg_qr_helper(Tensor self, str mode) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _linalg_qr_helper::call(const at::Tensor & self, c10::string_view mode) {
    static auto op = create__linalg_qr_helper_typed_handle();
    return op.call(self, mode);
}

// aten::_linalg_qr_helper(Tensor self, str mode) -> (Tensor, Tensor)
::std::tuple<at::Tensor,at::Tensor> _linalg_qr_helper::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::string_view mode) {
    static auto op = create__linalg_qr_helper_typed_handle();
    return op.redispatch(dispatchKeySet, self, mode);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_power, name, "aten::linalg_matrix_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_power, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_power, schema_str, "linalg_matrix_power(Tensor self, int n) -> Tensor")

// aten::linalg_matrix_power(Tensor self, int n) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_power::schema> create_linalg_matrix_power_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_power::name, linalg_matrix_power::overload_name)
      .typed<linalg_matrix_power::schema>();
}

// aten::linalg_matrix_power(Tensor self, int n) -> Tensor
at::Tensor linalg_matrix_power::call(const at::Tensor & self, int64_t n) {
    static auto op = create_linalg_matrix_power_typed_handle();
    return op.call(self, n);
}

// aten::linalg_matrix_power(Tensor self, int n) -> Tensor
at::Tensor linalg_matrix_power::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n) {
    static auto op = create_linalg_matrix_power_typed_handle();
    return op.redispatch(dispatchKeySet, self, n);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_power_out, name, "aten::linalg_matrix_power")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_power_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_power_out, schema_str, "linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_power_out::schema> create_linalg_matrix_power_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_power_out::name, linalg_matrix_power_out::overload_name)
      .typed<linalg_matrix_power_out::schema>();
}

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_power_out::call(const at::Tensor & self, int64_t n, at::Tensor & out) {
    static auto op = create_linalg_matrix_power_out_typed_handle();
    return op.call(self, n, out);
}

// aten::linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_power_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t n, at::Tensor & out) {
    static auto op = create_linalg_matrix_power_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, n, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank, schema_str, "linalg_matrix_rank(Tensor self, float? tol=None, bool hermitian=False) -> Tensor")

// aten::linalg_matrix_rank(Tensor self, float? tol=None, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank::schema> create_linalg_matrix_rank_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank::name, linalg_matrix_rank::overload_name)
      .typed<linalg_matrix_rank::schema>();
}

// aten::linalg_matrix_rank(Tensor self, float? tol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank::call(const at::Tensor & self, c10::optional<double> tol, bool hermitian) {
    static auto op = create_linalg_matrix_rank_typed_handle();
    return op.call(self, tol, hermitian);
}

// aten::linalg_matrix_rank(Tensor self, float? tol=None, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> tol, bool hermitian) {
    static auto op = create_linalg_matrix_rank_typed_handle();
    return op.redispatch(dispatchKeySet, self, tol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out, schema_str, "linalg_matrix_rank.out(Tensor self, float? tol=None, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_rank.out(Tensor self, float? tol=None, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_out::schema> create_linalg_matrix_rank_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_out::name, linalg_matrix_rank_out::overload_name)
      .typed<linalg_matrix_rank_out::schema>();
}

// aten::linalg_matrix_rank.out(Tensor self, float? tol=None, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out::call(const at::Tensor & self, c10::optional<double> tol, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_matrix_rank_out_typed_handle();
    return op.call(self, tol, hermitian, out);
}

// aten::linalg_matrix_rank.out(Tensor self, float? tol=None, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, c10::optional<double> tol, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_matrix_rank_out_typed_handle();
    return op.redispatch(dispatchKeySet, self, tol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_tol_tensor, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_tol_tensor, overload_name, "tol_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_tol_tensor, schema_str, "linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor")

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_tol_tensor::schema> create_linalg_matrix_rank_tol_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_tol_tensor::name, linalg_matrix_rank_tol_tensor::overload_name)
      .typed<linalg_matrix_rank_tol_tensor::schema>();
}

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_tol_tensor::call(const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
    static auto op = create_linalg_matrix_rank_tol_tensor_typed_handle();
    return op.call(input, tol, hermitian);
}

// aten::linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -> Tensor
at::Tensor linalg_matrix_rank_tol_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian) {
    static auto op = create_linalg_matrix_rank_tol_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, input, tol, hermitian);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out_tol_tensor, name, "aten::linalg_matrix_rank")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out_tol_tensor, overload_name, "out_tol_tensor")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_matrix_rank_out_tol_tensor, schema_str, "linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_matrix_rank_out_tol_tensor::schema> create_linalg_matrix_rank_out_tol_tensor_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_matrix_rank_out_tol_tensor::name, linalg_matrix_rank_out_tol_tensor::overload_name)
      .typed<linalg_matrix_rank_out_tol_tensor::schema>();
}

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out_tol_tensor::call(const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_matrix_rank_out_tol_tensor_typed_handle();
    return op.call(input, tol, hermitian, out);
}

// aten::linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_matrix_rank_out_tol_tensor::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & tol, bool hermitian, at::Tensor & out) {
    static auto op = create_linalg_matrix_rank_out_tol_tensor_typed_handle();
    return op.redispatch(dispatchKeySet, input, tol, hermitian, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot, name, "aten::linalg_multi_dot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot, schema_str, "linalg_multi_dot(Tensor[] tensors) -> Tensor")

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<linalg_multi_dot::schema> create_linalg_multi_dot_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_multi_dot::name, linalg_multi_dot::overload_name)
      .typed<linalg_multi_dot::schema>();
}

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
at::Tensor linalg_multi_dot::call(at::TensorList tensors) {
    static auto op = create_linalg_multi_dot_typed_handle();
    return op.call(tensors);
}

// aten::linalg_multi_dot(Tensor[] tensors) -> Tensor
at::Tensor linalg_multi_dot::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_linalg_multi_dot_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot_out, name, "aten::linalg_multi_dot")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot_out, overload_name, "out")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(linalg_multi_dot_out, schema_str, "linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)")

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
static C10_NOINLINE c10::TypedOperatorHandle<linalg_multi_dot_out::schema> create_linalg_multi_dot_out_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(linalg_multi_dot_out::name, linalg_multi_dot_out::overload_name)
      .typed<linalg_multi_dot_out::schema>();
}

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_multi_dot_out::call(at::TensorList tensors, at::Tensor & out) {
    static auto op = create_linalg_multi_dot_out_typed_handle();
    return op.call(tensors, out);
}

// aten::linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -> Tensor(a!)
at::Tensor & linalg_multi_dot_out::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors, at::Tensor & out) {
    static auto op = create_linalg_multi_dot_out_typed_handle();
    return op.redispatch(dispatchKeySet, tensors, out);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_serialization_subcmul, name, "aten::_test_serialization_subcmul")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_serialization_subcmul, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_serialization_subcmul, schema_str, "_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor")

// aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_serialization_subcmul::schema> create__test_serialization_subcmul_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_serialization_subcmul::name, _test_serialization_subcmul::overload_name)
      .typed<_test_serialization_subcmul::schema>();
}

// aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor
at::Tensor _test_serialization_subcmul::call(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create__test_serialization_subcmul_typed_handle();
    return op.call(self, other, alpha);
}

// aten::_test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -> Tensor
at::Tensor _test_serialization_subcmul::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    static auto op = create__test_serialization_subcmul_typed_handle();
    return op.redispatch(dispatchKeySet, self, other, alpha);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_intlist, name, "aten::_test_optional_intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_intlist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_intlist, schema_str, "_test_optional_intlist(Tensor values, int[]? addends) -> Tensor")

// aten::_test_optional_intlist(Tensor values, int[]? addends) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_optional_intlist::schema> create__test_optional_intlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_optional_intlist::name, _test_optional_intlist::overload_name)
      .typed<_test_optional_intlist::schema>();
}

// aten::_test_optional_intlist(Tensor values, int[]? addends) -> Tensor
at::Tensor _test_optional_intlist::call(const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
    static auto op = create__test_optional_intlist_typed_handle();
    return op.call(values, addends);
}

// aten::_test_optional_intlist(Tensor values, int[]? addends) -> Tensor
at::Tensor _test_optional_intlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
    static auto op = create__test_optional_intlist_typed_handle();
    return op.redispatch(dispatchKeySet, values, addends);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist, name, "aten::_test_optional_filled_intlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_filled_intlist, schema_str, "_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor")

// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_optional_filled_intlist::schema> create__test_optional_filled_intlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_optional_filled_intlist::name, _test_optional_filled_intlist::overload_name)
      .typed<_test_optional_filled_intlist::schema>();
}

// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor
at::Tensor _test_optional_filled_intlist::call(const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
    static auto op = create__test_optional_filled_intlist_typed_handle();
    return op.call(values, addends);
}

// aten::_test_optional_filled_intlist(Tensor values, int[2]? addends) -> Tensor
at::Tensor _test_optional_filled_intlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::IntArrayRef> addends) {
    static auto op = create__test_optional_filled_intlist_typed_handle();
    return op.redispatch(dispatchKeySet, values, addends);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_floatlist, name, "aten::_test_optional_floatlist")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_floatlist, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_optional_floatlist, schema_str, "_test_optional_floatlist(Tensor values, float[]? addends) -> Tensor")

// aten::_test_optional_floatlist(Tensor values, float[]? addends) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_optional_floatlist::schema> create__test_optional_floatlist_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_optional_floatlist::name, _test_optional_floatlist::overload_name)
      .typed<_test_optional_floatlist::schema>();
}

// aten::_test_optional_floatlist(Tensor values, float[]? addends) -> Tensor
at::Tensor _test_optional_floatlist::call(const at::Tensor & values, c10::optional<at::ArrayRef<double>> addends) {
    static auto op = create__test_optional_floatlist_typed_handle();
    return op.call(values, addends);
}

// aten::_test_optional_floatlist(Tensor values, float[]? addends) -> Tensor
at::Tensor _test_optional_floatlist::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & values, c10::optional<at::ArrayRef<double>> addends) {
    static auto op = create__test_optional_floatlist_typed_handle();
    return op.redispatch(dispatchKeySet, values, addends);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_string_default, name, "aten::_test_string_default")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_string_default, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_string_default, schema_str, "_test_string_default(Tensor dummy, str a=\"\\\"'\\\\\", str b='\"\\'\\\\') -> Tensor")

// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_string_default::schema> create__test_string_default_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_string_default::name, _test_string_default::overload_name)
      .typed<_test_string_default::schema>();
}

// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor
at::Tensor _test_string_default::call(const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
    static auto op = create__test_string_default_typed_handle();
    return op.call(dummy, a, b);
}

// aten::_test_string_default(Tensor dummy, str a="\"'\\", str b='"\'\\') -> Tensor
at::Tensor _test_string_default::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, c10::string_view a, c10::string_view b) {
    static auto op = create__test_string_default_typed_handle();
    return op.redispatch(dispatchKeySet, dummy, a, b);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_ambiguous_defaults_a, name, "aten::_test_ambiguous_defaults")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_ambiguous_defaults_a, overload_name, "a")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_ambiguous_defaults_a, schema_str, "_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -> Tensor")

// aten::_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_ambiguous_defaults_a::schema> create__test_ambiguous_defaults_a_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_ambiguous_defaults_a::name, _test_ambiguous_defaults_a::overload_name)
      .typed<_test_ambiguous_defaults_a::schema>();
}

// aten::_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -> Tensor
at::Tensor _test_ambiguous_defaults_a::call(const at::Tensor & dummy, int64_t a, int64_t b) {
    static auto op = create__test_ambiguous_defaults_a_typed_handle();
    return op.call(dummy, a, b);
}

// aten::_test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -> Tensor
at::Tensor _test_ambiguous_defaults_a::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, int64_t a, int64_t b) {
    static auto op = create__test_ambiguous_defaults_a_typed_handle();
    return op.redispatch(dispatchKeySet, dummy, a, b);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_ambiguous_defaults_b, name, "aten::_test_ambiguous_defaults")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_ambiguous_defaults_b, overload_name, "b")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_test_ambiguous_defaults_b, schema_str, "_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b=\"2\") -> Tensor")

// aten::_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b="2") -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_test_ambiguous_defaults_b::schema> create__test_ambiguous_defaults_b_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_test_ambiguous_defaults_b::name, _test_ambiguous_defaults_b::overload_name)
      .typed<_test_ambiguous_defaults_b::schema>();
}

// aten::_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b="2") -> Tensor
at::Tensor _test_ambiguous_defaults_b::call(const at::Tensor & dummy, int64_t a, c10::string_view b) {
    static auto op = create__test_ambiguous_defaults_b_typed_handle();
    return op.call(dummy, a, b);
}

// aten::_test_ambiguous_defaults.b(Tensor dummy, int a=2, str b="2") -> Tensor
at::Tensor _test_ambiguous_defaults_b::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & dummy, int64_t a, c10::string_view b) {
    static auto op = create__test_ambiguous_defaults_b_typed_handle();
    return op.redispatch(dispatchKeySet, dummy, a, b);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(segment_reduce, name, "aten::segment_reduce")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(segment_reduce, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(segment_reduce, schema_str, "segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor")

// aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<segment_reduce::schema> create_segment_reduce_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(segment_reduce::name, segment_reduce::overload_name)
      .typed<segment_reduce::schema>();
}

// aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor
at::Tensor segment_reduce::call(const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & indices, int64_t axis, bool unsafe, const c10::optional<at::Scalar> & initial) {
    static auto op = create_segment_reduce_typed_handle();
    return op.call(data, reduce, lengths, indices, axis, unsafe, initial);
}

// aten::segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, int axis=0, bool unsafe=False, Scalar? initial=None) -> Tensor
at::Tensor segment_reduce::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, const c10::optional<at::Tensor> & indices, int64_t axis, bool unsafe, const c10::optional<at::Scalar> & initial) {
    static auto op = create_segment_reduce_typed_handle();
    return op.redispatch(dispatchKeySet, data, reduce, lengths, indices, axis, unsafe, initial);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_segment_reduce_backward, name, "aten::_segment_reduce_backward")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_segment_reduce_backward, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(_segment_reduce_backward, schema_str, "_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, int axis=0) -> Tensor")

// aten::_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, int axis=0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<_segment_reduce_backward::schema> create__segment_reduce_backward_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(_segment_reduce_backward::name, _segment_reduce_backward::overload_name)
      .typed<_segment_reduce_backward::schema>();
}

// aten::_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, int axis=0) -> Tensor
at::Tensor _segment_reduce_backward::call(const at::Tensor & grad, const at::Tensor & output, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, int64_t axis) {
    static auto op = create__segment_reduce_backward_typed_handle();
    return op.call(grad, output, data, reduce, lengths, axis);
}

// aten::_segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, int axis=0) -> Tensor
at::Tensor _segment_reduce_backward::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & grad, const at::Tensor & output, const at::Tensor & data, c10::string_view reduce, const c10::optional<at::Tensor> & lengths, int64_t axis) {
    static auto op = create__segment_reduce_backward_typed_handle();
    return op.redispatch(dispatchKeySet, grad, output, data, reduce, lengths, axis);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad_sequence, name, "aten::pad_sequence")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad_sequence, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(pad_sequence, schema_str, "pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor")

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<pad_sequence::schema> create_pad_sequence_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(pad_sequence::name, pad_sequence::overload_name)
      .typed<pad_sequence::schema>();
}

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
at::Tensor pad_sequence::call(at::TensorList sequences, bool batch_first, double padding_value) {
    static auto op = create_pad_sequence_typed_handle();
    return op.call(sequences, batch_first, padding_value);
}

// aten::pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0) -> Tensor
at::Tensor pad_sequence::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList sequences, bool batch_first, double padding_value) {
    static auto op = create_pad_sequence_typed_handle();
    return op.redispatch(dispatchKeySet, sequences, batch_first, padding_value);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_dense_tensors, name, "aten::flatten_dense_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_dense_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(flatten_dense_tensors, schema_str, "flatten_dense_tensors(Tensor[] tensors) -> Tensor")

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
static C10_NOINLINE c10::TypedOperatorHandle<flatten_dense_tensors::schema> create_flatten_dense_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(flatten_dense_tensors::name, flatten_dense_tensors::overload_name)
      .typed<flatten_dense_tensors::schema>();
}

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
at::Tensor flatten_dense_tensors::call(at::TensorList tensors) {
    static auto op = create_flatten_dense_tensors_typed_handle();
    return op.call(tensors);
}

// aten::flatten_dense_tensors(Tensor[] tensors) -> Tensor
at::Tensor flatten_dense_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, at::TensorList tensors) {
    static auto op = create_flatten_dense_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, tensors);
}

STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_dense_tensors, name, "aten::unflatten_dense_tensors")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_dense_tensors, overload_name, "")
STATIC_CONST_STR_OUT_OF_LINE_FOR_WIN_CUDA(unflatten_dense_tensors, schema_str, "unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]")

// aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]
static C10_NOINLINE c10::TypedOperatorHandle<unflatten_dense_tensors::schema> create_unflatten_dense_tensors_typed_handle() {
  return c10::Dispatcher::singleton()
      .findSchemaOrThrow(unflatten_dense_tensors::name, unflatten_dense_tensors::overload_name)
      .typed<unflatten_dense_tensors::schema>();
}

// aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> unflatten_dense_tensors::call(const at::Tensor & flat, at::TensorList tensors) {
    static auto op = create_unflatten_dense_tensors_typed_handle();
    return op.call(flat, tensors);
}

// aten::unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -> Tensor[]
::std::vector<at::Tensor> unflatten_dense_tensors::redispatch(c10::DispatchKeySet dispatchKeySet, const at::Tensor & flat, at::TensorList tensors) {
    static auto op = create_unflatten_dense_tensors_typed_handle();
    return op.redispatch(dispatchKeySet, flat, tensors);
}

}} // namespace at::_ops
