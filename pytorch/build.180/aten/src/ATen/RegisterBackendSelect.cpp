// We register ops with a higher priority dispatch key (BackendSelect) than the usual backend-specific keys (e.g. CPU)
// which makes calls to the factory functions dispatch to here.
// We then 'manually' compute a lower-priority to re-dispatch to (e.g. CPU) to get to the eventually correct backend.
// @generated by tools/codegen/gen.py from RegisterBackendSelect.cpp

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <ATen/Operators.h>
#include <ATen/core/dispatch/Dispatcher.h>
#include <torch/library.h>
#include <c10/core/TensorOptions.h>

namespace at {

namespace {

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_cudnn_init_dropout_state", "")
    .typed<at::Tensor (double, bool, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, dropout, train, dropout_seed, dtype, layout, device, pin_memory);
}
// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor arange(const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::arange", "")
    .typed<at::Tensor (const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, end, dtype, layout, device, pin_memory);
}
// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor arange_start(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::arange", "start")
    .typed<at::Tensor (const at::Scalar &, const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, start, end, dtype, layout, device, pin_memory);
}
// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor arange_start_step(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::arange", "start_step")
    .typed<at::Tensor (const at::Scalar &, const at::Scalar &, const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, start, end, step, dtype, layout, device, pin_memory);
}
// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor bartlett_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::bartlett_window", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, dtype, layout, device, pin_memory);
}
// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor bartlett_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::bartlett_window", "periodic")
    .typed<at::Tensor (int64_t, bool, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor blackman_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::blackman_window", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, dtype, layout, device, pin_memory);
}
// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor blackman_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::blackman_window", "periodic")
    .typed<at::Tensor (int64_t, bool, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty", "names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, names, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_memory_format(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty", "memory_format")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory, memory_format);
}
// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _empty_affine_quantized(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_empty_affine_quantized", "")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, double, int64_t, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _empty_per_channel_affine_quantized(at::IntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_empty_per_channel_affine_quantized", "")
    .typed<at::Tensor (at::IntArrayRef, const at::Tensor &, const at::Tensor &, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(scales, zero_points);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, size, scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty_quantized", "")
    .typed<at::Tensor (at::IntArrayRef, const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(qtensor);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, size, qtensor, dtype, layout, device, pin_memory, memory_format);
}
// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor empty_strided(at::IntArrayRef size, at::IntArrayRef stride, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty_strided", "")
    .typed<at::Tensor (at::IntArrayRef, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, stride, dtype, layout, device, pin_memory);
}
// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor eye(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::eye", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, n, dtype, layout, device, pin_memory);
}
// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor eye_m(int64_t n, int64_t m, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::eye", "m")
    .typed<at::Tensor (int64_t, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, n, m, dtype, layout, device, pin_memory);
}
// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor full_names(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::full", "names")
    .typed<at::Tensor (at::IntArrayRef, const at::Scalar &, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, fill_value, names, dtype, layout, device, pin_memory);
}
// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor full(at::IntArrayRef size, const at::Scalar & fill_value, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::full", "")
    .typed<at::Tensor (at::IntArrayRef, const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, fill_value, dtype, layout, device, pin_memory);
}
// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor from_file(c10::string_view filename, c10::optional<bool> shared, c10::optional<int64_t> size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::from_file", "")
    .typed<at::Tensor (c10::string_view, c10::optional<bool>, c10::optional<int64_t>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, filename, shared, size, dtype, layout, device, pin_memory);
}
// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hann_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hann_window", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, dtype, layout, device, pin_memory);
}
// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hann_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hann_window", "periodic")
    .typed<at::Tensor (int64_t, bool, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, dtype, layout, device, pin_memory);
}
// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "periodic")
    .typed<at::Tensor (int64_t, bool, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window_periodic_alpha(int64_t window_length, bool periodic, double alpha, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "periodic_alpha")
    .typed<at::Tensor (int64_t, bool, double, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, alpha, dtype, layout, device, pin_memory);
}
// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor hamming_window_periodic_alpha_beta(int64_t window_length, bool periodic, double alpha, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "periodic_alpha_beta")
    .typed<at::Tensor (int64_t, bool, double, double, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, alpha, beta, dtype, layout, device, pin_memory);
}
// aten::kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor kaiser_window(int64_t window_length, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::kaiser_window", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, dtype, layout, device, pin_memory);
}
// aten::kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor kaiser_window_periodic(int64_t window_length, bool periodic, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::kaiser_window", "periodic")
    .typed<at::Tensor (int64_t, bool, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, dtype, layout, device, pin_memory);
}
// aten::kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor kaiser_window_beta(int64_t window_length, bool periodic, double beta, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::kaiser_window", "beta")
    .typed<at::Tensor (int64_t, bool, double, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, window_length, periodic, beta, dtype, layout, device, pin_memory);
}
// aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor linspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::linspace", "")
    .typed<at::Tensor (const at::Scalar &, const at::Scalar &, c10::optional<int64_t>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, start, end, steps, dtype, layout, device, pin_memory);
}
// aten::logspace(Scalar start, Scalar end, int? steps=None, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor logspace(const at::Scalar & start, const at::Scalar & end, c10::optional<int64_t> steps, double base, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::logspace", "")
    .typed<at::Tensor (const at::Scalar &, const at::Scalar &, c10::optional<int64_t>, double, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, start, end, steps, base, dtype, layout, device, pin_memory);
}
// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor ones_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::ones", "names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, names, dtype, layout, device, pin_memory);
}
// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor ones(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::ones", "")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory);
}
// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor scalar_tensor(const at::Scalar & s, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::scalar_tensor", "")
    .typed<at::Tensor (const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, s, dtype, layout, device, pin_memory);
}
// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, names, dtype, layout, device, pin_memory);
}
// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand_generator_with_names(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "generator_with_names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, generator, names, dtype, layout, device, pin_memory);
}
// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory);
}
// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor rand_generator(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "generator")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, generator, dtype, layout, device, pin_memory);
}
// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint(int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "")
    .typed<at::Tensor (int64_t, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, high, size, dtype, layout, device, pin_memory);
}
// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint_generator(int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "generator")
    .typed<at::Tensor (int64_t, at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, high, size, generator, dtype, layout, device, pin_memory);
}
// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint_low(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "low")
    .typed<at::Tensor (int64_t, int64_t, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, low, high, size, dtype, layout, device, pin_memory);
}
// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randint_low_generator(int64_t low, int64_t high, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "low_generator")
    .typed<at::Tensor (int64_t, int64_t, at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, low, high, size, generator, dtype, layout, device, pin_memory);
}
// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory);
}
// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn_generator(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "generator")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, generator, dtype, layout, device, pin_memory);
}
// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, names, dtype, layout, device, pin_memory);
}
// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randn_generator_with_names(at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "generator_with_names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, generator, names, dtype, layout, device, pin_memory);
}
// aten::randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randperm(int64_t n, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randperm", "")
    .typed<at::Tensor (int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, n, dtype, layout, device, pin_memory);
}
// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor randperm_generator(int64_t n, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randperm", "generator")
    .typed<at::Tensor (int64_t, c10::optional<at::Generator>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, n, generator, dtype, layout, device, pin_memory);
}
// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor range_step(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::range", "step")
    .typed<at::Tensor (const at::Scalar &, const at::Scalar &, const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, start, end, step, dtype, layout, device, pin_memory);
}
// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor range(const at::Scalar & start, const at::Scalar & end, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::range", "")
    .typed<at::Tensor (const at::Scalar &, const at::Scalar &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, start, end, dtype, layout, device, pin_memory);
}
// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor zeros_names(at::IntArrayRef size, c10::optional<at::DimnameList> names, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::zeros", "names")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::DimnameList>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, names, dtype, layout, device, pin_memory);
}
// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor zeros(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::zeros", "")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory);
}
// aten::sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_csr_tensor_crow_col_value_size(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_csr_tensor", "crow_col_value_size")
    .typed<at::Tensor (const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_csr_tensor_crow_col_value(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_csr_tensor", "crow_col_value")
    .typed<at::Tensor (const at::Tensor &, const at::Tensor &, const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, crow_indices, col_indices, values, dtype, layout, device, pin_memory);
}
// aten::_sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_csr_tensor_unsafe(const at::Tensor & crow_indices, const at::Tensor & col_indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_csr_tensor_unsafe", "")
    .typed<at::Tensor (const at::Tensor &, const at::Tensor &, const at::Tensor &, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(crow_indices, col_indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, crow_indices, col_indices, values, size, dtype, layout, device, pin_memory);
}
// aten::sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_coo_tensor_size(at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_coo_tensor", "size")
    .typed<at::Tensor (at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, size, dtype, layout, device, pin_memory);
}
// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_coo_tensor_indices(const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_coo_tensor", "indices")
    .typed<at::Tensor (const at::Tensor &, const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, indices, values, dtype, layout, device, pin_memory);
}
// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor sparse_coo_tensor_indices_size(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_coo_tensor", "indices_size")
    .typed<at::Tensor (const at::Tensor &, const at::Tensor &, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, indices, values, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_coo_tensor_unsafe(const at::Tensor & indices, const at::Tensor & values, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_coo_tensor_unsafe", "")
    .typed<at::Tensor (const at::Tensor &, const at::Tensor &, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, indices, values, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_coo_tensor_with_dims", "")
    .typed<at::Tensor (int64_t, int64_t, at::IntArrayRef, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}
// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_coo_tensor_with_dims_and_tensors", "")
    .typed<at::Tensor (int64_t, int64_t, at::IntArrayRef, const at::Tensor &, const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory);
}
// aten::_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor _to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_to_copy", "")
    .typed<at::Tensor (const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, bool, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(self);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}
// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor(a)
C10_ALWAYS_INLINE
at::Tensor to_dtype_layout(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, bool copy, c10::optional<at::MemoryFormat> memory_format) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::to", "dtype_layout")
    .typed<at::Tensor (const at::Tensor &, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>, bool, bool, c10::optional<at::MemoryFormat>)>();
  DispatchKeySet _dk_set = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device)) | c10::detail::multi_dispatch_key_set(self);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKeySet _dk = c10::impl::computeDispatchKeySet(_dk_set, _dk_mask);
  return op.redispatch(_dk, self, dtype, layout, device, pin_memory, non_blocking, copy, memory_format);
}
// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor tril_indices(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::tril_indices", "")
    .typed<at::Tensor (int64_t, int64_t, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, row, col, offset, dtype, layout, device, pin_memory);
}
// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor triu_indices(int64_t row, int64_t col, int64_t offset, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::triu_indices", "")
    .typed<at::Tensor (int64_t, int64_t, int64_t, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, row, col, offset, dtype, layout, device, pin_memory);
}
// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor normal_float_float(double mean, double std, at::IntArrayRef size, c10::optional<at::Generator> generator, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::normal", "float_float")
    .typed<at::Tensor (double, double, at::IntArrayRef, c10::optional<at::Generator>, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, mean, std, size, generator, dtype, layout, device, pin_memory);
}
// aten::fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor fft_fftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::fft_fftfreq", "")
    .typed<at::Tensor (int64_t, double, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, n, d, dtype, layout, device, pin_memory);
}
// aten::fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
C10_ALWAYS_INLINE
at::Tensor fft_rfftfreq(int64_t n, double d, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::fft_rfftfreq", "")
    .typed<at::Tensor (int64_t, double, c10::optional<at::ScalarType>, c10::optional<at::Layout>, c10::optional<at::Device>, c10::optional<bool>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(dtype, layout, device));
  return op.redispatch(_dk, n, d, dtype, layout, device, pin_memory);
}

bool is_pinned(const Tensor& self, c10::optional<at::Device> device) {
  // Only CPU tensors can be pinned
  if (!self.is_cpu()) {
    return false;
  }
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::is_pinned", "")
    .typed<bool (const Tensor&, c10::optional<at::Device>)>();
  // TODO: fetch scalar type from Tensor? But it doesn't really matter...
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(c10::nullopt, self.layout(), device.value_or(at::kCUDA)));
  return op.redispatch(_dk, self, device);
}

at::Tensor _pin_memory(const Tensor& self, c10::optional<at::Device> device) {
  TORCH_CHECK(self.device().is_cpu(), "cannot pin '", self.toString(), "' only dense CPU tensors can be pinned");
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_pin_memory", "")
    .typed<Tensor (const Tensor&, c10::optional<at::Device>)>();
  DispatchKeySet _dk = c10::DispatchKeySet(c10::computeDispatchKey(c10::nullopt, self.layout(), device.value_or(at::kCUDA)));
  return op.redispatch(_dk, self, device);
}

TORCH_LIBRARY_IMPL(aten, BackendSelect, m) {
  m.impl("aten::_cudnn_init_dropout_state", TORCH_FN(_cudnn_init_dropout_state));
  m.impl("aten::arange", TORCH_FN(arange));
  m.impl("aten::arange.start", TORCH_FN(arange_start));
  m.impl("aten::arange.start_step", TORCH_FN(arange_start_step));
  m.impl("aten::bartlett_window", TORCH_FN(bartlett_window));
  m.impl("aten::bartlett_window.periodic", TORCH_FN(bartlett_window_periodic));
  m.impl("aten::blackman_window", TORCH_FN(blackman_window));
  m.impl("aten::blackman_window.periodic", TORCH_FN(blackman_window_periodic));
  m.impl("aten::empty.names", TORCH_FN(empty_names));
  m.impl("aten::empty.memory_format", TORCH_FN(empty_memory_format));
  m.impl("aten::_empty_affine_quantized", TORCH_FN(_empty_affine_quantized));
  m.impl("aten::_empty_per_channel_affine_quantized", TORCH_FN(_empty_per_channel_affine_quantized));
  m.impl("aten::empty_quantized", TORCH_FN(empty_quantized));
  m.impl("aten::empty_strided", TORCH_FN(empty_strided));
  m.impl("aten::eye", TORCH_FN(eye));
  m.impl("aten::eye.m", TORCH_FN(eye_m));
  m.impl("aten::full.names", TORCH_FN(full_names));
  m.impl("aten::full", TORCH_FN(full));
  m.impl("aten::from_file", TORCH_FN(from_file));
  m.impl("aten::hann_window", TORCH_FN(hann_window));
  m.impl("aten::hann_window.periodic", TORCH_FN(hann_window_periodic));
  m.impl("aten::hamming_window", TORCH_FN(hamming_window));
  m.impl("aten::hamming_window.periodic", TORCH_FN(hamming_window_periodic));
  m.impl("aten::hamming_window.periodic_alpha", TORCH_FN(hamming_window_periodic_alpha));
  m.impl("aten::hamming_window.periodic_alpha_beta", TORCH_FN(hamming_window_periodic_alpha_beta));
  m.impl("aten::kaiser_window", TORCH_FN(kaiser_window));
  m.impl("aten::kaiser_window.periodic", TORCH_FN(kaiser_window_periodic));
  m.impl("aten::kaiser_window.beta", TORCH_FN(kaiser_window_beta));
  m.impl("aten::linspace", TORCH_FN(linspace));
  m.impl("aten::logspace", TORCH_FN(logspace));
  m.impl("aten::ones.names", TORCH_FN(ones_names));
  m.impl("aten::ones", TORCH_FN(ones));
  m.impl("aten::scalar_tensor", TORCH_FN(scalar_tensor));
  m.impl("aten::rand.names", TORCH_FN(rand_names));
  m.impl("aten::rand.generator_with_names", TORCH_FN(rand_generator_with_names));
  m.impl("aten::rand", TORCH_FN(rand));
  m.impl("aten::rand.generator", TORCH_FN(rand_generator));
  m.impl("aten::randint", TORCH_FN(randint));
  m.impl("aten::randint.generator", TORCH_FN(randint_generator));
  m.impl("aten::randint.low", TORCH_FN(randint_low));
  m.impl("aten::randint.low_generator", TORCH_FN(randint_low_generator));
  m.impl("aten::randn", TORCH_FN(randn));
  m.impl("aten::randn.generator", TORCH_FN(randn_generator));
  m.impl("aten::randn.names", TORCH_FN(randn_names));
  m.impl("aten::randn.generator_with_names", TORCH_FN(randn_generator_with_names));
  m.impl("aten::randperm", TORCH_FN(randperm));
  m.impl("aten::randperm.generator", TORCH_FN(randperm_generator));
  m.impl("aten::range.step", TORCH_FN(range_step));
  m.impl("aten::range", TORCH_FN(range));
  m.impl("aten::zeros.names", TORCH_FN(zeros_names));
  m.impl("aten::zeros", TORCH_FN(zeros));
  m.impl("aten::sparse_csr_tensor.crow_col_value_size", TORCH_FN(sparse_csr_tensor_crow_col_value_size));
  m.impl("aten::sparse_csr_tensor.crow_col_value", TORCH_FN(sparse_csr_tensor_crow_col_value));
  m.impl("aten::_sparse_csr_tensor_unsafe", TORCH_FN(_sparse_csr_tensor_unsafe));
  m.impl("aten::sparse_coo_tensor.size", TORCH_FN(sparse_coo_tensor_size));
  m.impl("aten::sparse_coo_tensor.indices", TORCH_FN(sparse_coo_tensor_indices));
  m.impl("aten::sparse_coo_tensor.indices_size", TORCH_FN(sparse_coo_tensor_indices_size));
  m.impl("aten::_sparse_coo_tensor_unsafe", TORCH_FN(_sparse_coo_tensor_unsafe));
  m.impl("aten::_sparse_coo_tensor_with_dims", TORCH_FN(_sparse_coo_tensor_with_dims));
  m.impl("aten::_sparse_coo_tensor_with_dims_and_tensors", TORCH_FN(_sparse_coo_tensor_with_dims_and_tensors));
  m.impl("aten::_to_copy", TORCH_FN(_to_copy));
  m.impl("aten::to.dtype_layout", TORCH_FN(to_dtype_layout));
  m.impl("aten::tril_indices", TORCH_FN(tril_indices));
  m.impl("aten::triu_indices", TORCH_FN(triu_indices));
  m.impl("aten::normal.float_float", TORCH_FN(normal_float_float));
  m.impl("aten::fft_fftfreq", TORCH_FN(fft_fftfreq));
  m.impl("aten::fft_rfftfreq", TORCH_FN(fft_rfftfreq));;
  m.impl(TORCH_SELECTIVE_NAME("aten::is_pinned"), TORCH_FN(is_pinned));
  m.impl(TORCH_SELECTIVE_NAME("aten::_pin_memory"), TORCH_FN(_pin_memory));
}

} // namespace
} // at
