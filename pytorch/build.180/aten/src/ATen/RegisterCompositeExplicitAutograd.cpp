// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// @generated by tools/codegen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/Functions.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>


#include <ATen/CompositeExplicitAutogradFunctions.h>

namespace at {

// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {


Tensor create_out(IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {

  
  if (strides.empty()) {
      return at::empty(sizes, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), options.memory_format_opt());
  } else {
      // TODO: assert options.memory_format_opt() is nullopt (debug only?)
      return at::empty_strided(sizes, strides, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
  }

}

void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      at::native::as_strided_(out, sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

namespace {

at::Tensor wrapper___fw_primal(const at::Tensor & self, int64_t level) {
    // No device check


  // DeviceGuard omitted
  return at::native::_fw_primal(self, level);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__abs(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::abs(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__abs_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::abs_(self);
}

} // anonymous namespace
struct structured_sgn_default_backend_functional final : public at::meta::structured_sgn {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sgn::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sgn(const at::Tensor & self) {
structured_sgn_default_backend_functional op;
op.meta(self);
at::sgn_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sgn_default_backend_inplace final : public at::meta::structured_sgn {
    structured_sgn_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sgn::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sgn_(at::Tensor & self) {
structured_sgn_default_backend_inplace op(self);
op.meta(self);
at::sgn_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper___conj(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_conj(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___conj_physical(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_conj_physical(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__conj_physical_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::conj_physical_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___neg_view(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_neg_view(self);
}

} // anonymous namespace
struct structured_acos_default_backend_functional final : public at::meta::structured_acos {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_acos::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_acos(const at::Tensor & self) {
structured_acos_default_backend_functional op;
op.meta(self);
at::acos_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_acos_default_backend_inplace final : public at::meta::structured_acos {
    structured_acos_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_acos::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_acos_(at::Tensor & self) {
structured_acos_default_backend_inplace op(self);
op.meta(self);
at::acos_outf(self, op.outputs_[0]);
return self;
}
struct structured_add_Tensor_default_backend_functional final : public at::meta::structured_add_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_add_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_add_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
structured_add_Tensor_default_backend_functional op;
op.meta(self, other, alpha);
at::add_outf(self, other, alpha, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_add_Tensor_default_backend_inplace final : public at::meta::structured_add_Tensor {
    structured_add_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_add_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_add__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
structured_add_Tensor_default_backend_inplace op(self);
op.meta(self, other, alpha);
at::add_outf(self, other, alpha, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_add_Scalar(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::add(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_add__Scalar(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::add_(self, other, alpha);
}

} // anonymous namespace
struct structured_addmv_default_backend_functional final : public at::meta::structured_addmv {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_addmv(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
structured_addmv_default_backend_functional op;
op.meta(self, mat, vec, beta, alpha);
at::addmv_outf(self, mat, vec, beta, alpha, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_addmv_default_backend_inplace final : public at::meta::structured_addmv {
    structured_addmv_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_addmv_(at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
structured_addmv_default_backend_inplace op(self);
op.meta(self, mat, vec, beta, alpha);
at::addmv_outf(self, mat, vec, beta, alpha, op.outputs_[0]);
return self;
}
namespace {

at::Tensor & wrapper__addr_(at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::addr_(self, vec1, vec2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__affine_grid_generator(const at::Tensor & theta, at::IntArrayRef size, bool align_corners) {
    // No device check


  // DeviceGuard omitted
  return at::native::affine_grid_generator(theta, size, align_corners);
}

} // anonymous namespace
struct structured_all_dim_default_backend_functional final : public at::meta::structured_all_dim {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_all_dim(const at::Tensor & self, int64_t dim, bool keepdim) {
structured_all_dim_default_backend_functional op;
auto precompute = op.meta(self, dim, keepdim);
(void)precompute;
at::all_outf(self, precompute.dim, keepdim, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_any_dim_default_backend_functional final : public at::meta::structured_any_dim {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_any_dim(const at::Tensor & self, int64_t dim, bool keepdim) {
structured_any_dim_default_backend_functional op;
auto precompute = op.meta(self, dim, keepdim);
(void)precompute;
at::any_outf(self, precompute.dim, keepdim, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_argmax_default_backend_functional final : public at::meta::structured_argmax {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_argmax(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
structured_argmax_default_backend_functional op;
op.meta(self, dim, keepdim);
at::argmax_outf(self, dim, keepdim, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_argmin_default_backend_functional final : public at::meta::structured_argmin {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_argmin(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
structured_argmin_default_backend_functional op;
op.meta(self, dim, keepdim);
at::argmin_outf(self, dim, keepdim, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_acosh_default_backend_functional final : public at::meta::structured_acosh {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_acosh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_acosh(const at::Tensor & self) {
structured_acosh_default_backend_functional op;
op.meta(self);
at::acosh_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_acosh_default_backend_inplace final : public at::meta::structured_acosh {
    structured_acosh_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_acosh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_acosh_(at::Tensor & self) {
structured_acosh_default_backend_inplace op(self);
op.meta(self);
at::acosh_outf(self, op.outputs_[0]);
return self;
}
struct structured_asinh_default_backend_functional final : public at::meta::structured_asinh {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_asinh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_asinh(const at::Tensor & self) {
structured_asinh_default_backend_functional op;
op.meta(self);
at::asinh_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_asinh_default_backend_inplace final : public at::meta::structured_asinh {
    structured_asinh_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_asinh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_asinh_(at::Tensor & self) {
structured_asinh_default_backend_inplace op(self);
op.meta(self);
at::asinh_outf(self, op.outputs_[0]);
return self;
}
struct structured_atanh_default_backend_functional final : public at::meta::structured_atanh {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_atanh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_atanh(const at::Tensor & self) {
structured_atanh_default_backend_functional op;
op.meta(self);
at::atanh_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_atanh_default_backend_inplace final : public at::meta::structured_atanh {
    structured_atanh_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_atanh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_atanh_(at::Tensor & self) {
structured_atanh_default_backend_inplace op(self);
op.meta(self);
at::atanh_outf(self, op.outputs_[0]);
return self;
}
namespace {

const at::Tensor & wrapper__as_strided_(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
    // No device check


  // DeviceGuard omitted
  return at::native::as_strided_(self, size, stride, storage_offset);
}

} // anonymous namespace
struct structured_asin_default_backend_functional final : public at::meta::structured_asin {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_asin::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_asin(const at::Tensor & self) {
structured_asin_default_backend_functional op;
op.meta(self);
at::asin_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_asin_default_backend_inplace final : public at::meta::structured_asin {
    structured_asin_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_asin::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_asin_(at::Tensor & self) {
structured_asin_default_backend_inplace op(self);
op.meta(self);
at::asin_outf(self, op.outputs_[0]);
return self;
}
struct structured_atan_default_backend_functional final : public at::meta::structured_atan {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_atan::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_atan(const at::Tensor & self) {
structured_atan_default_backend_functional op;
op.meta(self);
at::atan_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_atan_default_backend_inplace final : public at::meta::structured_atan {
    structured_atan_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_atan::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_atan_(at::Tensor & self) {
structured_atan_default_backend_inplace op(self);
op.meta(self);
at::atan_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__bernoulli(const at::Tensor & self, c10::optional<at::Generator> generator) {
    // No device check


  // DeviceGuard omitted
  return at::native::bernoulli(self, generator);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__binary_cross_entropy_with_logits(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::binary_cross_entropy_with_logits(self, target, weight, pos_weight, reduction);
}

} // anonymous namespace
struct structured_bitwise_not_default_backend_functional final : public at::meta::structured_bitwise_not {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_not::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_bitwise_not(const at::Tensor & self) {
structured_bitwise_not_default_backend_functional op;
op.meta(self);
at::bitwise_not_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_bitwise_not_default_backend_inplace final : public at::meta::structured_bitwise_not {
    structured_bitwise_not_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_not::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_bitwise_not_(at::Tensor & self) {
structured_bitwise_not_default_backend_inplace op(self);
op.meta(self);
at::bitwise_not_outf(self, op.outputs_[0]);
return self;
}
struct structured_copysign_Tensor_default_backend_functional final : public at::meta::structured_copysign_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_copysign_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_copysign_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_copysign_Tensor_default_backend_functional op;
op.meta(self, other);
at::copysign_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_copysign_Tensor_default_backend_inplace final : public at::meta::structured_copysign_Tensor {
    structured_copysign_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_copysign_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_copysign__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_copysign_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::copysign_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_copysign_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::copysign(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_copysign_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::copysign_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_copysign__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::copysign_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cat(at::TensorList tensors, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cat(tensors, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_cat_out_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::cat_out(tensors, dim, out);
}

} // anonymous namespace
struct structured_ceil_default_backend_functional final : public at::meta::structured_ceil {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ceil::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_ceil(const at::Tensor & self) {
structured_ceil_default_backend_functional op;
op.meta(self);
at::ceil_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_ceil_default_backend_inplace final : public at::meta::structured_ceil {
    structured_ceil_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ceil::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_ceil_(at::Tensor & self) {
structured_ceil_default_backend_inplace op(self);
op.meta(self);
at::ceil_outf(self, op.outputs_[0]);
return self;
}
struct structured_clamp_default_backend_functional final : public at::meta::structured_clamp {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_clamp::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
structured_clamp_default_backend_functional op;
op.meta(self, (min.has_value() ? at::OptionalScalarRef(&(min.value())) : at::OptionalScalarRef()), (max.has_value() ? at::OptionalScalarRef(&(max.value())) : at::OptionalScalarRef()));
at::clamp_outf(self, min, max, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_clamp_default_backend_inplace final : public at::meta::structured_clamp {
    structured_clamp_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_clamp::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_clamp_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
structured_clamp_default_backend_inplace op(self);
op.meta(self, (min.has_value() ? at::OptionalScalarRef(&(min.value())) : at::OptionalScalarRef()), (max.has_value() ? at::OptionalScalarRef(&(max.value())) : at::OptionalScalarRef()));
at::clamp_outf(self, min, max, op.outputs_[0]);
return self;
}
namespace {

at::Tensor & wrapper_Tensor_clamp__Tensor(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_(self, min, max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__clamp_max(const at::Tensor & self, const at::Scalar & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_max(self, max);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__clamp_max_(at::Tensor & self, const at::Scalar & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_max_(self, max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_clamp_max_Tensor(const at::Tensor & self, const at::Tensor & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_max(self, max);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_clamp_max__Tensor(at::Tensor & self, const at::Tensor & max) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_max_(self, max);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__clamp_min(const at::Tensor & self, const at::Scalar & min) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_min(self, min);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__clamp_min_(at::Tensor & self, const at::Scalar & min) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_min_(self, min);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_clamp_min_Tensor(const at::Tensor & self, const at::Tensor & min) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_min(self, min);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_clamp_min__Tensor(at::Tensor & self, const at::Tensor & min) {
    // No device check


  // DeviceGuard omitted
  return at::native::clamp_min_(self, min);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__complex(const at::Tensor & real, const at::Tensor & imag) {
    // No device check


  // DeviceGuard omitted
  return at::native::complex(real, imag);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__polar(const at::Tensor & abs, const at::Tensor & angle) {
    // No device check


  // DeviceGuard omitted
  return at::native::polar(abs, angle);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__constant_pad_nd(const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
    // No device check


  // DeviceGuard omitted
  return at::native::constant_pad_nd(self, pad, value);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__convolution_overrideable(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::convolution_overrideable(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__convolution_backward_overrideable(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::convolution_backward_overrideable(grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__conv_tbc(const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
    // No device check


  // DeviceGuard omitted
  return at::native::conv_tbc(self, weight, bias, pad);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check


  // DeviceGuard omitted
  return at::native::copy_(self, src, non_blocking);
}

} // anonymous namespace
struct structured_cos_default_backend_functional final : public at::meta::structured_cos {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_cos::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_cos(const at::Tensor & self) {
structured_cos_default_backend_functional op;
op.meta(self);
at::cos_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_cos_default_backend_inplace final : public at::meta::structured_cos {
    structured_cos_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_cos::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_cos_(at::Tensor & self) {
structured_cos_default_backend_inplace op(self);
op.meta(self);
at::cos_outf(self, op.outputs_[0]);
return self;
}
struct structured_cosh_default_backend_functional final : public at::meta::structured_cosh {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_cosh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_cosh(const at::Tensor & self) {
structured_cosh_default_backend_functional op;
op.meta(self);
at::cosh_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_cosh_default_backend_inplace final : public at::meta::structured_cosh {
    structured_cosh_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_cosh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_cosh_(at::Tensor & self) {
structured_cosh_default_backend_inplace op(self);
op.meta(self);
at::cosh_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__count_nonzero(const at::Tensor & self, c10::optional<int64_t> dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::count_nonzero(self, dim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__cummax(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummax(self, dim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_out_cummax_out_out(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummax_out(self, dim, values, indices);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__cummin(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummin(self, dim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_out_cummin_out_out(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::cummin_out(self, dim, values, indices);
}

} // anonymous namespace
struct structured_cumprod_default_backend_functional final : public at::meta::structured_cumprod {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_cumprod(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
structured_cumprod_default_backend_functional op;
op.meta(self, dim, dtype);
at::cumprod_outf(self, dim, dtype, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_cumprod_default_backend_inplace final : public at::meta::structured_cumprod {
    structured_cumprod_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_cumprod_(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
structured_cumprod_default_backend_inplace op(self);
op.meta(self, dim, dtype);
at::cumprod_outf(self, dim, dtype, op.outputs_[0]);
return self;
}
struct structured_cumsum_default_backend_functional final : public at::meta::structured_cumsum {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_cumsum(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
structured_cumsum_default_backend_functional op;
op.meta(self, dim, dtype);
at::cumsum_outf(self, dim, dtype, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_cumsum_default_backend_inplace final : public at::meta::structured_cumsum {
    structured_cumsum_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_cumsum_(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
structured_cumsum_default_backend_inplace op(self);
op.meta(self, dim, dtype);
at::cumsum_outf(self, dim, dtype, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__diagonal(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check


  // DeviceGuard omitted
  return at::native::diagonal(self, offset, dim1, dim2);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__diagonal_backward(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
    // No device check


  // DeviceGuard omitted
  return at::native::diagonal_backward(grad_output, input_sizes, offset, dim1, dim2);
}

} // anonymous namespace
struct structured_div_Tensor_default_backend_functional final : public at::meta::structured_div_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_div_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_div_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_div_Tensor_default_backend_functional op;
op.meta(self, other);
at::div_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_div_Tensor_default_backend_inplace final : public at::meta::structured_div_Tensor {
    structured_div_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_div_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_div__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_div_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::div_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_div_Tensor_mode_default_backend_functional final : public at::meta::structured_div_Tensor_mode {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_div_Tensor_mode::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_div_Tensor_mode(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
structured_div_Tensor_mode_default_backend_functional op;
op.meta(self, other, rounding_mode);
at::div_outf(self, other, rounding_mode, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_div_Tensor_mode_default_backend_inplace final : public at::meta::structured_div_Tensor_mode {
    structured_div_Tensor_mode_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_div_Tensor_mode::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_div__Tensor_mode(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
structured_div_Tensor_mode_default_backend_inplace op(self);
op.meta(self, other, rounding_mode);
at::div_outf(self, other, rounding_mode, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_div_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::div(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_div__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::div_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_mode_div_Scalar_mode(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::div(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_mode_div__Scalar_mode(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::div_(self, other, rounding_mode);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_dot_out_out(const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::dot_out(self, tensor, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_vdot_out_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::vdot_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__embedding(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
    // No device check


  // DeviceGuard omitted
  return at::native::embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}

} // anonymous namespace
struct structured_erf_default_backend_functional final : public at::meta::structured_erf {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_erf::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_erf(const at::Tensor & self) {
structured_erf_default_backend_functional op;
op.meta(self);
at::erf_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_erf_default_backend_inplace final : public at::meta::structured_erf {
    structured_erf_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_erf::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_erf_(at::Tensor & self) {
structured_erf_default_backend_inplace op(self);
op.meta(self);
at::erf_outf(self, op.outputs_[0]);
return self;
}
struct structured_erfc_default_backend_functional final : public at::meta::structured_erfc {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_erfc::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_erfc(const at::Tensor & self) {
structured_erfc_default_backend_functional op;
op.meta(self);
at::erfc_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_erfc_default_backend_inplace final : public at::meta::structured_erfc {
    structured_erfc_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_erfc::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_erfc_(at::Tensor & self) {
structured_erfc_default_backend_inplace op(self);
op.meta(self);
at::erfc_outf(self, op.outputs_[0]);
return self;
}
struct structured_exp_default_backend_functional final : public at::meta::structured_exp {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_exp::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_exp(const at::Tensor & self) {
structured_exp_default_backend_functional op;
op.meta(self);
at::exp_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_exp_default_backend_inplace final : public at::meta::structured_exp {
    structured_exp_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_exp::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_exp_(at::Tensor & self) {
structured_exp_default_backend_inplace op(self);
op.meta(self);
at::exp_outf(self, op.outputs_[0]);
return self;
}
struct structured_exp2_default_backend_functional final : public at::meta::structured_exp2 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_exp2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_exp2(const at::Tensor & self) {
structured_exp2_default_backend_functional op;
op.meta(self);
at::exp2_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_exp2_default_backend_inplace final : public at::meta::structured_exp2 {
    structured_exp2_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_exp2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_exp2_(at::Tensor & self) {
structured_exp2_default_backend_inplace op(self);
op.meta(self);
at::exp2_outf(self, op.outputs_[0]);
return self;
}
struct structured_expm1_default_backend_functional final : public at::meta::structured_expm1 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_expm1::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_expm1(const at::Tensor & self) {
structured_expm1_default_backend_functional op;
op.meta(self);
at::expm1_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_expm1_default_backend_inplace final : public at::meta::structured_expm1 {
    structured_expm1_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_expm1::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_expm1_(at::Tensor & self) {
structured_expm1_default_backend_inplace op(self);
op.meta(self);
at::expm1_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__expand(const at::Tensor & self, at::IntArrayRef size, bool implicit) {
    // No device check


  // DeviceGuard omitted
  return at::native::expand(self, size, implicit);
}

} // anonymous namespace
struct structured_floor_default_backend_functional final : public at::meta::structured_floor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_floor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_floor(const at::Tensor & self) {
structured_floor_default_backend_functional op;
op.meta(self);
at::floor_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_floor_default_backend_inplace final : public at::meta::structured_floor {
    structured_floor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_floor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_floor_(at::Tensor & self) {
structured_floor_default_backend_inplace op(self);
op.meta(self);
at::floor_outf(self, op.outputs_[0]);
return self;
}
struct structured_frac_default_backend_functional final : public at::meta::structured_frac {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_frac::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_frac(const at::Tensor & self) {
structured_frac_default_backend_functional op;
op.meta(self);
at::frac_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_frac_default_backend_inplace final : public at::meta::structured_frac {
    structured_frac_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_frac::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_frac_(at::Tensor & self) {
structured_frac_default_backend_inplace op(self);
op.meta(self);
at::frac_outf(self, op.outputs_[0]);
return self;
}
struct structured_gcd_default_backend_functional final : public at::meta::structured_gcd {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gcd::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_gcd(const at::Tensor & self, const at::Tensor & other) {
structured_gcd_default_backend_functional op;
op.meta(self, other);
at::gcd_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_gcd_default_backend_inplace final : public at::meta::structured_gcd {
    structured_gcd_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gcd::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_gcd_(at::Tensor & self, const at::Tensor & other) {
structured_gcd_default_backend_inplace op(self);
op.meta(self, other);
at::gcd_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_lcm_default_backend_functional final : public at::meta::structured_lcm {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lcm::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_lcm(const at::Tensor & self, const at::Tensor & other) {
structured_lcm_default_backend_functional op;
op.meta(self, other);
at::lcm_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_lcm_default_backend_inplace final : public at::meta::structured_lcm {
    structured_lcm_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lcm::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_lcm_(at::Tensor & self, const at::Tensor & other) {
structured_lcm_default_backend_inplace op(self);
op.meta(self, other);
at::lcm_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper___grid_sampler_2d_cpu_fallback(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
    // No device check


  // DeviceGuard omitted
  return at::native::_grid_sampler_2d_cpu_fallback(input, grid, interpolation_mode, padding_mode, align_corners);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__index_copy_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_copy_(self, dim, index, source);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__index_put_(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
    // No device check


  // DeviceGuard omitted
  return at::native::index_put_(self, indices, values, accumulate);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__inverse(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::inverse(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_inverse_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::inverse_out(self, out);
}

} // anonymous namespace
struct structured_isin_Tensor_Tensor_default_backend_functional final : public at::meta::structured_isin_Tensor_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_isin_Tensor_Tensor(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
structured_isin_Tensor_Tensor_default_backend_functional op;
op.meta(elements, test_elements, assume_unique, invert);
at::isin_outf(elements, test_elements, assume_unique, invert, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_isin_Tensor_Scalar_default_backend_functional final : public at::meta::structured_isin_Tensor_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_isin_Tensor_Scalar(const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert) {
structured_isin_Tensor_Scalar_default_backend_functional op;
op.meta(elements, test_element, assume_unique, invert);
at::isin_outf(elements, test_element, assume_unique, invert, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_isin_Scalar_Tensor_default_backend_functional final : public at::meta::structured_isin_Scalar_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_isin_Scalar_Tensor(const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert) {
structured_isin_Scalar_Tensor_default_backend_functional op;
op.meta(element, test_elements, assume_unique, invert);
at::isin_outf(element, test_elements, assume_unique, invert, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__kl_div(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
    // No device check


  // DeviceGuard omitted
  return at::native::kl_div(self, target, reduction, log_target);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__kthvalue(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::kthvalue(self, k, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__nan_to_num(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    // No device check


  // DeviceGuard omitted
  return at::native::nan_to_num(self, nan, posinf, neginf);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__nan_to_num_(at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
    // No device check


  // DeviceGuard omitted
  return at::native::nan_to_num_(self, nan, posinf, neginf);
}

} // anonymous namespace
struct structured_log_default_backend_functional final : public at::meta::structured_log {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_log(const at::Tensor & self) {
structured_log_default_backend_functional op;
op.meta(self);
at::log_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_log_default_backend_inplace final : public at::meta::structured_log {
    structured_log_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_log_(at::Tensor & self) {
structured_log_default_backend_inplace op(self);
op.meta(self);
at::log_outf(self, op.outputs_[0]);
return self;
}
struct structured_log10_default_backend_functional final : public at::meta::structured_log10 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log10::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_log10(const at::Tensor & self) {
structured_log10_default_backend_functional op;
op.meta(self);
at::log10_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_log10_default_backend_inplace final : public at::meta::structured_log10 {
    structured_log10_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log10::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_log10_(at::Tensor & self) {
structured_log10_default_backend_inplace op(self);
op.meta(self);
at::log10_outf(self, op.outputs_[0]);
return self;
}
struct structured_log1p_default_backend_functional final : public at::meta::structured_log1p {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log1p::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_log1p(const at::Tensor & self) {
structured_log1p_default_backend_functional op;
op.meta(self);
at::log1p_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_log1p_default_backend_inplace final : public at::meta::structured_log1p {
    structured_log1p_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log1p::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_log1p_(at::Tensor & self) {
structured_log1p_default_backend_inplace op(self);
op.meta(self);
at::log1p_outf(self, op.outputs_[0]);
return self;
}
struct structured_log2_default_backend_functional final : public at::meta::structured_log2 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_log2(const at::Tensor & self) {
structured_log2_default_backend_functional op;
op.meta(self);
at::log2_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_log2_default_backend_inplace final : public at::meta::structured_log2 {
    structured_log2_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_log2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_log2_(at::Tensor & self) {
structured_log2_default_backend_inplace op(self);
op.meta(self);
at::log2_outf(self, op.outputs_[0]);
return self;
}
struct structured_logaddexp_default_backend_functional final : public at::meta::structured_logaddexp {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_logaddexp::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_logaddexp(const at::Tensor & self, const at::Tensor & other) {
structured_logaddexp_default_backend_functional op;
op.meta(self, other);
at::logaddexp_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_logaddexp2_default_backend_functional final : public at::meta::structured_logaddexp2 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_logaddexp2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_logaddexp2(const at::Tensor & self, const at::Tensor & other) {
structured_logaddexp2_default_backend_functional op;
op.meta(self, other);
at::logaddexp2_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_xlogy_Tensor_default_backend_functional final : public at::meta::structured_xlogy_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_xlogy_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_xlogy_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_xlogy_Tensor_default_backend_functional op;
op.meta(self, other);
at::xlogy_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_xlogy_Tensor_default_backend_inplace final : public at::meta::structured_xlogy_Tensor {
    structured_xlogy_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_xlogy_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_xlogy__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_xlogy_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::xlogy_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_Self_xlogy_Scalar_Self(const at::Scalar & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::xlogy(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_OutScalar_Self_xlogy_out_OutScalar_Self(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::xlogy_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_Other_xlogy_Scalar_Other(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::xlogy(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_OutScalar_Other_xlogy_out_OutScalar_Other(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::xlogy_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_Other_xlogy__Scalar_Other(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::xlogy_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logdet(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::logdet(self);
}

} // anonymous namespace
struct structured__log_softmax_default_backend_functional final : public at::meta::structured__log_softmax {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper__log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
structured__log_softmax_default_backend_functional op;
op.meta(self, dim, half_to_float);
at::_log_softmax_outf(self, dim, half_to_float, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured__log_softmax_backward_data_default_backend_functional final : public at::meta::structured__log_softmax_backward_data {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper__log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
structured__log_softmax_backward_data_default_backend_functional op;
op.meta(grad_output, output, dim, self);
at::_log_softmax_backward_data_outf(grad_output, output, dim, self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__logcumsumexp(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::logcumsumexp(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_logcumsumexp_out_out(const at::Tensor & self, int64_t dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::logcumsumexp_out(self, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::logsumexp(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_logsumexp_out_out(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::logsumexp_out(self, dim, keepdim, out);
}

} // anonymous namespace
struct structured_aminmax_default_backend_functional final : public at::meta::structured_aminmax {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_aminmax(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
structured_aminmax_default_backend_functional op;
op.meta(self, dim, keepdim);
at::aminmax_outf(self, dim, keepdim, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
namespace {

at::Tensor wrapper__amax(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::amax(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mean(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::mean(self, dtype);
}

} // anonymous namespace
struct structured_mean_dim_default_backend_functional final : public at::meta::structured_mean_dim {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_mean_dim(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
structured_mean_dim_default_backend_functional op;
op.meta(self, dim, keepdim, dtype);
at::mean_outf(self, dim, keepdim, dtype, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dim_median_dim(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::median(self, dim, keepdim);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_dim_nanmedian_dim(const at::Tensor & self, int64_t dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::nanmedian(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__amin(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
    // No device check


  // DeviceGuard omitted
  return at::native::amin(self, dim, keepdim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mkldnn_convolution(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
    // No device check


  // DeviceGuard omitted
  return at::native::mkldnn_convolution(self, weight, bias, padding, stride, dilation, groups);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor> wrapper__mkldnn_convolution_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, ::std::array<bool,3> output_mask) {
    // No device check


  // DeviceGuard omitted
  return at::native::mkldnn_convolution_backward(self, grad_output, weight, padding, stride, dilation, groups, output_mask);
}

} // anonymous namespace
struct structured_mm_default_backend_functional final : public at::meta::structured_mm {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_mm(const at::Tensor & self, const at::Tensor & mat2) {
structured_mm_default_backend_functional op;
op.meta(self, mat2);
at::mm_outf(self, mat2, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_values_mode_out_values(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
    // No device check


  // DeviceGuard omitted
  return at::native::mode_out(self, dim, keepdim, values, indices);
}

} // anonymous namespace
struct structured_mul_Tensor_default_backend_functional final : public at::meta::structured_mul_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_mul_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_mul_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_mul_Tensor_default_backend_functional op;
op.meta(self, other);
at::mul_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_mul_Tensor_default_backend_inplace final : public at::meta::structured_mul_Tensor {
    structured_mul_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_mul_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_mul__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_mul_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::mul_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_mul_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::mul(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_mul__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::mul_(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_mv_out_out(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::mv_out(self, vec, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mvlgamma(const at::Tensor & self, int64_t p) {
    // No device check


  // DeviceGuard omitted
  return at::native::mvlgamma(self, p);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__mvlgamma_(at::Tensor & self, int64_t p) {
    // No device check


  // DeviceGuard omitted
  return at::native::mvlgamma_(self, p);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    // No device check


  // DeviceGuard omitted
  return at::native::narrow_copy_dense(self, dim, start, length);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___nnpack_spatial_convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride) {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnpack_spatial_convolution(input, weight, bias, padding, stride);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___euclidean_dist(const at::Tensor & x1, const at::Tensor & x2) {
    // No device check


  // DeviceGuard omitted
  return at::native::_euclidean_dist(x1, x2);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__permute(const at::Tensor & self, at::IntArrayRef dims) {
    // No device check


  // DeviceGuard omitted
  return at::native::permute(self, dims);
}

} // anonymous namespace
namespace {

bool wrapper__is_pinned(const at::Tensor & self, c10::optional<at::Device> device) {
    // No device check


  // DeviceGuard omitted
  return at::native::is_pinned_default(self, device);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rad2deg(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::rad2deg(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_rad2deg_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::rad2deg_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__rad2deg_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::rad2deg_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__deg2rad(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::deg2rad(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_deg2rad_out_out(const at::Tensor & self, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::deg2rad_out(self, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__deg2rad_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::deg2rad_(self);
}

} // anonymous namespace
struct structured_reciprocal_default_backend_functional final : public at::meta::structured_reciprocal {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_reciprocal::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_reciprocal(const at::Tensor & self) {
structured_reciprocal_default_backend_functional op;
op.meta(self);
at::reciprocal_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_reciprocal_default_backend_inplace final : public at::meta::structured_reciprocal {
    structured_reciprocal_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_reciprocal::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_reciprocal_(at::Tensor & self) {
structured_reciprocal_default_backend_inplace op(self);
op.meta(self);
at::reciprocal_outf(self, op.outputs_[0]);
return self;
}
struct structured_neg_default_backend_functional final : public at::meta::structured_neg {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_neg::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_neg(const at::Tensor & self) {
structured_neg_default_backend_functional op;
op.meta(self);
at::neg_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_neg_default_backend_inplace final : public at::meta::structured_neg {
    structured_neg_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_neg::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_neg_(at::Tensor & self) {
structured_neg_default_backend_inplace op(self);
op.meta(self);
at::neg_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__repeat(const at::Tensor & self, at::IntArrayRef repeats) {
    // No device check


  // DeviceGuard omitted
  return at::native::repeat(self, repeats);
}

} // anonymous namespace
struct structured_round_default_backend_functional final : public at::meta::structured_round {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_round::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_round(const at::Tensor & self) {
structured_round_default_backend_functional op;
op.meta(self);
at::round_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_round_default_backend_inplace final : public at::meta::structured_round {
    structured_round_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_round::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_round_(at::Tensor & self) {
structured_round_default_backend_inplace op(self);
op.meta(self);
at::round_outf(self, op.outputs_[0]);
return self;
}
struct structured_gelu_default_backend_functional final : public at::meta::structured_gelu {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gelu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_gelu(const at::Tensor & self) {
structured_gelu_default_backend_functional op;
op.meta(self);
at::gelu_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_gelu_backward_default_backend_functional final : public at::meta::structured_gelu_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gelu_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
structured_gelu_backward_default_backend_functional op;
op.meta(grad, self);
at::gelu_backward_outf(grad, self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_hardshrink_default_backend_functional final : public at::meta::structured_hardshrink {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hardshrink::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_hardshrink(const at::Tensor & self, const at::Scalar & lambd) {
structured_hardshrink_default_backend_functional op;
op.meta(self, lambd);
at::hardshrink_outf(self, lambd, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_hardshrink_backward_default_backend_functional final : public at::meta::structured_hardshrink_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hardshrink_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_hardshrink_backward(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
structured_hardshrink_backward_default_backend_functional op;
op.meta(grad_out, self, lambd);
at::hardshrink_backward_outf(grad_out, self, lambd, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_rsqrt_default_backend_functional final : public at::meta::structured_rsqrt {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_rsqrt::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_rsqrt(const at::Tensor & self) {
structured_rsqrt_default_backend_functional op;
op.meta(self);
at::rsqrt_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_rsqrt_default_backend_inplace final : public at::meta::structured_rsqrt {
    structured_rsqrt_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_rsqrt::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_rsqrt_(at::Tensor & self) {
structured_rsqrt_default_backend_inplace op(self);
op.meta(self);
at::rsqrt_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_int_select_int(const at::Tensor & self, int64_t dim, int64_t index) {
    // No device check


  // DeviceGuard omitted
  return at::native::select(self, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__select_backward(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t index) {
    // No device check


  // DeviceGuard omitted
  return at::native::select_backward(grad_output, input_sizes, dim, index);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__celu(const at::Tensor & self, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::celu(self, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__celu_(at::Tensor & self, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::celu_(self, alpha);
}

} // anonymous namespace
struct structured_silu_default_backend_functional final : public at::meta::structured_silu {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_silu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_silu(const at::Tensor & self) {
structured_silu_default_backend_functional op;
op.meta(self);
at::silu_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_silu_default_backend_inplace final : public at::meta::structured_silu {
    structured_silu_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_silu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_silu_(at::Tensor & self) {
structured_silu_default_backend_inplace op(self);
op.meta(self);
at::silu_outf(self, op.outputs_[0]);
return self;
}
struct structured_silu_backward_default_backend_functional final : public at::meta::structured_silu_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_silu_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
structured_silu_backward_default_backend_functional op;
op.meta(grad_output, self);
at::silu_backward_outf(grad_output, self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_mish_default_backend_functional final : public at::meta::structured_mish {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_mish::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_mish(const at::Tensor & self) {
structured_mish_default_backend_functional op;
op.meta(self);
at::mish_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_mish_default_backend_inplace final : public at::meta::structured_mish {
    structured_mish_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_mish::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_mish_(at::Tensor & self) {
structured_mish_default_backend_inplace op(self);
op.meta(self);
at::mish_outf(self, op.outputs_[0]);
return self;
}
struct structured_sigmoid_default_backend_functional final : public at::meta::structured_sigmoid {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sigmoid::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sigmoid(const at::Tensor & self) {
structured_sigmoid_default_backend_functional op;
op.meta(self);
at::sigmoid_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sigmoid_default_backend_inplace final : public at::meta::structured_sigmoid {
    structured_sigmoid_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sigmoid::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sigmoid_(at::Tensor & self) {
structured_sigmoid_default_backend_inplace op(self);
op.meta(self);
at::sigmoid_outf(self, op.outputs_[0]);
return self;
}
struct structured_sin_default_backend_functional final : public at::meta::structured_sin {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sin::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sin(const at::Tensor & self) {
structured_sin_default_backend_functional op;
op.meta(self);
at::sin_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sin_default_backend_inplace final : public at::meta::structured_sin {
    structured_sin_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sin::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sin_(at::Tensor & self) {
structured_sin_default_backend_inplace op(self);
op.meta(self);
at::sin_outf(self, op.outputs_[0]);
return self;
}
struct structured_sinc_default_backend_functional final : public at::meta::structured_sinc {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sinc::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sinc(const at::Tensor & self) {
structured_sinc_default_backend_functional op;
op.meta(self);
at::sinc_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sinc_default_backend_inplace final : public at::meta::structured_sinc {
    structured_sinc_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sinc::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sinc_(at::Tensor & self) {
structured_sinc_default_backend_inplace op(self);
op.meta(self);
at::sinc_outf(self, op.outputs_[0]);
return self;
}
struct structured_sinh_default_backend_functional final : public at::meta::structured_sinh {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sinh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sinh(const at::Tensor & self) {
structured_sinh_default_backend_functional op;
op.meta(self);
at::sinh_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sinh_default_backend_inplace final : public at::meta::structured_sinh {
    structured_sinh_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sinh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sinh_(at::Tensor & self) {
structured_sinh_default_backend_inplace op(self);
op.meta(self);
at::sinh_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__detach(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::detach(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__detach_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::detach_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Tensor_slice_Tensor(const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
    // No device check


  // DeviceGuard omitted
  return at::native::slice(self, dim, start, end, step);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__slice_backward(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
    // No device check


  // DeviceGuard omitted
  return at::native::slice_backward(grad_output, input_sizes, dim, start, end, step);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__slogdet(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::slogdet(self);
}

} // anonymous namespace
struct structured__softmax_default_backend_functional final : public at::meta::structured__softmax {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper__softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
structured__softmax_default_backend_functional op;
op.meta(self, dim, half_to_float);
at::_softmax_outf(self, dim, half_to_float, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured__softmax_backward_data_default_backend_functional final : public at::meta::structured__softmax_backward_data {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper__softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
structured__softmax_backward_data_default_backend_functional op;
op.meta(grad_output, output, dim, self);
at::_softmax_backward_data_outf(grad_output, output, dim, self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

::std::vector<at::Tensor> wrapper_Tensor_unsafe_split_Tensor(const at::Tensor & self, int64_t split_size, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unsafe_split(self, split_size, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper_Tensor_split_Tensor(const at::Tensor & self, int64_t split_size, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::split(self, split_size, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__unsafe_split_with_sizes(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unsafe_split_with_sizes(self, split_sizes, dim);
}

} // anonymous namespace
namespace {

::std::vector<at::Tensor> wrapper__split_with_sizes(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::split_with_sizes(self, split_sizes, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__squeeze(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::squeeze(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__squeeze_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::squeeze_(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim_squeeze_dim(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::squeeze(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_dim_squeeze__dim(at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::squeeze_(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__stack(at::TensorList tensors, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::stack(tensors, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_stack_out_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::stack_out(tensors, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___stack(at::TensorList tensors, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_stack(tensors, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out__stack_out_out(at::TensorList tensors, int64_t dim, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::_stack_out(tensors, dim, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::sum(self, dtype);
}

} // anonymous namespace
struct structured_sum_dim_IntList_default_backend_functional final : public at::meta::structured_sum_dim_IntList {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sum_dim_IntList(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
structured_sum_dim_IntList_default_backend_functional op;
op.meta(self, dim, keepdim, dtype);
at::sum_outf(self, dim, keepdim, dtype, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sqrt_default_backend_functional final : public at::meta::structured_sqrt {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sqrt::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sqrt(const at::Tensor & self) {
structured_sqrt_default_backend_functional op;
op.meta(self);
at::sqrt_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sqrt_default_backend_inplace final : public at::meta::structured_sqrt {
    structured_sqrt_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sqrt::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sqrt_(at::Tensor & self) {
structured_sqrt_default_backend_inplace op(self);
op.meta(self);
at::sqrt_outf(self, op.outputs_[0]);
return self;
}
struct structured_prod_dim_int_default_backend_functional final : public at::meta::structured_prod_dim_int {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_prod_dim_int(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
structured_prod_dim_int_default_backend_functional op;
op.meta(self, dim, keepdim, dtype);
at::prod_outf(self, dim, keepdim, dtype, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__t(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::t(self);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__t_(at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::t_(self);
}

} // anonymous namespace
struct structured_tan_default_backend_functional final : public at::meta::structured_tan {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_tan::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_tan(const at::Tensor & self) {
structured_tan_default_backend_functional op;
op.meta(self);
at::tan_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_tan_default_backend_inplace final : public at::meta::structured_tan {
    structured_tan_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_tan::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_tan_(at::Tensor & self) {
structured_tan_default_backend_inplace op(self);
op.meta(self);
at::tan_outf(self, op.outputs_[0]);
return self;
}
struct structured_tanh_default_backend_functional final : public at::meta::structured_tanh {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_tanh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_tanh(const at::Tensor & self) {
structured_tanh_default_backend_functional op;
op.meta(self);
at::tanh_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_tanh_default_backend_inplace final : public at::meta::structured_tanh {
    structured_tanh_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_tanh::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_tanh_(at::Tensor & self) {
structured_tanh_default_backend_inplace op(self);
op.meta(self);
at::tanh_outf(self, op.outputs_[0]);
return self;
}
struct structured_threshold_default_backend_functional final : public at::meta::structured_threshold {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_threshold::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
structured_threshold_default_backend_functional op;
op.meta(self, threshold, value);
at::threshold_outf(self, threshold, value, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_threshold_default_backend_inplace final : public at::meta::structured_threshold {
    structured_threshold_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_threshold::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_threshold_(at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
structured_threshold_default_backend_inplace op(self);
op.meta(self, threshold, value);
at::threshold_outf(self, threshold, value, op.outputs_[0]);
return self;
}
struct structured_threshold_backward_default_backend_functional final : public at::meta::structured_threshold_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_threshold_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
structured_threshold_backward_default_backend_functional op;
op.meta(grad_output, self, threshold);
at::threshold_backward_outf(grad_output, self, threshold, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper_int_transpose_int(const at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check


  // DeviceGuard omitted
  return at::native::transpose(self, dim0, dim1);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check


  // DeviceGuard omitted
  return at::native::transpose_(self, dim0, dim1);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__rot90(const at::Tensor & self, int64_t k, at::IntArrayRef dims) {
    // No device check


  // DeviceGuard omitted
  return at::native::rot90(self, k, dims);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___trilinear(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_trilinear(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}

} // anonymous namespace
struct structured_trunc_default_backend_functional final : public at::meta::structured_trunc {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_trunc::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_trunc(const at::Tensor & self) {
structured_trunc_default_backend_functional op;
op.meta(self);
at::trunc_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_trunc_default_backend_inplace final : public at::meta::structured_trunc {
    structured_trunc_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_trunc::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_trunc_(at::Tensor & self) {
structured_trunc_default_backend_inplace op(self);
op.meta(self);
at::trunc_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper___unsafe_view(const at::Tensor & self, at::IntArrayRef size) {
    // No device check


  // DeviceGuard omitted
  return at::native::_unsafe_view(self, size);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__unsqueeze(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unsqueeze(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper__unsqueeze_(at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unsqueeze_(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dim__sparse_sum_dim(const at::Tensor & self, at::IntArrayRef dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_sum(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_ScalarOpt_dtype_norm_ScalarOpt_dtype(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm(self, p, dtype);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_Scalar_norm_Scalar(const at::Tensor & self, const at::Scalar & p) {
    // No device check


  // DeviceGuard omitted
  return at::native::norm(self, p);
}

} // anonymous namespace
struct structured_norm_ScalarOpt_dim_dtype_default_backend_functional final : public at::meta::structured_norm_ScalarOpt_dim_dtype {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_norm_ScalarOpt_dim_dtype(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
structured_norm_ScalarOpt_dim_dtype_default_backend_functional op;
op.meta(self, (p.has_value() ? at::OptionalScalarRef(&(p.value())) : at::OptionalScalarRef()), dim, keepdim, dtype);
at::norm_outf(self, p, dim, keepdim, dtype, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_norm_ScalarOpt_dim_default_backend_functional final : public at::meta::structured_norm_ScalarOpt_dim {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_norm_ScalarOpt_dim(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
structured_norm_ScalarOpt_dim_default_backend_functional op;
op.meta(self, (p.has_value() ? at::OptionalScalarRef(&(p.value())) : at::OptionalScalarRef()), dim, keepdim);
at::norm_outf(self, p, dim, keepdim, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper_Tensor_frexp_Tensor(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::frexp(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::clone(self, memory_format);
}

} // anonymous namespace
namespace {

const at::Tensor & wrapper__resize_as_(const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::resize_as_(self, the_template, memory_format);
}

} // anonymous namespace
struct structured_sub_Tensor_default_backend_functional final : public at::meta::structured_sub_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sub_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sub_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
structured_sub_Tensor_default_backend_functional op;
op.meta(self, other, alpha);
at::sub_outf(self, other, alpha, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sub_Tensor_default_backend_inplace final : public at::meta::structured_sub_Tensor {
    structured_sub_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sub_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sub__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
structured_sub_Tensor_default_backend_inplace op(self);
op.meta(self, other, alpha);
at::sub_outf(self, other, alpha, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_sub_Scalar(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::sub(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_sub__Scalar(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::sub_(self, other, alpha);
}

} // anonymous namespace
struct structured_heaviside_default_backend_functional final : public at::meta::structured_heaviside {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_heaviside::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_heaviside(const at::Tensor & self, const at::Tensor & values) {
structured_heaviside_default_backend_functional op;
op.meta(self, values);
at::heaviside_outf(self, values, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_heaviside_default_backend_inplace final : public at::meta::structured_heaviside {
    structured_heaviside_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_heaviside::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_heaviside_(at::Tensor & self, const at::Tensor & values) {
structured_heaviside_default_backend_inplace op(self);
op.meta(self, values);
at::heaviside_outf(self, values, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_rsub_Scalar(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::rsub(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___sparse_addmm(const at::Tensor & self, const at::Tensor & sparse, const at::Tensor & dense, const at::Scalar & beta, const at::Scalar & alpha) {
    // No device check


  // DeviceGuard omitted
  return at::native::_sparse_addmm(self, sparse, dense, beta, alpha);
}

} // anonymous namespace
struct structured_addmm_default_backend_functional final : public at::meta::structured_addmm {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
structured_addmm_default_backend_functional op;
op.meta(self, mat1, mat2, beta, alpha);
at::addmm_outf(self, mat1, mat2, beta, alpha, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_addmm_default_backend_inplace final : public at::meta::structured_addmm {
    structured_addmm_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
structured_addmm_default_backend_inplace op(self);
op.meta(self, mat1, mat2, beta, alpha);
at::addmm_outf(self, mat1, mat2, beta, alpha, op.outputs_[0]);
return self;
}
namespace {

::std::vector<at::Tensor> wrapper_int_unbind_int(const at::Tensor & self, int64_t dim) {
    // No device check


  // DeviceGuard omitted
  return at::native::unbind(self, dim);
}

} // anonymous namespace
namespace {

at::Tensor wrapper___to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
    // No device check


  // DeviceGuard omitted
  return at::native::_to_copy(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper___pack_padded_sequence(const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
    // No device check


  // DeviceGuard omitted
  return at::native::_pack_padded_sequence(input, lengths, batch_first);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_dtype_view_dtype(const at::Tensor & self, at::ScalarType dtype) {
    // No device check


  // DeviceGuard omitted
  return at::native::view_dtype(self, dtype);
}

} // anonymous namespace
struct structured_scatter_src_default_backend_functional final : public at::meta::structured_scatter_src {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_scatter_src(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
structured_scatter_src_default_backend_functional op;
op.meta(self, dim, index, src);
at::scatter_outf(self, dim, index, src, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_scatter_src_default_backend_inplace final : public at::meta::structured_scatter_src {
    structured_scatter_src_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_scatter__src(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
structured_scatter_src_default_backend_inplace op(self);
op.meta(self, dim, index, src);
at::scatter_outf(self, dim, index, src, op.outputs_[0]);
return self;
}
struct structured_scatter_value_default_backend_functional final : public at::meta::structured_scatter_value {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_scatter_value(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
structured_scatter_value_default_backend_functional op;
op.meta(self, dim, index, value);
at::scatter_outf(self, dim, index, value, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_scatter_value_default_backend_inplace final : public at::meta::structured_scatter_value {
    structured_scatter_value_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_scatter__value(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
structured_scatter_value_default_backend_inplace op(self);
op.meta(self, dim, index, value);
at::scatter_outf(self, dim, index, value, op.outputs_[0]);
return self;
}
struct structured_scatter_reduce_default_backend_functional final : public at::meta::structured_scatter_reduce {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_scatter_reduce(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
structured_scatter_reduce_default_backend_functional op;
op.meta(self, dim, index, src, reduce);
at::scatter_outf(self, dim, index, src, reduce, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_scatter_reduce_default_backend_inplace final : public at::meta::structured_scatter_reduce {
    structured_scatter_reduce_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_scatter__reduce(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
structured_scatter_reduce_default_backend_inplace op(self);
op.meta(self, dim, index, src, reduce);
at::scatter_outf(self, dim, index, src, reduce, op.outputs_[0]);
return self;
}
struct structured_scatter_value_reduce_default_backend_functional final : public at::meta::structured_scatter_value_reduce {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_scatter_value_reduce(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
structured_scatter_value_reduce_default_backend_functional op;
op.meta(self, dim, index, value, reduce);
at::scatter_outf(self, dim, index, value, reduce, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_scatter_value_reduce_default_backend_inplace final : public at::meta::structured_scatter_value_reduce {
    structured_scatter_value_reduce_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_scatter__value_reduce(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
structured_scatter_value_reduce_default_backend_inplace op(self);
op.meta(self, dim, index, value, reduce);
at::scatter_outf(self, dim, index, value, reduce, op.outputs_[0]);
return self;
}
struct structured_scatter_add_default_backend_functional final : public at::meta::structured_scatter_add {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_scatter_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
structured_scatter_add_default_backend_functional op;
op.meta(self, dim, index, src);
at::scatter_add_outf(self, dim, index, src, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_scatter_add_default_backend_inplace final : public at::meta::structured_scatter_add {
    structured_scatter_add_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_scatter_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
structured_scatter_add_default_backend_inplace op(self);
op.meta(self, dim, index, src);
at::scatter_add_outf(self, dim, index, src, op.outputs_[0]);
return self;
}
struct structured_eq_Scalar_default_backend_functional final : public at::meta::structured_eq_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_eq_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_eq_Scalar(const at::Tensor & self, const at::Scalar & other) {
structured_eq_Scalar_default_backend_functional op;
op.meta(self, other);
at::eq_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_eq_Scalar_default_backend_inplace final : public at::meta::structured_eq_Scalar {
    structured_eq_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_eq_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_eq__Scalar(at::Tensor & self, const at::Scalar & other) {
structured_eq_Scalar_default_backend_inplace op(self);
op.meta(self, other);
at::eq_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_eq_Tensor_default_backend_functional final : public at::meta::structured_eq_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_eq_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_eq_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_eq_Tensor_default_backend_functional op;
op.meta(self, other);
at::eq_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_eq_Tensor_default_backend_inplace final : public at::meta::structured_eq_Tensor {
    structured_eq_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_eq_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_eq__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_eq_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::eq_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_bitwise_and_Tensor_default_backend_functional final : public at::meta::structured_bitwise_and_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_and_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_bitwise_and_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_bitwise_and_Tensor_default_backend_functional op;
op.meta(self, other);
at::bitwise_and_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_bitwise_and_Tensor_default_backend_inplace final : public at::meta::structured_bitwise_and_Tensor {
    structured_bitwise_and_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_and_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_bitwise_and__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_bitwise_and_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::bitwise_and_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_bitwise_and_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_and(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_bitwise_and_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_and_out(self, other, out);
}

} // anonymous namespace
struct structured_bitwise_or_Tensor_default_backend_functional final : public at::meta::structured_bitwise_or_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_or_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_bitwise_or_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_bitwise_or_Tensor_default_backend_functional op;
op.meta(self, other);
at::bitwise_or_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_bitwise_or_Tensor_default_backend_inplace final : public at::meta::structured_bitwise_or_Tensor {
    structured_bitwise_or_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_or_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_bitwise_or__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_bitwise_or_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::bitwise_or_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor & wrapper_Scalar_out_bitwise_or_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_or_out(self, other, out);
}

} // anonymous namespace
struct structured_bitwise_xor_Tensor_default_backend_functional final : public at::meta::structured_bitwise_xor_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_xor_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_bitwise_xor_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_bitwise_xor_Tensor_default_backend_functional op;
op.meta(self, other);
at::bitwise_xor_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_bitwise_xor_Tensor_default_backend_inplace final : public at::meta::structured_bitwise_xor_Tensor {
    structured_bitwise_xor_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_xor_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_bitwise_xor__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_bitwise_xor_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::bitwise_xor_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor & wrapper_Scalar_out_bitwise_xor_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::bitwise_xor_out(self, other, out);
}

} // anonymous namespace
struct structured_bitwise_left_shift_Tensor_default_backend_functional final : public at::meta::structured_bitwise_left_shift_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_left_shift_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_bitwise_left_shift_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_bitwise_left_shift_Tensor_default_backend_functional op;
op.meta(self, other);
at::bitwise_left_shift_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_bitwise_left_shift_Tensor_default_backend_inplace final : public at::meta::structured_bitwise_left_shift_Tensor {
    structured_bitwise_left_shift_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_left_shift_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_bitwise_left_shift__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_bitwise_left_shift_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::bitwise_left_shift_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_bitwise_right_shift_Tensor_default_backend_functional final : public at::meta::structured_bitwise_right_shift_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_right_shift_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_bitwise_right_shift_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_bitwise_right_shift_Tensor_default_backend_functional op;
op.meta(self, other);
at::bitwise_right_shift_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_bitwise_right_shift_Tensor_default_backend_inplace final : public at::meta::structured_bitwise_right_shift_Tensor {
    structured_bitwise_right_shift_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_bitwise_right_shift_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_bitwise_right_shift__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_bitwise_right_shift_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::bitwise_right_shift_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__tril(const at::Tensor & self, int64_t diagonal) {
    // No device check


  // DeviceGuard omitted
  return at::native::tril(self, diagonal);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__triu(const at::Tensor & self, int64_t diagonal) {
    // No device check


  // DeviceGuard omitted
  return at::native::triu(self, diagonal);
}

} // anonymous namespace
struct structured_digamma_default_backend_functional final : public at::meta::structured_digamma {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_digamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_digamma(const at::Tensor & self) {
structured_digamma_default_backend_functional op;
op.meta(self);
at::digamma_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_digamma_default_backend_inplace final : public at::meta::structured_digamma {
    structured_digamma_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_digamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_digamma_(at::Tensor & self) {
structured_digamma_default_backend_inplace op(self);
op.meta(self);
at::digamma_outf(self, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__diag(const at::Tensor & self, int64_t diagonal) {
    // No device check


  // DeviceGuard omitted
  return at::native::diag(self, diagonal);
}

} // anonymous namespace
struct structured_ne_Scalar_default_backend_functional final : public at::meta::structured_ne_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ne_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_ne_Scalar(const at::Tensor & self, const at::Scalar & other) {
structured_ne_Scalar_default_backend_functional op;
op.meta(self, other);
at::ne_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_ne_Scalar_default_backend_inplace final : public at::meta::structured_ne_Scalar {
    structured_ne_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ne_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_ne__Scalar(at::Tensor & self, const at::Scalar & other) {
structured_ne_Scalar_default_backend_inplace op(self);
op.meta(self, other);
at::ne_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_ne_Tensor_default_backend_functional final : public at::meta::structured_ne_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ne_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_ne_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_ne_Tensor_default_backend_functional op;
op.meta(self, other);
at::ne_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_ne_Tensor_default_backend_inplace final : public at::meta::structured_ne_Tensor {
    structured_ne_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ne_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_ne__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_ne_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::ne_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_ge_Scalar_default_backend_functional final : public at::meta::structured_ge_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ge_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_ge_Scalar(const at::Tensor & self, const at::Scalar & other) {
structured_ge_Scalar_default_backend_functional op;
op.meta(self, other);
at::ge_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_ge_Scalar_default_backend_inplace final : public at::meta::structured_ge_Scalar {
    structured_ge_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ge_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_ge__Scalar(at::Tensor & self, const at::Scalar & other) {
structured_ge_Scalar_default_backend_inplace op(self);
op.meta(self, other);
at::ge_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_ge_Tensor_default_backend_functional final : public at::meta::structured_ge_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ge_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_ge_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_ge_Tensor_default_backend_functional op;
op.meta(self, other);
at::ge_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_ge_Tensor_default_backend_inplace final : public at::meta::structured_ge_Tensor {
    structured_ge_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_ge_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_ge__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_ge_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::ge_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_le_Scalar_default_backend_functional final : public at::meta::structured_le_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_le_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_le_Scalar(const at::Tensor & self, const at::Scalar & other) {
structured_le_Scalar_default_backend_functional op;
op.meta(self, other);
at::le_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_le_Scalar_default_backend_inplace final : public at::meta::structured_le_Scalar {
    structured_le_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_le_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_le__Scalar(at::Tensor & self, const at::Scalar & other) {
structured_le_Scalar_default_backend_inplace op(self);
op.meta(self, other);
at::le_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_le_Tensor_default_backend_functional final : public at::meta::structured_le_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_le_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_le_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_le_Tensor_default_backend_functional op;
op.meta(self, other);
at::le_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_le_Tensor_default_backend_inplace final : public at::meta::structured_le_Tensor {
    structured_le_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_le_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_le__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_le_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::le_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_gt_Scalar_default_backend_functional final : public at::meta::structured_gt_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gt_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_gt_Scalar(const at::Tensor & self, const at::Scalar & other) {
structured_gt_Scalar_default_backend_functional op;
op.meta(self, other);
at::gt_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_gt_Scalar_default_backend_inplace final : public at::meta::structured_gt_Scalar {
    structured_gt_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gt_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_gt__Scalar(at::Tensor & self, const at::Scalar & other) {
structured_gt_Scalar_default_backend_inplace op(self);
op.meta(self, other);
at::gt_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_gt_Tensor_default_backend_functional final : public at::meta::structured_gt_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gt_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_gt_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_gt_Tensor_default_backend_functional op;
op.meta(self, other);
at::gt_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_gt_Tensor_default_backend_inplace final : public at::meta::structured_gt_Tensor {
    structured_gt_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_gt_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_gt__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_gt_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::gt_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_lt_Scalar_default_backend_functional final : public at::meta::structured_lt_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lt_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_lt_Scalar(const at::Tensor & self, const at::Scalar & other) {
structured_lt_Scalar_default_backend_functional op;
op.meta(self, other);
at::lt_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_lt_Scalar_default_backend_inplace final : public at::meta::structured_lt_Scalar {
    structured_lt_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lt_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_lt__Scalar(at::Tensor & self, const at::Scalar & other) {
structured_lt_Scalar_default_backend_inplace op(self);
op.meta(self, other);
at::lt_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_lt_Tensor_default_backend_functional final : public at::meta::structured_lt_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lt_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_lt_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_lt_Tensor_default_backend_functional op;
op.meta(self, other);
at::lt_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_lt_Tensor_default_backend_inplace final : public at::meta::structured_lt_Tensor {
    structured_lt_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lt_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_lt__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_lt_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::lt_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_gather_default_backend_functional final : public at::meta::structured_gather {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_gather(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
structured_gather_default_backend_functional op;
op.meta(self, dim, index, sparse_grad);
at::gather_outf(self, dim, index, sparse_grad, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_addcmul_default_backend_functional final : public at::meta::structured_addcmul {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_addcmul::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_addcmul(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
structured_addcmul_default_backend_functional op;
op.meta(self, tensor1, tensor2, value);
at::addcmul_outf(self, tensor1, tensor2, value, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_addcmul_default_backend_inplace final : public at::meta::structured_addcmul {
    structured_addcmul_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_addcmul::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_addcmul_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
structured_addcmul_default_backend_inplace op(self);
op.meta(self, tensor1, tensor2, value);
at::addcmul_outf(self, tensor1, tensor2, value, op.outputs_[0]);
return self;
}
struct structured_addcdiv_default_backend_functional final : public at::meta::structured_addcdiv {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_addcdiv::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_addcdiv(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
structured_addcdiv_default_backend_functional op;
op.meta(self, tensor1, tensor2, value);
at::addcdiv_outf(self, tensor1, tensor2, value, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_addcdiv_default_backend_inplace final : public at::meta::structured_addcdiv {
    structured_addcdiv_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_addcdiv::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_addcdiv_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
structured_addcdiv_default_backend_inplace op(self);
op.meta(self, tensor1, tensor2, value);
at::addcdiv_outf(self, tensor1, tensor2, value, op.outputs_[0]);
return self;
}
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__symeig(const at::Tensor & self, bool eigenvectors, bool upper) {
    // No device check


  // DeviceGuard omitted
  return at::native::symeig(self, eigenvectors, upper);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_e_symeig_out_e(const at::Tensor & self, bool eigenvectors, bool upper, at::Tensor & e, at::Tensor & V) {
    // No device check


  // DeviceGuard omitted
  return at::native::symeig_out(self, eigenvectors, upper, e, V);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__eig(const at::Tensor & self, bool eigenvectors) {
    // No device check


  // DeviceGuard omitted
  return at::native::eig(self, eigenvectors);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_e_eig_out_e(const at::Tensor & self, bool eigenvectors, at::Tensor & e, at::Tensor & v) {
    // No device check


  // DeviceGuard omitted
  return at::native::eig_out(self, eigenvectors, e, v);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__cholesky_solve(const at::Tensor & self, const at::Tensor & input2, bool upper) {
    // No device check


  // DeviceGuard omitted
  return at::native::cholesky_solve(self, input2, upper);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_cholesky_solve_out_out(const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::cholesky_solve_out(self, input2, upper, out);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__solve(const at::Tensor & self, const at::Tensor & A) {
    // No device check


  // DeviceGuard omitted
  return at::native::solve(self, A);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_solution_solve_out_solution(const at::Tensor & self, const at::Tensor & A, at::Tensor & solution, at::Tensor & lu) {
    // No device check


  // DeviceGuard omitted
  return at::native::solve_out(self, A, solution, lu);
}

} // anonymous namespace
struct structured_lgamma_default_backend_functional final : public at::meta::structured_lgamma {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lgamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_lgamma(const at::Tensor & self) {
structured_lgamma_default_backend_functional op;
op.meta(self);
at::lgamma_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_lgamma_default_backend_inplace final : public at::meta::structured_lgamma {
    structured_lgamma_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_lgamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_lgamma_(at::Tensor & self) {
structured_lgamma_default_backend_inplace op(self);
op.meta(self);
at::lgamma_outf(self, op.outputs_[0]);
return self;
}
struct structured_polygamma_default_backend_functional final : public at::meta::structured_polygamma {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_polygamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_polygamma(int64_t n, const at::Tensor & self) {
structured_polygamma_default_backend_functional op;
op.meta(n, self);
at::polygamma_outf(n, self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor & wrapper__polygamma_(at::Tensor & self, int64_t n) {
    // No device check


  // DeviceGuard omitted
  return at::native::polygamma_(self, n);
}

} // anonymous namespace
struct structured_erfinv_default_backend_functional final : public at::meta::structured_erfinv {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_erfinv::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_erfinv(const at::Tensor & self) {
structured_erfinv_default_backend_functional op;
op.meta(self);
at::erfinv_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_erfinv_default_backend_inplace final : public at::meta::structured_erfinv {
    structured_erfinv_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_erfinv::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_erfinv_(at::Tensor & self) {
structured_erfinv_default_backend_inplace op(self);
op.meta(self);
at::erfinv_outf(self, op.outputs_[0]);
return self;
}
struct structured_i0_default_backend_functional final : public at::meta::structured_i0 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_i0::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_i0(const at::Tensor & self) {
structured_i0_default_backend_functional op;
op.meta(self);
at::i0_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_i0_default_backend_inplace final : public at::meta::structured_i0 {
    structured_i0_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_i0::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_i0_(at::Tensor & self) {
structured_i0_default_backend_inplace op(self);
op.meta(self);
at::i0_outf(self, op.outputs_[0]);
return self;
}
struct structured_sign_default_backend_functional final : public at::meta::structured_sign {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sign::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sign(const at::Tensor & self) {
structured_sign_default_backend_functional op;
op.meta(self);
at::sign_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sign_default_backend_inplace final : public at::meta::structured_sign {
    structured_sign_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sign::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_sign_(at::Tensor & self) {
structured_sign_default_backend_inplace op(self);
op.meta(self);
at::sign_outf(self, op.outputs_[0]);
return self;
}
struct structured_signbit_default_backend_functional final : public at::meta::structured_signbit {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_signbit::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_signbit(const at::Tensor & self) {
structured_signbit_default_backend_functional op;
op.meta(self);
at::signbit_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__dist(const at::Tensor & self, const at::Tensor & other, const at::Scalar & p) {
    // No device check


  // DeviceGuard omitted
  return at::native::dist(self, other, p);
}

} // anonymous namespace
struct structured_atan2_default_backend_functional final : public at::meta::structured_atan2 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_atan2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_atan2(const at::Tensor & self, const at::Tensor & other) {
structured_atan2_default_backend_functional op;
op.meta(self, other);
at::atan2_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_atan2_default_backend_inplace final : public at::meta::structured_atan2 {
    structured_atan2_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_atan2::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_atan2_(at::Tensor & self, const at::Tensor & other) {
structured_atan2_default_backend_inplace op(self);
op.meta(self, other);
at::atan2_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_fmod_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::fmod(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_fmod_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::fmod_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_fmod__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::fmod_(self, other);
}

} // anonymous namespace
struct structured_fmod_Tensor_default_backend_functional final : public at::meta::structured_fmod_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_fmod_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_fmod_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_fmod_Tensor_default_backend_functional op;
op.meta(self, other);
at::fmod_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_fmod_Tensor_default_backend_inplace final : public at::meta::structured_fmod_Tensor {
    structured_fmod_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_fmod_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_fmod__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_fmod_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::fmod_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_hypot_default_backend_functional final : public at::meta::structured_hypot {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hypot::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_hypot(const at::Tensor & self, const at::Tensor & other) {
structured_hypot_default_backend_functional op;
op.meta(self, other);
at::hypot_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_hypot_default_backend_inplace final : public at::meta::structured_hypot {
    structured_hypot_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hypot::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_hypot_(at::Tensor & self, const at::Tensor & other) {
structured_hypot_default_backend_inplace op(self);
op.meta(self, other);
at::hypot_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_igamma_default_backend_functional final : public at::meta::structured_igamma {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_igamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_igamma(const at::Tensor & self, const at::Tensor & other) {
structured_igamma_default_backend_functional op;
op.meta(self, other);
at::igamma_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_igamma_default_backend_inplace final : public at::meta::structured_igamma {
    structured_igamma_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_igamma::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_igamma_(at::Tensor & self, const at::Tensor & other) {
structured_igamma_default_backend_inplace op(self);
op.meta(self, other);
at::igamma_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_igammac_default_backend_functional final : public at::meta::structured_igammac {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_igammac::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_igammac(const at::Tensor & self, const at::Tensor & other) {
structured_igammac_default_backend_functional op;
op.meta(self, other);
at::igammac_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_igammac_default_backend_inplace final : public at::meta::structured_igammac {
    structured_igammac_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_igammac::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_igammac_(at::Tensor & self, const at::Tensor & other) {
structured_igammac_default_backend_inplace op(self);
op.meta(self, other);
at::igammac_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_nextafter_default_backend_functional final : public at::meta::structured_nextafter {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_nextafter::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_nextafter(const at::Tensor & self, const at::Tensor & other) {
structured_nextafter_default_backend_functional op;
op.meta(self, other);
at::nextafter_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_nextafter_default_backend_inplace final : public at::meta::structured_nextafter {
    structured_nextafter_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_nextafter::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_nextafter_(at::Tensor & self, const at::Tensor & other) {
structured_nextafter_default_backend_inplace op(self);
op.meta(self, other);
at::nextafter_outf(self, other, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper_Scalar_remainder_Scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::remainder(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_out_remainder_out_Scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::remainder_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Scalar_remainder__Scalar(at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::remainder_(self, other);
}

} // anonymous namespace
struct structured_remainder_Tensor_default_backend_functional final : public at::meta::structured_remainder_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_remainder_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_remainder_Tensor(const at::Tensor & self, const at::Tensor & other) {
structured_remainder_Tensor_default_backend_functional op;
op.meta(self, other);
at::remainder_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_remainder_Tensor_default_backend_inplace final : public at::meta::structured_remainder_Tensor {
    structured_remainder_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_remainder_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_remainder__Tensor(at::Tensor & self, const at::Tensor & other) {
structured_remainder_Tensor_default_backend_inplace op(self);
op.meta(self, other);
at::remainder_outf(self, other, op.outputs_[0]);
return self;
}
struct structured_fmin_default_backend_functional final : public at::meta::structured_fmin {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_fmin::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_fmin(const at::Tensor & self, const at::Tensor & other) {
structured_fmin_default_backend_functional op;
op.meta(self, other);
at::fmin_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_fmax_default_backend_functional final : public at::meta::structured_fmax {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_fmax::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_fmax(const at::Tensor & self, const at::Tensor & other) {
structured_fmax_default_backend_functional op;
op.meta(self, other);
at::fmax_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_maximum_default_backend_functional final : public at::meta::structured_maximum {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_maximum::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_maximum(const at::Tensor & self, const at::Tensor & other) {
structured_maximum_default_backend_functional op;
op.meta(self, other);
at::maximum_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_minimum_default_backend_functional final : public at::meta::structured_minimum {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_minimum::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_minimum(const at::Tensor & self, const at::Tensor & other) {
structured_minimum_default_backend_functional op;
op.meta(self, other);
at::minimum_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_topk_default_backend_functional final : public at::meta::structured_topk {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_topk(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
structured_topk_default_backend_functional op;
op.meta(self, k, dim, largest, sorted);
at::topk_outf(self, k, dim, largest, sorted, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
struct structured_all_default_backend_functional final : public at::meta::structured_all {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_all(const at::Tensor & self) {
structured_all_default_backend_functional op;
op.meta(self);
at::all_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_any_default_backend_functional final : public at::meta::structured_any {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_any(const at::Tensor & self) {
structured_any_default_backend_functional op;
op.meta(self);
at::any_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_renorm_default_backend_functional final : public at::meta::structured_renorm {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_renorm(const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
structured_renorm_default_backend_functional op;
op.meta(self, p, dim, maxnorm);
at::renorm_outf(self, p, dim, maxnorm, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_renorm_default_backend_inplace final : public at::meta::structured_renorm {
    structured_renorm_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_renorm_(at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
structured_renorm_default_backend_inplace op(self);
op.meta(self, p, dim, maxnorm);
at::renorm_outf(self, p, dim, maxnorm, op.outputs_[0]);
return self;
}
struct structured_pow_Tensor_Tensor_default_backend_functional final : public at::meta::structured_pow_Tensor_Tensor {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_pow_Tensor_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_pow_Tensor_Tensor(const at::Tensor & self, const at::Tensor & exponent) {
structured_pow_Tensor_Tensor_default_backend_functional op;
op.meta(self, exponent);
at::pow_outf(self, exponent, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_pow_Tensor_Tensor_default_backend_inplace final : public at::meta::structured_pow_Tensor_Tensor {
    structured_pow_Tensor_Tensor_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_pow_Tensor_Tensor::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_pow__Tensor(at::Tensor & self, const at::Tensor & exponent) {
structured_pow_Tensor_Tensor_default_backend_inplace op(self);
op.meta(self, exponent);
at::pow_outf(self, exponent, op.outputs_[0]);
return self;
}
struct structured_pow_Scalar_default_backend_functional final : public at::meta::structured_pow_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_pow_Scalar(const at::Scalar & self, const at::Tensor & exponent) {
structured_pow_Scalar_default_backend_functional op;
op.meta(self, exponent);
at::pow_outf(self, exponent, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_pow_Tensor_Scalar_default_backend_functional final : public at::meta::structured_pow_Tensor_Scalar {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_pow_Tensor_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_pow_Tensor_Scalar(const at::Tensor & self, const at::Scalar & exponent) {
structured_pow_Tensor_Scalar_default_backend_functional op;
op.meta(self, exponent);
at::pow_outf(self, exponent, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_pow_Tensor_Scalar_default_backend_inplace final : public at::meta::structured_pow_Tensor_Scalar {
    structured_pow_Tensor_Scalar_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_pow_Tensor_Scalar::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_pow__Scalar(at::Tensor & self, const at::Scalar & exponent) {
structured_pow_Tensor_Scalar_default_backend_inplace op(self);
op.meta(self, exponent);
at::pow_outf(self, exponent, op.outputs_[0]);
return self;
}
namespace {

at::Tensor wrapper__alias(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::alias(self);
}

} // anonymous namespace
struct structured__convert_indices_from_coo_to_csr_default_backend_functional final : public at::meta::structured__convert_indices_from_coo_to_csr {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper__convert_indices_from_coo_to_csr(const at::Tensor & self, int64_t size, bool out_int32) {
structured__convert_indices_from_coo_to_csr_default_backend_functional op;
op.meta(self, size, out_int32);
at::_convert_indices_from_coo_to_csr_outf(self, size, out_int32, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::l1_loss(self, target, reduction);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_l1_loss_out_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::l1_loss_out(self, target, reduction, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::l1_loss_backward(grad_output, self, target, reduction);
}

} // anonymous namespace
struct structured_nll_loss_forward_default_backend_functional final : public at::meta::structured_nll_loss_forward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_nll_loss_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
structured_nll_loss_forward_default_backend_functional op;
op.meta(self, target, ((weight.has_value() && (*weight).defined()) ? at::OptionalTensorRef(*weight) : at::OptionalTensorRef()), reduction, ignore_index);
at::nll_loss_forward_outf(self, target, weight, reduction, ignore_index, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
struct structured_nll_loss_backward_default_backend_functional final : public at::meta::structured_nll_loss_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_nll_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
structured_nll_loss_backward_default_backend_functional op;
op.meta(grad_output, self, target, ((weight.has_value() && (*weight).defined()) ? at::OptionalTensorRef(*weight) : at::OptionalTensorRef()), reduction, ignore_index, total_weight);
at::nll_loss_backward_outf(grad_output, self, target, weight, reduction, ignore_index, total_weight, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__smooth_l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
    // No device check


  // DeviceGuard omitted
  return at::native::smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__huber_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
    // No device check


  // DeviceGuard omitted
  return at::native::huber_loss_backward(grad_output, self, target, reduction, delta);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__soft_margin_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::soft_margin_loss(self, target, reduction);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_soft_margin_loss_out_out(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::soft_margin_loss_out(self, target, reduction, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__soft_margin_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
    // No device check


  // DeviceGuard omitted
  return at::native::soft_margin_loss_backward(grad_output, self, target, reduction);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_grad_input_soft_margin_loss_backward_out_grad_input(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
    // No device check


  // DeviceGuard omitted
  return at::native::soft_margin_loss_backward_out(grad_output, self, target, reduction, grad_input);
}

} // anonymous namespace
struct structured_elu_default_backend_functional final : public at::meta::structured_elu {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_elu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_elu(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
structured_elu_default_backend_functional op;
op.meta(self, alpha, scale, input_scale);
at::elu_outf(self, alpha, scale, input_scale, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_elu_default_backend_inplace final : public at::meta::structured_elu {
    structured_elu_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_elu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_elu_(at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
structured_elu_default_backend_inplace op(self);
op.meta(self, alpha, scale, input_scale);
at::elu_outf(self, alpha, scale, input_scale, op.outputs_[0]);
return self;
}
struct structured_elu_backward_default_backend_functional final : public at::meta::structured_elu_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_elu_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_elu_backward(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
structured_elu_backward_default_backend_functional op;
op.meta(grad_output, alpha, scale, input_scale, is_result, self_or_result);
at::elu_backward_outf(grad_output, alpha, scale, input_scale, is_result, self_or_result, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_glu_default_backend_functional final : public at::meta::structured_glu {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_glu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_glu(const at::Tensor & self, int64_t dim) {
structured_glu_default_backend_functional op;
op.meta(self, dim);
at::glu_outf(self, dim, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_hardsigmoid_default_backend_functional final : public at::meta::structured_hardsigmoid {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hardsigmoid::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_hardsigmoid(const at::Tensor & self) {
structured_hardsigmoid_default_backend_functional op;
op.meta(self);
at::hardsigmoid_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_hardsigmoid_default_backend_inplace final : public at::meta::structured_hardsigmoid {
    structured_hardsigmoid_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hardsigmoid::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_hardsigmoid_(at::Tensor & self) {
structured_hardsigmoid_default_backend_inplace op(self);
op.meta(self);
at::hardsigmoid_outf(self, op.outputs_[0]);
return self;
}
struct structured_hardsigmoid_backward_default_backend_functional final : public at::meta::structured_hardsigmoid_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_hardsigmoid_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_hardsigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self) {
structured_hardsigmoid_backward_default_backend_functional op;
op.meta(grad_output, self);
at::hardsigmoid_backward_outf(grad_output, self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_leaky_relu_default_backend_functional final : public at::meta::structured_leaky_relu {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_leaky_relu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
structured_leaky_relu_default_backend_functional op;
op.meta(self, negative_slope);
at::leaky_relu_outf(self, negative_slope, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_leaky_relu_default_backend_inplace final : public at::meta::structured_leaky_relu {
    structured_leaky_relu_default_backend_inplace(Tensor& self) : outputs_{std::ref(self)} {}

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }

        if (!names.empty()) {
          namedinference::propagate_names(outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_leaky_relu::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return outputs_[output_idx];
    }
    std::array<std::reference_wrapper<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor & wrapper_leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
structured_leaky_relu_default_backend_inplace op(self);
op.meta(self, negative_slope);
at::leaky_relu_outf(self, negative_slope, op.outputs_[0]);
return self;
}
struct structured_leaky_relu_backward_default_backend_functional final : public at::meta::structured_leaky_relu_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_leaky_relu_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_leaky_relu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
structured_leaky_relu_backward_default_backend_functional op;
op.meta(grad_output, self, negative_slope, self_is_result);
at::leaky_relu_backward_outf(grad_output, self, negative_slope, self_is_result, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper__rrelu_with_noise_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
    // No device check


  // DeviceGuard omitted
  return at::native::rrelu_with_noise_backward(grad_output, self, noise, lower, upper, training, self_is_result);
}

} // anonymous namespace
struct structured_softplus_default_backend_functional final : public at::meta::structured_softplus {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_softplus::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_softplus(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
structured_softplus_default_backend_functional op;
op.meta(self, beta, threshold);
at::softplus_outf(self, beta, threshold, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_softplus_backward_default_backend_functional final : public at::meta::structured_softplus_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_softplus_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_softplus_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output) {
structured_softplus_backward_default_backend_functional op;
op.meta(grad_output, self, beta, threshold, output);
at::softplus_backward_outf(grad_output, self, beta, threshold, output, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_softshrink_default_backend_functional final : public at::meta::structured_softshrink {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_softshrink::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_softshrink(const at::Tensor & self, const at::Scalar & lambd) {
structured_softshrink_default_backend_functional op;
op.meta(self, lambd);
at::softshrink_outf(self, lambd, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_softshrink_backward_default_backend_functional final : public at::meta::structured_softshrink_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_softshrink_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_softshrink_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
structured_softshrink_backward_default_backend_functional op;
op.meta(grad_output, self, lambd);
at::softshrink_backward_outf(grad_output, self, lambd, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_adaptive_max_pool2d_default_backend_functional final : public at::meta::structured_adaptive_max_pool2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_adaptive_max_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
structured_adaptive_max_pool2d_default_backend_functional op;
op.meta(self, output_size);
at::adaptive_max_pool2d_outf(self, output_size, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
struct structured_adaptive_max_pool2d_backward_default_backend_functional final : public at::meta::structured_adaptive_max_pool2d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_adaptive_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
structured_adaptive_max_pool2d_backward_default_backend_functional op;
op.meta(grad_output, self, indices);
at::adaptive_max_pool2d_backward_outf(grad_output, self, indices, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_adaptive_max_pool3d_default_backend_functional final : public at::meta::structured_adaptive_max_pool3d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_adaptive_max_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
structured_adaptive_max_pool3d_default_backend_functional op;
op.meta(self, output_size);
at::adaptive_max_pool3d_outf(self, output_size, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
struct structured_adaptive_max_pool3d_backward_default_backend_functional final : public at::meta::structured_adaptive_max_pool3d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_adaptive_max_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
structured_adaptive_max_pool3d_backward_default_backend_functional op;
op.meta(grad_output, self, indices);
at::adaptive_max_pool3d_backward_outf(grad_output, self, indices, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_avg_pool2d_default_backend_functional final : public at::meta::structured_avg_pool2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
structured_avg_pool2d_default_backend_functional op;
auto precompute = op.meta(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
(void)precompute;
at::avg_pool2d_outf(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_avg_pool2d_backward_default_backend_functional final : public at::meta::structured_avg_pool2d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
structured_avg_pool2d_backward_default_backend_functional op;
op.meta(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
at::avg_pool2d_backward_outf(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_avg_pool3d_default_backend_functional final : public at::meta::structured_avg_pool3d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
structured_avg_pool3d_default_backend_functional op;
op.meta(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
at::avg_pool3d_outf(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_avg_pool3d_backward_default_backend_functional final : public at::meta::structured_avg_pool3d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
structured_avg_pool3d_backward_default_backend_functional op;
op.meta(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
at::avg_pool3d_backward_outf(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_fractional_max_pool2d_default_backend_functional final : public at::meta::structured_fractional_max_pool2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_fractional_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
structured_fractional_max_pool2d_default_backend_functional op;
op.meta(self, kernel_size, output_size, random_samples);
at::fractional_max_pool2d_outf(self, kernel_size, output_size, random_samples, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
struct structured_max_pool2d_with_indices_default_backend_functional final : public at::meta::structured_max_pool2d_with_indices {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 2> outputs_;
    c10::OptionalDeviceGuard guard_;
};

::std::tuple<at::Tensor,at::Tensor> wrapper_max_pool2d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
structured_max_pool2d_with_indices_default_backend_functional op;
op.meta(self, kernel_size, stride, padding, dilation, ceil_mode);
at::max_pool2d_with_indices_outf(self, kernel_size, stride, padding, dilation, ceil_mode, *op.outputs_[0], *op.outputs_[1]);
return std::make_tuple(std::move(op.outputs_[0]).take(), std::move(op.outputs_[1]).take());
}
struct structured_max_pool2d_with_indices_backward_default_backend_functional final : public at::meta::structured_max_pool2d_with_indices_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_max_pool2d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
structured_max_pool2d_with_indices_backward_default_backend_functional op;
op.meta(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
at::max_pool2d_with_indices_backward_outf(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_reflection_pad1d_default_backend_functional final : public at::meta::structured_reflection_pad1d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_reflection_pad1d(const at::Tensor & self, at::IntArrayRef padding) {
structured_reflection_pad1d_default_backend_functional op;
op.meta(self, padding);
at::reflection_pad1d_outf(self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_reflection_pad1d_backward_default_backend_functional final : public at::meta::structured_reflection_pad1d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_reflection_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
structured_reflection_pad1d_backward_default_backend_functional op;
op.meta(grad_output, self, padding);
at::reflection_pad1d_backward_outf(grad_output, self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_reflection_pad3d_default_backend_functional final : public at::meta::structured_reflection_pad3d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_reflection_pad3d(const at::Tensor & self, at::IntArrayRef padding) {
structured_reflection_pad3d_default_backend_functional op;
op.meta(self, padding);
at::reflection_pad3d_outf(self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_reflection_pad3d_backward_default_backend_functional final : public at::meta::structured_reflection_pad3d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_reflection_pad3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
structured_reflection_pad3d_backward_default_backend_functional op;
op.meta(grad_output, self, padding);
at::reflection_pad3d_backward_outf(grad_output, self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_replication_pad1d_default_backend_functional final : public at::meta::structured_replication_pad1d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_replication_pad1d(const at::Tensor & self, at::IntArrayRef padding) {
structured_replication_pad1d_default_backend_functional op;
op.meta(self, padding);
at::replication_pad1d_outf(self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_replication_pad1d_backward_default_backend_functional final : public at::meta::structured_replication_pad1d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_replication_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
structured_replication_pad1d_backward_default_backend_functional op;
op.meta(grad_output, self, padding);
at::replication_pad1d_backward_outf(grad_output, self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_replication_pad2d_default_backend_functional final : public at::meta::structured_replication_pad2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_replication_pad2d(const at::Tensor & self, at::IntArrayRef padding) {
structured_replication_pad2d_default_backend_functional op;
op.meta(self, padding);
at::replication_pad2d_outf(self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_replication_pad3d_default_backend_functional final : public at::meta::structured_replication_pad3d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_replication_pad3d(const at::Tensor & self, at::IntArrayRef padding) {
structured_replication_pad3d_default_backend_functional op;
op.meta(self, padding);
at::replication_pad3d_outf(self, padding, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper_vec_upsample_linear1d_vec(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_linear1d(input, output_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_linear1d_backward_vec(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_linear1d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_bilinear2d_vec(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_bilinear2d(input, output_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_bilinear2d_backward_vec(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_trilinear3d_vec(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_trilinear3d(input, output_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_trilinear3d_backward_vec(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_trilinear3d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_bicubic2d_vec(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_bicubic2d(input, output_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_bicubic2d_backward_vec(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_bicubic2d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_nearest1d_vec(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_nearest1d(input, output_size, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_nearest1d_backward_vec(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_nearest1d_backward(grad_output, output_size, input_size, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_nearest2d_vec(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_nearest2d(input, output_size, scale_factors);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_vec_upsample_nearest2d_backward_vec(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
    // No device check


  // DeviceGuard omitted
  return at::native::upsample_nearest2d_backward(grad_output, output_size, input_size, scale_factors);
}

} // anonymous namespace
struct structured_upsample_linear1d_default_backend_functional final : public at::meta::structured_upsample_linear1d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_linear1d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
structured_upsample_linear1d_default_backend_functional op;
op.meta(self, output_size, align_corners, scales);
at::upsample_linear1d_outf(self, output_size, align_corners, scales, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_linear1d_backward_default_backend_functional final : public at::meta::structured_upsample_linear1d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_linear1d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
structured_upsample_linear1d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, align_corners, scales);
at::upsample_linear1d_backward_outf(grad_output, output_size, input_size, align_corners, scales, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_bilinear2d_default_backend_functional final : public at::meta::structured_upsample_bilinear2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_bilinear2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_bilinear2d_default_backend_functional op;
op.meta(self, output_size, align_corners, scales_h, scales_w);
at::upsample_bilinear2d_outf(self, output_size, align_corners, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_bilinear2d_backward_default_backend_functional final : public at::meta::structured_upsample_bilinear2d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_bilinear2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_bilinear2d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
at::upsample_bilinear2d_backward_outf(grad_output, output_size, input_size, align_corners, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_bicubic2d_default_backend_functional final : public at::meta::structured_upsample_bicubic2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_bicubic2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_bicubic2d_default_backend_functional op;
op.meta(self, output_size, align_corners, scales_h, scales_w);
at::upsample_bicubic2d_outf(self, output_size, align_corners, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_bicubic2d_backward_default_backend_functional final : public at::meta::structured_upsample_bicubic2d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_bicubic2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_bicubic2d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
at::upsample_bicubic2d_backward_outf(grad_output, output_size, input_size, align_corners, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_trilinear3d_default_backend_functional final : public at::meta::structured_upsample_trilinear3d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_trilinear3d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_trilinear3d_default_backend_functional op;
op.meta(self, output_size, align_corners, scales_d, scales_h, scales_w);
at::upsample_trilinear3d_outf(self, output_size, align_corners, scales_d, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_trilinear3d_backward_default_backend_functional final : public at::meta::structured_upsample_trilinear3d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_trilinear3d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_trilinear3d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
at::upsample_trilinear3d_backward_outf(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_nearest1d_default_backend_functional final : public at::meta::structured_upsample_nearest1d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_nearest1d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales) {
structured_upsample_nearest1d_default_backend_functional op;
op.meta(self, output_size, scales);
at::upsample_nearest1d_outf(self, output_size, scales, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_nearest1d_backward_default_backend_functional final : public at::meta::structured_upsample_nearest1d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_nearest1d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales) {
structured_upsample_nearest1d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, scales);
at::upsample_nearest1d_backward_outf(grad_output, output_size, input_size, scales, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_nearest2d_default_backend_functional final : public at::meta::structured_upsample_nearest2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_nearest2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_nearest2d_default_backend_functional op;
op.meta(self, output_size, scales_h, scales_w);
at::upsample_nearest2d_outf(self, output_size, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_nearest2d_backward_default_backend_functional final : public at::meta::structured_upsample_nearest2d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_nearest2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_nearest2d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, scales_h, scales_w);
at::upsample_nearest2d_backward_outf(grad_output, output_size, input_size, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_nearest3d_default_backend_functional final : public at::meta::structured_upsample_nearest3d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_nearest3d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_nearest3d_default_backend_functional op;
op.meta(self, output_size, scales_d, scales_h, scales_w);
at::upsample_nearest3d_outf(self, output_size, scales_d, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_upsample_nearest3d_backward_default_backend_functional final : public at::meta::structured_upsample_nearest3d_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_upsample_nearest3d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
structured_upsample_nearest3d_backward_default_backend_functional op;
op.meta(grad_output, output_size, input_size, scales_d, scales_h, scales_w);
at::upsample_nearest3d_backward_outf(grad_output, output_size, input_size, scales_d, scales_h, scales_w, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_sigmoid_backward_default_backend_functional final : public at::meta::structured_sigmoid_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_sigmoid_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & output) {
structured_sigmoid_backward_default_backend_functional op;
op.meta(grad_output, output);
at::sigmoid_backward_outf(grad_output, output, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_logit_backward_default_backend_functional final : public at::meta::structured_logit_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_logit_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_logit_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
structured_logit_backward_default_backend_functional op;
op.meta(grad_output, self, eps);
at::logit_backward_outf(grad_output, self, eps, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_tanh_backward_default_backend_functional final : public at::meta::structured_tanh_backward {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_tanh_backward::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_tanh_backward(const at::Tensor & grad_output, const at::Tensor & output) {
structured_tanh_backward_default_backend_functional op;
op.meta(grad_output, output);
at::tanh_backward_outf(grad_output, output, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_slow_conv_transpose2d_default_backend_functional final : public at::meta::structured_slow_conv_transpose2d {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output

    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_slow_conv_transpose2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
structured_slow_conv_transpose2d_default_backend_functional op;
op.meta(self, weight, kernel_size, ((bias.has_value() && (*bias).defined()) ? at::OptionalTensorRef(*bias) : at::OptionalTensorRef()), stride, padding, output_padding, dilation);
at::slow_conv_transpose2d_outf(self, weight, kernel_size, bias, stride, padding, output_padding, dilation, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_isposinf_default_backend_functional final : public at::meta::structured_isposinf {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_isposinf::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_isposinf(const at::Tensor & self) {
structured_isposinf_default_backend_functional op;
op.meta(self);
at::isposinf_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_isneginf_default_backend_functional final : public at::meta::structured_isneginf {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_isneginf::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_isneginf(const at::Tensor & self) {
structured_isneginf_default_backend_functional op;
op.meta(self);
at::isneginf_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_special_entr_default_backend_functional final : public at::meta::structured_special_entr {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_entr::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_entr(const at::Tensor & self) {
structured_special_entr_default_backend_functional op;
op.meta(self);
at::special_entr_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_special_ndtri_default_backend_functional final : public at::meta::structured_special_ndtri {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_ndtri::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_ndtri(const at::Tensor & self) {
structured_special_ndtri_default_backend_functional op;
op.meta(self);
at::special_ndtri_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_special_erfcx_default_backend_functional final : public at::meta::structured_special_erfcx {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_erfcx::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_erfcx(const at::Tensor & self) {
structured_special_erfcx_default_backend_functional op;
op.meta(self);
at::special_erfcx_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_special_xlog1py_default_backend_functional final : public at::meta::structured_special_xlog1py {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_xlog1py::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_xlog1py(const at::Tensor & self, const at::Tensor & other) {
structured_special_xlog1py_default_backend_functional op;
op.meta(self, other);
at::special_xlog1py_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper_self_scalar_special_xlog1py_self_scalar(const at::Scalar & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlog1py(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_self_scalar_out_special_xlog1py_out_self_scalar_out(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlog1py_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_scalar_special_xlog1py_other_scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlog1py(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_other_scalar_out_special_xlog1py_out_other_scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_xlog1py_out(self, other, out);
}

} // anonymous namespace
struct structured_special_zeta_default_backend_functional final : public at::meta::structured_special_zeta {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_zeta::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_zeta(const at::Tensor & self, const at::Tensor & other) {
structured_special_zeta_default_backend_functional op;
op.meta(self, other);
at::special_zeta_outf(self, other, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

at::Tensor wrapper_self_scalar_special_zeta_self_scalar(const at::Scalar & self, const at::Tensor & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_zeta(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_self_scalar_out_special_zeta_out_self_scalar_out(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_zeta_out(self, other, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper_other_scalar_special_zeta_other_scalar(const at::Tensor & self, const at::Scalar & other) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_zeta(self, other);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_other_scalar_out_special_zeta_out_other_scalar_out(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
    // No device check


  // DeviceGuard omitted
  return at::native::special_zeta_out(self, other, out);
}

} // anonymous namespace
struct structured_special_i0e_default_backend_functional final : public at::meta::structured_special_i0e {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_i0e::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_i0e(const at::Tensor & self) {
structured_special_i0e_default_backend_functional op;
op.meta(self);
at::special_i0e_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_special_i1_default_backend_functional final : public at::meta::structured_special_i1 {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_i1::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_i1(const at::Tensor & self) {
structured_special_i1_default_backend_functional op;
op.meta(self);
at::special_i1_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
struct structured_special_i1e_default_backend_functional final : public at::meta::structured_special_i1e {

    void set_output(int64_t output_idx, IntArrayRef sizes, IntArrayRef strides,
                    TensorOptions options, DimnameList names) override {

        auto current_device = guard_.current_device();
        if (C10_UNLIKELY(current_device.has_value())) {
          TORCH_INTERNAL_ASSERT(*current_device == options.device(),
            "structured kernels don't support multi-device outputs");
        } else {
          guard_.reset_device(options.device());
        }


        outputs_[output_idx] = create_out(sizes, strides, options);
        if (!names.empty()) {
          namedinference::propagate_names(*outputs_[output_idx], names);
        }
        // super must happen after, so that downstream can use maybe_get_output
        // to retrieve the output
        at::meta::structured_special_i1e::set_output(output_idx, sizes, strides, options, names);
    }

    const Tensor& maybe_get_output(int64_t output_idx) override {
        return *outputs_[output_idx];
    }
    std::array<c10::ExclusivelyOwned<Tensor>, 1> outputs_;
    c10::OptionalDeviceGuard guard_;
};

at::Tensor wrapper_special_i1e(const at::Tensor & self) {
structured_special_i1e_default_backend_functional op;
op.meta(self);
at::special_i1e_outf(self, *op.outputs_[0]);
return std::move(op.outputs_[0]).take();
}
namespace {

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> wrapper__linalg_lstsq(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_lstsq(self, b, rcond, driver);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__linalg_inv_ex(const at::Tensor & self, bool check_errors) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_inv_ex(self, check_errors);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_inverse_linalg_inv_ex_out_inverse(const at::Tensor & self, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_inv_ex_out(self, check_errors, inverse, info);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor,at::Tensor> wrapper__linalg_qr(const at::Tensor & self, c10::string_view mode) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_qr(self, mode);
}

} // anonymous namespace
namespace {

::std::tuple<at::Tensor &,at::Tensor &> wrapper_out_linalg_qr_out_out(const at::Tensor & self, c10::string_view mode, at::Tensor & Q, at::Tensor & R) {
    // No device check


  // DeviceGuard omitted
  return at::native::linalg_qr_out(self, mode, Q, R);
}

} // anonymous namespace

TORCH_LIBRARY_IMPL(aten, CompositeExplicitAutograd, m) {
  m.impl("_fw_primal",
  TORCH_FN(wrapper___fw_primal));
  m.impl("abs",
  TORCH_FN(wrapper__abs));
  m.impl("abs_",
  TORCH_FN(wrapper__abs_));
  m.impl("sgn", TORCH_FN(wrapper_sgn));
  m.impl("sgn_", TORCH_FN(wrapper_sgn_));
  m.impl("_conj",
  TORCH_FN(wrapper___conj));
  m.impl("_conj_physical",
  TORCH_FN(wrapper___conj_physical));
  m.impl("conj_physical_",
  TORCH_FN(wrapper__conj_physical_));
  m.impl("_neg_view",
  TORCH_FN(wrapper___neg_view));
  m.impl("acos", TORCH_FN(wrapper_acos));
  m.impl("acos_", TORCH_FN(wrapper_acos_));
  m.impl("add.Tensor", TORCH_FN(wrapper_add_Tensor));
  m.impl("add_.Tensor", TORCH_FN(wrapper_add__Tensor));
  m.impl("add.Scalar",
  TORCH_FN(wrapper_Scalar_add_Scalar));
  m.impl("add_.Scalar",
  TORCH_FN(wrapper_Scalar_add__Scalar));
  m.impl("addmv", TORCH_FN(wrapper_addmv));
  m.impl("addmv_", TORCH_FN(wrapper_addmv_));
  m.impl("addr_",
  TORCH_FN(wrapper__addr_));
  m.impl("affine_grid_generator",
  TORCH_FN(wrapper__affine_grid_generator));
  m.impl("all.dim", TORCH_FN(wrapper_all_dim));
  m.impl("any.dim", TORCH_FN(wrapper_any_dim));
  m.impl("argmax", TORCH_FN(wrapper_argmax));
  m.impl("argmin", TORCH_FN(wrapper_argmin));
  m.impl("acosh", TORCH_FN(wrapper_acosh));
  m.impl("acosh_", TORCH_FN(wrapper_acosh_));
  m.impl("asinh", TORCH_FN(wrapper_asinh));
  m.impl("asinh_", TORCH_FN(wrapper_asinh_));
  m.impl("atanh", TORCH_FN(wrapper_atanh));
  m.impl("atanh_", TORCH_FN(wrapper_atanh_));
  m.impl("as_strided_",
  TORCH_FN(wrapper__as_strided_));
  m.impl("asin", TORCH_FN(wrapper_asin));
  m.impl("asin_", TORCH_FN(wrapper_asin_));
  m.impl("atan", TORCH_FN(wrapper_atan));
  m.impl("atan_", TORCH_FN(wrapper_atan_));
  m.impl("bernoulli",
  TORCH_FN(wrapper__bernoulli));
  m.impl("binary_cross_entropy_with_logits",
  TORCH_FN(wrapper__binary_cross_entropy_with_logits));
  m.impl("bitwise_not", TORCH_FN(wrapper_bitwise_not));
  m.impl("bitwise_not_", TORCH_FN(wrapper_bitwise_not_));
  m.impl("copysign.Tensor", TORCH_FN(wrapper_copysign_Tensor));
  m.impl("copysign_.Tensor", TORCH_FN(wrapper_copysign__Tensor));
  m.impl("copysign.Scalar",
  TORCH_FN(wrapper_Scalar_copysign_Scalar));
  m.impl("copysign.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_copysign_out_Scalar_out));
  m.impl("copysign_.Scalar",
  TORCH_FN(wrapper_Scalar_copysign__Scalar));
  m.impl("cat",
  TORCH_FN(wrapper__cat));
  m.impl("cat.out",
  TORCH_FN(wrapper_out_cat_out_out));
  m.impl("ceil", TORCH_FN(wrapper_ceil));
  m.impl("ceil_", TORCH_FN(wrapper_ceil_));
  m.impl("clamp", TORCH_FN(wrapper_clamp));
  m.impl("clamp_", TORCH_FN(wrapper_clamp_));
  m.impl("clamp_.Tensor",
  TORCH_FN(wrapper_Tensor_clamp__Tensor));
  m.impl("clamp_max",
  TORCH_FN(wrapper__clamp_max));
  m.impl("clamp_max_",
  TORCH_FN(wrapper__clamp_max_));
  m.impl("clamp_max.Tensor",
  TORCH_FN(wrapper_Tensor_clamp_max_Tensor));
  m.impl("clamp_max_.Tensor",
  TORCH_FN(wrapper_Tensor_clamp_max__Tensor));
  m.impl("clamp_min",
  TORCH_FN(wrapper__clamp_min));
  m.impl("clamp_min_",
  TORCH_FN(wrapper__clamp_min_));
  m.impl("clamp_min.Tensor",
  TORCH_FN(wrapper_Tensor_clamp_min_Tensor));
  m.impl("clamp_min_.Tensor",
  TORCH_FN(wrapper_Tensor_clamp_min__Tensor));
  m.impl("complex",
  TORCH_FN(wrapper__complex));
  m.impl("polar",
  TORCH_FN(wrapper__polar));
  m.impl("constant_pad_nd",
  TORCH_FN(wrapper__constant_pad_nd));
  m.impl("convolution_overrideable",
  TORCH_FN(wrapper__convolution_overrideable));
  m.impl("convolution_backward_overrideable",
  TORCH_FN(wrapper__convolution_backward_overrideable));
  m.impl("conv_tbc",
  TORCH_FN(wrapper__conv_tbc));
  m.impl("copy_",
  TORCH_FN(wrapper__copy_));
  m.impl("cos", TORCH_FN(wrapper_cos));
  m.impl("cos_", TORCH_FN(wrapper_cos_));
  m.impl("cosh", TORCH_FN(wrapper_cosh));
  m.impl("cosh_", TORCH_FN(wrapper_cosh_));
  m.impl("count_nonzero",
  TORCH_FN(wrapper__count_nonzero));
  m.impl("cummax",
  TORCH_FN(wrapper__cummax));
  m.impl("cummax.out",
  TORCH_FN(wrapper_out_cummax_out_out));
  m.impl("cummin",
  TORCH_FN(wrapper__cummin));
  m.impl("cummin.out",
  TORCH_FN(wrapper_out_cummin_out_out));
  m.impl("cumprod", TORCH_FN(wrapper_cumprod));
  m.impl("cumprod_", TORCH_FN(wrapper_cumprod_));
  m.impl("cumsum", TORCH_FN(wrapper_cumsum));
  m.impl("cumsum_", TORCH_FN(wrapper_cumsum_));
  m.impl("diagonal",
  TORCH_FN(wrapper__diagonal));
  m.impl("diagonal_backward",
  TORCH_FN(wrapper__diagonal_backward));
  m.impl("div.Tensor", TORCH_FN(wrapper_div_Tensor));
  m.impl("div_.Tensor", TORCH_FN(wrapper_div__Tensor));
  m.impl("div.Tensor_mode", TORCH_FN(wrapper_div_Tensor_mode));
  m.impl("div_.Tensor_mode", TORCH_FN(wrapper_div__Tensor_mode));
  m.impl("div.Scalar",
  TORCH_FN(wrapper_Scalar_div_Scalar));
  m.impl("div_.Scalar",
  TORCH_FN(wrapper_Scalar_div__Scalar));
  m.impl("div.Scalar_mode",
  TORCH_FN(wrapper_Scalar_mode_div_Scalar_mode));
  m.impl("div_.Scalar_mode",
  TORCH_FN(wrapper_Scalar_mode_div__Scalar_mode));
  m.impl("dot.out",
  TORCH_FN(wrapper_out_dot_out_out));
  m.impl("vdot.out",
  TORCH_FN(wrapper_out_vdot_out_out));
  m.impl("embedding",
  TORCH_FN(wrapper__embedding));
  m.impl("erf", TORCH_FN(wrapper_erf));
  m.impl("erf_", TORCH_FN(wrapper_erf_));
  m.impl("erfc", TORCH_FN(wrapper_erfc));
  m.impl("erfc_", TORCH_FN(wrapper_erfc_));
  m.impl("exp", TORCH_FN(wrapper_exp));
  m.impl("exp_", TORCH_FN(wrapper_exp_));
  m.impl("exp2", TORCH_FN(wrapper_exp2));
  m.impl("exp2_", TORCH_FN(wrapper_exp2_));
  m.impl("expm1", TORCH_FN(wrapper_expm1));
  m.impl("expm1_", TORCH_FN(wrapper_expm1_));
  m.impl("expand",
  TORCH_FN(wrapper__expand));
  m.impl("floor", TORCH_FN(wrapper_floor));
  m.impl("floor_", TORCH_FN(wrapper_floor_));
  m.impl("frac", TORCH_FN(wrapper_frac));
  m.impl("frac_", TORCH_FN(wrapper_frac_));
  m.impl("gcd", TORCH_FN(wrapper_gcd));
  m.impl("gcd_", TORCH_FN(wrapper_gcd_));
  m.impl("lcm", TORCH_FN(wrapper_lcm));
  m.impl("lcm_", TORCH_FN(wrapper_lcm_));
  m.impl("_grid_sampler_2d_cpu_fallback",
  TORCH_FN(wrapper___grid_sampler_2d_cpu_fallback));
  m.impl("index_copy_",
  TORCH_FN(wrapper__index_copy_));
  m.impl("index_put_",
  TORCH_FN(wrapper__index_put_));
  m.impl("inverse",
  TORCH_FN(wrapper__inverse));
  m.impl("inverse.out",
  TORCH_FN(wrapper_out_inverse_out_out));
  m.impl("isin.Tensor_Tensor", TORCH_FN(wrapper_isin_Tensor_Tensor));
  m.impl("isin.Tensor_Scalar", TORCH_FN(wrapper_isin_Tensor_Scalar));
  m.impl("isin.Scalar_Tensor", TORCH_FN(wrapper_isin_Scalar_Tensor));
  m.impl("kl_div",
  TORCH_FN(wrapper__kl_div));
  m.impl("kthvalue",
  TORCH_FN(wrapper__kthvalue));
  m.impl("nan_to_num",
  TORCH_FN(wrapper__nan_to_num));
  m.impl("nan_to_num_",
  TORCH_FN(wrapper__nan_to_num_));
  m.impl("log", TORCH_FN(wrapper_log));
  m.impl("log_", TORCH_FN(wrapper_log_));
  m.impl("log10", TORCH_FN(wrapper_log10));
  m.impl("log10_", TORCH_FN(wrapper_log10_));
  m.impl("log1p", TORCH_FN(wrapper_log1p));
  m.impl("log1p_", TORCH_FN(wrapper_log1p_));
  m.impl("log2", TORCH_FN(wrapper_log2));
  m.impl("log2_", TORCH_FN(wrapper_log2_));
  m.impl("logaddexp", TORCH_FN(wrapper_logaddexp));
  m.impl("logaddexp2", TORCH_FN(wrapper_logaddexp2));
  m.impl("xlogy.Tensor", TORCH_FN(wrapper_xlogy_Tensor));
  m.impl("xlogy_.Tensor", TORCH_FN(wrapper_xlogy__Tensor));
  m.impl("xlogy.Scalar_Self",
  TORCH_FN(wrapper_Scalar_Self_xlogy_Scalar_Self));
  m.impl("xlogy.OutScalar_Self",
  TORCH_FN(wrapper_OutScalar_Self_xlogy_out_OutScalar_Self));
  m.impl("xlogy.Scalar_Other",
  TORCH_FN(wrapper_Scalar_Other_xlogy_Scalar_Other));
  m.impl("xlogy.OutScalar_Other",
  TORCH_FN(wrapper_OutScalar_Other_xlogy_out_OutScalar_Other));
  m.impl("xlogy_.Scalar_Other",
  TORCH_FN(wrapper_Scalar_Other_xlogy__Scalar_Other));
  m.impl("logdet",
  TORCH_FN(wrapper__logdet));
  m.impl("_log_softmax", TORCH_FN(wrapper__log_softmax));
  m.impl("_log_softmax_backward_data", TORCH_FN(wrapper__log_softmax_backward_data));
  m.impl("logcumsumexp",
  TORCH_FN(wrapper__logcumsumexp));
  m.impl("logcumsumexp.out",
  TORCH_FN(wrapper_out_logcumsumexp_out_out));
  m.impl("logsumexp",
  TORCH_FN(wrapper__logsumexp));
  m.impl("logsumexp.out",
  TORCH_FN(wrapper_out_logsumexp_out_out));
  m.impl("aminmax", TORCH_FN(wrapper_aminmax));
  m.impl("amax",
  TORCH_FN(wrapper__amax));
  m.impl("mean",
  TORCH_FN(wrapper__mean));
  m.impl("mean.dim", TORCH_FN(wrapper_mean_dim));
  m.impl("median.dim",
  TORCH_FN(wrapper_dim_median_dim));
  m.impl("nanmedian.dim",
  TORCH_FN(wrapper_dim_nanmedian_dim));
  m.impl("amin",
  TORCH_FN(wrapper__amin));
  m.impl("mkldnn_convolution",
  TORCH_FN(wrapper__mkldnn_convolution));
  m.impl("mkldnn_convolution_backward",
  TORCH_FN(wrapper__mkldnn_convolution_backward));
  m.impl("mm", TORCH_FN(wrapper_mm));
  m.impl("mode.values",
  TORCH_FN(wrapper_values_mode_out_values));
  m.impl("mul.Tensor", TORCH_FN(wrapper_mul_Tensor));
  m.impl("mul_.Tensor", TORCH_FN(wrapper_mul__Tensor));
  m.impl("mul.Scalar",
  TORCH_FN(wrapper_Scalar_mul_Scalar));
  m.impl("mul_.Scalar",
  TORCH_FN(wrapper_Scalar_mul__Scalar));
  m.impl("mv.out",
  TORCH_FN(wrapper_out_mv_out_out));
  m.impl("mvlgamma",
  TORCH_FN(wrapper__mvlgamma));
  m.impl("mvlgamma_",
  TORCH_FN(wrapper__mvlgamma_));
  m.impl("narrow_copy",
  TORCH_FN(wrapper__narrow_copy));
  m.impl("_nnpack_spatial_convolution",
  TORCH_FN(wrapper___nnpack_spatial_convolution));
  m.impl("_euclidean_dist",
  TORCH_FN(wrapper___euclidean_dist));
  m.impl("permute",
  TORCH_FN(wrapper__permute));
  m.impl("is_pinned",
  TORCH_FN(wrapper__is_pinned));
  m.impl("rad2deg",
  TORCH_FN(wrapper__rad2deg));
  m.impl("rad2deg.out",
  TORCH_FN(wrapper_out_rad2deg_out_out));
  m.impl("rad2deg_",
  TORCH_FN(wrapper__rad2deg_));
  m.impl("deg2rad",
  TORCH_FN(wrapper__deg2rad));
  m.impl("deg2rad.out",
  TORCH_FN(wrapper_out_deg2rad_out_out));
  m.impl("deg2rad_",
  TORCH_FN(wrapper__deg2rad_));
  m.impl("reciprocal", TORCH_FN(wrapper_reciprocal));
  m.impl("reciprocal_", TORCH_FN(wrapper_reciprocal_));
  m.impl("neg", TORCH_FN(wrapper_neg));
  m.impl("neg_", TORCH_FN(wrapper_neg_));
  m.impl("repeat",
  TORCH_FN(wrapper__repeat));
  m.impl("round", TORCH_FN(wrapper_round));
  m.impl("round_", TORCH_FN(wrapper_round_));
  m.impl("gelu", TORCH_FN(wrapper_gelu));
  m.impl("gelu_backward", TORCH_FN(wrapper_gelu_backward));
  m.impl("hardshrink", TORCH_FN(wrapper_hardshrink));
  m.impl("hardshrink_backward", TORCH_FN(wrapper_hardshrink_backward));
  m.impl("rsqrt", TORCH_FN(wrapper_rsqrt));
  m.impl("rsqrt_", TORCH_FN(wrapper_rsqrt_));
  m.impl("select.int",
  TORCH_FN(wrapper_int_select_int));
  m.impl("select_backward",
  TORCH_FN(wrapper__select_backward));
  m.impl("celu",
  TORCH_FN(wrapper__celu));
  m.impl("celu_",
  TORCH_FN(wrapper__celu_));
  m.impl("silu", TORCH_FN(wrapper_silu));
  m.impl("silu_", TORCH_FN(wrapper_silu_));
  m.impl("silu_backward", TORCH_FN(wrapper_silu_backward));
  m.impl("mish", TORCH_FN(wrapper_mish));
  m.impl("mish_", TORCH_FN(wrapper_mish_));
  m.impl("sigmoid", TORCH_FN(wrapper_sigmoid));
  m.impl("sigmoid_", TORCH_FN(wrapper_sigmoid_));
  m.impl("sin", TORCH_FN(wrapper_sin));
  m.impl("sin_", TORCH_FN(wrapper_sin_));
  m.impl("sinc", TORCH_FN(wrapper_sinc));
  m.impl("sinc_", TORCH_FN(wrapper_sinc_));
  m.impl("sinh", TORCH_FN(wrapper_sinh));
  m.impl("sinh_", TORCH_FN(wrapper_sinh_));
  m.impl("detach",
  TORCH_FN(wrapper__detach));
  m.impl("detach_",
  TORCH_FN(wrapper__detach_));
  m.impl("slice.Tensor",
  TORCH_FN(wrapper_Tensor_slice_Tensor));
  m.impl("slice_backward",
  TORCH_FN(wrapper__slice_backward));
  m.impl("slogdet",
  TORCH_FN(wrapper__slogdet));
  m.impl("_softmax", TORCH_FN(wrapper__softmax));
  m.impl("_softmax_backward_data", TORCH_FN(wrapper__softmax_backward_data));
  m.impl("unsafe_split.Tensor",
  TORCH_FN(wrapper_Tensor_unsafe_split_Tensor));
  m.impl("split.Tensor",
  TORCH_FN(wrapper_Tensor_split_Tensor));
  m.impl("unsafe_split_with_sizes",
  TORCH_FN(wrapper__unsafe_split_with_sizes));
  m.impl("split_with_sizes",
  TORCH_FN(wrapper__split_with_sizes));
  m.impl("squeeze",
  TORCH_FN(wrapper__squeeze));
  m.impl("squeeze_",
  TORCH_FN(wrapper__squeeze_));
  m.impl("squeeze.dim",
  TORCH_FN(wrapper_dim_squeeze_dim));
  m.impl("squeeze_.dim",
  TORCH_FN(wrapper_dim_squeeze__dim));
  m.impl("stack",
  TORCH_FN(wrapper__stack));
  m.impl("stack.out",
  TORCH_FN(wrapper_out_stack_out_out));
  m.impl("_stack",
  TORCH_FN(wrapper___stack));
  m.impl("_stack.out",
  TORCH_FN(wrapper_out__stack_out_out));
  m.impl("sum",
  TORCH_FN(wrapper__sum));
  m.impl("sum.dim_IntList", TORCH_FN(wrapper_sum_dim_IntList));
  m.impl("sqrt", TORCH_FN(wrapper_sqrt));
  m.impl("sqrt_", TORCH_FN(wrapper_sqrt_));
  m.impl("prod.dim_int", TORCH_FN(wrapper_prod_dim_int));
  m.impl("t",
  TORCH_FN(wrapper__t));
  m.impl("t_",
  TORCH_FN(wrapper__t_));
  m.impl("tan", TORCH_FN(wrapper_tan));
  m.impl("tan_", TORCH_FN(wrapper_tan_));
  m.impl("tanh", TORCH_FN(wrapper_tanh));
  m.impl("tanh_", TORCH_FN(wrapper_tanh_));
  m.impl("threshold", TORCH_FN(wrapper_threshold));
  m.impl("threshold_", TORCH_FN(wrapper_threshold_));
  m.impl("threshold_backward", TORCH_FN(wrapper_threshold_backward));
  m.impl("transpose.int",
  TORCH_FN(wrapper_int_transpose_int));
  m.impl("transpose_",
  TORCH_FN(wrapper__transpose_));
  m.impl("rot90",
  TORCH_FN(wrapper__rot90));
  m.impl("_trilinear",
  TORCH_FN(wrapper___trilinear));
  m.impl("trunc", TORCH_FN(wrapper_trunc));
  m.impl("trunc_", TORCH_FN(wrapper_trunc_));
  m.impl("_unsafe_view",
  TORCH_FN(wrapper___unsafe_view));
  m.impl("unsqueeze",
  TORCH_FN(wrapper__unsqueeze));
  m.impl("unsqueeze_",
  TORCH_FN(wrapper__unsqueeze_));
  m.impl("_sparse_sum.dim",
  TORCH_FN(wrapper_dim__sparse_sum_dim));
  m.impl("norm.ScalarOpt_dtype",
  TORCH_FN(wrapper_ScalarOpt_dtype_norm_ScalarOpt_dtype));
  m.impl("norm.Scalar",
  TORCH_FN(wrapper_Scalar_norm_Scalar));
  m.impl("norm.ScalarOpt_dim_dtype", TORCH_FN(wrapper_norm_ScalarOpt_dim_dtype));
  m.impl("norm.ScalarOpt_dim", TORCH_FN(wrapper_norm_ScalarOpt_dim));
  m.impl("frexp.Tensor",
  TORCH_FN(wrapper_Tensor_frexp_Tensor));
  m.impl("clone",
  TORCH_FN(wrapper__clone));
  m.impl("resize_as_",
  TORCH_FN(wrapper__resize_as_));
  m.impl("sub.Tensor", TORCH_FN(wrapper_sub_Tensor));
  m.impl("sub_.Tensor", TORCH_FN(wrapper_sub__Tensor));
  m.impl("sub.Scalar",
  TORCH_FN(wrapper_Scalar_sub_Scalar));
  m.impl("sub_.Scalar",
  TORCH_FN(wrapper_Scalar_sub__Scalar));
  m.impl("heaviside", TORCH_FN(wrapper_heaviside));
  m.impl("heaviside_", TORCH_FN(wrapper_heaviside_));
  m.impl("rsub.Scalar",
  TORCH_FN(wrapper_Scalar_rsub_Scalar));
  m.impl("_sparse_addmm",
  TORCH_FN(wrapper___sparse_addmm));
  m.impl("addmm", TORCH_FN(wrapper_addmm));
  m.impl("addmm_", TORCH_FN(wrapper_addmm_));
  m.impl("unbind.int",
  TORCH_FN(wrapper_int_unbind_int));
  m.impl("_to_copy",
  TORCH_FN(wrapper___to_copy));
  m.impl("_pack_padded_sequence",
  TORCH_FN(wrapper___pack_padded_sequence));
  m.impl("view.dtype",
  TORCH_FN(wrapper_dtype_view_dtype));
  m.impl("scatter.src", TORCH_FN(wrapper_scatter_src));
  m.impl("scatter_.src", TORCH_FN(wrapper_scatter__src));
  m.impl("scatter.value", TORCH_FN(wrapper_scatter_value));
  m.impl("scatter_.value", TORCH_FN(wrapper_scatter__value));
  m.impl("scatter.reduce", TORCH_FN(wrapper_scatter_reduce));
  m.impl("scatter_.reduce", TORCH_FN(wrapper_scatter__reduce));
  m.impl("scatter.value_reduce", TORCH_FN(wrapper_scatter_value_reduce));
  m.impl("scatter_.value_reduce", TORCH_FN(wrapper_scatter__value_reduce));
  m.impl("scatter_add", TORCH_FN(wrapper_scatter_add));
  m.impl("scatter_add_", TORCH_FN(wrapper_scatter_add_));
  m.impl("eq.Scalar", TORCH_FN(wrapper_eq_Scalar));
  m.impl("eq_.Scalar", TORCH_FN(wrapper_eq__Scalar));
  m.impl("eq.Tensor", TORCH_FN(wrapper_eq_Tensor));
  m.impl("eq_.Tensor", TORCH_FN(wrapper_eq__Tensor));
  m.impl("bitwise_and.Tensor", TORCH_FN(wrapper_bitwise_and_Tensor));
  m.impl("bitwise_and_.Tensor", TORCH_FN(wrapper_bitwise_and__Tensor));
  m.impl("bitwise_and.Scalar",
  TORCH_FN(wrapper_Scalar_bitwise_and_Scalar));
  m.impl("bitwise_and.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_bitwise_and_out_Scalar_out));
  m.impl("bitwise_or.Tensor", TORCH_FN(wrapper_bitwise_or_Tensor));
  m.impl("bitwise_or_.Tensor", TORCH_FN(wrapper_bitwise_or__Tensor));
  m.impl("bitwise_or.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_bitwise_or_out_Scalar_out));
  m.impl("bitwise_xor.Tensor", TORCH_FN(wrapper_bitwise_xor_Tensor));
  m.impl("bitwise_xor_.Tensor", TORCH_FN(wrapper_bitwise_xor__Tensor));
  m.impl("bitwise_xor.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_bitwise_xor_out_Scalar_out));
  m.impl("bitwise_left_shift.Tensor", TORCH_FN(wrapper_bitwise_left_shift_Tensor));
  m.impl("bitwise_left_shift_.Tensor", TORCH_FN(wrapper_bitwise_left_shift__Tensor));
  m.impl("bitwise_right_shift.Tensor", TORCH_FN(wrapper_bitwise_right_shift_Tensor));
  m.impl("bitwise_right_shift_.Tensor", TORCH_FN(wrapper_bitwise_right_shift__Tensor));
  m.impl("tril",
  TORCH_FN(wrapper__tril));
  m.impl("triu",
  TORCH_FN(wrapper__triu));
  m.impl("digamma", TORCH_FN(wrapper_digamma));
  m.impl("digamma_", TORCH_FN(wrapper_digamma_));
  m.impl("diag",
  TORCH_FN(wrapper__diag));
  m.impl("ne.Scalar", TORCH_FN(wrapper_ne_Scalar));
  m.impl("ne_.Scalar", TORCH_FN(wrapper_ne__Scalar));
  m.impl("ne.Tensor", TORCH_FN(wrapper_ne_Tensor));
  m.impl("ne_.Tensor", TORCH_FN(wrapper_ne__Tensor));
  m.impl("ge.Scalar", TORCH_FN(wrapper_ge_Scalar));
  m.impl("ge_.Scalar", TORCH_FN(wrapper_ge__Scalar));
  m.impl("ge.Tensor", TORCH_FN(wrapper_ge_Tensor));
  m.impl("ge_.Tensor", TORCH_FN(wrapper_ge__Tensor));
  m.impl("le.Scalar", TORCH_FN(wrapper_le_Scalar));
  m.impl("le_.Scalar", TORCH_FN(wrapper_le__Scalar));
  m.impl("le.Tensor", TORCH_FN(wrapper_le_Tensor));
  m.impl("le_.Tensor", TORCH_FN(wrapper_le__Tensor));
  m.impl("gt.Scalar", TORCH_FN(wrapper_gt_Scalar));
  m.impl("gt_.Scalar", TORCH_FN(wrapper_gt__Scalar));
  m.impl("gt.Tensor", TORCH_FN(wrapper_gt_Tensor));
  m.impl("gt_.Tensor", TORCH_FN(wrapper_gt__Tensor));
  m.impl("lt.Scalar", TORCH_FN(wrapper_lt_Scalar));
  m.impl("lt_.Scalar", TORCH_FN(wrapper_lt__Scalar));
  m.impl("lt.Tensor", TORCH_FN(wrapper_lt_Tensor));
  m.impl("lt_.Tensor", TORCH_FN(wrapper_lt__Tensor));
  m.impl("gather", TORCH_FN(wrapper_gather));
  m.impl("addcmul", TORCH_FN(wrapper_addcmul));
  m.impl("addcmul_", TORCH_FN(wrapper_addcmul_));
  m.impl("addcdiv", TORCH_FN(wrapper_addcdiv));
  m.impl("addcdiv_", TORCH_FN(wrapper_addcdiv_));
  m.impl("symeig",
  TORCH_FN(wrapper__symeig));
  m.impl("symeig.e",
  TORCH_FN(wrapper_e_symeig_out_e));
  m.impl("eig",
  TORCH_FN(wrapper__eig));
  m.impl("eig.e",
  TORCH_FN(wrapper_e_eig_out_e));
  m.impl("cholesky_solve",
  TORCH_FN(wrapper__cholesky_solve));
  m.impl("cholesky_solve.out",
  TORCH_FN(wrapper_out_cholesky_solve_out_out));
  m.impl("solve",
  TORCH_FN(wrapper__solve));
  m.impl("solve.solution",
  TORCH_FN(wrapper_solution_solve_out_solution));
  m.impl("lgamma", TORCH_FN(wrapper_lgamma));
  m.impl("lgamma_", TORCH_FN(wrapper_lgamma_));
  m.impl("polygamma", TORCH_FN(wrapper_polygamma));
  m.impl("polygamma_",
  TORCH_FN(wrapper__polygamma_));
  m.impl("erfinv", TORCH_FN(wrapper_erfinv));
  m.impl("erfinv_", TORCH_FN(wrapper_erfinv_));
  m.impl("i0", TORCH_FN(wrapper_i0));
  m.impl("i0_", TORCH_FN(wrapper_i0_));
  m.impl("sign", TORCH_FN(wrapper_sign));
  m.impl("sign_", TORCH_FN(wrapper_sign_));
  m.impl("signbit", TORCH_FN(wrapper_signbit));
  m.impl("dist",
  TORCH_FN(wrapper__dist));
  m.impl("atan2", TORCH_FN(wrapper_atan2));
  m.impl("atan2_", TORCH_FN(wrapper_atan2_));
  m.impl("fmod.Scalar",
  TORCH_FN(wrapper_Scalar_fmod_Scalar));
  m.impl("fmod.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_fmod_out_Scalar_out));
  m.impl("fmod_.Scalar",
  TORCH_FN(wrapper_Scalar_fmod__Scalar));
  m.impl("fmod.Tensor", TORCH_FN(wrapper_fmod_Tensor));
  m.impl("fmod_.Tensor", TORCH_FN(wrapper_fmod__Tensor));
  m.impl("hypot", TORCH_FN(wrapper_hypot));
  m.impl("hypot_", TORCH_FN(wrapper_hypot_));
  m.impl("igamma", TORCH_FN(wrapper_igamma));
  m.impl("igamma_", TORCH_FN(wrapper_igamma_));
  m.impl("igammac", TORCH_FN(wrapper_igammac));
  m.impl("igammac_", TORCH_FN(wrapper_igammac_));
  m.impl("nextafter", TORCH_FN(wrapper_nextafter));
  m.impl("nextafter_", TORCH_FN(wrapper_nextafter_));
  m.impl("remainder.Scalar",
  TORCH_FN(wrapper_Scalar_remainder_Scalar));
  m.impl("remainder.Scalar_out",
  TORCH_FN(wrapper_Scalar_out_remainder_out_Scalar_out));
  m.impl("remainder_.Scalar",
  TORCH_FN(wrapper_Scalar_remainder__Scalar));
  m.impl("remainder.Tensor", TORCH_FN(wrapper_remainder_Tensor));
  m.impl("remainder_.Tensor", TORCH_FN(wrapper_remainder__Tensor));
  m.impl("fmin", TORCH_FN(wrapper_fmin));
  m.impl("fmax", TORCH_FN(wrapper_fmax));
  m.impl("maximum", TORCH_FN(wrapper_maximum));
  m.impl("minimum", TORCH_FN(wrapper_minimum));
  m.impl("topk", TORCH_FN(wrapper_topk));
  m.impl("all", TORCH_FN(wrapper_all));
  m.impl("any", TORCH_FN(wrapper_any));
  m.impl("renorm", TORCH_FN(wrapper_renorm));
  m.impl("renorm_", TORCH_FN(wrapper_renorm_));
  m.impl("pow.Tensor_Tensor", TORCH_FN(wrapper_pow_Tensor_Tensor));
  m.impl("pow_.Tensor", TORCH_FN(wrapper_pow__Tensor));
  m.impl("pow.Scalar", TORCH_FN(wrapper_pow_Scalar));
  m.impl("pow.Tensor_Scalar", TORCH_FN(wrapper_pow_Tensor_Scalar));
  m.impl("pow_.Scalar", TORCH_FN(wrapper_pow__Scalar));
  m.impl("alias",
  TORCH_FN(wrapper__alias));
  m.impl("_convert_indices_from_coo_to_csr", TORCH_FN(wrapper__convert_indices_from_coo_to_csr));
  m.impl("l1_loss",
  TORCH_FN(wrapper__l1_loss));
  m.impl("l1_loss.out",
  TORCH_FN(wrapper_out_l1_loss_out_out));
  m.impl("l1_loss_backward",
  TORCH_FN(wrapper__l1_loss_backward));
  m.impl("nll_loss_forward", TORCH_FN(wrapper_nll_loss_forward));
  m.impl("nll_loss_backward", TORCH_FN(wrapper_nll_loss_backward));
  m.impl("smooth_l1_loss_backward",
  TORCH_FN(wrapper__smooth_l1_loss_backward));
  m.impl("huber_loss_backward",
  TORCH_FN(wrapper__huber_loss_backward));
  m.impl("soft_margin_loss",
  TORCH_FN(wrapper__soft_margin_loss));
  m.impl("soft_margin_loss.out",
  TORCH_FN(wrapper_out_soft_margin_loss_out_out));
  m.impl("soft_margin_loss_backward",
  TORCH_FN(wrapper__soft_margin_loss_backward));
  m.impl("soft_margin_loss_backward.grad_input",
  TORCH_FN(wrapper_grad_input_soft_margin_loss_backward_out_grad_input));
  m.impl("elu", TORCH_FN(wrapper_elu));
  m.impl("elu_", TORCH_FN(wrapper_elu_));
  m.impl("elu_backward", TORCH_FN(wrapper_elu_backward));
  m.impl("glu", TORCH_FN(wrapper_glu));
  m.impl("hardsigmoid", TORCH_FN(wrapper_hardsigmoid));
  m.impl("hardsigmoid_", TORCH_FN(wrapper_hardsigmoid_));
  m.impl("hardsigmoid_backward", TORCH_FN(wrapper_hardsigmoid_backward));
  m.impl("leaky_relu", TORCH_FN(wrapper_leaky_relu));
  m.impl("leaky_relu_", TORCH_FN(wrapper_leaky_relu_));
  m.impl("leaky_relu_backward", TORCH_FN(wrapper_leaky_relu_backward));
  m.impl("rrelu_with_noise_backward",
  TORCH_FN(wrapper__rrelu_with_noise_backward));
  m.impl("softplus", TORCH_FN(wrapper_softplus));
  m.impl("softplus_backward", TORCH_FN(wrapper_softplus_backward));
  m.impl("softshrink", TORCH_FN(wrapper_softshrink));
  m.impl("softshrink_backward", TORCH_FN(wrapper_softshrink_backward));
  m.impl("adaptive_max_pool2d", TORCH_FN(wrapper_adaptive_max_pool2d));
  m.impl("adaptive_max_pool2d_backward", TORCH_FN(wrapper_adaptive_max_pool2d_backward));
  m.impl("adaptive_max_pool3d", TORCH_FN(wrapper_adaptive_max_pool3d));
  m.impl("adaptive_max_pool3d_backward", TORCH_FN(wrapper_adaptive_max_pool3d_backward));
  m.impl("avg_pool2d", TORCH_FN(wrapper_avg_pool2d));
  m.impl("avg_pool2d_backward", TORCH_FN(wrapper_avg_pool2d_backward));
  m.impl("avg_pool3d", TORCH_FN(wrapper_avg_pool3d));
  m.impl("avg_pool3d_backward", TORCH_FN(wrapper_avg_pool3d_backward));
  m.impl("fractional_max_pool2d", TORCH_FN(wrapper_fractional_max_pool2d));
  m.impl("max_pool2d_with_indices", TORCH_FN(wrapper_max_pool2d_with_indices));
  m.impl("max_pool2d_with_indices_backward", TORCH_FN(wrapper_max_pool2d_with_indices_backward));
  m.impl("reflection_pad1d", TORCH_FN(wrapper_reflection_pad1d));
  m.impl("reflection_pad1d_backward", TORCH_FN(wrapper_reflection_pad1d_backward));
  m.impl("reflection_pad3d", TORCH_FN(wrapper_reflection_pad3d));
  m.impl("reflection_pad3d_backward", TORCH_FN(wrapper_reflection_pad3d_backward));
  m.impl("replication_pad1d", TORCH_FN(wrapper_replication_pad1d));
  m.impl("replication_pad1d_backward", TORCH_FN(wrapper_replication_pad1d_backward));
  m.impl("replication_pad2d", TORCH_FN(wrapper_replication_pad2d));
  m.impl("replication_pad3d", TORCH_FN(wrapper_replication_pad3d));
  m.impl("upsample_linear1d.vec",
  TORCH_FN(wrapper_vec_upsample_linear1d_vec));
  m.impl("upsample_linear1d_backward.vec",
  TORCH_FN(wrapper_vec_upsample_linear1d_backward_vec));
  m.impl("upsample_bilinear2d.vec",
  TORCH_FN(wrapper_vec_upsample_bilinear2d_vec));
  m.impl("upsample_bilinear2d_backward.vec",
  TORCH_FN(wrapper_vec_upsample_bilinear2d_backward_vec));
  m.impl("upsample_trilinear3d.vec",
  TORCH_FN(wrapper_vec_upsample_trilinear3d_vec));
  m.impl("upsample_trilinear3d_backward.vec",
  TORCH_FN(wrapper_vec_upsample_trilinear3d_backward_vec));
  m.impl("upsample_bicubic2d.vec",
  TORCH_FN(wrapper_vec_upsample_bicubic2d_vec));
  m.impl("upsample_bicubic2d_backward.vec",
  TORCH_FN(wrapper_vec_upsample_bicubic2d_backward_vec));
  m.impl("upsample_nearest1d.vec",
  TORCH_FN(wrapper_vec_upsample_nearest1d_vec));
  m.impl("upsample_nearest1d_backward.vec",
  TORCH_FN(wrapper_vec_upsample_nearest1d_backward_vec));
  m.impl("upsample_nearest2d.vec",
  TORCH_FN(wrapper_vec_upsample_nearest2d_vec));
  m.impl("upsample_nearest2d_backward.vec",
  TORCH_FN(wrapper_vec_upsample_nearest2d_backward_vec));
  m.impl("upsample_linear1d", TORCH_FN(wrapper_upsample_linear1d));
  m.impl("upsample_linear1d_backward", TORCH_FN(wrapper_upsample_linear1d_backward));
  m.impl("upsample_bilinear2d", TORCH_FN(wrapper_upsample_bilinear2d));
  m.impl("upsample_bilinear2d_backward", TORCH_FN(wrapper_upsample_bilinear2d_backward));
  m.impl("upsample_bicubic2d", TORCH_FN(wrapper_upsample_bicubic2d));
  m.impl("upsample_bicubic2d_backward", TORCH_FN(wrapper_upsample_bicubic2d_backward));
  m.impl("upsample_trilinear3d", TORCH_FN(wrapper_upsample_trilinear3d));
  m.impl("upsample_trilinear3d_backward", TORCH_FN(wrapper_upsample_trilinear3d_backward));
  m.impl("upsample_nearest1d", TORCH_FN(wrapper_upsample_nearest1d));
  m.impl("upsample_nearest1d_backward", TORCH_FN(wrapper_upsample_nearest1d_backward));
  m.impl("upsample_nearest2d", TORCH_FN(wrapper_upsample_nearest2d));
  m.impl("upsample_nearest2d_backward", TORCH_FN(wrapper_upsample_nearest2d_backward));
  m.impl("upsample_nearest3d", TORCH_FN(wrapper_upsample_nearest3d));
  m.impl("upsample_nearest3d_backward", TORCH_FN(wrapper_upsample_nearest3d_backward));
  m.impl("sigmoid_backward", TORCH_FN(wrapper_sigmoid_backward));
  m.impl("logit_backward", TORCH_FN(wrapper_logit_backward));
  m.impl("tanh_backward", TORCH_FN(wrapper_tanh_backward));
  m.impl("slow_conv_transpose2d", TORCH_FN(wrapper_slow_conv_transpose2d));
  m.impl("isposinf", TORCH_FN(wrapper_isposinf));
  m.impl("isneginf", TORCH_FN(wrapper_isneginf));
  m.impl("special_entr", TORCH_FN(wrapper_special_entr));
  m.impl("special_ndtri", TORCH_FN(wrapper_special_ndtri));
  m.impl("special_erfcx", TORCH_FN(wrapper_special_erfcx));
  m.impl("special_xlog1py", TORCH_FN(wrapper_special_xlog1py));
  m.impl("special_xlog1py.self_scalar",
  TORCH_FN(wrapper_self_scalar_special_xlog1py_self_scalar));
  m.impl("special_xlog1py.self_scalar_out",
  TORCH_FN(wrapper_self_scalar_out_special_xlog1py_out_self_scalar_out));
  m.impl("special_xlog1py.other_scalar",
  TORCH_FN(wrapper_other_scalar_special_xlog1py_other_scalar));
  m.impl("special_xlog1py.other_scalar_out",
  TORCH_FN(wrapper_other_scalar_out_special_xlog1py_out_other_scalar_out));
  m.impl("special_zeta", TORCH_FN(wrapper_special_zeta));
  m.impl("special_zeta.self_scalar",
  TORCH_FN(wrapper_self_scalar_special_zeta_self_scalar));
  m.impl("special_zeta.self_scalar_out",
  TORCH_FN(wrapper_self_scalar_out_special_zeta_out_self_scalar_out));
  m.impl("special_zeta.other_scalar",
  TORCH_FN(wrapper_other_scalar_special_zeta_other_scalar));
  m.impl("special_zeta.other_scalar_out",
  TORCH_FN(wrapper_other_scalar_out_special_zeta_out_other_scalar_out));
  m.impl("special_i0e", TORCH_FN(wrapper_special_i0e));
  m.impl("special_i1", TORCH_FN(wrapper_special_i1));
  m.impl("special_i1e", TORCH_FN(wrapper_special_i1e));
  m.impl("linalg_lstsq",
  TORCH_FN(wrapper__linalg_lstsq));
  m.impl("linalg_inv_ex",
  TORCH_FN(wrapper__linalg_inv_ex));
  m.impl("linalg_inv_ex.inverse",
  TORCH_FN(wrapper_inverse_linalg_inv_ex_out_inverse));
  m.impl("linalg_qr",
  TORCH_FN(wrapper__linalg_qr));
  m.impl("linalg_qr.out",
  TORCH_FN(wrapper_out_linalg_qr_out_out));
}

} // anonymous namespace

namespace compositeexplicitautograd {


at::Tensor _fw_primal(const at::Tensor & self, int64_t level) {
return wrapper___fw_primal(self, level);
}

at::Tensor abs(const at::Tensor & self) {
return wrapper__abs(self);
}

at::Tensor & abs_(at::Tensor & self) {
return wrapper__abs_(self);
}

at::Tensor sgn(const at::Tensor & self) {
return wrapper_sgn(self);
}

at::Tensor & sgn_(at::Tensor & self) {
return wrapper_sgn_(self);
}

at::Tensor _conj(const at::Tensor & self) {
return wrapper___conj(self);
}

at::Tensor _conj_physical(const at::Tensor & self) {
return wrapper___conj_physical(self);
}

at::Tensor & conj_physical_(at::Tensor & self) {
return wrapper__conj_physical_(self);
}

at::Tensor _neg_view(const at::Tensor & self) {
return wrapper___neg_view(self);
}

at::Tensor acos(const at::Tensor & self) {
return wrapper_acos(self);
}

at::Tensor & acos_(at::Tensor & self) {
return wrapper_acos_(self);
}

at::Tensor add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_add_Tensor(self, other, alpha);
}

at::Tensor & add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_add__Tensor(self, other, alpha);
}

at::Tensor add(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_add_Scalar(self, other, alpha);
}

at::Tensor & add_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_add__Scalar(self, other, alpha);
}

at::Tensor addmv(const at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_addmv(self, mat, vec, beta, alpha);
}

at::Tensor & addmv_(at::Tensor & self, const at::Tensor & mat, const at::Tensor & vec, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_addmv_(self, mat, vec, beta, alpha);
}

at::Tensor & addr_(at::Tensor & self, const at::Tensor & vec1, const at::Tensor & vec2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper__addr_(self, vec1, vec2, beta, alpha);
}

at::Tensor affine_grid_generator(const at::Tensor & theta, at::IntArrayRef size, bool align_corners) {
return wrapper__affine_grid_generator(theta, size, align_corners);
}

at::Tensor all(const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_all_dim(self, dim, keepdim);
}

at::Tensor any(const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_any_dim(self, dim, keepdim);
}

at::Tensor argmax(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_argmax(self, dim, keepdim);
}

at::Tensor argmin(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_argmin(self, dim, keepdim);
}

at::Tensor acosh(const at::Tensor & self) {
return wrapper_acosh(self);
}

at::Tensor & acosh_(at::Tensor & self) {
return wrapper_acosh_(self);
}

at::Tensor asinh(const at::Tensor & self) {
return wrapper_asinh(self);
}

at::Tensor & asinh_(at::Tensor & self) {
return wrapper_asinh_(self);
}

at::Tensor atanh(const at::Tensor & self) {
return wrapper_atanh(self);
}

at::Tensor & atanh_(at::Tensor & self) {
return wrapper_atanh_(self);
}

const at::Tensor & as_strided_(const at::Tensor & self, at::IntArrayRef size, at::IntArrayRef stride, c10::optional<int64_t> storage_offset) {
return wrapper__as_strided_(self, size, stride, storage_offset);
}

at::Tensor asin(const at::Tensor & self) {
return wrapper_asin(self);
}

at::Tensor & asin_(at::Tensor & self) {
return wrapper_asin_(self);
}

at::Tensor atan(const at::Tensor & self) {
return wrapper_atan(self);
}

at::Tensor & atan_(at::Tensor & self) {
return wrapper_atan_(self);
}

at::Tensor bernoulli(const at::Tensor & self, c10::optional<at::Generator> generator) {
return wrapper__bernoulli(self, generator);
}

at::Tensor binary_cross_entropy_with_logits(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & pos_weight, int64_t reduction) {
return wrapper__binary_cross_entropy_with_logits(self, target, weight, pos_weight, reduction);
}

at::Tensor bitwise_not(const at::Tensor & self) {
return wrapper_bitwise_not(self);
}

at::Tensor & bitwise_not_(at::Tensor & self) {
return wrapper_bitwise_not_(self);
}

at::Tensor copysign(const at::Tensor & self, const at::Tensor & other) {
return wrapper_copysign_Tensor(self, other);
}

at::Tensor & copysign_(at::Tensor & self, const at::Tensor & other) {
return wrapper_copysign__Tensor(self, other);
}

at::Tensor copysign(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_copysign_Scalar(self, other);
}

at::Tensor & copysign_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_copysign_out_Scalar_out(self, other, out);
}

at::Tensor & copysign_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_copysign_out_Scalar_out(self, other, out);
}

at::Tensor & copysign_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_copysign__Scalar(self, other);
}

at::Tensor cat(at::TensorList tensors, int64_t dim) {
return wrapper__cat(tensors, dim);
}

at::Tensor & cat_out(at::Tensor & out, at::TensorList tensors, int64_t dim) {
return wrapper_out_cat_out_out(tensors, dim, out);
}

at::Tensor & cat_outf(at::TensorList tensors, int64_t dim, at::Tensor & out) {
return wrapper_out_cat_out_out(tensors, dim, out);
}

at::Tensor ceil(const at::Tensor & self) {
return wrapper_ceil(self);
}

at::Tensor & ceil_(at::Tensor & self) {
return wrapper_ceil_(self);
}

at::Tensor clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_clamp(self, min, max);
}

at::Tensor & clamp_(at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
return wrapper_clamp_(self, min, max);
}

at::Tensor & clamp_(at::Tensor & self, const c10::optional<at::Tensor> & min, const c10::optional<at::Tensor> & max) {
return wrapper_Tensor_clamp__Tensor(self, min, max);
}

at::Tensor clamp_max(const at::Tensor & self, const at::Scalar & max) {
return wrapper__clamp_max(self, max);
}

at::Tensor & clamp_max_(at::Tensor & self, const at::Scalar & max) {
return wrapper__clamp_max_(self, max);
}

at::Tensor clamp_max(const at::Tensor & self, const at::Tensor & max) {
return wrapper_Tensor_clamp_max_Tensor(self, max);
}

at::Tensor & clamp_max_(at::Tensor & self, const at::Tensor & max) {
return wrapper_Tensor_clamp_max__Tensor(self, max);
}

at::Tensor clamp_min(const at::Tensor & self, const at::Scalar & min) {
return wrapper__clamp_min(self, min);
}

at::Tensor & clamp_min_(at::Tensor & self, const at::Scalar & min) {
return wrapper__clamp_min_(self, min);
}

at::Tensor clamp_min(const at::Tensor & self, const at::Tensor & min) {
return wrapper_Tensor_clamp_min_Tensor(self, min);
}

at::Tensor & clamp_min_(at::Tensor & self, const at::Tensor & min) {
return wrapper_Tensor_clamp_min__Tensor(self, min);
}

at::Tensor complex(const at::Tensor & real, const at::Tensor & imag) {
return wrapper__complex(real, imag);
}

at::Tensor polar(const at::Tensor & abs, const at::Tensor & angle) {
return wrapper__polar(abs, angle);
}

at::Tensor constant_pad_nd(const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
return wrapper__constant_pad_nd(self, pad, value);
}

at::Tensor convolution_overrideable(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
return wrapper__convolution_overrideable(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> convolution_backward_overrideable(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
return wrapper__convolution_backward_overrideable(grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}

at::Tensor conv_tbc(const at::Tensor & self, const at::Tensor & weight, const at::Tensor & bias, int64_t pad) {
return wrapper__conv_tbc(self, weight, bias, pad);
}

at::Tensor & copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
return wrapper__copy_(self, src, non_blocking);
}

at::Tensor cos(const at::Tensor & self) {
return wrapper_cos(self);
}

at::Tensor & cos_(at::Tensor & self) {
return wrapper_cos_(self);
}

at::Tensor cosh(const at::Tensor & self) {
return wrapper_cosh(self);
}

at::Tensor & cosh_(at::Tensor & self) {
return wrapper_cosh_(self);
}

at::Tensor count_nonzero(const at::Tensor & self, c10::optional<int64_t> dim) {
return wrapper__count_nonzero(self, dim);
}

::std::tuple<at::Tensor,at::Tensor> cummax(const at::Tensor & self, int64_t dim) {
return wrapper__cummax(self, dim);
}

::std::tuple<at::Tensor &,at::Tensor &> cummax_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, int64_t dim) {
return wrapper_out_cummax_out_out(self, dim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> cummax_outf(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
return wrapper_out_cummax_out_out(self, dim, values, indices);
}

::std::tuple<at::Tensor,at::Tensor> cummin(const at::Tensor & self, int64_t dim) {
return wrapper__cummin(self, dim);
}

::std::tuple<at::Tensor &,at::Tensor &> cummin_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, int64_t dim) {
return wrapper_out_cummin_out_out(self, dim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> cummin_outf(const at::Tensor & self, int64_t dim, at::Tensor & values, at::Tensor & indices) {
return wrapper_out_cummin_out_out(self, dim, values, indices);
}

at::Tensor cumprod(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_cumprod(self, dim, dtype);
}

at::Tensor & cumprod_(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_cumprod_(self, dim, dtype);
}

at::Tensor cumsum(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_cumsum(self, dim, dtype);
}

at::Tensor & cumsum_(at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
return wrapper_cumsum_(self, dim, dtype);
}

at::Tensor diagonal(const at::Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) {
return wrapper__diagonal(self, offset, dim1, dim2);
}

at::Tensor diagonal_backward(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {
return wrapper__diagonal_backward(grad_output, input_sizes, offset, dim1, dim2);
}

at::Tensor div(const at::Tensor & self, const at::Tensor & other) {
return wrapper_div_Tensor(self, other);
}

at::Tensor & div_(at::Tensor & self, const at::Tensor & other) {
return wrapper_div__Tensor(self, other);
}

at::Tensor div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_div_Tensor_mode(self, other, rounding_mode);
}

at::Tensor & div_(at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_div__Tensor_mode(self, other, rounding_mode);
}

at::Tensor div(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_div_Scalar(self, other);
}

at::Tensor & div_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_div__Scalar(self, other);
}

at::Tensor div(const at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Scalar_mode_div_Scalar_mode(self, other, rounding_mode);
}

at::Tensor & div_(at::Tensor & self, const at::Scalar & other, c10::optional<c10::string_view> rounding_mode) {
return wrapper_Scalar_mode_div__Scalar_mode(self, other, rounding_mode);
}

at::Tensor & dot_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & tensor) {
return wrapper_out_dot_out_out(self, tensor, out);
}

at::Tensor & dot_outf(const at::Tensor & self, const at::Tensor & tensor, at::Tensor & out) {
return wrapper_out_dot_out_out(self, tensor, out);
}

at::Tensor & vdot_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
return wrapper_out_vdot_out_out(self, other, out);
}

at::Tensor & vdot_outf(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_out_vdot_out_out(self, other, out);
}

at::Tensor embedding(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
return wrapper__embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
}

at::Tensor erf(const at::Tensor & self) {
return wrapper_erf(self);
}

at::Tensor & erf_(at::Tensor & self) {
return wrapper_erf_(self);
}

at::Tensor erfc(const at::Tensor & self) {
return wrapper_erfc(self);
}

at::Tensor & erfc_(at::Tensor & self) {
return wrapper_erfc_(self);
}

at::Tensor exp(const at::Tensor & self) {
return wrapper_exp(self);
}

at::Tensor & exp_(at::Tensor & self) {
return wrapper_exp_(self);
}

at::Tensor exp2(const at::Tensor & self) {
return wrapper_exp2(self);
}

at::Tensor & exp2_(at::Tensor & self) {
return wrapper_exp2_(self);
}

at::Tensor expm1(const at::Tensor & self) {
return wrapper_expm1(self);
}

at::Tensor & expm1_(at::Tensor & self) {
return wrapper_expm1_(self);
}

at::Tensor expand(const at::Tensor & self, at::IntArrayRef size, bool implicit) {
return wrapper__expand(self, size, implicit);
}

at::Tensor floor(const at::Tensor & self) {
return wrapper_floor(self);
}

at::Tensor & floor_(at::Tensor & self) {
return wrapper_floor_(self);
}

at::Tensor frac(const at::Tensor & self) {
return wrapper_frac(self);
}

at::Tensor & frac_(at::Tensor & self) {
return wrapper_frac_(self);
}

at::Tensor gcd(const at::Tensor & self, const at::Tensor & other) {
return wrapper_gcd(self, other);
}

at::Tensor & gcd_(at::Tensor & self, const at::Tensor & other) {
return wrapper_gcd_(self, other);
}

at::Tensor lcm(const at::Tensor & self, const at::Tensor & other) {
return wrapper_lcm(self, other);
}

at::Tensor & lcm_(at::Tensor & self, const at::Tensor & other) {
return wrapper_lcm_(self, other);
}

at::Tensor _grid_sampler_2d_cpu_fallback(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
return wrapper___grid_sampler_2d_cpu_fallback(input, grid, interpolation_mode, padding_mode, align_corners);
}

at::Tensor & index_copy_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & source) {
return wrapper__index_copy_(self, dim, index, source);
}

at::Tensor & index_put_(at::Tensor & self, const c10::List<c10::optional<at::Tensor>> & indices, const at::Tensor & values, bool accumulate) {
return wrapper__index_put_(self, indices, values, accumulate);
}

at::Tensor inverse(const at::Tensor & self) {
return wrapper__inverse(self);
}

at::Tensor & inverse_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_inverse_out_out(self, out);
}

at::Tensor & inverse_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_inverse_out_out(self, out);
}

at::Tensor isin(const at::Tensor & elements, const at::Tensor & test_elements, bool assume_unique, bool invert) {
return wrapper_isin_Tensor_Tensor(elements, test_elements, assume_unique, invert);
}

at::Tensor isin(const at::Tensor & elements, const at::Scalar & test_element, bool assume_unique, bool invert) {
return wrapper_isin_Tensor_Scalar(elements, test_element, assume_unique, invert);
}

at::Tensor isin(const at::Scalar & element, const at::Tensor & test_elements, bool assume_unique, bool invert) {
return wrapper_isin_Scalar_Tensor(element, test_elements, assume_unique, invert);
}

at::Tensor kl_div(const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
return wrapper__kl_div(self, target, reduction, log_target);
}

::std::tuple<at::Tensor,at::Tensor> kthvalue(const at::Tensor & self, int64_t k, int64_t dim, bool keepdim) {
return wrapper__kthvalue(self, k, dim, keepdim);
}

at::Tensor nan_to_num(const at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
return wrapper__nan_to_num(self, nan, posinf, neginf);
}

at::Tensor & nan_to_num_(at::Tensor & self, c10::optional<double> nan, c10::optional<double> posinf, c10::optional<double> neginf) {
return wrapper__nan_to_num_(self, nan, posinf, neginf);
}

at::Tensor log(const at::Tensor & self) {
return wrapper_log(self);
}

at::Tensor & log_(at::Tensor & self) {
return wrapper_log_(self);
}

at::Tensor log10(const at::Tensor & self) {
return wrapper_log10(self);
}

at::Tensor & log10_(at::Tensor & self) {
return wrapper_log10_(self);
}

at::Tensor log1p(const at::Tensor & self) {
return wrapper_log1p(self);
}

at::Tensor & log1p_(at::Tensor & self) {
return wrapper_log1p_(self);
}

at::Tensor log2(const at::Tensor & self) {
return wrapper_log2(self);
}

at::Tensor & log2_(at::Tensor & self) {
return wrapper_log2_(self);
}

at::Tensor logaddexp(const at::Tensor & self, const at::Tensor & other) {
return wrapper_logaddexp(self, other);
}

at::Tensor logaddexp2(const at::Tensor & self, const at::Tensor & other) {
return wrapper_logaddexp2(self, other);
}

at::Tensor xlogy(const at::Tensor & self, const at::Tensor & other) {
return wrapper_xlogy_Tensor(self, other);
}

at::Tensor & xlogy_(at::Tensor & self, const at::Tensor & other) {
return wrapper_xlogy__Tensor(self, other);
}

at::Tensor xlogy(const at::Scalar & self, const at::Tensor & other) {
return wrapper_Scalar_Self_xlogy_Scalar_Self(self, other);
}

at::Tensor & xlogy_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & other) {
return wrapper_OutScalar_Self_xlogy_out_OutScalar_Self(self, other, out);
}

at::Tensor & xlogy_outf(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_OutScalar_Self_xlogy_out_OutScalar_Self(self, other, out);
}

at::Tensor xlogy(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_Other_xlogy_Scalar_Other(self, other);
}

at::Tensor & xlogy_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_OutScalar_Other_xlogy_out_OutScalar_Other(self, other, out);
}

at::Tensor & xlogy_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_OutScalar_Other_xlogy_out_OutScalar_Other(self, other, out);
}

at::Tensor & xlogy_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_Other_xlogy__Scalar_Other(self, other);
}

at::Tensor logdet(const at::Tensor & self) {
return wrapper__logdet(self);
}

at::Tensor _log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper__log_softmax(self, dim, half_to_float);
}

at::Tensor _log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
return wrapper__log_softmax_backward_data(grad_output, output, dim, self);
}

at::Tensor logcumsumexp(const at::Tensor & self, int64_t dim) {
return wrapper__logcumsumexp(self, dim);
}

at::Tensor & logcumsumexp_out(at::Tensor & out, const at::Tensor & self, int64_t dim) {
return wrapper_out_logcumsumexp_out_out(self, dim, out);
}

at::Tensor & logcumsumexp_outf(const at::Tensor & self, int64_t dim, at::Tensor & out) {
return wrapper_out_logcumsumexp_out_out(self, dim, out);
}

at::Tensor logsumexp(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper__logsumexp(self, dim, keepdim);
}

at::Tensor & logsumexp_out(at::Tensor & out, const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper_out_logsumexp_out_out(self, dim, keepdim, out);
}

at::Tensor & logsumexp_outf(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, at::Tensor & out) {
return wrapper_out_logsumexp_out_out(self, dim, keepdim, out);
}

::std::tuple<at::Tensor,at::Tensor> aminmax(const at::Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
return wrapper_aminmax(self, dim, keepdim);
}

at::Tensor amax(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper__amax(self, dim, keepdim);
}

at::Tensor mean(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
return wrapper__mean(self, dtype);
}

at::Tensor mean(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_mean_dim(self, dim, keepdim, dtype);
}

::std::tuple<at::Tensor,at::Tensor> median(const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_dim_median_dim(self, dim, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> nanmedian(const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_dim_nanmedian_dim(self, dim, keepdim);
}

at::Tensor amin(const at::Tensor & self, at::IntArrayRef dim, bool keepdim) {
return wrapper__amin(self, dim, keepdim);
}

at::Tensor mkldnn_convolution(const at::Tensor & self, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups) {
return wrapper__mkldnn_convolution(self, weight, bias, padding, stride, dilation, groups);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor> mkldnn_convolution_backward(const at::Tensor & self, const at::Tensor & grad_output, const at::Tensor & weight, at::IntArrayRef padding, at::IntArrayRef stride, at::IntArrayRef dilation, int64_t groups, ::std::array<bool,3> output_mask) {
return wrapper__mkldnn_convolution_backward(self, grad_output, weight, padding, stride, dilation, groups, output_mask);
}

at::Tensor mm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_mm(self, mat2);
}

::std::tuple<at::Tensor &,at::Tensor &> mode_out(at::Tensor & values, at::Tensor & indices, const at::Tensor & self, int64_t dim, bool keepdim) {
return wrapper_values_mode_out_values(self, dim, keepdim, values, indices);
}

::std::tuple<at::Tensor &,at::Tensor &> mode_outf(const at::Tensor & self, int64_t dim, bool keepdim, at::Tensor & values, at::Tensor & indices) {
return wrapper_values_mode_out_values(self, dim, keepdim, values, indices);
}

at::Tensor mul(const at::Tensor & self, const at::Tensor & other) {
return wrapper_mul_Tensor(self, other);
}

at::Tensor & mul_(at::Tensor & self, const at::Tensor & other) {
return wrapper_mul__Tensor(self, other);
}

at::Tensor mul(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_mul_Scalar(self, other);
}

at::Tensor & mul_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_mul__Scalar(self, other);
}

at::Tensor & mv_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & vec) {
return wrapper_out_mv_out_out(self, vec, out);
}

at::Tensor & mv_outf(const at::Tensor & self, const at::Tensor & vec, at::Tensor & out) {
return wrapper_out_mv_out_out(self, vec, out);
}

at::Tensor mvlgamma(const at::Tensor & self, int64_t p) {
return wrapper__mvlgamma(self, p);
}

at::Tensor & mvlgamma_(at::Tensor & self, int64_t p) {
return wrapper__mvlgamma_(self, p);
}

at::Tensor narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
return wrapper__narrow_copy(self, dim, start, length);
}

at::Tensor _nnpack_spatial_convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef padding, at::IntArrayRef stride) {
return wrapper___nnpack_spatial_convolution(input, weight, bias, padding, stride);
}

at::Tensor _euclidean_dist(const at::Tensor & x1, const at::Tensor & x2) {
return wrapper___euclidean_dist(x1, x2);
}

at::Tensor permute(const at::Tensor & self, at::IntArrayRef dims) {
return wrapper__permute(self, dims);
}

bool is_pinned(const at::Tensor & self, c10::optional<at::Device> device) {
return wrapper__is_pinned(self, device);
}

at::Tensor rad2deg(const at::Tensor & self) {
return wrapper__rad2deg(self);
}

at::Tensor & rad2deg_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_rad2deg_out_out(self, out);
}

at::Tensor & rad2deg_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_rad2deg_out_out(self, out);
}

at::Tensor & rad2deg_(at::Tensor & self) {
return wrapper__rad2deg_(self);
}

at::Tensor deg2rad(const at::Tensor & self) {
return wrapper__deg2rad(self);
}

at::Tensor & deg2rad_out(at::Tensor & out, const at::Tensor & self) {
return wrapper_out_deg2rad_out_out(self, out);
}

at::Tensor & deg2rad_outf(const at::Tensor & self, at::Tensor & out) {
return wrapper_out_deg2rad_out_out(self, out);
}

at::Tensor & deg2rad_(at::Tensor & self) {
return wrapper__deg2rad_(self);
}

at::Tensor reciprocal(const at::Tensor & self) {
return wrapper_reciprocal(self);
}

at::Tensor & reciprocal_(at::Tensor & self) {
return wrapper_reciprocal_(self);
}

at::Tensor neg(const at::Tensor & self) {
return wrapper_neg(self);
}

at::Tensor & neg_(at::Tensor & self) {
return wrapper_neg_(self);
}

at::Tensor repeat(const at::Tensor & self, at::IntArrayRef repeats) {
return wrapper__repeat(self, repeats);
}

at::Tensor round(const at::Tensor & self) {
return wrapper_round(self);
}

at::Tensor & round_(at::Tensor & self) {
return wrapper_round_(self);
}

at::Tensor gelu(const at::Tensor & self) {
return wrapper_gelu(self);
}

at::Tensor gelu_backward(const at::Tensor & grad, const at::Tensor & self) {
return wrapper_gelu_backward(grad, self);
}

at::Tensor hardshrink(const at::Tensor & self, const at::Scalar & lambd) {
return wrapper_hardshrink(self, lambd);
}

at::Tensor hardshrink_backward(const at::Tensor & grad_out, const at::Tensor & self, const at::Scalar & lambd) {
return wrapper_hardshrink_backward(grad_out, self, lambd);
}

at::Tensor rsqrt(const at::Tensor & self) {
return wrapper_rsqrt(self);
}

at::Tensor & rsqrt_(at::Tensor & self) {
return wrapper_rsqrt_(self);
}

at::Tensor select(const at::Tensor & self, int64_t dim, int64_t index) {
return wrapper_int_select_int(self, dim, index);
}

at::Tensor select_backward(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t index) {
return wrapper__select_backward(grad_output, input_sizes, dim, index);
}

at::Tensor celu(const at::Tensor & self, const at::Scalar & alpha) {
return wrapper__celu(self, alpha);
}

at::Tensor & celu_(at::Tensor & self, const at::Scalar & alpha) {
return wrapper__celu_(self, alpha);
}

at::Tensor silu(const at::Tensor & self) {
return wrapper_silu(self);
}

at::Tensor & silu_(at::Tensor & self) {
return wrapper_silu_(self);
}

at::Tensor silu_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper_silu_backward(grad_output, self);
}

at::Tensor mish(const at::Tensor & self) {
return wrapper_mish(self);
}

at::Tensor & mish_(at::Tensor & self) {
return wrapper_mish_(self);
}

at::Tensor sigmoid(const at::Tensor & self) {
return wrapper_sigmoid(self);
}

at::Tensor & sigmoid_(at::Tensor & self) {
return wrapper_sigmoid_(self);
}

at::Tensor sin(const at::Tensor & self) {
return wrapper_sin(self);
}

at::Tensor & sin_(at::Tensor & self) {
return wrapper_sin_(self);
}

at::Tensor sinc(const at::Tensor & self) {
return wrapper_sinc(self);
}

at::Tensor & sinc_(at::Tensor & self) {
return wrapper_sinc_(self);
}

at::Tensor sinh(const at::Tensor & self) {
return wrapper_sinh(self);
}

at::Tensor & sinh_(at::Tensor & self) {
return wrapper_sinh_(self);
}

at::Tensor detach(const at::Tensor & self) {
return wrapper__detach(self);
}

at::Tensor & detach_(at::Tensor & self) {
return wrapper__detach_(self);
}

at::Tensor slice(const at::Tensor & self, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
return wrapper_Tensor_slice_Tensor(self, dim, start, end, step);
}

at::Tensor slice_backward(const at::Tensor & grad_output, at::IntArrayRef input_sizes, int64_t dim, int64_t start, int64_t end, int64_t step) {
return wrapper__slice_backward(grad_output, input_sizes, dim, start, end, step);
}

::std::tuple<at::Tensor,at::Tensor> slogdet(const at::Tensor & self) {
return wrapper__slogdet(self);
}

at::Tensor _softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
return wrapper__softmax(self, dim, half_to_float);
}

at::Tensor _softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, const at::Tensor & self) {
return wrapper__softmax_backward_data(grad_output, output, dim, self);
}

::std::vector<at::Tensor> unsafe_split(const at::Tensor & self, int64_t split_size, int64_t dim) {
return wrapper_Tensor_unsafe_split_Tensor(self, split_size, dim);
}

::std::vector<at::Tensor> split(const at::Tensor & self, int64_t split_size, int64_t dim) {
return wrapper_Tensor_split_Tensor(self, split_size, dim);
}

::std::vector<at::Tensor> unsafe_split_with_sizes(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
return wrapper__unsafe_split_with_sizes(self, split_sizes, dim);
}

::std::vector<at::Tensor> split_with_sizes(const at::Tensor & self, at::IntArrayRef split_sizes, int64_t dim) {
return wrapper__split_with_sizes(self, split_sizes, dim);
}

at::Tensor squeeze(const at::Tensor & self) {
return wrapper__squeeze(self);
}

at::Tensor & squeeze_(at::Tensor & self) {
return wrapper__squeeze_(self);
}

at::Tensor squeeze(const at::Tensor & self, int64_t dim) {
return wrapper_dim_squeeze_dim(self, dim);
}

at::Tensor & squeeze_(at::Tensor & self, int64_t dim) {
return wrapper_dim_squeeze__dim(self, dim);
}

at::Tensor stack(at::TensorList tensors, int64_t dim) {
return wrapper__stack(tensors, dim);
}

at::Tensor & stack_out(at::Tensor & out, at::TensorList tensors, int64_t dim) {
return wrapper_out_stack_out_out(tensors, dim, out);
}

at::Tensor & stack_outf(at::TensorList tensors, int64_t dim, at::Tensor & out) {
return wrapper_out_stack_out_out(tensors, dim, out);
}

at::Tensor _stack(at::TensorList tensors, int64_t dim) {
return wrapper___stack(tensors, dim);
}

at::Tensor & _stack_out(at::Tensor & out, at::TensorList tensors, int64_t dim) {
return wrapper_out__stack_out_out(tensors, dim, out);
}

at::Tensor & _stack_outf(at::TensorList tensors, int64_t dim, at::Tensor & out) {
return wrapper_out__stack_out_out(tensors, dim, out);
}

at::Tensor sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
return wrapper__sum(self, dtype);
}

at::Tensor sum(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_sum_dim_IntList(self, dim, keepdim, dtype);
}

at::Tensor sqrt(const at::Tensor & self) {
return wrapper_sqrt(self);
}

at::Tensor & sqrt_(at::Tensor & self) {
return wrapper_sqrt_(self);
}

at::Tensor prod(const at::Tensor & self, int64_t dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
return wrapper_prod_dim_int(self, dim, keepdim, dtype);
}

at::Tensor t(const at::Tensor & self) {
return wrapper__t(self);
}

at::Tensor & t_(at::Tensor & self) {
return wrapper__t_(self);
}

at::Tensor tan(const at::Tensor & self) {
return wrapper_tan(self);
}

at::Tensor & tan_(at::Tensor & self) {
return wrapper_tan_(self);
}

at::Tensor tanh(const at::Tensor & self) {
return wrapper_tanh(self);
}

at::Tensor & tanh_(at::Tensor & self) {
return wrapper_tanh_(self);
}

at::Tensor threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
return wrapper_threshold(self, threshold, value);
}

at::Tensor & threshold_(at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
return wrapper_threshold_(self, threshold, value);
}

at::Tensor threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
return wrapper_threshold_backward(grad_output, self, threshold);
}

at::Tensor transpose(const at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper_int_transpose_int(self, dim0, dim1);
}

at::Tensor & transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
return wrapper__transpose_(self, dim0, dim1);
}

at::Tensor rot90(const at::Tensor & self, int64_t k, at::IntArrayRef dims) {
return wrapper__rot90(self, k, dims);
}

at::Tensor _trilinear(const at::Tensor & i1, const at::Tensor & i2, const at::Tensor & i3, at::IntArrayRef expand1, at::IntArrayRef expand2, at::IntArrayRef expand3, at::IntArrayRef sumdim, int64_t unroll_dim) {
return wrapper___trilinear(i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}

at::Tensor trunc(const at::Tensor & self) {
return wrapper_trunc(self);
}

at::Tensor & trunc_(at::Tensor & self) {
return wrapper_trunc_(self);
}

at::Tensor _unsafe_view(const at::Tensor & self, at::IntArrayRef size) {
return wrapper___unsafe_view(self, size);
}

at::Tensor unsqueeze(const at::Tensor & self, int64_t dim) {
return wrapper__unsqueeze(self, dim);
}

at::Tensor & unsqueeze_(at::Tensor & self, int64_t dim) {
return wrapper__unsqueeze_(self, dim);
}

at::Tensor _sparse_sum(const at::Tensor & self, at::IntArrayRef dim) {
return wrapper_dim__sparse_sum_dim(self, dim);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::ScalarType dtype) {
return wrapper_ScalarOpt_dtype_norm_ScalarOpt_dtype(self, p, dtype);
}

at::Tensor norm(const at::Tensor & self, const at::Scalar & p) {
return wrapper_Scalar_norm_Scalar(self, p);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
return wrapper_norm_ScalarOpt_dim_dtype(self, p, dim, keepdim, dtype);
}

at::Tensor norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
return wrapper_norm_ScalarOpt_dim(self, p, dim, keepdim);
}

::std::tuple<at::Tensor,at::Tensor> frexp(const at::Tensor & self) {
return wrapper_Tensor_frexp_Tensor(self);
}

at::Tensor clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__clone(self, memory_format);
}

const at::Tensor & resize_as_(const at::Tensor & self, const at::Tensor & the_template, c10::optional<at::MemoryFormat> memory_format) {
return wrapper__resize_as_(self, the_template, memory_format);
}

at::Tensor sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_sub_Tensor(self, other, alpha);
}

at::Tensor & sub_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_sub__Tensor(self, other, alpha);
}

at::Tensor sub(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_sub_Scalar(self, other, alpha);
}

at::Tensor & sub_(at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_sub__Scalar(self, other, alpha);
}

at::Tensor heaviside(const at::Tensor & self, const at::Tensor & values) {
return wrapper_heaviside(self, values);
}

at::Tensor & heaviside_(at::Tensor & self, const at::Tensor & values) {
return wrapper_heaviside_(self, values);
}

at::Tensor rsub(const at::Tensor & self, const at::Scalar & other, const at::Scalar & alpha) {
return wrapper_Scalar_rsub_Scalar(self, other, alpha);
}

at::Tensor _sparse_addmm(const at::Tensor & self, const at::Tensor & sparse, const at::Tensor & dense, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper___sparse_addmm(self, sparse, dense, beta, alpha);
}

at::Tensor addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_addmm(self, mat1, mat2, beta, alpha);
}

at::Tensor & addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_addmm_(self, mat1, mat2, beta, alpha);
}

::std::vector<at::Tensor> unbind(const at::Tensor & self, int64_t dim) {
return wrapper_int_unbind_int(self, dim);
}

at::Tensor _to_copy(const at::Tensor & self, at::TensorOptions options, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
return wrapper___to_copy(self, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), non_blocking, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format));
}

at::Tensor _to_copy(const at::Tensor & self, c10::optional<at::ScalarType> dtype, c10::optional<at::Layout> layout, c10::optional<at::Device> device, c10::optional<bool> pin_memory, bool non_blocking, c10::optional<at::MemoryFormat> memory_format) {
return wrapper___to_copy(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}

::std::tuple<at::Tensor,at::Tensor> _pack_padded_sequence(const at::Tensor & input, const at::Tensor & lengths, bool batch_first) {
return wrapper___pack_padded_sequence(input, lengths, batch_first);
}

at::Tensor view(const at::Tensor & self, at::ScalarType dtype) {
return wrapper_dtype_view_dtype(self, dtype);
}

at::Tensor scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_scatter_src(self, dim, index, src);
}

at::Tensor & scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_scatter__src(self, dim, index, src);
}

at::Tensor scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_scatter_value(self, dim, index, value);
}

at::Tensor & scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value) {
return wrapper_scatter__value(self, dim, index, value);
}

at::Tensor scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
return wrapper_scatter_reduce(self, dim, index, src, reduce);
}

at::Tensor & scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce) {
return wrapper_scatter__reduce(self, dim, index, src, reduce);
}

at::Tensor scatter(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
return wrapper_scatter_value_reduce(self, dim, index, value, reduce);
}

at::Tensor & scatter_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Scalar & value, c10::string_view reduce) {
return wrapper_scatter__value_reduce(self, dim, index, value, reduce);
}

at::Tensor scatter_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_scatter_add(self, dim, index, src);
}

at::Tensor & scatter_add_(at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
return wrapper_scatter_add_(self, dim, index, src);
}

at::Tensor eq(const at::Tensor & self, const at::Scalar & other) {
return wrapper_eq_Scalar(self, other);
}

at::Tensor & eq_(at::Tensor & self, const at::Scalar & other) {
return wrapper_eq__Scalar(self, other);
}

at::Tensor eq(const at::Tensor & self, const at::Tensor & other) {
return wrapper_eq_Tensor(self, other);
}

at::Tensor & eq_(at::Tensor & self, const at::Tensor & other) {
return wrapper_eq__Tensor(self, other);
}

at::Tensor bitwise_and(const at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_and_Tensor(self, other);
}

at::Tensor & bitwise_and_(at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_and__Tensor(self, other);
}

at::Tensor bitwise_and(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_bitwise_and_Scalar(self, other);
}

at::Tensor & bitwise_and_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_bitwise_and_out_Scalar_out(self, other, out);
}

at::Tensor & bitwise_and_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_bitwise_and_out_Scalar_out(self, other, out);
}

at::Tensor bitwise_or(const at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_or_Tensor(self, other);
}

at::Tensor & bitwise_or_(at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_or__Tensor(self, other);
}

at::Tensor & bitwise_or_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_bitwise_or_out_Scalar_out(self, other, out);
}

at::Tensor & bitwise_or_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_bitwise_or_out_Scalar_out(self, other, out);
}

at::Tensor bitwise_xor(const at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_xor_Tensor(self, other);
}

at::Tensor & bitwise_xor_(at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_xor__Tensor(self, other);
}

at::Tensor & bitwise_xor_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_bitwise_xor_out_Scalar_out(self, other, out);
}

at::Tensor & bitwise_xor_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_bitwise_xor_out_Scalar_out(self, other, out);
}

at::Tensor bitwise_left_shift(const at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_left_shift_Tensor(self, other);
}

at::Tensor & bitwise_left_shift_(at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_left_shift__Tensor(self, other);
}

at::Tensor bitwise_right_shift(const at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_right_shift_Tensor(self, other);
}

at::Tensor & bitwise_right_shift_(at::Tensor & self, const at::Tensor & other) {
return wrapper_bitwise_right_shift__Tensor(self, other);
}

at::Tensor tril(const at::Tensor & self, int64_t diagonal) {
return wrapper__tril(self, diagonal);
}

at::Tensor triu(const at::Tensor & self, int64_t diagonal) {
return wrapper__triu(self, diagonal);
}

at::Tensor digamma(const at::Tensor & self) {
return wrapper_digamma(self);
}

at::Tensor & digamma_(at::Tensor & self) {
return wrapper_digamma_(self);
}

at::Tensor diag(const at::Tensor & self, int64_t diagonal) {
return wrapper__diag(self, diagonal);
}

at::Tensor ne(const at::Tensor & self, const at::Scalar & other) {
return wrapper_ne_Scalar(self, other);
}

at::Tensor & ne_(at::Tensor & self, const at::Scalar & other) {
return wrapper_ne__Scalar(self, other);
}

at::Tensor ne(const at::Tensor & self, const at::Tensor & other) {
return wrapper_ne_Tensor(self, other);
}

at::Tensor & ne_(at::Tensor & self, const at::Tensor & other) {
return wrapper_ne__Tensor(self, other);
}

at::Tensor ge(const at::Tensor & self, const at::Scalar & other) {
return wrapper_ge_Scalar(self, other);
}

at::Tensor & ge_(at::Tensor & self, const at::Scalar & other) {
return wrapper_ge__Scalar(self, other);
}

at::Tensor ge(const at::Tensor & self, const at::Tensor & other) {
return wrapper_ge_Tensor(self, other);
}

at::Tensor & ge_(at::Tensor & self, const at::Tensor & other) {
return wrapper_ge__Tensor(self, other);
}

at::Tensor le(const at::Tensor & self, const at::Scalar & other) {
return wrapper_le_Scalar(self, other);
}

at::Tensor & le_(at::Tensor & self, const at::Scalar & other) {
return wrapper_le__Scalar(self, other);
}

at::Tensor le(const at::Tensor & self, const at::Tensor & other) {
return wrapper_le_Tensor(self, other);
}

at::Tensor & le_(at::Tensor & self, const at::Tensor & other) {
return wrapper_le__Tensor(self, other);
}

at::Tensor gt(const at::Tensor & self, const at::Scalar & other) {
return wrapper_gt_Scalar(self, other);
}

at::Tensor & gt_(at::Tensor & self, const at::Scalar & other) {
return wrapper_gt__Scalar(self, other);
}

at::Tensor gt(const at::Tensor & self, const at::Tensor & other) {
return wrapper_gt_Tensor(self, other);
}

at::Tensor & gt_(at::Tensor & self, const at::Tensor & other) {
return wrapper_gt__Tensor(self, other);
}

at::Tensor lt(const at::Tensor & self, const at::Scalar & other) {
return wrapper_lt_Scalar(self, other);
}

at::Tensor & lt_(at::Tensor & self, const at::Scalar & other) {
return wrapper_lt__Scalar(self, other);
}

at::Tensor lt(const at::Tensor & self, const at::Tensor & other) {
return wrapper_lt_Tensor(self, other);
}

at::Tensor & lt_(at::Tensor & self, const at::Tensor & other) {
return wrapper_lt__Tensor(self, other);
}

at::Tensor gather(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
return wrapper_gather(self, dim, index, sparse_grad);
}

at::Tensor addcmul(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
return wrapper_addcmul(self, tensor1, tensor2, value);
}

at::Tensor & addcmul_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
return wrapper_addcmul_(self, tensor1, tensor2, value);
}

at::Tensor addcdiv(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
return wrapper_addcdiv(self, tensor1, tensor2, value);
}

at::Tensor & addcdiv_(at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
return wrapper_addcdiv_(self, tensor1, tensor2, value);
}

::std::tuple<at::Tensor,at::Tensor> symeig(const at::Tensor & self, bool eigenvectors, bool upper) {
return wrapper__symeig(self, eigenvectors, upper);
}

::std::tuple<at::Tensor &,at::Tensor &> symeig_out(at::Tensor & e, at::Tensor & V, const at::Tensor & self, bool eigenvectors, bool upper) {
return wrapper_e_symeig_out_e(self, eigenvectors, upper, e, V);
}

::std::tuple<at::Tensor &,at::Tensor &> symeig_outf(const at::Tensor & self, bool eigenvectors, bool upper, at::Tensor & e, at::Tensor & V) {
return wrapper_e_symeig_out_e(self, eigenvectors, upper, e, V);
}

::std::tuple<at::Tensor,at::Tensor> eig(const at::Tensor & self, bool eigenvectors) {
return wrapper__eig(self, eigenvectors);
}

::std::tuple<at::Tensor &,at::Tensor &> eig_out(at::Tensor & e, at::Tensor & v, const at::Tensor & self, bool eigenvectors) {
return wrapper_e_eig_out_e(self, eigenvectors, e, v);
}

::std::tuple<at::Tensor &,at::Tensor &> eig_outf(const at::Tensor & self, bool eigenvectors, at::Tensor & e, at::Tensor & v) {
return wrapper_e_eig_out_e(self, eigenvectors, e, v);
}

at::Tensor cholesky_solve(const at::Tensor & self, const at::Tensor & input2, bool upper) {
return wrapper__cholesky_solve(self, input2, upper);
}

at::Tensor & cholesky_solve_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & input2, bool upper) {
return wrapper_out_cholesky_solve_out_out(self, input2, upper, out);
}

at::Tensor & cholesky_solve_outf(const at::Tensor & self, const at::Tensor & input2, bool upper, at::Tensor & out) {
return wrapper_out_cholesky_solve_out_out(self, input2, upper, out);
}

::std::tuple<at::Tensor,at::Tensor> solve(const at::Tensor & self, const at::Tensor & A) {
return wrapper__solve(self, A);
}

::std::tuple<at::Tensor &,at::Tensor &> solve_out(at::Tensor & solution, at::Tensor & lu, const at::Tensor & self, const at::Tensor & A) {
return wrapper_solution_solve_out_solution(self, A, solution, lu);
}

::std::tuple<at::Tensor &,at::Tensor &> solve_outf(const at::Tensor & self, const at::Tensor & A, at::Tensor & solution, at::Tensor & lu) {
return wrapper_solution_solve_out_solution(self, A, solution, lu);
}

at::Tensor lgamma(const at::Tensor & self) {
return wrapper_lgamma(self);
}

at::Tensor & lgamma_(at::Tensor & self) {
return wrapper_lgamma_(self);
}

at::Tensor polygamma(int64_t n, const at::Tensor & self) {
return wrapper_polygamma(n, self);
}

at::Tensor & polygamma_(at::Tensor & self, int64_t n) {
return wrapper__polygamma_(self, n);
}

at::Tensor erfinv(const at::Tensor & self) {
return wrapper_erfinv(self);
}

at::Tensor & erfinv_(at::Tensor & self) {
return wrapper_erfinv_(self);
}

at::Tensor i0(const at::Tensor & self) {
return wrapper_i0(self);
}

at::Tensor & i0_(at::Tensor & self) {
return wrapper_i0_(self);
}

at::Tensor sign(const at::Tensor & self) {
return wrapper_sign(self);
}

at::Tensor & sign_(at::Tensor & self) {
return wrapper_sign_(self);
}

at::Tensor signbit(const at::Tensor & self) {
return wrapper_signbit(self);
}

at::Tensor dist(const at::Tensor & self, const at::Tensor & other, const at::Scalar & p) {
return wrapper__dist(self, other, p);
}

at::Tensor atan2(const at::Tensor & self, const at::Tensor & other) {
return wrapper_atan2(self, other);
}

at::Tensor & atan2_(at::Tensor & self, const at::Tensor & other) {
return wrapper_atan2_(self, other);
}

at::Tensor fmod(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_fmod_Scalar(self, other);
}

at::Tensor & fmod_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_fmod_out_Scalar_out(self, other, out);
}

at::Tensor & fmod_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_fmod_out_Scalar_out(self, other, out);
}

at::Tensor & fmod_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_fmod__Scalar(self, other);
}

at::Tensor fmod(const at::Tensor & self, const at::Tensor & other) {
return wrapper_fmod_Tensor(self, other);
}

at::Tensor & fmod_(at::Tensor & self, const at::Tensor & other) {
return wrapper_fmod__Tensor(self, other);
}

at::Tensor hypot(const at::Tensor & self, const at::Tensor & other) {
return wrapper_hypot(self, other);
}

at::Tensor & hypot_(at::Tensor & self, const at::Tensor & other) {
return wrapper_hypot_(self, other);
}

at::Tensor igamma(const at::Tensor & self, const at::Tensor & other) {
return wrapper_igamma(self, other);
}

at::Tensor & igamma_(at::Tensor & self, const at::Tensor & other) {
return wrapper_igamma_(self, other);
}

at::Tensor igammac(const at::Tensor & self, const at::Tensor & other) {
return wrapper_igammac(self, other);
}

at::Tensor & igammac_(at::Tensor & self, const at::Tensor & other) {
return wrapper_igammac_(self, other);
}

at::Tensor nextafter(const at::Tensor & self, const at::Tensor & other) {
return wrapper_nextafter(self, other);
}

at::Tensor & nextafter_(at::Tensor & self, const at::Tensor & other) {
return wrapper_nextafter_(self, other);
}

at::Tensor remainder(const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_remainder_Scalar(self, other);
}

at::Tensor & remainder_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_out_remainder_out_Scalar_out(self, other, out);
}

at::Tensor & remainder_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_Scalar_out_remainder_out_Scalar_out(self, other, out);
}

at::Tensor & remainder_(at::Tensor & self, const at::Scalar & other) {
return wrapper_Scalar_remainder__Scalar(self, other);
}

at::Tensor remainder(const at::Tensor & self, const at::Tensor & other) {
return wrapper_remainder_Tensor(self, other);
}

at::Tensor & remainder_(at::Tensor & self, const at::Tensor & other) {
return wrapper_remainder__Tensor(self, other);
}

at::Tensor fmin(const at::Tensor & self, const at::Tensor & other) {
return wrapper_fmin(self, other);
}

at::Tensor fmax(const at::Tensor & self, const at::Tensor & other) {
return wrapper_fmax(self, other);
}

at::Tensor maximum(const at::Tensor & self, const at::Tensor & other) {
return wrapper_maximum(self, other);
}

at::Tensor minimum(const at::Tensor & self, const at::Tensor & other) {
return wrapper_minimum(self, other);
}

::std::tuple<at::Tensor,at::Tensor> topk(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
return wrapper_topk(self, k, dim, largest, sorted);
}

at::Tensor all(const at::Tensor & self) {
return wrapper_all(self);
}

at::Tensor any(const at::Tensor & self) {
return wrapper_any(self);
}

at::Tensor renorm(const at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
return wrapper_renorm(self, p, dim, maxnorm);
}

at::Tensor & renorm_(at::Tensor & self, const at::Scalar & p, int64_t dim, const at::Scalar & maxnorm) {
return wrapper_renorm_(self, p, dim, maxnorm);
}

at::Tensor pow(const at::Tensor & self, const at::Tensor & exponent) {
return wrapper_pow_Tensor_Tensor(self, exponent);
}

at::Tensor & pow_(at::Tensor & self, const at::Tensor & exponent) {
return wrapper_pow__Tensor(self, exponent);
}

at::Tensor pow(const at::Scalar & self, const at::Tensor & exponent) {
return wrapper_pow_Scalar(self, exponent);
}

at::Tensor pow(const at::Tensor & self, const at::Scalar & exponent) {
return wrapper_pow_Tensor_Scalar(self, exponent);
}

at::Tensor & pow_(at::Tensor & self, const at::Scalar & exponent) {
return wrapper_pow__Scalar(self, exponent);
}

at::Tensor alias(const at::Tensor & self) {
return wrapper__alias(self);
}

at::Tensor _convert_indices_from_coo_to_csr(const at::Tensor & self, int64_t size, bool out_int32) {
return wrapper__convert_indices_from_coo_to_csr(self, size, out_int32);
}

at::Tensor l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper__l1_loss(self, target, reduction);
}

at::Tensor & l1_loss_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_out_l1_loss_out_out(self, target, reduction, out);
}

at::Tensor & l1_loss_outf(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
return wrapper_out_l1_loss_out_out(self, target, reduction, out);
}

at::Tensor l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper__l1_loss_backward(grad_output, self, target, reduction);
}

::std::tuple<at::Tensor,at::Tensor> nll_loss_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
return wrapper_nll_loss_forward(self, target, weight, reduction, ignore_index);
}

at::Tensor nll_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
return wrapper_nll_loss_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
}

at::Tensor smooth_l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
return wrapper__smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
}

at::Tensor huber_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double delta) {
return wrapper__huber_loss_backward(grad_output, self, target, reduction, delta);
}

at::Tensor soft_margin_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper__soft_margin_loss(self, target, reduction);
}

at::Tensor & soft_margin_loss_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_out_soft_margin_loss_out_out(self, target, reduction, out);
}

at::Tensor & soft_margin_loss_outf(const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & out) {
return wrapper_out_soft_margin_loss_out_out(self, target, reduction, out);
}

at::Tensor soft_margin_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper__soft_margin_loss_backward(grad_output, self, target, reduction);
}

at::Tensor & soft_margin_loss_backward_out(at::Tensor & grad_input, const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
return wrapper_grad_input_soft_margin_loss_backward_out_grad_input(grad_output, self, target, reduction, grad_input);
}

at::Tensor & soft_margin_loss_backward_outf(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, at::Tensor & grad_input) {
return wrapper_grad_input_soft_margin_loss_backward_out_grad_input(grad_output, self, target, reduction, grad_input);
}

at::Tensor elu(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
return wrapper_elu(self, alpha, scale, input_scale);
}

at::Tensor & elu_(at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
return wrapper_elu_(self, alpha, scale, input_scale);
}

at::Tensor elu_backward(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
return wrapper_elu_backward(grad_output, alpha, scale, input_scale, is_result, self_or_result);
}

at::Tensor glu(const at::Tensor & self, int64_t dim) {
return wrapper_glu(self, dim);
}

at::Tensor hardsigmoid(const at::Tensor & self) {
return wrapper_hardsigmoid(self);
}

at::Tensor & hardsigmoid_(at::Tensor & self) {
return wrapper_hardsigmoid_(self);
}

at::Tensor hardsigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self) {
return wrapper_hardsigmoid_backward(grad_output, self);
}

at::Tensor leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
return wrapper_leaky_relu(self, negative_slope);
}

at::Tensor & leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
return wrapper_leaky_relu_(self, negative_slope);
}

at::Tensor leaky_relu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
return wrapper_leaky_relu_backward(grad_output, self, negative_slope, self_is_result);
}

at::Tensor rrelu_with_noise_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & noise, const at::Scalar & lower, const at::Scalar & upper, bool training, bool self_is_result) {
return wrapper__rrelu_with_noise_backward(grad_output, self, noise, lower, upper, training, self_is_result);
}

at::Tensor softplus(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
return wrapper_softplus(self, beta, threshold);
}

at::Tensor softplus_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold, const at::Tensor & output) {
return wrapper_softplus_backward(grad_output, self, beta, threshold, output);
}

at::Tensor softshrink(const at::Tensor & self, const at::Scalar & lambd) {
return wrapper_softshrink(self, lambd);
}

at::Tensor softshrink_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & lambd) {
return wrapper_softshrink_backward(grad_output, self, lambd);
}

::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_adaptive_max_pool2d(self, output_size);
}

at::Tensor adaptive_max_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
return wrapper_adaptive_max_pool2d_backward(grad_output, self, indices);
}

::std::tuple<at::Tensor,at::Tensor> adaptive_max_pool3d(const at::Tensor & self, at::IntArrayRef output_size) {
return wrapper_adaptive_max_pool3d(self, output_size);
}

at::Tensor adaptive_max_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & indices) {
return wrapper_adaptive_max_pool3d_backward(grad_output, self, indices);
}

at::Tensor avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

at::Tensor avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

at::Tensor avg_pool3d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_avg_pool3d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

at::Tensor avg_pool3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
return wrapper_avg_pool3d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

::std::tuple<at::Tensor,at::Tensor> fractional_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef output_size, const at::Tensor & random_samples) {
return wrapper_fractional_max_pool2d(self, kernel_size, output_size, random_samples);
}

::std::tuple<at::Tensor,at::Tensor> max_pool2d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
return wrapper_max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}

at::Tensor max_pool2d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
return wrapper_max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}

at::Tensor reflection_pad1d(const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_reflection_pad1d(self, padding);
}

at::Tensor reflection_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_reflection_pad1d_backward(grad_output, self, padding);
}

at::Tensor reflection_pad3d(const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_reflection_pad3d(self, padding);
}

at::Tensor reflection_pad3d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_reflection_pad3d_backward(grad_output, self, padding);
}

at::Tensor replication_pad1d(const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_replication_pad1d(self, padding);
}

at::Tensor replication_pad1d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_replication_pad1d_backward(grad_output, self, padding);
}

at::Tensor replication_pad2d(const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_replication_pad2d(self, padding);
}

at::Tensor replication_pad3d(const at::Tensor & self, at::IntArrayRef padding) {
return wrapper_replication_pad3d(self, padding);
}

at::Tensor upsample_linear1d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_linear1d_vec(input, output_size, align_corners, scale_factors);
}

at::Tensor upsample_linear1d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_linear1d_backward_vec(grad_output, output_size, input_size, align_corners, scale_factors);
}

at::Tensor upsample_bilinear2d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_bilinear2d_vec(input, output_size, align_corners, scale_factors);
}

at::Tensor upsample_bilinear2d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_bilinear2d_backward_vec(grad_output, output_size, input_size, align_corners, scale_factors);
}

at::Tensor upsample_trilinear3d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_trilinear3d_vec(input, output_size, align_corners, scale_factors);
}

at::Tensor upsample_trilinear3d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_trilinear3d_backward_vec(grad_output, output_size, input_size, align_corners, scale_factors);
}

at::Tensor upsample_bicubic2d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_bicubic2d_vec(input, output_size, align_corners, scale_factors);
}

at::Tensor upsample_bicubic2d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_bicubic2d_backward_vec(grad_output, output_size, input_size, align_corners, scale_factors);
}

at::Tensor upsample_nearest1d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_nearest1d_vec(input, output_size, scale_factors);
}

at::Tensor upsample_nearest1d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_nearest1d_backward_vec(grad_output, output_size, input_size, scale_factors);
}

at::Tensor upsample_nearest2d(const at::Tensor & input, c10::optional<at::IntArrayRef> output_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_nearest2d_vec(input, output_size, scale_factors);
}

at::Tensor upsample_nearest2d_backward(const at::Tensor & grad_output, c10::optional<at::IntArrayRef> output_size, at::IntArrayRef input_size, c10::optional<at::ArrayRef<double>> scale_factors) {
return wrapper_vec_upsample_nearest2d_backward_vec(grad_output, output_size, input_size, scale_factors);
}

at::Tensor upsample_linear1d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
return wrapper_upsample_linear1d(self, output_size, align_corners, scales);
}

at::Tensor upsample_linear1d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
return wrapper_upsample_linear1d_backward(grad_output, output_size, input_size, align_corners, scales);
}

at::Tensor upsample_bilinear2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
}

at::Tensor upsample_bilinear2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

at::Tensor upsample_bicubic2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_bicubic2d(self, output_size, align_corners, scales_h, scales_w);
}

at::Tensor upsample_bicubic2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_bicubic2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}

at::Tensor upsample_trilinear3d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_trilinear3d(self, output_size, align_corners, scales_d, scales_h, scales_w);
}

at::Tensor upsample_trilinear3d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_trilinear3d_backward(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}

at::Tensor upsample_nearest1d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales) {
return wrapper_upsample_nearest1d(self, output_size, scales);
}

at::Tensor upsample_nearest1d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales) {
return wrapper_upsample_nearest1d_backward(grad_output, output_size, input_size, scales);
}

at::Tensor upsample_nearest2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_nearest2d(self, output_size, scales_h, scales_w);
}

at::Tensor upsample_nearest2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_nearest2d_backward(grad_output, output_size, input_size, scales_h, scales_w);
}

at::Tensor upsample_nearest3d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_nearest3d(self, output_size, scales_d, scales_h, scales_w);
}

at::Tensor upsample_nearest3d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
return wrapper_upsample_nearest3d_backward(grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}

at::Tensor sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & output) {
return wrapper_sigmoid_backward(grad_output, output);
}

at::Tensor logit_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::optional<double> eps) {
return wrapper_logit_backward(grad_output, self, eps);
}

at::Tensor tanh_backward(const at::Tensor & grad_output, const at::Tensor & output) {
return wrapper_tanh_backward(grad_output, output);
}

at::Tensor slow_conv_transpose2d(const at::Tensor & self, const at::Tensor & weight, at::IntArrayRef kernel_size, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef output_padding, at::IntArrayRef dilation) {
return wrapper_slow_conv_transpose2d(self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}

at::Tensor isposinf(const at::Tensor & self) {
return wrapper_isposinf(self);
}

at::Tensor isneginf(const at::Tensor & self) {
return wrapper_isneginf(self);
}

at::Tensor special_entr(const at::Tensor & self) {
return wrapper_special_entr(self);
}

at::Tensor special_ndtri(const at::Tensor & self) {
return wrapper_special_ndtri(self);
}

at::Tensor special_erfcx(const at::Tensor & self) {
return wrapper_special_erfcx(self);
}

at::Tensor special_xlog1py(const at::Tensor & self, const at::Tensor & other) {
return wrapper_special_xlog1py(self, other);
}

at::Tensor special_xlog1py(const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_special_xlog1py_self_scalar(self, other);
}

at::Tensor & special_xlog1py_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_out_special_xlog1py_out_self_scalar_out(self, other, out);
}

at::Tensor & special_xlog1py_outf(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_self_scalar_out_special_xlog1py_out_self_scalar_out(self, other, out);
}

at::Tensor special_xlog1py(const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_special_xlog1py_other_scalar(self, other);
}

at::Tensor & special_xlog1py_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_out_special_xlog1py_out_other_scalar_out(self, other, out);
}

at::Tensor & special_xlog1py_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_other_scalar_out_special_xlog1py_out_other_scalar_out(self, other, out);
}

at::Tensor special_zeta(const at::Tensor & self, const at::Tensor & other) {
return wrapper_special_zeta(self, other);
}

at::Tensor special_zeta(const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_special_zeta_self_scalar(self, other);
}

at::Tensor & special_zeta_out(at::Tensor & out, const at::Scalar & self, const at::Tensor & other) {
return wrapper_self_scalar_out_special_zeta_out_self_scalar_out(self, other, out);
}

at::Tensor & special_zeta_outf(const at::Scalar & self, const at::Tensor & other, at::Tensor & out) {
return wrapper_self_scalar_out_special_zeta_out_self_scalar_out(self, other, out);
}

at::Tensor special_zeta(const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_special_zeta_other_scalar(self, other);
}

at::Tensor & special_zeta_out(at::Tensor & out, const at::Tensor & self, const at::Scalar & other) {
return wrapper_other_scalar_out_special_zeta_out_other_scalar_out(self, other, out);
}

at::Tensor & special_zeta_outf(const at::Tensor & self, const at::Scalar & other, at::Tensor & out) {
return wrapper_other_scalar_out_special_zeta_out_other_scalar_out(self, other, out);
}

at::Tensor special_i0e(const at::Tensor & self) {
return wrapper_special_i0e(self);
}

at::Tensor special_i1(const at::Tensor & self) {
return wrapper_special_i1(self);
}

at::Tensor special_i1e(const at::Tensor & self) {
return wrapper_special_i1e(self);
}

::std::tuple<at::Tensor,at::Tensor,at::Tensor,at::Tensor> linalg_lstsq(const at::Tensor & self, const at::Tensor & b, c10::optional<double> rcond, c10::optional<c10::string_view> driver) {
return wrapper__linalg_lstsq(self, b, rcond, driver);
}

::std::tuple<at::Tensor,at::Tensor> linalg_inv_ex(const at::Tensor & self, bool check_errors) {
return wrapper__linalg_inv_ex(self, check_errors);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_out(at::Tensor & inverse, at::Tensor & info, const at::Tensor & self, bool check_errors) {
return wrapper_inverse_linalg_inv_ex_out_inverse(self, check_errors, inverse, info);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_inv_ex_outf(const at::Tensor & self, bool check_errors, at::Tensor & inverse, at::Tensor & info) {
return wrapper_inverse_linalg_inv_ex_out_inverse(self, check_errors, inverse, info);
}

::std::tuple<at::Tensor,at::Tensor> linalg_qr(const at::Tensor & self, c10::string_view mode) {
return wrapper__linalg_qr(self, mode);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_qr_out(at::Tensor & Q, at::Tensor & R, const at::Tensor & self, c10::string_view mode) {
return wrapper_out_linalg_qr_out_out(self, mode, Q, R);
}

::std::tuple<at::Tensor &,at::Tensor &> linalg_qr_outf(const at::Tensor & self, c10::string_view mode, at::Tensor & Q, at::Tensor & R) {
return wrapper_out_linalg_qr_out_out(self, mode, Q, R);
}

} // namespace compositeexplicitautograd

} // namespace at
