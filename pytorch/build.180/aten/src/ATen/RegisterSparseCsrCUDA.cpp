// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// @generated by tools/codegen/gen.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/Tensor.h>
#include <ATen/Functions.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>



namespace at {

// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {


void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      at::native::as_strided_(out, sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}

namespace {

at::Tensor wrapper_Tensor_add_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_sparse_csr(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_add_out_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_out_sparse_csr_cuda(self, other, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_Tensor_add__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
    // No device check


  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::add_sparse_csr_(self, other, alpha);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mm(const at::Tensor & self, const at::Tensor & mat2) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__mm", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__mm", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_mm(self, mat2);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_mm_out_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_mm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_mm_out_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_mm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::_sparse_csr_mm_out(self, mat2, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__mv(const at::Tensor & self, const at::Tensor & vec) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__mv", "self");
  c10::impl::check_and_update_common_device(common_device, vec, "wrapper__mv", "vec");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::mv_sparse(self, vec);
}

} // anonymous namespace
namespace {

const at::Tensor & wrapper__resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__resize_as_sparse_", "self");
  c10::impl::check_and_update_common_device(common_device, the_template, "wrapper__resize_as_sparse_", "the_template");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::resize_as_sparse_csr_(self, the_template);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__addmm", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper__addmm", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper__addmm", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::addmm_sparse_csr_dense(self, mat1, mat2, beta, alpha);
}

} // anonymous namespace
namespace {

at::Tensor & wrapper_out_addmm_out_out(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, out, "wrapper_out_addmm_out_out", "out");
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_out_addmm_out_out", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_out_addmm_out_out", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_out_addmm_out_out", "mat2");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::addmm_out_sparse_csr_dense_cuda(self, mat1, mat2, beta, alpha, out);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
  c10::optional<Device> common_device = nullopt;
(void)common_device; // Suppress unused variable warning

  c10::impl::check_and_update_common_device(common_device, self, "wrapper__to_dense", "self");

  const OptionalDeviceGuard device_guard(device_of(self));
  return at::native::sparse_to_dense(self, dtype);
}

} // anonymous namespace
namespace {

int64_t wrapper___nnz(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::_nnz_sparse_csr(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__values(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::values_sparse_csr(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__crow_indices(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::crow_indices_sparse_csr(self);
}

} // anonymous namespace
namespace {

at::Tensor wrapper__col_indices(const at::Tensor & self) {
    // No device check


  // DeviceGuard omitted
  return at::native::col_indices_sparse_csr(self);
}

} // anonymous namespace

TORCH_LIBRARY_IMPL(aten, SparseCsrCUDA, m) {
  m.impl("add.Tensor",
  TORCH_FN(wrapper_Tensor_add_Tensor));
  m.impl("add.out",
  TORCH_FN(wrapper_out_add_out_out));
  m.impl("add_.Tensor",
  TORCH_FN(wrapper_Tensor_add__Tensor));
  m.impl("mm",
  TORCH_FN(wrapper__mm));
  m.impl("mm.out",
  TORCH_FN(wrapper_out_mm_out_out));
  m.impl("mv",
  TORCH_FN(wrapper__mv));
  m.impl("resize_as_sparse_",
  TORCH_FN(wrapper__resize_as_sparse_));
  m.impl("addmm",
  TORCH_FN(wrapper__addmm));
  m.impl("addmm.out",
  TORCH_FN(wrapper_out_addmm_out_out));
  m.impl("to_dense",
  TORCH_FN(wrapper__to_dense));
  m.impl("_nnz",
  TORCH_FN(wrapper___nnz));
  m.impl("values",
  TORCH_FN(wrapper__values));
  m.impl("crow_indices",
  TORCH_FN(wrapper__crow_indices));
  m.impl("col_indices",
  TORCH_FN(wrapper__col_indices));
}

} // anonymous namespace

namespace sparsecsrcuda {


at::Tensor add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_add_Tensor(self, other, alpha);
}

at::Tensor & add_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_out_add_out_out(self, other, alpha, out);
}

at::Tensor & add_outf(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_add_out_out(self, other, alpha, out);
}

at::Tensor & add_(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
return wrapper_Tensor_add__Tensor(self, other, alpha);
}

at::Tensor mm(const at::Tensor & self, const at::Tensor & mat2) {
return wrapper__mm(self, mat2);
}

at::Tensor & mm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
return wrapper_out_mm_out_out(self, mat2, out);
}

at::Tensor & mm_outf(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
return wrapper_out_mm_out_out(self, mat2, out);
}

at::Tensor mv(const at::Tensor & self, const at::Tensor & vec) {
return wrapper__mv(self, vec);
}

const at::Tensor & resize_as_sparse_(const at::Tensor & self, const at::Tensor & the_template) {
return wrapper__resize_as_sparse_(self, the_template);
}

at::Tensor addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper__addmm(self, mat1, mat2, beta, alpha);
}

at::Tensor & addmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
return wrapper_out_addmm_out_out(self, mat1, mat2, beta, alpha, out);
}

at::Tensor & addmm_outf(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha, at::Tensor & out) {
return wrapper_out_addmm_out_out(self, mat1, mat2, beta, alpha, out);
}

at::Tensor to_dense(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
return wrapper__to_dense(self, dtype);
}

int64_t _nnz(const at::Tensor & self) {
return wrapper___nnz(self);
}

at::Tensor values(const at::Tensor & self) {
return wrapper__values(self);
}

at::Tensor crow_indices(const at::Tensor & self) {
return wrapper__crow_indices(self);
}

at::Tensor col_indices(const at::Tensor & self) {
return wrapper__col_indices(self);
}

} // namespace sparsecsrcuda

} // namespace at
